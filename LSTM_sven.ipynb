{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task A: Creating a Performance Predictor\n",
    "\n",
    "In this task, you will use training data from 2000 configurations on a single OpenML dataset to train a performance predictor. The data will be splitted into train, test and validation set and we will only use the first 10 epochs of the learning curves for predicitons. You are provided with the full benchmark logs for Fashion-MNIST, that is learning curves, config parameters and gradient statistics, and you can use them freely.\n",
    "\n",
    "For questions, you can contact zimmerl@informatik.uni-freiburg.\n",
    "\n",
    "__Note: Please use the dataloading and splits you are provided with in this notebook.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifications:\n",
    "\n",
    "* Data: fashion_mnist.json\n",
    "* Number of datasets: 1\n",
    "* Number of configurations: 2000\n",
    "* Number of epochs seed during prediction: 10\n",
    "* Available data: Learning curves, architecture parameters and hyperparameters, gradient statistics \n",
    "* Target: Final validation accuracy\n",
    "* Evaluation metric: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and splitting data\n",
    "\n",
    "__Note__: There are 51 steps logged, 50 epochs plus the 0th epoch, prior to any weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd ..\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils_prep\n",
    "from api import Benchmark\n",
    "import utils_prep2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading data...\n",
      "==> No cached data found or cache set to False.\n",
      "==> Reading json data...\n",
      "==> Done.\n"
     ]
    }
   ],
   "source": [
    "bench_dir = \"/home/sven/LCBench/data/11604705/fashion_mnist.json\"\n",
    "bench = Benchmark(bench_dir, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1000\n",
      "Validation: 500\n",
      "Test: 500\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "def cut_data(data, cut_position):\n",
    "    targets = []\n",
    "    for dp in data:\n",
    "        targets.append(dp[\"Train/val_accuracy\"][50])\n",
    "        for tag in dp:\n",
    "            if tag.startswith(\"Train/\"):\n",
    "                dp[tag] = dp[tag][0:cut_position]\n",
    "    return data, targets\n",
    "\n",
    "def read_data():\n",
    "    dataset_name = 'Fashion-MNIST'\n",
    "    n_configs = bench.get_number_of_configs(dataset_name)\n",
    "    \n",
    "    # Query API\n",
    "    data = []\n",
    "    for config_id in range(n_configs):\n",
    "        data_point = dict()\n",
    "        data_point[\"config\"] = bench.query(dataset_name=dataset_name, tag=\"config\", config_id=config_id)\n",
    "        for tag in bench.get_queriable_tags(dataset_name=dataset_name, config_id=config_id):\n",
    "            if tag.startswith(\"Train/\"):\n",
    "                data_point[tag] = bench.query(dataset_name=dataset_name, tag=tag, config_id=config_id)    \n",
    "        data.append(data_point)\n",
    "        \n",
    "    # Split: 50% train, 25% validation, 25% test (the data is already shuffled)\n",
    "    indices = np.arange(n_configs)\n",
    "    ind_train = indices[0:int(np.floor(0.5*n_configs))]\n",
    "    ind_val = indices[int(np.floor(0.5*n_configs)):int(np.floor(0.75*n_configs))]\n",
    "    ind_test = indices[int(np.floor(0.75*n_configs)):]\n",
    "\n",
    "    array_data = np.array(data)\n",
    "    train_data = array_data[ind_train]\n",
    "    val_data = array_data[ind_val]\n",
    "    test_data = array_data[ind_test]\n",
    "    \n",
    "    # Cut curves for validation and test\n",
    "    cut_position = 11\n",
    "    val_data, val_targets = cut_data(val_data, cut_position)\n",
    "    test_data, test_targets = cut_data(test_data, cut_position)\n",
    "    train_data, train_targets = cut_data(train_data, 51)   # Cut last value as it is repeated\n",
    "    \n",
    "    return train_data, val_data, test_data, train_targets, val_targets, test_targets\n",
    "    \n",
    "train_data, val_data, test_data, train_targets, val_targets, test_targets = read_data()\n",
    "\n",
    "print(\"Train:\", len(train_data))\n",
    "print(\"Validation:\", len(val_data))\n",
    "print(\"Test:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains the configuration of the trained model and learning curves as well as global and layer-wise gradient statistics.\n",
    "\n",
    "__Note__: Not all parameters vary across different configurations. The varying parameters are batch_size, max_dropout, max_units, num_layers, learning_rate, momentum, weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config example: {'batch_size': 93, 'imputation_strategy': 'mean', 'learning_rate_scheduler': 'cosine_annealing', 'loss': 'cross_entropy_weighted', 'network': 'shapedmlpnet', 'max_dropout': 0.06145480624960298, 'normalization_strategy': 'standardize', 'optimizer': 'sgd', 'cosine_annealing_T_max': 50, 'cosine_annealing_eta_min': 1e-08, 'activation': 'relu', 'max_units': 402, 'mlp_shape': 'funnel', 'num_layers': True, 'learning_rate': 0.07306153347321286, 'momentum': 0.5844418984083981, 'weight_decay': 0.05967268273584057}\n",
      "\n",
      "\n",
      "DATA Keys: dict_keys(['config', 'Train/loss', 'Train/train_accuracy', 'Train/val_accuracy', 'Train/train_cross_entropy', 'Train/val_cross_entropy', 'Train/train_balanced_accuracy', 'Train/val_balanced_accuracy', 'Train/test_result', 'Train/test_cross_entropy', 'Train/test_balanced_accuracy', 'Train/gradient_max', 'Train/gradient_mean', 'Train/gradient_median', 'Train/gradient_std', 'Train/gradient_q10', 'Train/gradient_q25', 'Train/gradient_q75', 'Train/gradient_q90', 'Train/layer_wise_gradient_max_layer_0', 'Train/layer_wise_gradient_mean_layer_0', 'Train/layer_wise_gradient_median_layer_0', 'Train/layer_wise_gradient_std_layer_0', 'Train/layer_wise_gradient_q10_layer_0', 'Train/layer_wise_gradient_q25_layer_0', 'Train/layer_wise_gradient_q75_layer_0', 'Train/layer_wise_gradient_q90_layer_0', 'Train/gradient_norm', 'Train/lr'])\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "print(\"Config example:\", test_data[0][\"config\"])\n",
    "print(\"\\n\")\n",
    "print(\"DATA Keys:\", test_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb5f0e81fd0>]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Learning curve\n",
    "plt.plot(train_data[10][\"Train/val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb5f1c27ef0>]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gradient statistics\n",
    "plt.plot(train_data[10][\"Train/layer_wise_gradient_mean_layer_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = utils_prep2.check_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_classes, embedding_dim, hidden_size, seq_length, \n",
    "                 num_layers, config_size, bidirectional = False, drop_prob=0.5, relative_size = 0.2):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.config_size = config_size\n",
    "        \n",
    "        # self.forecaster = torch.nn.Embedding(100, embedding_dim)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(drop_prob)\n",
    "        self.lstm = torch.nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_dim, \n",
    "                            num_layers = self.num_layers, dropout =drop_prob, bidirectional = bidirectional)\n",
    "        \n",
    " \n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(self.config_size, int(self.hidden_dim*relative_size))\n",
    "                                       \n",
    "                                       \n",
    "        self.linear2 = torch.nn.Linear(int(self.hidden_dim*relative_size),\n",
    "                                       int(self.hidden_dim*relative_size))\n",
    "                                       \n",
    "                                       \n",
    "        self.linear3 = torch.nn.Linear(self.hidden_dim + int(self.hidden_dim*relative_size),\n",
    "                                       self.hidden_dim + int(self.hidden_dim*relative_size))\n",
    "\n",
    "        \n",
    "        self.linear4 = torch.nn.Linear(self.hidden_dim + int(self.hidden_dim*relative_size), self.hidden_dim//2)\n",
    "        \n",
    "        self.linear5 = torch.nn.Linear(self.hidden_dim//2, num_classes)\n",
    "        \n",
    "        \n",
    "        self.dropout2 = torch.nn.Dropout(drop_prob)\n",
    "        \n",
    "        \n",
    "        self.linear_reduce1 = torch.nn.Linear(self.hidden_dim*10, self.hidden_dim*5)\n",
    "\n",
    "        self.linear_reduce2 = torch.nn.Linear(self.hidden_dim*5, self.hidden_dim)\n",
    "\n",
    "        self.linear_reduce3 = torch.nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x, configs, hidden):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = torch.t(x)\n",
    "\n",
    "        x=x.unsqueeze(-1)\n",
    "        \n",
    "        x = x.float()\n",
    "        \n",
    "        #forecast = self.forecaster(x)\n",
    "\n",
    "        lstm_x, hidden = self.lstm(x, hidden)\n",
    "            \n",
    "        lstm_x = lstm_x.permute(1,0,2)\n",
    "\n",
    "        \n",
    "        lstm_x = lstm_x.contiguous().view(batch_size, -1)\n",
    "        \n",
    "\n",
    "\n",
    "        # lstm_x = lstm_x.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        x = self.dropout(lstm_x)\n",
    "        \n",
    "        # try \n",
    "        \n",
    "        \n",
    "        x = torch.nn.functional.relu(self.linear_reduce1(x))\n",
    "        x = torch.nn.functional.relu(self.linear_reduce2(x))\n",
    "        x = torch.nn.functional.relu(self.linear_reduce3(x))\n",
    "\n",
    "\n",
    "                                       \n",
    "        x_config = torch.nn.functional.relu(self.linear1(configs))\n",
    "        x_config = torch.nn.functional.relu(self.linear2(x_config))\n",
    "\n",
    "        x_cat =  torch.cat([x, x_config], dim =1)\n",
    "\n",
    "        \n",
    "        x_cat = torch.nn.functional.relu(self.linear3(x_cat))\n",
    "        \n",
    "        #x_cat = self.dropout2(x_cat)\n",
    "        \n",
    "        x_cat = torch.nn.functional.relu(self.linear4(x_cat))\n",
    "        x_cat = self.linear5(x_cat)\n",
    "\n",
    "        \n",
    "        x_cat = x_cat.view(batch_size, -1)\n",
    "        x_cat = x_cat[:,-1]\n",
    "        \n",
    "        return x_cat, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size = 32):\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "### alternative weighted MSE Loss\n",
    "\n",
    "\n",
    "\n",
    "def weighted_mse_loss(prediction, target, weight):\n",
    "    ### construct weight vector\n",
    "        \n",
    "    \n",
    "    \n",
    "    return torch.sum((prediction-target)**2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### calculate weights\n",
    "\n",
    "weights = [1/(len([x for x in train_targets if x < 80.0])), 1/(len([x for x in train_targets if x >= 80.0]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hidden_dim, num_layers, input_size = 10, lr= 0.001, relative_size = 0.75, weight_decay = 0.001):\n",
    "    input_size = 1\n",
    "    outcome_dim = 1\n",
    "    embedding_dim = 400\n",
    "    seq_length = 10\n",
    "    config_size = 7\n",
    "\n",
    "    model = LSTM_Net(input_size, outcome_dim, embedding_dim, hidden_dim, seq_length, num_layers, config_size,\n",
    "                     relative_size = relative_size)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_model(model, optimizer, criterion):\n",
    "    \n",
    "    epochs = 2000\n",
    "    counter = 0\n",
    "    print_every = 200\n",
    "\n",
    "    clip = 5\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "\n",
    "        for batches, configs , labels in train_data_loader:\n",
    "            counter += 1\n",
    "\n",
    "            batch_size_calc = len(labels)\n",
    "            hidden = model.init_hidden(batch_size=batch_size_calc)\n",
    "\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            output, hidden = model(batches, configs, hidden)\n",
    "\n",
    "\n",
    "            loss = abs(criterion(output.squeeze(), labels.float()))\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            if counter%print_every == 0:\n",
    "\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "\n",
    "                for inp,configs, lab in val_data_loader:\n",
    "\n",
    "                    batch_size_calc = len(lab)\n",
    "\n",
    "                    val_h = model.init_hidden(batch_size = batch_size_calc)\n",
    "\n",
    "\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    inp, lab = inp.to(device), lab.to(device)\n",
    "                    out, val_h = model(inp, configs,val_h)\n",
    "                    val_loss = abs(criterion(out.squeeze(), lab.float()))\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                    print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "\n",
    "                if np.mean(val_losses) <= valid_loss_min:\n",
    "                    torch.save(model.state_dict(), '/home/sven/LCBench/state_dict.pt')\n",
    "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                    valid_loss_min = np.mean(val_losses)\n",
    "                \n",
    "                model.train()\n",
    "\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion):\n",
    "    \n",
    "    model.load_state_dict(torch.load('/home/sven/LCBench/state_dict.pt'))\n",
    "\n",
    "    test_losses = []\n",
    "    msqrt = 0\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    for inputs,configs, labels in test_data_loader:\n",
    "        counter = counter +1\n",
    "        batch_size_calc = len(labels)\n",
    "        h = model.init_hidden(batch_size_calc)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output, h = model(inputs,configs, h)\n",
    "        test_loss = criterion(output.squeeze(), labels.float())\n",
    "        test_losses.append(test_loss.item())\n",
    "        pred = (output.squeeze())\n",
    "\n",
    "        msqrt = msqrt + mean_squared_error(labels.detach().numpy(), pred.detach().numpy())\n",
    "    \n",
    "        print((labels.detach()))\n",
    "        print((pred.detach()))\n",
    "    print(counter)\n",
    "    total_test_acc = msqrt\n",
    "    print(\"TOTAL Loss: {:.3f}\".format(total_test_acc))\n",
    "    test_acc2 = msqrt/counter\n",
    "    \n",
    "    print(\"Msqrt: {:.3f}\".format(test_acc2))\n",
    "\n",
    "    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "    \n",
    "    return test_acc2, np.mean(test_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 11.260777\n",
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 9.119049\n",
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 10.492034\n",
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 9.348111\n",
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 10.455791\n",
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 13.318169\n",
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 12.019214\n",
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 11.244556\n",
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 10.971863\n",
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 10.561783\n",
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 10.445814\n",
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 10.904299\n",
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 10.710390\n",
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 11.413108\n",
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 11.774523\n",
      "Epoch: 7/2000... Step: 200... Loss: 18.971664... Val Loss: 11.664431\n",
      "Validation loss decreased (inf --> 11.664431).  Saving model ...\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 10.987135\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 7.838225\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 9.552011\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 8.680019\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 9.296661\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 11.604749\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 10.650426\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 10.009377\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 9.909342\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 9.594019\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 9.571122\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 9.793754\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 9.683388\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 10.279370\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 10.527466\n",
      "Epoch: 13/2000... Step: 400... Loss: 17.652136... Val Loss: 10.700616\n",
      "Validation loss decreased (11.664431 --> 10.700616).  Saving model ...\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 38.921730\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 36.072607\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 36.329100\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 34.867573\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 36.444931\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 38.603097\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 36.927606\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 36.038651\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 35.657034\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 34.960432\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 34.870185\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 35.567444\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 34.994203\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 35.894504\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 36.494322\n",
      "Epoch: 19/2000... Step: 600... Loss: 39.062756... Val Loss: 35.773143\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 29.061035\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 26.422712\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 27.921319\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 26.179608\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 28.528325\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 31.772484\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 29.882596\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 28.828219\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 28.246803\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 27.621229\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 27.428824\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 28.402293\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 27.818765\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 28.520054\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 29.256075\n",
      "Epoch: 25/2000... Step: 800... Loss: 60.824722... Val Loss: 28.748299\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 6.851937\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 6.558893\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 9.298163\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 8.025437\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 9.423537\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 11.533042\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 10.600800\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 9.875641\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 9.619605\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 9.191899\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 8.860404\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 9.661143\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 9.725088\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 10.214371\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 10.532212\n",
      "Epoch: 32/2000... Step: 1000... Loss: 17.075424... Val Loss: 10.608344\n",
      "Validation loss decreased (10.700616 --> 10.608344).  Saving model ...\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 11.971006\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 8.954284\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 10.061130\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 9.211825\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 9.754543\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 10.965164\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 10.140901\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 9.608823\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 9.598889\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 9.402396\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 9.571493\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 9.850190\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 9.710553\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 10.366178\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 10.664635\n",
      "Epoch: 38/2000... Step: 1200... Loss: 15.462573... Val Loss: 10.665515\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 25.493204\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 23.336412\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 24.400726\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 22.876359\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 24.927565\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 25.855686\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 24.417068\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 23.669701\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 23.341876\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 22.791117\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 22.699299\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 23.669120\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 23.279811\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 24.049963\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 24.763466\n",
      "Epoch: 44/2000... Step: 1400... Loss: 13.569252... Val Loss: 24.321571\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 39.117920\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 41.686026\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 45.401257\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 44.164914\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 44.328358\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 43.459628\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 43.765687\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 43.507529\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 43.883438\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 43.788964\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 43.375827\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 44.147479\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 44.806772\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 45.189613\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 45.002455\n",
      "Epoch: 50/2000... Step: 1600... Loss: 18.596600... Val Loss: 45.250395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 11.003328\n",
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 11.954572\n",
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 15.078635\n",
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 13.732626\n",
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 15.068315\n",
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 14.996149\n",
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 14.549915\n",
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 14.087809\n",
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 14.030524\n",
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 13.725020\n",
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 13.376948\n",
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 14.338558\n",
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 14.602130\n",
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 15.089789\n",
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 15.363166\n",
      "Epoch: 57/2000... Step: 1800... Loss: 31.099972... Val Loss: 15.528907\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 7.313426\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 6.530399\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 8.738375\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 7.652600\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 8.582123\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 10.750425\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 9.829907\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 9.180522\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 8.958147\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 8.557196\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 8.288544\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 8.870458\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 8.895657\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 9.408009\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 9.665410\n",
      "Epoch: 63/2000... Step: 2000... Loss: 12.041409... Val Loss: 9.731144\n",
      "Validation loss decreased (10.608344 --> 9.731144).  Saving model ...\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 8.454637\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 7.050847\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 8.855798\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 7.578648\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 8.874011\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 9.886375\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 8.895684\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 8.281768\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 8.084089\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 7.724161\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 7.603835\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 8.361270\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 8.256281\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 8.901932\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 9.345802\n",
      "Epoch: 69/2000... Step: 2200... Loss: 24.951645... Val Loss: 9.228540\n",
      "Validation loss decreased (9.731144 --> 9.228540).  Saving model ...\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 14.712090\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 12.757981\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 15.071017\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 13.344149\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 15.912283\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 17.314115\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 15.798856\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 14.994480\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 14.440756\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 14.123366\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 13.982402\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 15.221719\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 14.892774\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 15.572697\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 16.407713\n",
      "Epoch: 75/2000... Step: 2400... Loss: 9.936726... Val Loss: 16.239790\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 11.747616\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 9.634992\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 11.650977\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 10.412088\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 11.694962\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 13.240155\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 12.236565\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 11.510454\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 11.293402\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 10.803492\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 10.740577\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 11.447696\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 11.163556\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 11.726313\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 12.135952\n",
      "Epoch: 82/2000... Step: 2600... Loss: 15.658968... Val Loss: 11.938009\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 7.394545\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 6.964918\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 9.358214\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 8.158280\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 9.213772\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 9.265629\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 8.670295\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 8.222266\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 8.140495\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 7.911542\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 7.684806\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 8.442972\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 8.554442\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 9.137211\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 9.452109\n",
      "Epoch: 88/2000... Step: 2800... Loss: 9.036950... Val Loss: 9.532406\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 13.263352\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 9.843483\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 10.992261\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 10.055816\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 10.533247\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 10.829015\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 10.455007\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 9.995454\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 9.999672\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 10.274070\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 11.116685\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 11.348783\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 11.033920\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 11.695482\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 11.911219\n",
      "Epoch: 94/2000... Step: 3000... Loss: 27.180376... Val Loss: 11.723385\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 9.670365\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 7.935525\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 11.310252\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 9.349145\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 12.255376\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 13.897187\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 12.500090\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 11.652614\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 10.972388\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 11.016077\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 11.283636\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 12.661936\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 12.326060\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 12.879823\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 13.709084\n",
      "Epoch: 100/2000... Step: 3200... Loss: 25.546150... Val Loss: 13.728811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 26.508224\n",
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 24.024890\n",
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 25.574373\n",
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 23.949897\n",
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 26.150722\n",
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 26.637222\n",
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 25.299293\n",
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 24.580237\n",
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 24.255930\n",
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 23.759118\n",
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 23.637895\n",
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 24.729748\n",
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 24.307368\n",
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 25.072851\n",
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 25.837136\n",
      "Epoch: 107/2000... Step: 3400... Loss: 12.783144... Val Loss: 25.419173\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 8.441854\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 7.109882\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 9.068924\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 7.933574\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 8.598041\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 8.150515\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 7.652759\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 7.320569\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 7.353383\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 7.322480\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 7.337363\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 7.938939\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 8.009773\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 8.673588\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 8.946317\n",
      "Epoch: 113/2000... Step: 3600... Loss: 9.446056... Val Loss: 8.981481\n",
      "Validation loss decreased (9.228540 --> 8.981481).  Saving model ...\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 9.704861\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 8.450446\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 10.545785\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 9.025802\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 10.990416\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 11.452346\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 10.345220\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 9.706911\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 9.384425\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 9.089482\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 9.022278\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 10.106681\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 9.936127\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 10.597608\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 11.245435\n",
      "Epoch: 119/2000... Step: 3800... Loss: 19.115465... Val Loss: 11.088936\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 9.273109\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 9.697925\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 12.355035\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 11.028499\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 12.014782\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 11.660332\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 11.191556\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 10.780447\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 10.770569\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 10.623781\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 10.356366\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 11.232323\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 11.447142\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 12.007104\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 12.279867\n",
      "Epoch: 125/2000... Step: 4000... Loss: 19.128271... Val Loss: 12.337342\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 10.880087\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 10.985731\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 13.449549\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 12.315455\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 13.038735\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 12.849539\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 12.398163\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 12.038435\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 12.024985\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 11.805580\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 11.521617\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 12.218135\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 12.441869\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 12.993603\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 13.208135\n",
      "Epoch: 132/2000... Step: 4200... Loss: 23.872931... Val Loss: 13.350549\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 26.975178\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 24.221765\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 25.655339\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 24.233548\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 25.860376\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 26.382870\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 25.341096\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 24.620698\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 24.503919\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 23.948594\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 23.920671\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 24.787353\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 24.322657\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 25.003441\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 25.574000\n",
      "Epoch: 138/2000... Step: 4400... Loss: 32.869888... Val Loss: 25.079457\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 29.867077\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 26.324030\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 27.160774\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 25.857519\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 26.959115\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 27.229922\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 26.449374\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 25.836763\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 25.808702\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 25.608314\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 26.110688\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 26.665379\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 26.137821\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 26.908558\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 27.307810\n",
      "Epoch: 144/2000... Step: 4600... Loss: 17.007820... Val Loss: 26.709216\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 7.834317\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 6.506509\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 8.441006\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 7.367894\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 8.095794\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 8.299654\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 7.689505\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 7.273154\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 7.200960\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 7.045166\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 6.963861\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 7.536501\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 7.536243\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 8.152355\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 8.436703\n",
      "Epoch: 150/2000... Step: 4800... Loss: 12.107497... Val Loss: 8.455485\n",
      "Validation loss decreased (8.981481 --> 8.455485).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 6.998013\n",
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 5.929393\n",
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 7.841706\n",
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 6.670689\n",
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 7.670543\n",
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 7.799607\n",
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 7.078308\n",
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 6.615705\n",
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 6.533791\n",
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 6.292691\n",
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 6.150174\n",
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 6.867529\n",
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 6.873715\n",
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 7.495553\n",
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 7.869145\n",
      "Epoch: 157/2000... Step: 5000... Loss: 16.158943... Val Loss: 7.837911\n",
      "Validation loss decreased (8.455485 --> 7.837911).  Saving model ...\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 17.318974\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 14.943395\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 16.926361\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 15.520967\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 17.109949\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 17.228913\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 16.315939\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 15.675241\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 15.526642\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 15.099822\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 15.014078\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 15.928513\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 15.582014\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 16.251964\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 16.811470\n",
      "Epoch: 163/2000... Step: 5200... Loss: 14.785316... Val Loss: 16.503489\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 17.153069\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 13.716753\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 15.083774\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 14.073060\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 14.652628\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 15.081969\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 14.440149\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 13.947805\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 14.001082\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 13.752249\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 13.885527\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 14.292476\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 13.964006\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 14.580220\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 14.871942\n",
      "Epoch: 169/2000... Step: 5400... Loss: 10.364965... Val Loss: 14.645718\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 6.336232\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 5.593381\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 7.800651\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 6.503927\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 7.855825\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 7.902500\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 7.122610\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 6.617914\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 6.446810\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 6.134597\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 5.955265\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 6.845258\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 6.802925\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 7.403630\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 7.853067\n",
      "Epoch: 175/2000... Step: 5600... Loss: 10.449374... Val Loss: 7.800801\n",
      "Validation loss decreased (7.837911 --> 7.800801).  Saving model ...\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 93.311378\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 90.101578\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 90.423002\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 88.600008\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 91.691240\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 95.867175\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 93.565480\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 92.222648\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 91.438361\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 90.343221\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 90.420472\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 91.579934\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 90.619461\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 91.396599\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 92.339750\n",
      "Epoch: 182/2000... Step: 5800... Loss: 60.127644... Val Loss: 91.272252\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 31.555767\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 25.803845\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 26.644841\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 25.730370\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 25.694698\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 26.157278\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 25.785965\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 25.279560\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 25.544593\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 25.663055\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 26.568421\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 26.594030\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 25.947219\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 26.565243\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 26.706668\n",
      "Epoch: 188/2000... Step: 6000... Loss: 29.655806... Val Loss: 26.255977\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 8.513413\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 6.646566\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 8.355118\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 7.355149\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 7.942538\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 7.763426\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 7.173283\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 6.822686\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 6.818026\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 6.698650\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 6.679271\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 7.187375\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 7.172423\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 7.808882\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 8.111162\n",
      "Epoch: 194/2000... Step: 6200... Loss: 10.181611... Val Loss: 8.137983\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 45.133171\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 42.204268\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 42.627837\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 41.209910\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 42.846959\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 43.967198\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 42.583541\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 41.814617\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 41.571594\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 40.765573\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 40.732259\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 41.530231\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 40.993056\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 41.856267\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 42.483816\n",
      "Epoch: 200/2000... Step: 6400... Loss: 30.802219... Val Loss: 41.754009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 6.947193\n",
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 7.422479\n",
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 10.250289\n",
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 8.735045\n",
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 10.223600\n",
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 11.106914\n",
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 10.286108\n",
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 9.666549\n",
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 9.410029\n",
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 9.096185\n",
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 8.796848\n",
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 9.822903\n",
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 9.882413\n",
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 10.399187\n",
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 10.772526\n",
      "Epoch: 207/2000... Step: 6600... Loss: 10.354958... Val Loss: 10.735493\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 19.300722\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 16.136569\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 17.541354\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 16.382091\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 17.214039\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 17.480756\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 16.763263\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 16.213080\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 16.236273\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 15.917174\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 16.027013\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 16.568019\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 16.203248\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 16.871042\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 17.231017\n",
      "Epoch: 213/2000... Step: 6800... Loss: 21.806967... Val Loss: 16.895315\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 8.512606\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 7.400560\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 9.681059\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 8.091760\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 10.004987\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 10.801254\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 9.648144\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 8.996588\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 8.617868\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 8.301622\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 8.071259\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 9.146047\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 8.986743\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 9.663639\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 10.279035\n",
      "Epoch: 219/2000... Step: 7000... Loss: 10.891996... Val Loss: 10.136679\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 7.475354\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 8.035619\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 11.225306\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 9.577147\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 11.462310\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 11.943795\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 11.134846\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 10.515422\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 10.224113\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 9.943657\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 9.634249\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 10.828343\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 10.879841\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 11.381599\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 11.845655\n",
      "Epoch: 225/2000... Step: 7200... Loss: 25.626759... Val Loss: 11.869270\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 7.505282\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 6.474495\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 8.687602\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 7.201664\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 8.835021\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 9.837470\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 8.776942\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 8.132109\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 7.800152\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 7.473503\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 7.353258\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 8.298538\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 8.150362\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 8.786604\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 9.300328\n",
      "Epoch: 232/2000... Step: 7400... Loss: 8.827784... Val Loss: 9.146223\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 14.698200\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 14.422767\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 16.299438\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 15.623726\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 15.775422\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 15.938440\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 15.452118\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 15.219345\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 15.169669\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 15.058391\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 14.739415\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 15.104617\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 15.414011\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 15.947022\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 16.112757\n",
      "Epoch: 238/2000... Step: 7600... Loss: 11.268763... Val Loss: 16.474301\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 14.306907\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 12.221140\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 13.597157\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 12.305266\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 13.540536\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 15.031195\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 13.886526\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 13.217283\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 12.959353\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 12.509003\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 12.422892\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 13.133229\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 12.859069\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 13.566126\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 14.036725\n",
      "Epoch: 244/2000... Step: 7800... Loss: 11.426777... Val Loss: 13.733730\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 15.457638\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 14.931489\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 16.741528\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 15.931479\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 15.942022\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 15.353400\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 15.113125\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 14.953585\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 15.020800\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 15.233665\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 15.280108\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 15.615606\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 15.836114\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 16.410052\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 16.505973\n",
      "Epoch: 250/2000... Step: 8000... Loss: 7.280149... Val Loss: 16.741464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 7.557038\n",
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 6.427498\n",
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 8.358809\n",
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 7.282574\n",
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 8.079888\n",
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 8.184050\n",
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 7.528288\n",
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 7.109395\n",
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 7.031055\n",
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 6.796289\n",
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 6.646007\n",
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 7.251704\n",
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 7.254743\n",
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 7.867040\n",
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 8.180923\n",
      "Epoch: 257/2000... Step: 8200... Loss: 12.611261... Val Loss: 8.182706\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 8.660875\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 7.260291\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 8.934525\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 8.038510\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 8.512161\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 8.736682\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 8.125514\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 7.773025\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 7.711586\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 7.511990\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 7.363935\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 7.838738\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 7.844829\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 8.446614\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 8.708990\n",
      "Epoch: 263/2000... Step: 8400... Loss: 8.027915... Val Loss: 8.753481\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 26.829784\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 23.839472\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 24.464037\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 23.268849\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 24.231654\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 24.371663\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 23.476351\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 22.941602\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 22.930391\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 22.435637\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 22.543440\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 23.111609\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 22.729746\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 23.528358\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 23.968989\n",
      "Epoch: 269/2000... Step: 8600... Loss: 17.310368... Val Loss: 23.498940\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 15.273684\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 12.132705\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 13.167089\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 11.956829\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 12.690283\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 13.059510\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 12.404784\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 11.927532\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 11.797186\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 11.687810\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 12.053270\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 12.542608\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 12.233188\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 13.076218\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 13.401365\n",
      "Epoch: 275/2000... Step: 8800... Loss: 15.516236... Val Loss: 13.056808\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 6.533254\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 5.838082\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 7.776628\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 6.595029\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 7.509470\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 8.551416\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 7.739871\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 7.189351\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 6.988818\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 6.677939\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 6.427072\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 7.126694\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 7.090482\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 7.666310\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 7.986485\n",
      "Epoch: 282/2000... Step: 9000... Loss: 10.046276... Val Loss: 7.903949\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 9.154856\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 7.303998\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 8.659629\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 7.654398\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 8.238116\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 8.474921\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 7.819843\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 7.411338\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 7.364869\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 7.200890\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 7.153639\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 7.640914\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 7.523673\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 8.145114\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 8.462279\n",
      "Epoch: 288/2000... Step: 9200... Loss: 7.810917... Val Loss: 8.383676\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 6.791108\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 5.922473\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 7.678270\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 6.431400\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 7.355621\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 7.056480\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 6.452988\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 6.017598\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 5.905182\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 5.800394\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 5.678620\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 6.555333\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 6.493938\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 7.136441\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 7.503812\n",
      "Epoch: 294/2000... Step: 9400... Loss: 13.569838... Val Loss: 7.401899\n",
      "Validation loss decreased (7.800801 --> 7.401899).  Saving model ...\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 9.005695\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 9.694050\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 11.721790\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 10.639472\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 11.133089\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 10.854431\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 10.521167\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 10.162265\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 10.128832\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 10.177681\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 10.000568\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 10.856241\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 11.045054\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 11.565027\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 11.775133\n",
      "Epoch: 300/2000... Step: 9600... Loss: 7.090217... Val Loss: 11.790172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 6.017720\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 5.563405\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 7.808679\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 6.392846\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 7.800531\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 7.411605\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 6.626478\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 6.149833\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 5.943999\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 5.770386\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 5.522253\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 6.713277\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 6.650999\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 7.285718\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 7.765187\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.237052... Val Loss: 7.701069\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 7.769061\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 7.125851\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 8.794188\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 7.701537\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 7.969004\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 8.687573\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 8.029204\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 7.604617\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 7.433615\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 7.408726\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 7.141617\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 7.655654\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 7.695067\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 8.209312\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 8.442982\n",
      "Epoch: 313/2000... Step: 10000... Loss: 10.831181... Val Loss: 8.551159\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 15.131076\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 11.428050\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 12.437133\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 11.354703\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 11.482602\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 12.042262\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 11.324919\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 10.931532\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 10.942970\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 10.836968\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 10.793807\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 11.101199\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 10.815951\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 11.436169\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 11.739665\n",
      "Epoch: 319/2000... Step: 10200... Loss: 8.350017... Val Loss: 11.591992\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 25.676397\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 23.753931\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 24.595938\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 23.250750\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 24.819337\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 25.115322\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 24.069513\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 23.443963\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 23.199247\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 22.783051\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 22.683359\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 23.747170\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 23.428178\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 24.215847\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 24.787256\n",
      "Epoch: 325/2000... Step: 10400... Loss: 7.920634... Val Loss: 24.365448\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 7.467352\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 7.052463\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 8.470185\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 7.383806\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 7.719890\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 7.766550\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 7.263807\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 6.934928\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 6.809576\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 6.911105\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 6.726590\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 7.265976\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 7.317941\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 7.835458\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 8.129833\n",
      "Epoch: 332/2000... Step: 10600... Loss: 10.443299... Val Loss: 8.199592\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 7.903162\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 6.839379\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 8.110860\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 7.013206\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 7.775510\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 8.200478\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 7.504994\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 7.014179\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 6.878974\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 6.773715\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 6.636791\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 7.399239\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 7.276339\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 7.862329\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 8.228165\n",
      "Epoch: 338/2000... Step: 10800... Loss: 4.631731... Val Loss: 8.085385\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 15.185854\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 13.299411\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 14.364691\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 13.257207\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 14.253483\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 14.503894\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 13.760622\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 13.200774\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 13.121247\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 12.890294\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 12.828583\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 13.723044\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 13.455981\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 14.106485\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 14.539402\n",
      "Epoch: 344/2000... Step: 11000... Loss: 4.359905... Val Loss: 14.246322\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 7.408613\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 6.639334\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 8.252419\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 7.023929\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 8.459373\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 9.529718\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 8.508260\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 7.864138\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 7.553084\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 7.294956\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 7.210676\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 8.216196\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 8.056860\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 8.669267\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 9.161074\n",
      "Epoch: 350/2000... Step: 11200... Loss: 10.340525... Val Loss: 8.986216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 8.984810\n",
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 7.778167\n",
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 9.241178\n",
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 8.005197\n",
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 9.263447\n",
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 9.889417\n",
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 8.994275\n",
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 8.412443\n",
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 8.175263\n",
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 7.912016\n",
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 7.814806\n",
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 8.627539\n",
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 8.441921\n",
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 9.064934\n",
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 9.523498\n",
      "Epoch: 357/2000... Step: 11400... Loss: 12.181624... Val Loss: 9.317081\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 8.097756\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 6.983370\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 7.932331\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 7.130388\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 7.621184\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 7.233527\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 6.863442\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 6.543028\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 6.504295\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 6.636323\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 6.642821\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 7.182244\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 7.113860\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 7.629244\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 8.054329\n",
      "Epoch: 363/2000... Step: 11600... Loss: 4.909319... Val Loss: 8.062437\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 5.929705\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 5.528162\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 6.807870\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 5.694488\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 6.582400\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 6.402226\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 5.761981\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 5.353654\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 5.206782\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 5.112074\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 4.957453\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 5.721971\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 5.695953\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 6.281852\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 6.726220\n",
      "Epoch: 369/2000... Step: 11800... Loss: 6.053251... Val Loss: 6.644956\n",
      "Validation loss decreased (7.401899 --> 6.644956).  Saving model ...\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 11.181562\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 9.843222\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 11.062054\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 9.878777\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 11.591292\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 12.577027\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 11.529184\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 10.840752\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 10.524788\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 10.201861\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 10.213037\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 11.188824\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 10.943547\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 11.647564\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 12.287189\n",
      "Epoch: 375/2000... Step: 12000... Loss: 6.342490... Val Loss: 12.027643\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 9.177245\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 7.658324\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 8.789131\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 7.647956\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 8.426243\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 8.278024\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 7.618832\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 7.206324\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 7.102404\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 6.951931\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 6.850942\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 7.411538\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 7.291344\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 7.864844\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 8.340831\n",
      "Epoch: 382/2000... Step: 12200... Loss: 5.546143... Val Loss: 8.224357\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 9.895917\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 8.979124\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 10.046823\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 9.102244\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 9.186246\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 8.506527\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 8.240806\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 8.143223\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 8.053284\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 8.306518\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 8.437455\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 8.709113\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 8.794830\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 9.248197\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 9.631038\n",
      "Epoch: 388/2000... Step: 12400... Loss: 15.042800... Val Loss: 9.853495\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 8.450507\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 7.509285\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 8.890875\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 7.544152\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 9.471562\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 9.728368\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 8.703241\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 8.108986\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 7.782871\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 7.518354\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 7.390181\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 8.552110\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 8.414894\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 9.094222\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 9.869996\n",
      "Epoch: 394/2000... Step: 12600... Loss: 14.348065... Val Loss: 9.710032\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 5.168492\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 5.478140\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 6.579357\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 5.534119\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 6.622439\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 6.165200\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 5.586876\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 5.228703\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 5.097704\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 4.929815\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 4.783881\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 5.628175\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 5.669056\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 6.233443\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 6.823037\n",
      "Epoch: 400/2000... Step: 12800... Loss: 5.019062... Val Loss: 6.747037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 6.106924\n",
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 5.804120\n",
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 6.734816\n",
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 5.822369\n",
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 6.672657\n",
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 6.774937\n",
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 6.148998\n",
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 5.766509\n",
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 5.610325\n",
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 5.419206\n",
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 5.283542\n",
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 5.926751\n",
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 5.900774\n",
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 6.403731\n",
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 6.985988\n",
      "Epoch: 407/2000... Step: 13000... Loss: 4.712675... Val Loss: 6.907868\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 6.842111\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 6.313983\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 7.366758\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 6.194393\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 6.857972\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 6.381418\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 5.809993\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 5.559107\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 5.421184\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 5.329511\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 5.151786\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 5.738996\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 5.746551\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 6.284853\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 6.784255\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.082715... Val Loss: 6.759616\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 5.911886\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 5.896580\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 7.414830\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 6.226175\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 7.936182\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 8.225044\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 7.356102\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 6.812528\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 6.489658\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 6.236279\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 6.127259\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 7.242266\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 7.102403\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 7.619187\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 8.290867\n",
      "Epoch: 419/2000... Step: 13400... Loss: 7.575798... Val Loss: 8.123567\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 16.141090\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 14.636377\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 14.993770\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 13.917270\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 16.524906\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 21.325064\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 19.745792\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 18.546917\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 17.742018\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 17.528596\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 17.893435\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 18.729231\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 18.171718\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 18.830696\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 19.637462\n",
      "Epoch: 425/2000... Step: 13600... Loss: 26.186586... Val Loss: 19.161507\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 5.667521\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 5.939458\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 7.374340\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 6.084594\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 7.794122\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 7.633435\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 6.781375\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 6.292532\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 6.016156\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 5.831362\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 5.580823\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 6.794585\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 6.758778\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 7.433337\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 8.190356\n",
      "Epoch: 432/2000... Step: 13800... Loss: 4.597075... Val Loss: 8.043447\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 8.389527\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 7.051876\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 7.475746\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 6.873664\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 7.135297\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 6.672368\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 6.369604\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 6.382845\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 6.357736\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 6.524402\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 6.650538\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 7.060975\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 7.075037\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 7.533112\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 8.157931\n",
      "Epoch: 438/2000... Step: 14000... Loss: 8.943293... Val Loss: 8.231678\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 6.283872\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 6.117184\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 7.573544\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 6.195145\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 7.919117\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 7.458422\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 6.605462\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 6.164947\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 5.887112\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 5.717364\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 5.476191\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 6.642516\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 6.561919\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 7.171914\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 7.871039\n",
      "Epoch: 444/2000... Step: 14200... Loss: 5.333337... Val Loss: 7.729782\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 7.652634\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 7.060723\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 8.166227\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 6.853820\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 8.647809\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 8.215556\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 7.332185\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 6.865837\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 6.608587\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 6.453471\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 6.264443\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 7.351971\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 7.238640\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 7.884589\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 8.629260\n",
      "Epoch: 450/2000... Step: 14400... Loss: 10.309850... Val Loss: 8.431279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 19.229595\n",
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 19.508088\n",
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 20.513094\n",
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 19.679874\n",
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 19.585918\n",
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 18.692157\n",
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 18.697895\n",
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 18.779125\n",
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 18.806676\n",
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 19.063363\n",
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 19.300271\n",
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 19.670011\n",
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 19.903298\n",
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 20.299085\n",
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 20.615798\n",
      "Epoch: 457/2000... Step: 14600... Loss: 20.687077... Val Loss: 20.740754\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 9.167871\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 7.852464\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 8.703906\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 7.978931\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 8.184891\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 7.628706\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 7.364194\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 7.260566\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 7.121973\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 7.204252\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 7.286476\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 7.774342\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 7.749849\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 8.207340\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 8.922330\n",
      "Epoch: 463/2000... Step: 14800... Loss: 6.485158... Val Loss: 9.036901\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 6.668522\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 7.773806\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 8.992052\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 7.828522\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 8.734539\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 8.270818\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 7.736861\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 7.437346\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 7.271099\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 7.066622\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 6.845657\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 7.733601\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 7.838603\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 8.324408\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 8.883197\n",
      "Epoch: 469/2000... Step: 15000... Loss: 4.455636... Val Loss: 8.827891\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 5.228352\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 5.502932\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 6.402895\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 5.364653\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 6.254057\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 5.976667\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 5.441652\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 5.126989\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 5.007710\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 4.910854\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 4.815323\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 5.568338\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 5.677393\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 6.190410\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 6.794838\n",
      "Epoch: 475/2000... Step: 15200... Loss: 7.319087... Val Loss: 6.751927\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 6.032176\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 5.652828\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 6.441192\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 5.437618\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 6.438888\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 6.070323\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 5.481306\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 5.144999\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 4.999547\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 4.884396\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 4.746141\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 5.562149\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 5.500109\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 6.101212\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 6.649427\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.196973... Val Loss: 6.536874\n",
      "Validation loss decreased (6.644956 --> 6.536874).  Saving model ...\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 9.618972\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 7.895093\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 8.354940\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 7.499912\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 7.832769\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 7.545307\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 7.254279\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 7.094607\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 7.047676\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 7.238884\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 7.488973\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 7.907150\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 7.763609\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 8.287484\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 8.849176\n",
      "Epoch: 488/2000... Step: 15600... Loss: 8.557100... Val Loss: 8.786116\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 5.388484\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 5.449080\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 5.983945\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 5.239420\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 5.979701\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 5.824972\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 5.301681\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 5.031606\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 4.902504\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 4.788565\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 4.792703\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 5.466741\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 5.508535\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 6.025004\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 6.774566\n",
      "Epoch: 494/2000... Step: 15800... Loss: 4.865942... Val Loss: 6.755752\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 9.951665\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 8.862648\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 9.184139\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 8.229877\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 9.195839\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 10.127744\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 9.317403\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 8.748851\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 8.575901\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 8.371441\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 8.455421\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 9.063235\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 8.906462\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 9.592287\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 10.153217\n",
      "Epoch: 500/2000... Step: 16000... Loss: 11.113860... Val Loss: 9.886890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 9.909952\n",
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 9.769280\n",
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 10.577637\n",
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 9.391629\n",
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 9.472520\n",
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 8.829481\n",
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 8.444715\n",
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 8.362728\n",
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 8.214001\n",
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 8.415536\n",
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 8.321130\n",
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 8.688982\n",
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 8.803811\n",
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 9.235842\n",
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 9.679875\n",
      "Epoch: 507/2000... Step: 16200... Loss: 7.837017... Val Loss: 9.866768\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 14.690075\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 11.578835\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 11.689669\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 10.799447\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 10.694271\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 10.696296\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 10.540500\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 10.307841\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 10.357085\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 10.832010\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 11.396335\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 11.484802\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 11.212766\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 11.818088\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 12.212720\n",
      "Epoch: 513/2000... Step: 16400... Loss: 11.254375... Val Loss: 12.113952\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 5.368551\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 5.570713\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 6.178298\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 5.057004\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 5.910197\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 5.758567\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 5.149807\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 4.774487\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 4.647545\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 4.576707\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 4.496995\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 5.213498\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 5.267707\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 5.950483\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 6.480252\n",
      "Epoch: 519/2000... Step: 16600... Loss: 3.399033... Val Loss: 6.385374\n",
      "Validation loss decreased (6.536874 --> 6.385374).  Saving model ...\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 6.131311\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 6.123783\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 7.382654\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 5.989917\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 7.295469\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 7.018674\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 6.263503\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 5.820219\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 5.600651\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 5.408332\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 5.183294\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 6.079779\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 6.035464\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 6.652762\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 7.203987\n",
      "Epoch: 525/2000... Step: 16800... Loss: 4.346035... Val Loss: 7.048122\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 5.254843\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 5.542284\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 6.368818\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 5.402555\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 6.424990\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 6.357025\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 5.708829\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 5.350102\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 5.140178\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 4.902533\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 4.738580\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 5.553020\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 5.526961\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 6.036046\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 6.713151\n",
      "Epoch: 532/2000... Step: 17000... Loss: 2.941040... Val Loss: 6.599415\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 8.548761\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 9.523115\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 10.198551\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 9.263125\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 9.734017\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 9.120190\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 8.675938\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 8.590079\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 8.434218\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 8.313482\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 8.084536\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 8.612371\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 8.752735\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 9.200494\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 9.695764\n",
      "Epoch: 538/2000... Step: 17200... Loss: 6.657781... Val Loss: 9.817974\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 8.288755\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 7.831790\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 8.138164\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 6.981028\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 8.529939\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 8.446386\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 7.625573\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 7.148535\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 6.920895\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 6.755759\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 6.700119\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 7.751922\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 7.624680\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 8.353208\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 9.114974\n",
      "Epoch: 544/2000... Step: 17400... Loss: 8.299359... Val Loss: 8.878540\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 7.335204\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 7.398199\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 8.443734\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 7.267575\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 7.669500\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 7.256417\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 6.733530\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 6.533429\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 6.368182\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 6.290639\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 6.080667\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 6.610878\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 6.657402\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 7.138975\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 7.590984\n",
      "Epoch: 550/2000... Step: 17600... Loss: 2.689965... Val Loss: 7.598150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 6.605202\n",
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 6.254741\n",
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 7.029758\n",
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 5.891166\n",
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 6.793520\n",
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 6.733689\n",
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 6.069248\n",
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 5.707722\n",
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 5.530789\n",
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 5.331357\n",
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 5.179875\n",
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 5.870724\n",
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 5.784522\n",
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 6.337175\n",
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 6.862607\n",
      "Epoch: 557/2000... Step: 17800... Loss: 5.096449... Val Loss: 6.715615\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 6.748245\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 6.044033\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 6.399421\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 5.517114\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 6.258144\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 5.981098\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 5.491724\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 5.329436\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 5.227566\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 5.149517\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 5.109154\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 5.783259\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 5.731957\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 6.272529\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 6.926817\n",
      "Epoch: 563/2000... Step: 18000... Loss: 4.603101... Val Loss: 6.823500\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 6.909777\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 6.989027\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 8.192732\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 6.627622\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 7.851493\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 10.213117\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 9.187935\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 8.363588\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 7.890074\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 7.866096\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 8.205768\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 8.888078\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 8.766133\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 9.290604\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 9.667351\n",
      "Epoch: 569/2000... Step: 18200... Loss: 8.639493... Val Loss: 9.405692\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 6.585196\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 6.298711\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 7.914690\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 6.576229\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 8.315050\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 9.853048\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 8.829985\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 8.102618\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 7.668609\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 7.536399\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 7.691772\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 8.702149\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 8.517109\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 9.054096\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 9.655032\n",
      "Epoch: 575/2000... Step: 18400... Loss: 7.115263... Val Loss: 9.437995\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 6.530230\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 6.153697\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 6.762133\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 5.739266\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 7.082975\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 7.076791\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 6.344510\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 5.904093\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 5.722852\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 5.526613\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 5.391001\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 6.320982\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 6.234719\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 6.856888\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 7.552962\n",
      "Epoch: 582/2000... Step: 18600... Loss: 5.139235... Val Loss: 7.356113\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 7.587659\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 6.911028\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 7.474919\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 6.433317\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 7.504752\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 7.334347\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 6.693661\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 6.357492\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 6.221017\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 6.033632\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 5.891282\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 6.671418\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 6.538017\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 7.137203\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 7.719575\n",
      "Epoch: 588/2000... Step: 18800... Loss: 5.530479... Val Loss: 7.525624\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 5.129819\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 5.327464\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 5.692077\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 4.867466\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 5.984650\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 5.797806\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 5.210926\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 4.901588\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 4.739899\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 4.547304\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 4.451067\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 5.295985\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 5.247260\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 5.737075\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 6.428405\n",
      "Epoch: 594/2000... Step: 19000... Loss: 3.966418... Val Loss: 6.305190\n",
      "Validation loss decreased (6.385374 --> 6.305190).  Saving model ...\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 11.718910\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 8.752657\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 8.584171\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 7.900242\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 7.893675\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 7.634713\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 7.256528\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 7.126804\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 7.270127\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 7.557965\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 7.784924\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 7.915879\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 7.720355\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 8.272922\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 8.675472\n",
      "Epoch: 600/2000... Step: 19200... Loss: 2.741350... Val Loss: 8.701096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 5.556987\n",
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 5.429062\n",
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 5.854851\n",
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 5.011540\n",
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 5.970013\n",
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 5.937334\n",
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 5.357470\n",
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 5.062073\n",
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 4.926464\n",
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 4.772157\n",
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 4.696071\n",
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 5.477571\n",
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 5.449378\n",
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 5.990315\n",
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 6.669602\n",
      "Epoch: 607/2000... Step: 19400... Loss: 2.853083... Val Loss: 6.554188\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 10.027059\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 8.872258\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 9.079911\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 8.202937\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 9.070462\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 9.093363\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 8.524236\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 8.205303\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 8.139479\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 8.035882\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 7.999664\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 8.636212\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 8.471028\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 9.103773\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 9.662737\n",
      "Epoch: 613/2000... Step: 19600... Loss: 10.607748... Val Loss: 9.444010\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 5.610427\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 5.677875\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 6.594281\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 5.577312\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 7.623647\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 7.309392\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 6.493124\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 5.993316\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 5.724872\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 5.512346\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 5.375832\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 6.686715\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 6.602219\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 7.177624\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 8.143891\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.505885... Val Loss: 8.037768\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 10.572957\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 8.683179\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 9.177063\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 7.988160\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 7.773775\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 7.321962\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 6.835690\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 6.778925\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 6.623932\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 6.709572\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 6.673929\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 6.867826\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 6.775802\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 7.256037\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 7.737677\n",
      "Epoch: 625/2000... Step: 20000... Loss: 3.239160... Val Loss: 7.890853\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 5.865415\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 5.829261\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 6.528044\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 5.415694\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 6.694704\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 6.555793\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 5.841647\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 5.445466\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 5.245547\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 5.047564\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 4.921372\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 5.777476\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 5.707384\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 6.312839\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 6.925916\n",
      "Epoch: 632/2000... Step: 20200... Loss: 2.554794... Val Loss: 6.758630\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 7.440448\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 6.603753\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 6.995795\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 6.172939\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 6.959100\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 6.797704\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 6.350549\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 6.069730\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 5.987487\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 6.011214\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 6.089127\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 6.672644\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 6.585150\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 7.187876\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 7.763343\n",
      "Epoch: 638/2000... Step: 20400... Loss: 8.308041... Val Loss: 7.667206\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 5.794507\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 6.461812\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 7.957715\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 6.740068\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 8.782302\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 8.883095\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 8.021225\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 7.473915\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 7.138810\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 6.863447\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 6.685799\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 7.925854\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 7.838532\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 8.341421\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 9.128037\n",
      "Epoch: 644/2000... Step: 20600... Loss: 4.103987... Val Loss: 8.997352\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 10.865291\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 13.004421\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 14.773940\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 13.713720\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 15.572274\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 15.293646\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 14.702659\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 14.281049\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 13.987865\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 13.614669\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 13.518652\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 14.771524\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 14.871005\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 15.163849\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 16.003784\n",
      "Epoch: 650/2000... Step: 20800... Loss: 13.732037... Val Loss: 15.904939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 6.328410\n",
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 6.252977\n",
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 7.247739\n",
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 5.826568\n",
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 6.936384\n",
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 6.714553\n",
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 5.947106\n",
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 5.581682\n",
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 5.338535\n",
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 5.227612\n",
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 5.014129\n",
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 5.844425\n",
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 5.818617\n",
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 6.435766\n",
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 6.954221\n",
      "Epoch: 657/2000... Step: 21000... Loss: 2.788664... Val Loss: 6.805383\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 8.419158\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 7.706657\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 8.027451\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 7.421009\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 8.815708\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 8.812269\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 8.206321\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 7.841962\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 7.729158\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 7.464690\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 7.389693\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 8.386029\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 8.187192\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 8.691155\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 9.544912\n",
      "Epoch: 663/2000... Step: 21200... Loss: 6.665121... Val Loss: 9.285680\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 6.083291\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 5.773248\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 6.295136\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 5.295215\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 6.434434\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 6.484027\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 5.791360\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 5.410326\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 5.217935\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 5.026206\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 4.926199\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 5.818977\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 5.743906\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 6.307001\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 6.958331\n",
      "Epoch: 669/2000... Step: 21400... Loss: 4.575490... Val Loss: 6.771666\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 6.505758\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 6.909155\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 7.688284\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 6.511110\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 7.101633\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 7.243336\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 6.633107\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 6.335173\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 6.127661\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 5.987251\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 5.882357\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 6.448506\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 6.506339\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 6.918832\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 7.413868\n",
      "Epoch: 675/2000... Step: 21600... Loss: 7.087403... Val Loss: 7.384455\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 6.963678\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 6.745983\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 7.116051\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 6.217339\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 6.702837\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 6.206268\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 5.737862\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 5.644215\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 5.499033\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 5.388857\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 5.208075\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 5.765386\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 5.656573\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 6.132057\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 6.647466\n",
      "Epoch: 682/2000... Step: 21800... Loss: 5.019031... Val Loss: 6.592030\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 6.713162\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 6.220130\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 6.525096\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 5.596816\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 6.149411\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 6.222784\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 5.652549\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 5.436786\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 5.283220\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 5.130255\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 5.047089\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 5.615281\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 5.589470\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 6.003204\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 6.623341\n",
      "Epoch: 688/2000... Step: 22000... Loss: 4.167770... Val Loss: 6.542924\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 5.225497\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 5.515202\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 6.052845\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 5.138750\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 6.642077\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 6.556875\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 5.846099\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 5.438209\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 5.205030\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 4.979987\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 4.902020\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 5.950720\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 5.846157\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 6.370531\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 7.183474\n",
      "Epoch: 694/2000... Step: 22200... Loss: 5.151803... Val Loss: 7.000424\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 12.220641\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 11.215898\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 11.326106\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 10.498843\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 11.913075\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 12.211764\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 11.454450\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 10.922523\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 10.758032\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 10.533317\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 10.533046\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 11.441870\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 11.194530\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 11.824692\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 12.606089\n",
      "Epoch: 700/2000... Step: 22400... Loss: 8.338296... Val Loss: 12.254426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 5.002669\n",
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 5.296895\n",
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 5.774683\n",
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 4.943832\n",
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 6.168818\n",
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 5.892613\n",
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 5.333046\n",
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 5.108514\n",
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 4.900433\n",
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 4.703957\n",
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 4.597098\n",
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 5.490814\n",
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 5.474770\n",
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 5.886469\n",
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 6.728677\n",
      "Epoch: 707/2000... Step: 22600... Loss: 3.132545... Val Loss: 6.595965\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 6.196470\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 6.400403\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 6.845935\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 5.896171\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 6.620997\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 6.236285\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 5.774705\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 5.587176\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 5.395500\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 5.298815\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 5.216763\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 5.855415\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 5.801102\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 6.276068\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 6.846471\n",
      "Epoch: 713/2000... Step: 22800... Loss: 4.447173... Val Loss: 6.773012\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 7.944862\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 7.033571\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 7.203464\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 6.209345\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 6.814776\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 6.845125\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 6.283515\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 6.100644\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 5.960613\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 5.909114\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 5.908964\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 6.379543\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 6.262365\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 6.652955\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 7.199333\n",
      "Epoch: 719/2000... Step: 23000... Loss: 5.835275... Val Loss: 7.123803\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 6.566978\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 6.632687\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 7.807171\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 6.667359\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 8.377552\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 9.401938\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 8.511623\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 7.846817\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 7.466784\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 7.348148\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 7.563013\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 8.638102\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 8.438517\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 8.945020\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 9.653189\n",
      "Epoch: 725/2000... Step: 23200... Loss: 7.223214... Val Loss: 9.468437\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 5.704892\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 5.635751\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 5.829641\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 4.914175\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 5.942080\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 5.877916\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 5.248440\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 4.941113\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 4.773798\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 4.648133\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 4.575342\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 5.410723\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 5.369001\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 5.940353\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 6.625078\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.871834... Val Loss: 6.456398\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 6.590519\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 6.941138\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 7.273616\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 6.495032\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 7.078976\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 6.789654\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 6.408878\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 6.320158\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 6.102019\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 6.122897\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 6.129503\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 6.694501\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 6.678724\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 7.100816\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 7.753723\n",
      "Epoch: 738/2000... Step: 23600... Loss: 5.752905... Val Loss: 7.799980\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 5.267934\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 5.439817\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 5.811563\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 4.882075\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 5.909931\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 5.592989\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 5.026766\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 4.758745\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 4.557818\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 4.439785\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 4.293198\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 5.197396\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 5.149869\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 5.636319\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 6.388079\n",
      "Epoch: 744/2000... Step: 23800... Loss: 3.859472... Val Loss: 6.221149\n",
      "Validation loss decreased (6.305190 --> 6.221149).  Saving model ...\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 5.666622\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 6.012684\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 6.724611\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 5.503440\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 6.701767\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 6.436766\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 5.760328\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 5.426444\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 5.207586\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 5.059266\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 4.858895\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 5.817194\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 5.800083\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 6.308817\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 6.933708\n",
      "Epoch: 750/2000... Step: 24000... Loss: 6.537317... Val Loss: 6.776889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 6.389874\n",
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 6.073219\n",
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 6.441287\n",
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 5.581303\n",
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 6.823541\n",
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 6.953875\n",
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 6.263613\n",
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 5.859786\n",
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 5.662625\n",
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 5.466410\n",
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 5.382443\n",
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 6.315776\n",
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 6.188562\n",
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 6.741674\n",
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 7.466100\n",
      "Epoch: 757/2000... Step: 24200... Loss: 4.056296... Val Loss: 7.277009\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 8.605508\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 7.932965\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 8.194842\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 7.241946\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 8.779344\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 9.101789\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 8.259519\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 7.717235\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 7.488321\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 7.322535\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 7.308093\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 8.293107\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 8.128502\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 8.818963\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 9.566717\n",
      "Epoch: 763/2000... Step: 24400... Loss: 3.916711... Val Loss: 9.299811\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 5.890146\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 6.292118\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 7.543103\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 6.290501\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 8.417979\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 8.268870\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 7.375821\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 6.871101\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 6.541031\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 6.370462\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 6.144721\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 7.437172\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 7.356768\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 7.891047\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 8.735284\n",
      "Epoch: 769/2000... Step: 24600... Loss: 5.220328... Val Loss: 8.555018\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 5.748559\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 7.038854\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 7.735802\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 6.689160\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 7.687810\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 7.600418\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 7.006563\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 6.689596\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 6.451041\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 6.257025\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 6.095569\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 6.974565\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 7.015328\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 7.457137\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 8.120664\n",
      "Epoch: 775/2000... Step: 24800... Loss: 5.777778... Val Loss: 8.029275\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 6.958213\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 6.492737\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 6.886095\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 5.828366\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 7.151906\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 7.228428\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 6.576456\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 6.159407\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 5.949371\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 5.808657\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 5.750104\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 6.507740\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 6.373420\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 6.925332\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 7.595699\n",
      "Epoch: 782/2000... Step: 25000... Loss: 3.308339... Val Loss: 7.357837\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 6.935974\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 6.487329\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 6.530683\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 5.763177\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 6.374997\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 6.578958\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 6.092109\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 5.979687\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 5.822065\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 5.793980\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 5.769317\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 6.339462\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 6.260285\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 6.667346\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 7.323905\n",
      "Epoch: 788/2000... Step: 25200... Loss: 5.064171... Val Loss: 7.245305\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 5.379506\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 5.989518\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 6.301293\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 5.290674\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 6.200161\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 6.204962\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 5.617911\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 5.399215\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 5.163446\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 5.050837\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 4.882740\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 5.589388\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 5.585547\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 6.054211\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 6.692821\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.658960... Val Loss: 6.598121\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 7.004344\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 6.602696\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 6.995567\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 6.040391\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 7.480114\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 7.421246\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 6.666322\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 6.232810\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 5.987970\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 5.795903\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 5.767577\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 6.814586\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 6.676414\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 7.234426\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 8.008949\n",
      "Epoch: 800/2000... Step: 25600... Loss: 5.703186... Val Loss: 7.815909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 5.567781\n",
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 5.613039\n",
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 5.864662\n",
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 4.946822\n",
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 6.042582\n",
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 6.107640\n",
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 5.477002\n",
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 5.143973\n",
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 4.973868\n",
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 4.816460\n",
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 4.725254\n",
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 5.550352\n",
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 5.473869\n",
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 5.973560\n",
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 6.614389\n",
      "Epoch: 807/2000... Step: 25800... Loss: 4.270213... Val Loss: 6.462223\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 13.893024\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 12.302940\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 11.910545\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 11.073114\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 11.902706\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 12.116179\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 11.631273\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 11.368964\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 11.367814\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 11.322780\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 11.440039\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 12.037391\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 11.797094\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 12.322525\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 12.891699\n",
      "Epoch: 813/2000... Step: 26000... Loss: 10.895942... Val Loss: 12.561551\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 7.497225\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 6.997531\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 7.220947\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 6.260817\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 7.756151\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 7.623993\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 6.914593\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 6.519993\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 6.333379\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 6.136115\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 6.003285\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 6.994615\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 6.848534\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 7.410863\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 8.188676\n",
      "Epoch: 819/2000... Step: 26200... Loss: 5.999779... Val Loss: 7.939200\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 10.748664\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 9.768012\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 9.940183\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 8.877426\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 10.106284\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 10.028252\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 9.314538\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 8.915517\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 8.746890\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 8.610858\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 8.471095\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 9.419738\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 9.246307\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 9.826303\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 10.532377\n",
      "Epoch: 825/2000... Step: 26400... Loss: 7.080475... Val Loss: 10.248956\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 6.146205\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 6.223274\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 6.508957\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 5.529696\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 6.477087\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 6.101270\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 5.531975\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 5.384563\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 5.155991\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 5.011091\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 4.910236\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 5.651143\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 5.626615\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 6.033351\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 6.768956\n",
      "Epoch: 832/2000... Step: 26600... Loss: 4.008000... Val Loss: 6.673722\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 10.007825\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 8.340906\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 8.511868\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 7.367093\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 7.553725\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 7.651850\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 7.043900\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 6.927166\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 6.790526\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 6.783337\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 6.729319\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 7.021256\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 6.870552\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 7.334319\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 7.864879\n",
      "Epoch: 838/2000... Step: 26800... Loss: 6.244626... Val Loss: 7.889758\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 6.020520\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 7.187712\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 7.856434\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 6.809567\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 8.089694\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 7.743826\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 7.155658\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 6.824878\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 6.621840\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 6.430297\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 6.353587\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 7.378542\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 7.416184\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 7.859428\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 8.613587\n",
      "Epoch: 844/2000... Step: 27000... Loss: 6.128330... Val Loss: 8.451642\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 6.216753\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 7.446940\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 8.354360\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 7.196118\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 8.008488\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 7.608728\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 7.035846\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 6.716925\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 6.489908\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 6.294234\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 6.115988\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 6.917946\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 6.956535\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 7.453169\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 8.035974\n",
      "Epoch: 850/2000... Step: 27200... Loss: 5.784881... Val Loss: 7.954277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 8.969638\n",
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 9.858862\n",
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 10.620674\n",
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 9.176749\n",
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 9.608928\n",
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 9.155480\n",
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 8.646884\n",
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 8.450332\n",
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 8.211495\n",
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 8.082173\n",
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 7.852636\n",
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 8.434673\n",
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 8.515859\n",
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 8.921730\n",
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 9.464818\n",
      "Epoch: 857/2000... Step: 27400... Loss: 6.300362... Val Loss: 9.427005\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 7.764529\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 7.043378\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 7.472791\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 6.406077\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 6.803223\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 6.788348\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 6.238558\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 5.987398\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 5.832875\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 5.766617\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 5.674790\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 6.145605\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 6.023238\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 6.538606\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 7.003324\n",
      "Epoch: 863/2000... Step: 27600... Loss: 3.294644... Val Loss: 6.938409\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 5.814517\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 5.939791\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 6.336607\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 5.401130\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 6.929748\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 7.061644\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 6.279907\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 5.814726\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 5.571987\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 5.356780\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 5.245207\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 6.208923\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 6.070153\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 6.639634\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 7.379006\n",
      "Epoch: 869/2000... Step: 27800... Loss: 4.749193... Val Loss: 7.157128\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 6.992518\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 8.626767\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 8.988235\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 7.953180\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 8.708056\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 8.189625\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 7.773926\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 7.663044\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 7.470516\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 7.431070\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 7.308524\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 8.013858\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 8.112568\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 8.506185\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 9.104959\n",
      "Epoch: 875/2000... Step: 28000... Loss: 3.761228... Val Loss: 9.095877\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 8.997808\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 8.398639\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 8.193348\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 7.426686\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 9.039713\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 9.079170\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 8.361022\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 7.944065\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 7.741997\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 7.528770\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 7.475790\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 8.448864\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 8.267630\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 8.746083\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 9.613101\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.350745... Val Loss: 9.314753\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 7.020257\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 6.356367\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 6.551305\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 5.650492\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 6.091557\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 5.905876\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 5.456678\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 5.277399\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 5.167521\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 5.228608\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 5.244501\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 5.705250\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 5.620004\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 6.153369\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 6.668548\n",
      "Epoch: 888/2000... Step: 28400... Loss: 2.106731... Val Loss: 6.623740\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 6.818148\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 6.630152\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 6.678983\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 5.584941\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 6.363261\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 6.364785\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 5.735830\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 5.423172\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 5.252819\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 5.142196\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 5.014122\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 5.704607\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 5.618653\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 6.104018\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 6.676190\n",
      "Epoch: 894/2000... Step: 28600... Loss: 5.197202... Val Loss: 6.501270\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 5.853109\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 5.980903\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 6.603409\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 5.608423\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 7.465694\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 7.457656\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 6.662794\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 6.155084\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 5.852273\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 5.754903\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 5.842103\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 6.963219\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 6.789660\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 7.376631\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 8.219786\n",
      "Epoch: 900/2000... Step: 28800... Loss: 4.195764... Val Loss: 8.022402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 7.541636\n",
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 6.799582\n",
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 7.234077\n",
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 6.250169\n",
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 6.754314\n",
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 6.441212\n",
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 5.896522\n",
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 5.756598\n",
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 5.579849\n",
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 5.412972\n",
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 5.248873\n",
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 5.780307\n",
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 5.673793\n",
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 6.115272\n",
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 6.746067\n",
      "Epoch: 907/2000... Step: 29000... Loss: 3.222414... Val Loss: 6.650820\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 7.190456\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 6.574974\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 6.765924\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 6.058192\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 6.871574\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 6.646229\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 6.162856\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 6.066660\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 5.936160\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 5.794704\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 5.738913\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 6.407821\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 6.317799\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 6.701889\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 7.471621\n",
      "Epoch: 913/2000... Step: 29200... Loss: 2.939576... Val Loss: 7.354703\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 9.326781\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 11.022306\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 11.978554\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 10.705658\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 11.468655\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 11.028799\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 10.590399\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 10.323699\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 10.165898\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 10.049175\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 10.013841\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 10.907056\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 11.030467\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 11.409770\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 11.951054\n",
      "Epoch: 919/2000... Step: 29400... Loss: 5.336899... Val Loss: 11.802680\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 6.270189\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 6.001771\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 6.223317\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 5.506023\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 7.114995\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 6.887447\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 6.303883\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 6.073807\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 5.885663\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 5.642218\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 5.568293\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 6.540886\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 6.400104\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 6.782017\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 7.721301\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.760576... Val Loss: 7.475219\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 5.078240\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 5.628175\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 5.966344\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 5.019376\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 5.980351\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 5.850294\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 5.259265\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 5.036264\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 4.804738\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 4.612226\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 4.504829\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 5.329233\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 5.279061\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 5.708992\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 6.391689\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.878361... Val Loss: 6.255346\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 12.036218\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 10.453217\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 10.159940\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 9.447415\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 10.282887\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 10.276752\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 9.688443\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 9.365373\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 9.384690\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 9.185309\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 9.111388\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 9.866539\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 9.652397\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 10.225848\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 10.847460\n",
      "Epoch: 938/2000... Step: 30000... Loss: 10.650083... Val Loss: 10.543025\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 14.775002\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 13.633952\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 13.659544\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 12.604016\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 14.782378\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 15.049600\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 14.068543\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 13.415128\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 13.145292\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 13.017398\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 12.966859\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 13.912220\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 13.607434\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 14.330262\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 15.107389\n",
      "Epoch: 944/2000... Step: 30200... Loss: 13.029490... Val Loss: 14.784689\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 5.694714\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 6.885480\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 7.547406\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 6.575630\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 7.935439\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 7.494493\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 6.949425\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 6.710555\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 6.460142\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 6.267282\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 6.045118\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 7.037264\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 7.041500\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 7.460114\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 8.209763\n",
      "Epoch: 950/2000... Step: 30400... Loss: 6.670498... Val Loss: 8.127976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 6.737012\n",
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 6.605983\n",
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 7.074867\n",
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 6.019095\n",
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 6.640362\n",
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 6.238561\n",
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 5.719869\n",
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 5.541618\n",
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 5.380660\n",
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 5.252982\n",
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 5.110811\n",
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 5.692895\n",
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 5.568213\n",
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 6.112190\n",
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 6.595090\n",
      "Epoch: 957/2000... Step: 30600... Loss: 3.868709... Val Loss: 6.505357\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 5.926479\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 6.525989\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 7.003800\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 5.796825\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 6.568866\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 6.263782\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 5.705923\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 5.486146\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 5.311963\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 5.207638\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 5.047292\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 5.770575\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 5.793862\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 6.280880\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 6.762338\n",
      "Epoch: 963/2000... Step: 30800... Loss: 3.953963... Val Loss: 6.673145\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 10.260139\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 9.407161\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 9.773376\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 8.580413\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 10.325631\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 10.248752\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 9.313908\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 8.801376\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 8.505677\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 8.386069\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 8.302421\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 9.408881\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 9.255856\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 9.915317\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 10.732101\n",
      "Epoch: 969/2000... Step: 31000... Loss: 6.812928... Val Loss: 10.501752\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 5.116008\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 5.458652\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 5.798782\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 4.790384\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 5.909907\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 5.639106\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 5.022263\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 4.708904\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 4.523492\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 4.375213\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 4.267453\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 5.138182\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 5.093660\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 5.630721\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 6.268908\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.167085... Val Loss: 6.127864\n",
      "Validation loss decreased (6.221149 --> 6.127864).  Saving model ...\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 6.064552\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 6.159324\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 6.302075\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 5.133548\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 6.427712\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 6.077642\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 5.407747\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 5.142586\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 4.944976\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 4.763970\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 4.635422\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 5.574957\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 5.525885\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 6.031346\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 6.739667\n",
      "Epoch: 982/2000... Step: 31400... Loss: 3.615650... Val Loss: 6.552853\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 8.525010\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 7.366241\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 7.291772\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 6.448102\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 6.887437\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 6.802746\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 6.456501\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 6.457507\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 6.318179\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 6.400888\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 6.574932\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 7.058821\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 6.879087\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 7.297262\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 7.884607\n",
      "Epoch: 988/2000... Step: 31600... Loss: 5.147398... Val Loss: 7.790240\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 4.998722\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 5.680608\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 5.783614\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 4.808800\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 5.896766\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 5.627781\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 5.041849\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 4.810487\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 4.622209\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 4.495963\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 4.393889\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 5.250718\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 5.217766\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 5.693777\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 6.365917\n",
      "Epoch: 994/2000... Step: 31800... Loss: 5.089913... Val Loss: 6.244585\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 6.734761\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 6.698776\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 6.423026\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 5.303027\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 5.954736\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 5.759533\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 5.256686\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 5.151379\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 4.950344\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 4.944994\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 4.942427\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 5.527382\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 5.429248\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 5.827717\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 6.432768\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 1.012630... Val Loss: 6.330945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 9.727832\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 8.735134\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 9.285856\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 7.503828\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 7.852363\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 7.796173\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 6.964989\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 6.805531\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 6.497725\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 6.334990\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 6.057682\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 6.482179\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 6.327531\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 6.818590\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 7.291206\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 9.164244... Val Loss: 7.228556\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 8.683299\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 8.030752\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 8.110035\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 7.256218\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 8.215757\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 8.146353\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 7.648102\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 7.405503\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 7.305382\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 7.148613\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 7.073441\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 7.830509\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 7.637706\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 8.105801\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 8.732375\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 3.920691... Val Loss: 8.480309\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 11.939625\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 10.999354\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 11.147800\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 10.041413\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 12.126035\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 12.306158\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 11.337222\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 10.677714\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 10.377647\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 10.300351\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 10.364244\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 11.421733\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 11.174597\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 11.833699\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 12.698200\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 11.462561... Val Loss: 12.437242\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 6.457456\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 6.203485\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 6.690753\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 5.648246\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 7.166706\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 7.142480\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 6.376377\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 5.935194\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 5.691350\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 5.502145\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 5.485127\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 6.417100\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 6.297284\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 6.830142\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 7.583322\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.487314... Val Loss: 7.335381\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 5.634459\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 5.403395\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 5.982458\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 5.006822\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 6.204281\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 5.896878\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 5.330383\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 5.061807\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 4.848917\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 4.680895\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 4.569412\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 5.408899\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 5.334109\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 5.780921\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 6.504915\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 4.681262... Val Loss: 6.325432\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 5.424095\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 5.680241\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 6.189642\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 5.342530\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 6.351455\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 5.940289\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 5.449765\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 5.343846\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 5.138971\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 5.028200\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 4.948706\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 5.704545\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 5.747098\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 6.144152\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 6.943925\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 2.207434... Val Loss: 6.880622\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 9.596234\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 8.785557\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 9.070168\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 8.079281\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 9.939257\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 9.990043\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 9.172989\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 8.649239\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 8.378630\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 8.195697\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 8.209435\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 9.340476\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 9.102448\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 9.616710\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 10.479788\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 13.777723... Val Loss: 10.250548\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 6.416538\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 6.439084\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 6.840201\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 5.726712\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 7.210113\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 7.120071\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 6.301779\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 5.846358\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 5.583857\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 5.483626\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 5.363377\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 6.278233\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 6.198544\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 6.855909\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 7.594243\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.836054... Val Loss: 7.373543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 6.021425\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 5.904737\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 6.329686\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 5.261439\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 6.340252\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 6.051026\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 5.376964\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 5.052372\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 4.843182\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 4.712753\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 4.554462\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 5.451314\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 5.362338\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 5.911631\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 6.597672\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 3.711508... Val Loss: 6.405536\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 4.385886\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 5.468753\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 5.740748\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 4.825282\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 5.952459\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 5.716010\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 5.186679\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 4.953375\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 4.776823\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 4.756211\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 4.793204\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 5.615597\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 5.690025\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 6.076774\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 6.752383\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 1.832414... Val Loss: 6.688242\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 6.997094\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 6.381026\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 6.537147\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 5.461819\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 6.228988\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 6.081609\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 5.519235\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 5.318432\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 5.139041\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 5.055050\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 4.985498\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 5.644540\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 5.539087\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 5.990089\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 6.624376\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 3.262375... Val Loss: 6.470238\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 4.551302\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 5.283003\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 5.649944\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 4.739146\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 6.426065\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 6.050641\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 5.382381\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 5.044470\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 4.781069\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 4.586814\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 4.507899\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 5.581418\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 5.518020\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 5.975363\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 6.840096\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 3.994442... Val Loss: 6.672231\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 11.559203\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 13.305331\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 14.409818\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 13.015069\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 13.485229\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 14.085253\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 13.447032\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 13.106965\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 12.795544\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 12.491576\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 12.205004\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 12.844611\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 12.929145\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 13.234284\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 13.625430\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 8.197996... Val Loss: 13.542908\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 12.624766\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 10.957696\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 10.868054\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 10.117100\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 10.550576\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 10.621463\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 10.291544\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 10.183513\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 10.207767\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 10.238681\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 10.355995\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 10.791893\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 10.539257\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 10.985544\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 11.493768\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 6.573285... Val Loss: 11.284716\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 5.508354\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 5.864254\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 6.107179\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 5.057030\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 5.925087\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 5.687533\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 5.145652\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 5.023879\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 4.820391\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 4.670870\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 4.583349\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 5.257006\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 5.245495\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 5.641211\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 6.317033\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 3.202897... Val Loss: 6.236004\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 5.135454\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 5.650904\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 5.983150\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 4.924378\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 6.102080\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 5.872732\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 5.212279\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 4.886891\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 4.622010\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 4.425392\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 4.230487\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 5.148283\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 5.079851\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 5.604132\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 6.318415\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 4.384528... Val Loss: 6.183734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 7.839473\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 8.678975\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 9.570820\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 8.332085\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 9.153397\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 8.813928\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 8.299075\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 8.068176\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 7.853780\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 7.700954\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 7.507226\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 8.318542\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 8.409655\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 8.800727\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 9.350921\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 5.802655... Val Loss: 9.279312\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 5.786672\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 5.766736\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 5.829305\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 4.900311\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 6.050296\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 5.701255\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 5.159458\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 5.145392\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 4.922193\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 4.809484\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 4.708092\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 5.308433\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 5.225039\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 5.596079\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 6.317959\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 3.711123... Val Loss: 6.227461\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 6.216921\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 6.108276\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 5.924460\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 5.075567\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 5.954020\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 5.971743\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 5.417910\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 5.195845\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 5.053503\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 4.978803\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 4.967968\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 5.608969\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 5.520169\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 5.998917\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 6.664652\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.058672... Val Loss: 6.500014\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 5.735061\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 6.558530\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 6.957386\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 5.877674\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 7.046685\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 6.503758\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 5.951953\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 5.760964\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 5.505764\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 5.291633\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 5.092793\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 6.029575\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 5.996398\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 6.373993\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 7.049185\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 3.641372... Val Loss: 6.917320\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 6.233076\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 6.316987\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 7.049957\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 6.152963\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 7.721355\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 7.545495\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 6.866431\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 6.442418\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 6.229515\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 6.007187\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 5.874444\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 6.832179\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 6.636050\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 7.158202\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 7.852433\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.562418... Val Loss: 7.642146\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 6.523251\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 6.452413\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 6.724208\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 5.517176\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 5.922033\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 5.678452\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 5.129713\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 4.964535\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 4.801856\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 4.748177\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 4.631604\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 5.148206\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 5.110981\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 5.626124\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 6.110225\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.948006... Val Loss: 6.033921\n",
      "Validation loss decreased (6.127864 --> 6.033921).  Saving model ...\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 7.117545\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 6.895939\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 7.009909\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 6.040373\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 7.870908\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 7.613668\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 6.822621\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 6.385937\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 6.131773\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 5.923857\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 5.829845\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 6.993210\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 6.834465\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 7.304781\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 8.159753\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 6.713000... Val Loss: 7.915099\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 6.474967\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 6.280538\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 6.452342\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 5.609900\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 6.563896\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 6.719892\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 6.099848\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 5.759618\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 5.586513\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 5.471467\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 5.387650\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 6.074227\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 5.936391\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 6.462636\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 7.083056\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 10.169663... Val Loss: 6.893274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 5.556385\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 5.644516\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 5.583509\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 4.753359\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 5.627693\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 6.015826\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 5.435722\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 5.248738\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 5.040652\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 4.871179\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 4.763041\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 5.530306\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 5.409770\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 5.793865\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 6.456655\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 2.684809... Val Loss: 6.297524\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 12.172170\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 11.707062\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 11.152694\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 10.291023\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 11.771606\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 12.098570\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 11.289571\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 10.745229\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 10.593039\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 10.361227\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 10.402840\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 11.200457\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 10.966013\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 11.487751\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 12.224580\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 10.240338... Val Loss: 11.915611\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 4.952645\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 5.636354\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 5.497697\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 4.525509\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 5.756011\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 5.506565\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 4.878908\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 4.604184\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 4.372288\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 4.253842\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 4.125729\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 5.047134\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 4.982733\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 5.441682\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 6.251280\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 4.032858... Val Loss: 6.085937\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 5.560064\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 6.370647\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 6.604152\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 5.338828\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 7.100158\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 7.097811\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 6.272738\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 5.810173\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 5.474167\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 5.334710\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 5.199192\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 6.266013\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 6.201592\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 6.700059\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 7.494183\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 6.064365... Val Loss: 7.345607\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 8.232746\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 7.988254\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 8.193366\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 7.123970\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 9.291393\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 9.433362\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 8.477251\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 7.874191\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 7.583567\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 7.402825\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 7.307195\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 8.378672\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 8.198286\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 8.752366\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 9.575994\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 4.844960... Val Loss: 9.342034\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 6.907784\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 6.458128\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 6.716422\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 5.609861\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 6.035624\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 6.100187\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 5.541018\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 5.380196\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 5.209823\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 5.119263\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 5.044273\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 5.499471\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 5.421858\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 5.820984\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 6.350110\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 2.301384... Val Loss: 6.298483\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 4.873740\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 5.353984\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 5.330862\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 4.499717\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 5.580633\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 5.348463\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 4.856532\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 4.771116\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 4.578768\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 4.469123\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 4.420142\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 5.217128\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 5.197116\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 5.552269\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 6.289649\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 5.684412... Val Loss: 6.183350\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 5.073001\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 5.442997\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 5.664058\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 4.701353\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 6.273693\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 6.125353\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 5.431133\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 5.060574\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 4.802550\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 4.618136\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 4.507766\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 5.538903\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 5.445153\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 5.878650\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 6.649429\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 6.820833... Val Loss: 6.482367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 8.769723\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 7.813331\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 8.428600\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 7.004393\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 7.242471\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 6.999276\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 6.332524\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 6.183741\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 5.967544\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 5.820723\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 5.602823\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 6.036429\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 5.879508\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 6.317532\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 6.776972\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 7.843672... Val Loss: 6.682978\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 10.512786\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 9.931517\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 9.559859\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 8.785103\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 10.065103\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 10.117288\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 9.348783\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 8.908406\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 8.750443\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 8.598591\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 8.618103\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 9.537306\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 9.352594\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 9.885969\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 10.677762\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 4.024355... Val Loss: 10.414863\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 5.762781\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 5.765940\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 6.254756\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 5.129339\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 6.064507\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 6.463086\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 5.750081\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 5.445737\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 5.170809\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 4.958673\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 4.766371\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 5.593018\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 5.488094\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 5.867010\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 6.468988\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.886807... Val Loss: 6.302488\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 5.173128\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 5.683700\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 5.691098\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 4.722707\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 6.057918\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 5.953230\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 5.301084\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 4.985035\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 4.735261\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 4.637326\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 4.603454\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 5.488620\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 5.390866\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 5.806020\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 6.574051\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.384317... Val Loss: 6.402203\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 5.805021\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 6.258365\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 6.292837\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 5.335573\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 6.157218\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 6.002487\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 5.491234\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 5.465382\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 5.240012\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 5.141092\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 5.032525\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 5.705746\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 5.684723\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 5.995566\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 6.632198\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 6.858473... Val Loss: 6.572236\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 4.974568\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 5.724781\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 5.875813\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 4.810691\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 6.273574\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 5.981244\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 5.360346\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 5.162535\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 4.903562\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 4.706882\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 4.624212\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 5.575512\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 5.525614\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 5.874433\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 6.574016\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.492870... Val Loss: 6.415915\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 7.296551\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 8.704026\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 9.545032\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 8.229961\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 9.715629\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 10.280703\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 9.524815\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 8.991525\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 8.675389\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 8.485516\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 8.431099\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 9.472214\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 9.470247\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 9.831404\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 10.424930\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 6.302874... Val Loss: 10.226365\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 5.987183\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 6.662316\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 6.968569\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 6.044147\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 6.817768\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 6.479479\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 5.994946\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 5.964242\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 5.736650\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 5.545870\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 5.392412\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 6.176635\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 6.201385\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 6.497370\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 7.221300\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 4.152517... Val Loss: 7.153925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 7.735302\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 6.854627\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 7.072953\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 5.916886\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 6.154076\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 6.078377\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 5.519606\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 5.516479\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 5.377917\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 5.381147\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 5.367366\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 5.700265\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 5.642737\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 5.993186\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 6.484260\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 3.363418... Val Loss: 6.518798\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 8.898293\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 7.601536\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 7.714922\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 6.810887\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 7.349833\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 7.295922\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 6.811602\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 6.646219\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 6.538942\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 6.508992\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 6.544728\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 7.070366\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 6.891388\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 7.283236\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 7.868272\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 3.781001... Val Loss: 7.680433\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 5.662683\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 5.943286\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 5.839406\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 4.838031\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 6.312842\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 6.089754\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 5.424837\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 5.140579\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 4.909476\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 4.818285\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 4.832767\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 5.706832\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 5.597715\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 6.092703\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 6.801979\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.645048... Val Loss: 6.607945\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 7.411777\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 7.219673\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 7.392072\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 6.206484\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 6.729851\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 6.946418\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 6.307834\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 6.107907\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 5.912582\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 5.870241\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 5.741315\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 6.261912\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 6.120467\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 6.585685\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 7.086963\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 4.791863... Val Loss: 6.932742\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 6.679774\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 6.444175\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 6.512910\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 5.609120\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 6.548149\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 6.314261\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 5.773875\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 5.716536\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 5.519438\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 5.360010\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 5.331621\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 6.094700\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 5.948555\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 6.268958\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 6.978842\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 5.437518... Val Loss: 6.800597\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 7.015863\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 6.670476\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 6.401340\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 5.510699\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 6.280083\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 6.074283\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 5.598383\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 5.517376\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 5.343529\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 5.304627\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 5.312241\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 5.964364\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 5.809905\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 6.221330\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 6.909643\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 5.421396... Val Loss: 6.727356\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 8.857382\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 8.489536\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 8.469762\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 7.600976\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 9.157668\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 9.309424\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 8.540714\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 8.103882\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 7.898862\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 7.760870\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 7.668739\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 8.631170\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 8.437872\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 8.931472\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 9.730931\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 6.208261... Val Loss: 9.459304\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 9.729467\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 8.828822\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 8.405681\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 7.535072\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 8.097034\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 8.278082\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 7.861374\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 7.690477\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 7.603648\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 7.716257\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 7.824350\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 8.373207\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 8.167459\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 8.600864\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 9.188378\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 9.500653... Val Loss: 8.971929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 8.222459\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 7.702070\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 8.182317\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 6.873415\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 9.060513\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 9.213473\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 8.196120\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 7.508931\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 7.096603\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 7.152784\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 7.235038\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 8.544240\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 8.418796\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 8.967916\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 9.795013\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 3.500120... Val Loss: 9.698425\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 9.095937\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 8.991514\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 8.436034\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 7.617549\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 9.304577\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 9.476473\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 8.661050\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 8.126014\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 7.961718\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 7.845549\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 7.935044\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 8.883043\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 8.690646\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 9.287011\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 9.969816\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 7.123225... Val Loss: 9.689515\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 5.491376\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 5.867982\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 5.610243\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 4.729430\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 5.823869\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 5.749762\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 5.130549\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 4.889476\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 4.682150\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 4.644721\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 4.622356\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 5.368586\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 5.280811\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 5.761215\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 6.510157\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 3.235256... Val Loss: 6.351363\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 5.250522\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 5.612358\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 5.676734\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 4.687122\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 5.970018\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 6.007408\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 5.342605\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 5.030544\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 4.795832\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 4.670314\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 4.624466\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 5.584146\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 5.536345\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 5.942854\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 6.672838\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.542966... Val Loss: 6.511594\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 8.790051\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 8.809611\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 8.994588\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 7.919297\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 8.335039\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 7.844433\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 7.414757\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 7.495966\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 7.276609\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 7.193689\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 7.087082\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 7.776827\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 7.742259\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 8.053377\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 8.623134\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 4.570467... Val Loss: 8.558699\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 6.688407\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 6.766950\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 6.591597\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 5.613314\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 7.080524\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 6.950436\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 6.204951\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 5.800374\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 5.577751\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 5.428009\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 5.383241\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 6.338217\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 6.216940\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 6.708150\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 7.458295\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 2.378551... Val Loss: 7.237633\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 5.488549\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 5.970182\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 5.725048\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 4.869121\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 6.711055\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 6.269998\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 5.609082\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 5.419153\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 5.135600\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 4.915665\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 4.829240\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 6.001347\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 5.835585\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 6.193741\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 7.204552\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 3.828356... Val Loss: 6.981355\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 11.380647\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 10.817552\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 10.063624\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 9.286132\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 10.812400\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 10.897241\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 10.116617\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 9.603630\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 9.414425\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 9.431237\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 9.560773\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 10.471499\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 10.277834\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 10.942980\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 11.729593\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 8.755430... Val Loss: 11.461364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 7.597291\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 8.491403\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 9.034255\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 7.921287\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 8.549638\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 7.965400\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 7.497963\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 7.418513\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 7.164087\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 7.007860\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 6.811699\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 7.551612\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 7.548053\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 7.931828\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 8.548867\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 6.453062... Val Loss: 8.497868\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 6.083283\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 6.149820\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 6.059214\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 5.194562\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 6.097504\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 5.920858\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 5.352263\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 5.183107\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 4.989174\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 4.890951\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 4.900138\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 5.635308\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 5.529031\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 5.928185\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 6.660160\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 4.035227... Val Loss: 6.494642\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 5.014472\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 5.707275\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 5.609998\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 4.631238\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 5.980162\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 5.630268\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 5.046538\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 4.851696\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 4.620468\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 4.472351\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 4.458380\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 5.439298\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 5.356763\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 5.683535\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 6.398815\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 3.441499... Val Loss: 6.244123\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 12.912685\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 12.007792\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 11.875663\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 10.907817\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 12.509484\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 12.619870\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 11.799162\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 11.260781\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 11.054994\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 10.998083\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 10.949342\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 11.852669\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 11.631058\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 12.190051\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 12.899434\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 6.959978... Val Loss: 12.640221\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 6.569066\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 7.722912\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 8.260292\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 7.170774\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 8.403221\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 8.055993\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 7.514061\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 7.283242\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 7.004382\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 6.871589\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 6.903979\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 7.926040\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 7.929464\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 8.279578\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 8.965229\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 4.010750... Val Loss: 8.842130\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 4.411509\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 5.515831\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 5.794259\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 4.678421\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 6.459271\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 6.133235\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 5.422363\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 5.099590\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 4.772040\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 4.633390\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 4.477571\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 5.471040\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 5.361558\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 5.806388\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 6.505539\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 2.446296... Val Loss: 6.321976\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 5.895868\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 6.097037\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 6.253647\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 5.235760\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 6.625112\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 6.394231\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 5.681628\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 5.319981\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 5.057341\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 4.911183\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 4.809657\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 5.753672\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 5.599021\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 6.023299\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 6.689793\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 5.459250... Val Loss: 6.484215\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 8.790413\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 8.729729\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 8.943284\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 7.913119\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 8.120947\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 7.690401\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 7.331665\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 7.480520\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 7.215465\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 7.348386\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 7.441871\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 7.974742\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 7.934282\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 8.231054\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 8.875482\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 3.542857... Val Loss: 8.905945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 5.112628\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 5.229059\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 5.696143\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 4.744402\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 5.736180\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 5.563574\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 4.988594\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 4.807001\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 4.579459\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 4.465857\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 4.339871\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 5.178207\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 5.117389\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 5.518936\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 6.194961\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 2.693536... Val Loss: 6.058814\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 9.033225\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 8.426145\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 8.381522\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 7.513492\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 8.780631\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 8.717640\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 8.126099\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 7.841692\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 7.711269\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 7.636406\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 7.667501\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 8.496164\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 8.281847\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 8.717465\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 9.364088\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 7.650945... Val Loss: 9.097514\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 6.613708\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 6.491829\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 6.125261\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 5.260040\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 6.279448\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 6.037616\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 5.465605\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 5.258744\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 5.075468\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 5.003443\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 4.990027\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 5.833443\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 5.686348\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 6.093125\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 6.824823\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 5.497146... Val Loss: 6.617373\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 9.818344\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 9.485749\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 9.173889\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 8.461641\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 9.594044\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 9.719832\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 9.109204\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 8.752642\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 8.641529\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 8.555757\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 8.536651\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 9.316162\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 9.127135\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 9.613760\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 10.348147\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 4.189737... Val Loss: 10.075513\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 5.792292\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 5.698056\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 5.892274\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 4.905373\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 5.714611\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 5.693803\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 5.099312\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 4.895940\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 4.686236\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 4.619386\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 4.625155\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 5.360478\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 5.275326\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 5.679436\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 6.253762\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 3.354891... Val Loss: 6.128029\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 5.800936\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 5.875902\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 5.915872\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 5.159841\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 6.246301\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 6.012477\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 5.472118\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 5.355973\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 5.154167\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 4.981327\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 4.945335\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 5.807752\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 5.668348\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 6.016085\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 6.746231\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 2.414741... Val Loss: 6.577978\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 4.994862\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 5.486370\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 5.997104\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 5.072537\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 7.261028\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 6.885119\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 6.124444\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 5.755211\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 5.434782\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 5.256101\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 5.135951\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 6.304546\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 6.113437\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 6.525876\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 7.372555\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 4.240513... Val Loss: 7.139582\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 4.708745\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 6.039774\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 6.457162\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 5.319790\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 6.645235\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 6.469224\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 5.834788\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 5.640638\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 5.314468\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 5.117129\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 4.969223\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 5.834762\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 5.776651\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 6.140318\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 6.824768\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 5.576762... Val Loss: 6.693816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 5.434168\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 5.993136\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 6.323663\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 5.182928\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 6.227516\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 6.128296\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 5.482810\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 5.208621\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 4.942243\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 4.800891\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 4.700244\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 5.576935\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 5.528009\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 5.949997\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 6.633494\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.971556... Val Loss: 6.486020\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 5.683584\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 6.263420\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 6.385256\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 5.400739\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 7.674418\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 7.317583\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 6.481222\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 5.983243\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 5.706276\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 5.518624\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 5.537366\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 6.399925\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 6.298223\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 6.764816\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 7.579830\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 7.200805... Val Loss: 7.385416\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 4.757292\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 5.138997\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 5.284738\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 4.460091\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 5.690821\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 5.695406\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 5.042383\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 4.774277\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 4.549230\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 4.426119\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 4.311805\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 5.107495\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 5.056616\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 5.504610\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 6.341940\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.989068... Val Loss: 6.200107\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 5.022326\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 5.255037\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 5.856490\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 4.989795\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 6.687936\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 6.583597\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 5.884158\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 5.510890\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 5.204009\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 4.982437\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 4.958058\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 6.150241\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 5.965262\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 6.301526\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 7.076114\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 5.058455... Val Loss: 6.897476\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 6.146662\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 6.126686\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 6.179841\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 5.108036\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 6.202509\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 6.258913\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 5.596687\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 5.249687\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 5.036305\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 4.915865\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 4.863394\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 5.703868\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 5.586127\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 6.002325\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 6.575363\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 3.409036... Val Loss: 6.377932\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 10.760843\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 10.104266\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 10.031677\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 9.209275\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 10.616477\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 10.865866\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 10.135752\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 9.664608\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 9.481780\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 9.324183\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 9.324804\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 10.206127\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 9.969473\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 10.456203\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 11.127845\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 4.790401... Val Loss: 10.795405\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 5.692317\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 6.572906\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 6.636865\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 5.651747\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 6.754521\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 6.351309\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 5.759233\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 5.557963\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 5.336285\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 5.162657\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 5.016284\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 5.972946\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 5.946327\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 6.335446\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 7.154185\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.246529... Val Loss: 6.992339\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 5.553721\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 5.842810\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 5.897485\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 4.791736\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 5.690670\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 5.762927\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 5.127407\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 4.865787\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 4.639321\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 4.568214\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 4.500545\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 5.117770\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 5.030372\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 5.468057\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 6.038060\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 2.600226... Val Loss: 5.908805\n",
      "Validation loss decreased (6.033921 --> 5.908805).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 6.960152\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 7.546156\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 8.057009\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 6.710049\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 7.130538\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 6.885722\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 6.309746\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 6.121549\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 5.895046\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 5.771608\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 5.635968\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 6.214103\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 6.189894\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 6.547059\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 7.052726\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 7.645486... Val Loss: 6.998812\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 6.131406\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 5.842140\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 5.898517\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 5.041763\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 5.868506\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 5.730271\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 5.297061\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 5.219255\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 5.034364\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 5.000747\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 4.995844\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 5.702532\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 5.552058\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 5.931635\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 6.552259\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 5.096014... Val Loss: 6.383619\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 5.983027\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 6.384918\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 6.146484\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 5.153790\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 6.793640\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 6.751642\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 5.972745\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 5.502777\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 5.232471\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 5.189662\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 5.169108\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 6.114355\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 6.007006\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 6.503232\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 7.229721\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 3.866874... Val Loss: 7.032732\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 10.272996\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 9.785170\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 9.409284\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 8.629369\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 10.233232\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 10.411572\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 9.631704\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 9.117614\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 8.921059\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 8.821244\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 8.798951\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 9.750650\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 9.550068\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 10.073624\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 10.841540\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 13.154655... Val Loss: 10.562541\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 5.334066\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 5.540965\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 5.650995\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 4.770735\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 6.020771\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 6.082772\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 5.422126\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 5.077191\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 4.850996\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 4.673707\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 4.653281\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 5.477086\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 5.354281\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 5.769825\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 6.434015\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 3.203236... Val Loss: 6.242191\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 25.686441\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 24.689138\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 23.386259\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 22.816638\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 23.920469\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 24.330179\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 23.798984\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 23.239354\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 23.374544\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 23.358954\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 23.561131\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 24.246648\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 23.883724\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 24.410676\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 24.897579\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 23.098457... Val Loss: 24.388105\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 6.034666\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 6.578985\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 6.309790\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 5.618833\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 7.313433\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 7.256056\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 6.556575\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 6.163026\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 5.956961\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 5.862009\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 5.842466\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 6.778252\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 6.646638\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 7.209775\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 8.004290\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 4.898480... Val Loss: 7.756971\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 5.351105\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 5.624582\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 5.786208\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 4.804355\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 6.779368\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 6.466147\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 5.718941\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 5.313205\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 5.034572\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 4.860556\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 4.798478\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 6.012077\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 5.854605\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 6.229309\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 7.034692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1550/2000... Step: 49600... Loss: 3.760720... Val Loss: 6.883278\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 5.393198\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 6.167242\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 6.450012\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 5.342721\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 6.228806\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 5.944365\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 5.333044\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 5.077562\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 4.853594\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 4.721558\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 4.589960\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 5.417205\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 5.408676\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 5.854844\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 6.567561\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.587266... Val Loss: 6.433974\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 5.390422\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 6.097582\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 6.313759\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 5.214037\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 6.259271\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 6.043625\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 5.441679\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 5.180538\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 4.975332\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 4.914561\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 4.900065\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 5.720992\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 5.731351\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 6.168507\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 6.793527\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 1.687255... Val Loss: 6.659975\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 5.993389\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 6.257226\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 6.016260\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 5.164709\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 6.459264\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 6.302408\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 5.693069\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 5.432916\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 5.239846\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 5.185878\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 5.076605\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 5.990254\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 5.851258\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 6.272853\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 7.053115\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 5.068550... Val Loss: 6.839086\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 13.503392\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 12.438189\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 12.412566\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 11.315294\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 11.836389\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 12.007698\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 11.419769\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 11.086083\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 11.018071\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 11.058859\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 11.036144\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 11.543318\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 11.327939\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 11.837714\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 12.297173\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 9.585468... Val Loss: 12.035639\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 6.242762\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 6.107406\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 6.237477\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 5.204437\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 6.134323\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 6.138875\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 5.552971\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 5.325310\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 5.109544\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 4.952521\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 4.854188\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 5.666736\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 5.516196\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 5.884874\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 6.449121\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 3.289270... Val Loss: 6.266044\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 5.342829\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 5.983663\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 5.910060\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 4.855138\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 5.401169\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 5.250790\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 4.764967\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 4.793873\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 4.547714\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 4.589857\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 4.540634\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 5.159560\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 5.098691\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 5.437763\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 6.132276\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 2.731573... Val Loss: 6.078047\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 4.831236\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 5.631871\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 5.873614\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 4.988693\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 6.491054\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 6.619042\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 5.943614\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 5.595121\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 5.311221\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 5.139728\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 5.125510\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 6.119127\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 6.047799\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 6.421634\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 7.245603\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 2.539704... Val Loss: 7.080854\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 6.174039\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 6.435957\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 6.209630\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 5.239048\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 6.947636\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 6.984609\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 6.203508\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 5.712350\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 5.462815\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 5.350542\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 5.300651\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 6.250493\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 6.126136\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 6.598981\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 7.335172\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 7.328542... Val Loss: 7.161725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 5.378686\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 5.471996\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 5.572435\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 4.589306\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 5.564187\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 5.332984\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 4.799660\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 4.615597\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 4.417205\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 4.365535\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 4.378337\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 5.094820\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 5.002416\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 5.414522\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 6.015811\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.204118... Val Loss: 5.881189\n",
      "Validation loss decreased (5.908805 --> 5.881189).  Saving model ...\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 6.253470\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 6.626844\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 6.609367\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 5.594811\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 5.958153\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 5.615009\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 5.162649\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 5.188538\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 4.999591\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 4.985390\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 4.930149\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 5.532622\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 5.427429\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 5.752132\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 6.314393\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 3.196805... Val Loss: 6.280280\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 5.559715\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 5.691475\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 6.225431\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 5.121853\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 6.118951\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 6.144030\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 5.428336\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 5.085518\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 4.842417\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 4.784397\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 4.649523\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 5.446149\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 5.382980\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 5.886297\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 6.518328\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 4.031878... Val Loss: 6.347234\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 7.199235\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 7.320479\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 6.979053\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 6.037164\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 7.459480\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 7.411481\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 6.734915\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 6.392519\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 6.175901\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 6.127084\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 6.095404\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 6.845236\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 6.677486\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 7.173195\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 7.815504\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.759525... Val Loss: 7.574529\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 7.749023\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 7.378328\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 7.341467\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 6.349826\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 6.936308\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 7.111883\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 6.627575\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 6.514233\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 6.351849\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 6.294240\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 6.200157\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 6.924875\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 6.731330\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 7.086028\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 7.655976\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 6.102657... Val Loss: 7.441921\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 5.266391\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 6.150074\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 6.375607\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 5.381339\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 6.428515\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 6.034448\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 5.521781\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 5.435702\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 5.230292\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 5.074924\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 5.020614\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 5.779560\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 5.771941\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 6.100072\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 6.707616\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.660104... Val Loss: 6.636968\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 4.523676\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 5.331089\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 5.253021\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 4.309853\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 6.023160\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 5.673827\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 5.011063\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 4.685839\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 4.441322\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 4.289494\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 4.181566\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 5.005706\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 4.949638\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 5.361227\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 6.138181\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 2.795181... Val Loss: 6.047888\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 7.717723\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 6.810604\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 7.426655\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 6.339953\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 6.756519\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 6.834757\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 6.323346\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 6.107633\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 5.952554\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 5.861325\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 5.758923\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 6.314795\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 6.168912\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 6.611104\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 7.150346\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 5.077972... Val Loss: 7.011761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 4.788278\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 5.644228\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 5.390444\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 4.308241\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 5.507557\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 5.351228\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 4.727997\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 4.468711\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 4.222239\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 4.209135\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 4.187289\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 5.028449\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 4.963944\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 5.366272\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 6.015700\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.986523... Val Loss: 5.886551\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 11.204941\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 9.656245\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 9.798210\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 8.998893\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 9.359572\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 9.277609\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 8.981468\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 8.966532\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 8.939264\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 8.996634\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 9.054074\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 9.685920\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 9.432294\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 9.766282\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 10.259166\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 6.477355... Val Loss: 10.006880\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 4.548968\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 5.477447\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 5.452653\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 4.447380\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 6.210009\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 5.907971\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 5.250667\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 4.955301\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 4.690319\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 4.565371\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 4.581233\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 5.554879\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 5.454462\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 5.858738\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 6.600219\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.034104... Val Loss: 6.438421\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 9.725757\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 9.270790\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 8.905002\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 8.105574\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 9.421573\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 9.490058\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 8.817456\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 8.418101\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 8.262358\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 8.105580\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 8.114693\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 8.953765\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 8.741238\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 9.235723\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 9.915422\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 5.819645... Val Loss: 9.622369\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 5.527454\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 5.677429\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 5.596714\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 4.642778\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 5.634590\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 5.658811\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 5.069192\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 4.815129\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 4.608229\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 4.502468\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 4.456896\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 5.347671\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 5.227658\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 5.584013\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 6.236124\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 3.731388... Val Loss: 6.053546\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 8.790962\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 7.448762\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 7.386559\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 6.347728\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 6.502102\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 6.504033\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 6.064986\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 6.071880\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 5.962219\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 6.174680\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 6.419436\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 6.753937\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 6.572182\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 6.926491\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 7.397416\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 1.996047... Val Loss: 7.439032\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 6.198541\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 6.824514\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 6.300892\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 5.510807\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 7.150056\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 7.083653\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 6.392652\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 6.011713\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 5.808193\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 5.683146\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 5.730047\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 6.722168\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 6.530516\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 6.947106\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 7.697723\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 6.223860... Val Loss: 7.482910\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 5.664287\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 6.030822\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 5.767767\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 4.867409\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 5.965791\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 5.893016\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 5.353729\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 5.143987\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 4.959885\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 4.860136\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 4.897769\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 5.668890\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 5.516609\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 5.866706\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 6.513063\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 2.562441... Val Loss: 6.334900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 6.566339\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 6.397312\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 6.291323\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 5.558483\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 6.926976\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 6.750629\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 6.202267\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 6.016414\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 5.822438\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 5.672645\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 5.595513\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 6.618259\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 6.419294\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 6.760314\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 7.578379\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.540291... Val Loss: 7.318669\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 6.473701\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 6.450716\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 6.059014\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 5.034364\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 6.528918\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 6.131725\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 5.452481\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 5.268690\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 5.088773\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 4.938843\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 4.869858\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 5.660313\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 5.518112\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 5.910489\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 6.582587\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 3.036286... Val Loss: 6.399259\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 5.157631\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 5.681505\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 5.595441\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 4.680650\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 5.899691\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 5.846402\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 5.241651\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 5.008326\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 4.798734\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 4.708429\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 4.752468\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 5.360927\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 5.243361\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 5.645073\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 6.222473\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 4.134101... Val Loss: 6.095438\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 5.535975\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 5.891779\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 6.220164\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 5.203170\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 5.689333\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 5.577055\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 5.091194\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 5.077634\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 4.859273\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 4.729569\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 4.583053\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 5.260436\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 5.163208\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 5.481745\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 6.119122\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 3.634927... Val Loss: 6.033864\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 5.261114\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 5.866754\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 5.819603\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 4.709040\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 5.645311\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 5.557005\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 4.935264\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 4.636751\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 4.400564\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 4.335749\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 4.257783\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 4.993396\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 4.880869\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 5.293203\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 5.817660\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 1.791384... Val Loss: 5.678400\n",
      "Validation loss decreased (5.881189 --> 5.678400).  Saving model ...\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 7.513427\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 7.387771\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 7.306962\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 6.283583\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 6.932645\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 6.726598\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 6.264747\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 6.113624\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 5.970456\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 5.934781\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 5.881445\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 6.572924\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 6.382734\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 6.731241\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 7.254733\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 2.279704... Val Loss: 7.060151\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 4.764490\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 5.553059\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 5.630963\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 4.679254\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 5.669682\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 5.585035\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 4.953795\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 4.675129\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 4.457010\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 4.325209\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 4.234525\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 4.902063\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 4.808730\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 5.240435\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 5.927607\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 3.208788... Val Loss: 5.788988\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 5.325197\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 5.599034\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 5.423259\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 4.506042\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 5.764849\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 5.585021\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 5.100099\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 5.022449\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 4.818109\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 4.763024\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 4.800277\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 5.692965\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 5.532344\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 5.833620\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 6.474779\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 4.019673... Val Loss: 6.276545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 5.167668\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 6.094158\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 6.291968\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 5.181555\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 6.097159\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 5.946914\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 5.404419\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 5.254486\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 4.992637\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 4.875346\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 4.812151\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 5.633241\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 5.560150\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 5.896413\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 6.456831\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 4.285389... Val Loss: 6.344485\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 6.276819\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 6.264615\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 6.797696\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 5.587351\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 6.481939\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 6.335400\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 5.743509\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 5.600586\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 5.387909\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 5.300431\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 5.328653\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 5.854370\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 5.800119\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 6.153320\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 6.656146\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 3.938277... Val Loss: 6.609051\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 8.908134\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 9.771276\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 10.185268\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 9.068981\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 9.724832\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 9.208504\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 8.833734\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 9.001448\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 8.741179\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 8.688082\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 8.626247\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 9.419478\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 9.469374\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 9.715973\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 10.301634\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 4.899011... Val Loss: 10.291680\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 7.579596\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 6.944595\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 6.707395\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 5.961936\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 6.904299\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 6.660498\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 6.069994\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 5.927481\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 5.844481\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 5.694047\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 5.563440\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 6.192728\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 5.990831\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 6.363540\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 6.953524\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 1.891193... Val Loss: 6.762661\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 6.704280\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 7.792608\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 7.994877\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 6.833036\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 7.441433\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 6.971547\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 6.538988\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 6.594510\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 6.358303\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 6.289889\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 6.262127\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 6.933672\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 6.919651\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 7.211418\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 7.739820\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 5.628726... Val Loss: 7.723788\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 8.276864\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 7.788257\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 7.751778\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 6.886208\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 7.012986\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 6.767212\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 6.513827\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 6.879596\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 6.694887\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 6.823179\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 7.016302\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 7.537829\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 7.369940\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 7.609531\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 8.305319\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 4.363428... Val Loss: 8.278242\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 5.604235\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 6.255161\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 6.697367\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 5.474634\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 7.010900\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 6.629243\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 5.880314\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 5.525372\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 5.241145\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 5.043196\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 4.900687\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 5.674212\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 5.531601\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 6.002550\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 6.554324\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 5.358031... Val Loss: 6.337993\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 4.987153\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 6.110636\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 6.211784\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 5.210169\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 6.244016\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 5.814415\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 5.299856\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 5.263515\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 4.971761\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 4.778729\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 4.669070\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 5.631931\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 5.528233\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 5.836087\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 6.512486\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.431152... Val Loss: 6.389003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 6.523568\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 6.611225\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 6.592086\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 5.625165\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 7.209439\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 7.016798\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 6.213835\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 5.723641\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 5.446685\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 5.376679\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 5.323800\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 6.104424\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 5.972343\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 6.465679\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 7.136991\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.908753... Val Loss: 6.936308\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 11.000271\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 10.163004\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 9.968932\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 9.202235\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 9.855192\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 9.890194\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 9.551520\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 9.414259\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 9.383192\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 9.423445\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 9.577731\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 10.182108\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 9.926281\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 10.268196\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 10.903135\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 7.367298... Val Loss: 10.610443\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 4.739451\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 5.484793\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 5.752644\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 4.712548\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 6.077984\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 5.917164\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 5.250100\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 4.902110\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 4.640256\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 4.485331\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 4.456279\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 5.464853\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 5.353958\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 5.740052\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 6.405799\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 2.895982... Val Loss: 6.255641\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 5.432991\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 6.183620\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 5.622077\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 5.015287\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 6.628868\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 6.532013\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 5.918437\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 5.689335\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 5.490106\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 5.389775\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 5.420096\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 6.245970\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 6.052687\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 6.403500\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 7.173537\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 5.045235... Val Loss: 6.952723\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 4.771025\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 5.723974\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 5.784425\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 4.703413\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 5.870042\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 5.563307\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 4.944998\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 4.656450\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 4.417971\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 4.353251\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 4.251687\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 5.175844\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 5.103029\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 5.528635\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 6.213998\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.528403... Val Loss: 6.062856\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 7.238506\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 7.392775\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 7.001768\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 6.203708\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 7.322119\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 7.320751\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 6.693788\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 6.363813\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 6.223173\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 6.092093\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 6.125158\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 6.801109\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 6.642081\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 7.103531\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 7.738298\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 2.534425... Val Loss: 7.510275\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 5.775611\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 6.929204\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 7.317361\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 6.144459\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 7.639990\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 7.119790\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 6.532058\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 6.317271\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 6.058390\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 5.846280\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 5.669730\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 6.573401\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 6.568467\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 6.909572\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 7.555813\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 5.904026... Val Loss: 7.479031\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 4.723095\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 5.784396\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 5.988372\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 4.806830\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 5.909418\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 5.715840\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 5.085534\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 4.821248\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 4.538311\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 4.428708\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 4.328250\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 5.222959\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 5.135214\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 5.499049\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 6.115884\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 5.348639... Val Loss: 5.990240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 11.102675\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 12.796229\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 13.309634\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 12.235543\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 12.552217\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 11.821751\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 11.490803\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 11.875232\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 11.553527\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 11.446108\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 11.295264\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 12.081560\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 12.227691\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 12.455496\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 13.150929\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 6.854161... Val Loss: 13.235841\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 7.718130\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 7.219254\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 7.396697\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 6.294401\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 6.306638\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 6.131698\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 5.811860\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 6.026922\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 5.845031\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 6.123525\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 6.351911\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 6.714468\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 6.640773\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 6.941916\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 7.486993\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 3.954754... Val Loss: 7.591773\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 4.617507\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 5.325671\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 5.388210\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 4.431285\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 5.680208\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 5.383694\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 4.793054\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 4.617159\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 4.386748\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 4.286489\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 4.217989\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 4.844574\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 4.751721\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 5.147410\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 5.796402\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 3.213582... Val Loss: 5.666834\n",
      "Validation loss decreased (5.678400 --> 5.666834).  Saving model ...\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 5.273934\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 6.061408\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 5.885485\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 4.948378\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 5.648617\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 5.324229\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 4.886134\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 4.902612\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 4.725140\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 4.755495\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 4.812340\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 5.385307\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 5.324904\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 5.655301\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 6.178459\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 0.941389... Val Loss: 6.145332\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 5.068296\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 5.635429\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 5.379005\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 4.582127\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 6.563696\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 6.485354\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 5.759060\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 5.451502\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 5.169479\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 5.007879\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 5.023703\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 5.940106\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 5.769940\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 6.157908\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 6.955368\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 3.400517... Val Loss: 6.736740\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 7.931105\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 7.938920\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 7.463129\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 6.739901\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 7.956516\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 8.075060\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 7.415142\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 7.019695\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 6.885439\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 6.830799\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 6.853714\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 7.500363\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 7.344035\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 7.825882\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 8.366084\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 4.310217... Val Loss: 8.153695\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 4.450431\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 5.346708\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 5.653466\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 4.672126\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 6.674606\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 6.182154\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 5.513306\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 5.357710\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 5.015304\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 4.841581\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 4.610672\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 5.465120\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 5.335202\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 5.676831\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 6.614397\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 3.952759... Val Loss: 6.506623\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 5.341277\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 5.752728\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 5.492752\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 4.610711\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 5.967645\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 5.632734\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 5.117309\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 5.074806\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 4.855614\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 4.733382\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 4.699697\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 5.480685\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 5.328263\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 5.637268\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 6.250610\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.162393... Val Loss: 6.108348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 5.156871\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 5.733949\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 5.729310\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 4.656637\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 5.516213\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 5.315805\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 4.746913\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 4.573996\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 4.365469\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 4.297092\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 4.201524\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 4.976639\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 4.864158\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 5.232000\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 5.870893\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 2.010924... Val Loss: 5.730218\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 5.061923\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 5.594668\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 5.806864\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 4.950084\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 7.118090\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 6.802102\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 6.012013\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 5.533665\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 5.294919\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 5.158629\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 5.217582\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 6.239366\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 6.143600\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 6.593953\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 7.433186\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 7.722404... Val Loss: 7.301069\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 5.141406\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 5.783106\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 5.868995\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 4.806866\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 6.071500\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 5.685255\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 5.182319\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 5.013171\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 4.741101\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 4.686560\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 4.674927\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 5.605577\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 5.489464\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 5.830188\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 6.440818\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 3.443673... Val Loss: 6.279424\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 6.926272\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 7.016382\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 7.008094\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 6.057830\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 7.091702\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 7.243511\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 6.582393\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 6.193103\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 6.000276\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 5.974132\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 5.930277\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 6.741780\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 6.573678\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 7.061188\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 7.638016\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 7.037562... Val Loss: 7.429170\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 7.525057\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 9.347975\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 9.685831\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 8.478908\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 9.083395\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 8.794468\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 8.330929\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 8.333526\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 8.014555\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 7.931821\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 7.810617\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 8.571063\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 8.600267\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 8.890694\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 9.471817\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 5.719864... Val Loss: 9.469959\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 11.799188\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 11.015161\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 10.605947\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 9.775721\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 10.458122\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 10.600342\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 10.173409\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 9.955288\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 9.916012\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 9.945951\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 10.223964\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 10.632087\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 10.353501\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 10.750606\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 11.192285\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 6.065315... Val Loss: 10.950605\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 10.535192\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 10.123410\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 9.849930\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 9.186268\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 10.927780\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 11.065478\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 10.228258\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 9.649510\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 9.455828\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 9.264741\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 9.271841\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 10.178016\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 9.941040\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 10.474604\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 11.256648\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 10.610240... Val Loss: 10.963760\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 5.249677\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 6.569860\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 7.013097\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 5.898633\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 6.897664\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 6.547098\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 6.012263\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 5.949584\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 5.640646\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 5.450871\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 5.334596\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 6.171846\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 6.128941\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 6.428315\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 6.999035\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 3.595295... Val Loss: 6.936637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 5.637792\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 5.817828\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 5.744976\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 4.686036\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 5.808572\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 5.589281\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 4.988222\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 4.740626\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 4.509959\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 4.446635\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 4.371997\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 5.205474\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 5.078484\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 5.513793\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 6.115654\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 2.254097... Val Loss: 5.939175\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 6.228133\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 5.847427\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 5.958049\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 5.143412\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 5.726877\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 5.714775\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 5.296382\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 5.205476\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 5.049281\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 5.048336\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 5.086458\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 5.750692\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 5.603009\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 5.975276\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 6.587074\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 3.918258... Val Loss: 6.423110\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 5.835712\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 6.140007\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 5.795843\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 5.068311\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 6.283956\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 6.234743\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 5.603737\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 5.271992\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 5.118689\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 5.050260\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 5.110441\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 5.926136\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 5.805882\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 6.246349\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 6.903242\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 3.862816... Val Loss: 6.699863\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 5.801731\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 5.848788\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 6.103000\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 5.120224\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 6.464543\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 6.241822\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 5.599604\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 5.311952\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 5.054360\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 4.904248\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 4.801637\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 5.830489\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 5.653686\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 6.029790\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 6.714991\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 7.901520... Val Loss: 6.531250\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 5.360643\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 5.629826\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 6.427606\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 5.371397\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 5.946531\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 6.029416\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 5.454137\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 5.221916\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 5.039661\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 4.896918\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 4.885495\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 5.627544\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 5.499388\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 5.848785\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 6.384920\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 3.565571... Val Loss: 6.225835\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 8.974597\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 7.818346\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 7.937554\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 7.088137\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 7.707212\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 7.397835\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 6.975706\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 7.253971\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 7.083674\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 7.050010\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 7.148698\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 7.562232\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 7.298755\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 7.584139\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 8.146903\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 3.581536... Val Loss: 8.050001\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 4.107931\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 5.136322\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 5.272538\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 4.286336\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 5.600064\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 5.343308\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 4.779095\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 4.641793\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 4.410877\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 4.337256\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 4.354871\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 5.123301\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 5.122378\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 5.441099\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 6.072488\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.908339... Val Loss: 5.997793\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 4.326989\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 5.211063\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 5.345319\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 4.390889\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 6.137745\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 5.847224\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 5.203494\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 4.938023\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 4.661955\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 4.460808\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 4.397021\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 5.275860\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 5.206161\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 5.559689\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 6.271439\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 3.138766... Val Loss: 6.177089\n"
     ]
    }
   ],
   "source": [
    "from utils import prep_data\n",
    "\n",
    "\n",
    "\n",
    "train_data_loader = prep_data(train_data, train_targets, batch_size = 32,first_n_epochs = 10 )\n",
    "test_data_loader = prep_data(test_data, test_targets, batch_size = 32,first_n_epochs = 10 )\n",
    "val_data_loader = prep_data(val_data, val_targets, batch_size = 32, first_n_epochs=10)\n",
    "\n",
    "\n",
    "model, optimizer, criterion = create_model(50, 3, input_size = 10)\n",
    "train_validate_model(model, optimizer, criterion)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([83.5056, 81.3736, 80.0233, 81.2056, 83.1180, 81.3025, 84.1388, 80.9148,\n",
      "        84.1646, 70.7779, 59.4650, 70.4419, 81.4640, 80.5401, 82.1489, 83.8610,\n",
      "        81.8969, 59.4327, 79.8294, 83.2860, 83.6607, 78.5890, 71.3012, 75.5847,\n",
      "        51.6346, 73.3687, 78.2272, 81.9421, 81.5157, 82.1166, 76.8510, 81.7095])\n",
      "tensor([82.7910, 81.6401, 80.0444, 82.3157, 83.9442, 81.3170, 84.2825, 80.9289,\n",
      "        82.9997, 72.6544, 56.7210, 69.6633, 81.9440, 78.5815, 81.5258, 82.3043,\n",
      "        82.1150, 66.5303, 80.4695, 83.8613, 83.0810, 80.2495, 69.2251, 76.0652,\n",
      "        60.7655, 74.2313, 80.1827, 82.6443, 81.2971, 82.9368, 79.8167, 82.6443])\n",
      "tensor([82.3298, 83.9643, 59.0322, 83.9191, 84.3455, 84.1840, 80.6177, 84.4812,\n",
      "        80.9665, 82.7756, 84.0871, 82.8983, 83.1890, 78.0204, 88.0346, 84.0806,\n",
      "        83.8351, 82.6916, 89.1717, 80.6952, 68.0062, 77.3679, 72.8195, 84.2938,\n",
      "        83.0727, 84.6233, 74.5768, 83.2472, 84.1194, 82.5688, 82.0390, 83.3247])\n",
      "tensor([82.1462, 84.5968, 55.5063, 82.5367, 84.2608, 83.7371, 81.2397, 84.6765,\n",
      "        82.0821, 82.7462, 84.2595, 83.5265, 83.8150, 80.2784, 86.8550, 82.0518,\n",
      "        83.1733, 82.3638, 87.5060, 80.7686, 67.9813, 79.7252, 77.4191, 81.1688,\n",
      "        83.2302, 85.1310, 73.4638, 83.8651, 84.5015, 80.2269, 81.2684, 83.9324])\n",
      "tensor([83.7124, 84.2680, 84.9787, 79.2415, 57.7465, 84.3197, 83.4022, 84.5393,\n",
      "        76.3212, 60.0078, 84.8947, 73.8726, 57.7852, 76.7606, 89.1782, 75.0549,\n",
      "        83.3699, 84.6169, 83.9385, 84.5329, 73.2265, 81.4511, 78.6794, 85.0627,\n",
      "        85.2436, 84.7913, 82.9888, 84.2680, 78.8474, 81.4317, 81.2379, 83.9837])\n",
      "tensor([84.0603, 84.3152, 84.5722, 79.0043, 52.1407, 84.7978, 83.7529, 83.5082,\n",
      "        77.5274, 57.2387, 81.0021, 74.6115, 60.3617, 77.0650, 87.8312, 74.9315,\n",
      "        78.8315, 85.0083, 84.3411, 85.1063, 69.9113, 80.5084, 78.4216, 85.3318,\n",
      "        84.3960, 85.1823, 83.1731, 84.4765, 80.9979, 81.7841, 82.6445, 84.0893])\n",
      "tensor([60.8218, 45.2901, 83.2536, 57.2490, 79.4870, 79.8747, 84.3391, 58.6316,\n",
      "        72.1669, 83.6671, 83.5379, 83.4346, 79.7842, 79.2092, 75.8108, 24.5962,\n",
      "        76.5603, 77.1224, 85.4116, 83.9256, 81.3090, 82.0843, 79.2609, 79.6162,\n",
      "        76.8316, 82.3298, 78.7763, 82.8918, 62.3853, 83.4798, 84.2874, 76.7541])\n",
      "tensor([54.3799, 45.6828, 83.9985, 58.0245, 79.4662, 81.4830, 84.3010, 67.2746,\n",
      "        72.6083, 83.3769, 83.2165, 83.7454, 80.4963, 78.9388, 76.1985, 26.2411,\n",
      "        78.8639, 77.1997, 84.5167, 84.5505, 82.4780, 81.9779, 77.8271, 78.3594,\n",
      "        73.3777, 83.2630, 79.5445, 83.7987, 68.5644, 83.7374, 84.0573, 80.0067])\n",
      "tensor([77.1288, 77.8524, 81.8840, 82.9565, 82.8466, 71.7987, 83.2536, 63.3674,\n",
      "        83.1309, 81.6126, 71.1138, 83.6607, 79.9910, 60.4406, 83.5508, 83.6413,\n",
      "        63.4643, 81.1281, 30.0943, 77.8137, 85.0239, 67.1469, 85.0110, 81.7871,\n",
      "        73.0004, 84.7655, 81.3800, 80.7469, 82.4913, 82.6463, 82.0067, 77.3291])\n",
      "tensor([79.4810, 79.0792, 81.8073, 83.4467, 82.9391, 68.4980, 83.6874, 59.0788,\n",
      "        82.6484, 82.1719, 71.4813, 83.4444, 81.0351, 54.3257, 84.1686, 83.4325,\n",
      "        57.5885, 81.8310, 36.8769, 79.7731, 83.9006, 63.6562, 85.2614, 81.8655,\n",
      "        73.1876, 83.6819, 82.0425, 81.6140, 82.8614, 83.1163, 82.7830, 78.5750])\n",
      "tensor([74.9128, 74.6737, 83.2278, 81.3800, 85.7475, 83.6090, 78.8280, 81.6708,\n",
      "        82.4331, 81.1668, 80.4884, 83.5379, 69.1175, 87.7568, 83.5250, 82.0067,\n",
      "        83.9256, 83.4152, 85.5925, 84.1969, 57.7077, 68.0256, 83.6994, 76.3600,\n",
      "        84.2357, 54.1155, 81.5997, 76.6830, 79.6033, 87.7116, 83.2278, 86.1352])\n",
      "tensor([76.2800, 74.3423, 82.7450, 82.1585, 85.3510, 83.3245, 81.0695, 81.7144,\n",
      "        83.1451, 81.2549, 81.9736, 83.5972, 65.4260, 86.6476, 84.2829, 83.1627,\n",
      "        83.5355, 81.0525, 83.8495, 83.9986, 56.2678, 69.7365, 84.2749, 77.0941,\n",
      "        82.5236, 58.8261, 82.6709, 77.3169, 81.4184, 86.0904, 83.2065, 86.1065])\n",
      "tensor([82.3556, 55.3883, 75.8948, 78.8086, 76.2631, 78.2788, 80.8309, 60.7507,\n",
      "        82.8079, 73.5754, 80.5530, 77.9041, 70.6164, 81.3154, 59.1872, 78.5308,\n",
      "        66.0421, 82.1618, 79.2350, 83.9514, 82.9694, 75.7591, 84.2421, 70.4484,\n",
      "        79.7519, 81.2120, 81.2185, 73.2976, 83.7124, 74.2538, 79.9199, 83.3053])\n",
      "tensor([83.2164, 56.7895, 76.5651, 79.8946, 79.6328, 78.1258, 80.0693, 56.6311,\n",
      "        80.7844, 73.5770, 80.5087, 77.8468, 69.7704, 82.4125, 58.3243, 78.2594,\n",
      "        65.4788, 82.7418, 79.8885, 84.2564, 83.7799, 76.5404, 84.7386, 73.8506,\n",
      "        79.8679, 81.3404, 81.9876, 73.8543, 83.8918, 71.7253, 79.2522, 82.8130])\n",
      "tensor([70.2416, 84.2486, 60.2274, 83.2407, 83.5056, 83.9450, 79.2092, 82.9758,\n",
      "        83.7641, 84.1646, 86.1481, 58.2569, 83.1890, 82.7432, 88.9521, 79.3255,\n",
      "        76.0822, 75.2229, 79.9199, 83.1438, 69.6279, 81.8775, 77.6909, 80.1654,\n",
      "        83.8287, 84.1065, 80.3592, 82.4848, 70.8425, 82.8595, 79.9070, 70.9717])\n",
      "tensor([70.4621, 84.7487, 58.8822, 83.9989, 84.0497, 83.6282, 77.9920, 82.6111,\n",
      "        83.8020, 84.7653, 86.1540, 58.4879, 83.9978, 82.7082, 86.4782, 81.2446,\n",
      "        77.8050, 77.6045, 79.9395, 82.9474, 72.3144, 82.0609, 78.1514, 80.8633,\n",
      "        83.2645, 83.7410, 80.4843, 82.8368, 72.4366, 83.2296, 81.1576, 67.2950])\n",
      "tensor([82.3039, 77.3485, 83.3312, 83.2020, 76.7283, 84.1969, 72.4835, 82.9306,\n",
      "        83.2795, 84.1775, 79.4612, 85.6829, 74.3959, 77.6715, 79.4353, 69.8217,\n",
      "        70.9329, 84.0742, 84.6104, 82.6528, 71.6372, 83.0469, 83.5702, 78.9508,\n",
      "        75.9917, 85.4439, 79.5322, 83.1503, 83.6155, 81.5609, 80.9924, 83.8028])\n",
      "tensor([83.2430, 77.8294, 83.0608, 83.7445, 79.7090, 84.4909, 70.6071, 83.8347,\n",
      "        82.8659, 84.7400, 79.9413, 85.0499, 74.0453, 78.8662, 79.3987, 75.5304,\n",
      "        74.9968, 83.8180, 81.9867, 83.7243, 66.9531, 83.7791, 83.9987, 78.6039,\n",
      "        76.9745, 83.7078, 78.8711, 82.7433, 84.1083, 82.0497, 81.9629, 83.9495])\n",
      "tensor([84.2745, 83.1761, 83.0275, 58.9094, 81.3477, 68.8203, 82.2135, 80.2429,\n",
      "        84.2228, 84.4618, 83.4152, 61.1255, 75.9400, 76.2049, 80.7533, 79.5775,\n",
      "        67.2438, 82.9500, 80.7921, 77.0384, 78.3241, 83.9062, 77.5552, 67.3278,\n",
      "        60.7249, 81.0053, 83.2536, 79.5258, 82.9435, 84.1323, 81.6061, 84.5329])\n",
      "tensor([84.7358, 80.7636, 83.6084, 52.9175, 81.5249, 66.6192, 82.3238, 80.1843,\n",
      "        84.0574, 84.5213, 83.0771, 59.0709, 76.6195, 71.8671, 82.2909, 79.8542,\n",
      "        64.0202, 82.4233, 82.1935, 78.0576, 80.4902, 77.7337, 80.1410, 68.1884,\n",
      "        58.5084, 81.2490, 82.6362, 77.2354, 83.6472, 84.4025, 79.7297, 84.6396])\n",
      "tensor([83.5185, 76.2825, 81.9098, 83.3699, 80.2429, 80.9472, 86.9944, 83.6413,\n",
      "        81.7418, 81.2831, 80.9665, 76.1533, 76.8898, 81.1474, 82.1424, 80.4820,\n",
      "        82.8725, 83.3829, 84.9464, 80.2623, 79.6938, 81.9873, 72.0119, 83.8545,\n",
      "        73.2394, 84.1711, 84.2615, 84.1775, 73.3234, 77.6974, 66.0163, 59.4457])\n",
      "tensor([83.4034, 77.3950, 82.9374, 81.5905, 81.6190, 81.4657, 86.3046, 83.9473,\n",
      "        81.8445, 82.5745, 81.7421, 76.8219, 77.9685, 81.0446, 83.3866, 79.8064,\n",
      "        82.1468, 83.0681, 85.3074, 81.8527, 81.4110, 82.0992, 73.4065, 83.5418,\n",
      "        73.6094, 84.7116, 84.4610, 82.0483, 68.8740, 77.4331, 64.0689, 58.9520])\n",
      "tensor([78.7763, 83.2084, 84.6427, 75.6428, 81.5480, 76.3342, 84.2098, 72.6580,\n",
      "        70.6487, 82.1166, 83.0340, 84.6556, 76.3212, 46.4078, 71.8891, 83.1180,\n",
      "        83.7253, 83.5315, 82.3298, 82.1166, 75.2746, 85.0174, 80.5078, 82.8725,\n",
      "        81.3929, 75.5330, 73.4074, 84.1646, 78.6730, 82.1166, 77.9558, 73.0069])\n",
      "tensor([80.8732, 80.3890, 84.2055, 77.4120, 81.6273, 76.6762, 81.9912, 72.9080,\n",
      "        63.9868, 83.0350, 83.4513, 83.2992, 76.5988, 60.1467, 67.5762, 83.1515,\n",
      "        83.0158, 83.1213, 81.1068, 81.2976, 76.4392, 83.8649, 81.9123, 83.3447,\n",
      "        82.1353, 76.2027, 69.2173, 84.3736, 78.6006, 80.8430, 78.7117, 71.5982])\n",
      "tensor([46.7955, 84.7332, 83.6025, 75.7850, 84.6556, 84.4230, 74.4088, 84.1905,\n",
      "        80.2042, 84.9658, 75.8754, 87.0784, 82.1812, 85.8767, 85.7152, 84.1840,\n",
      "        84.0483, 85.3276, 78.0656, 59.8979, 82.8725, 83.7770, 84.7655, 74.9645,\n",
      "        83.6607, 82.3491, 73.8274, 75.8884, 72.6450, 89.2945, 84.2745, 80.4626])\n",
      "tensor([57.6001, 80.4442, 83.5904, 76.8346, 85.1069, 84.6896, 74.3415, 84.2875,\n",
      "        81.7172, 85.4106, 76.6673, 82.1681, 82.1096, 85.7001, 83.8418, 84.6298,\n",
      "        84.2817, 83.9628, 78.6743, 66.1003, 79.1864, 83.7817, 84.8644, 78.6712,\n",
      "        80.8814, 79.5269, 74.5393, 76.0394, 70.1713, 87.9616, 84.1464, 80.0775])\n",
      "tensor([79.5839, 88.2995, 83.4087, 82.7949, 79.9716, 83.0275, 72.0571, 81.7354,\n",
      "        78.8668, 77.7168, 83.4669, 87.1883, 81.1345, 83.2795, 53.7020, 81.3865,\n",
      "        56.8743, 60.7701, 79.0025, 75.7398, 82.1101, 82.8466, 82.8660, 55.9698,\n",
      "        82.6011, 86.7037, 61.8555, 76.3342, 84.5264, 76.5861, 65.1570, 84.0677])\n",
      "tensor([80.4278, 86.7596, 83.1080, 80.3303, 80.2412, 83.0929, 72.3218, 81.1138,\n",
      "        78.7006, 77.7047, 84.1109, 85.2160, 81.2569, 81.8994, 65.2650, 79.8999,\n",
      "        66.8087, 58.7086, 78.7162, 76.4396, 82.0407, 82.7750, 82.7051, 56.9885,\n",
      "        82.3479, 86.1863, 59.5289, 76.3574, 84.9613, 79.9286, 67.4663, 83.9246])\n",
      "tensor([83.9450, 83.0663, 80.7792, 79.6292, 84.5200, 76.2049, 84.9851, 77.9817,\n",
      "        87.3239, 81.5092, 68.1354, 79.2738, 76.0951, 80.8696, 82.1036, 80.9665,\n",
      "        63.3480, 79.9328, 80.1912, 80.1008, 81.2250, 75.1971, 82.9435, 83.3441,\n",
      "        84.2809, 84.5716, 76.9738, 77.3808, 77.1159, 76.7153, 66.6688, 81.2379])\n",
      "tensor([83.7438, 82.9520, 80.7682, 79.6998, 84.7050, 77.0104, 85.2410, 79.7574,\n",
      "        85.9200, 82.6977, 74.0036, 79.0221, 78.8902, 81.5164, 83.1996, 81.1828,\n",
      "        68.8023, 79.9771, 81.3792, 79.3497, 81.1790, 75.3107, 83.6066, 82.8038,\n",
      "        83.3574, 84.6505, 77.1943, 78.4153, 77.1494, 77.5341, 66.5536, 82.2566])\n",
      "tensor([82.6205, 84.3326, 81.6061, 69.6925, 84.0871, 75.9207, 82.0196, 84.4037,\n",
      "        85.1854, 83.2536, 70.0478, 82.9371, 83.4087, 69.6343, 85.1854, 75.6105,\n",
      "        74.0276, 85.8444, 75.5976, 81.5092])\n",
      "tensor([83.3883, 84.9004, 82.0948, 67.5296, 83.4700, 77.8971, 81.5400, 82.0384,\n",
      "        85.2212, 83.9711, 66.2547, 82.5186, 84.0599, 74.3635, 84.5474, 76.5556,\n",
      "        72.0545, 85.6132, 75.9406, 82.4195])\n",
      "16\n",
      "TOTAL Loss: 70.781\n",
      "Msqrt: 4.424\n",
      "Test loss: 4.424\n"
     ]
    }
   ],
   "source": [
    "test_acc, msqrt = test_model(model, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382\n",
      "618\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(len([x for x in train_targets if x < 80.0]))\n",
    "print(len([x for x in train_targets if x > 80.0]))\n",
    "\n",
    "\n",
    "plt.hist(train_targets,bins=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sven/.pyenv/versions/3.7.0/envs/ceat/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import logging\n",
    "\n",
    "from hpbandster.core.worker import Worker\n",
    "import hpbandster.core.nameserver as hpns\n",
    "import hpbandster.core.result as hpres\n",
    "from hpbandster.optimizers import BOHB\n",
    "\n",
    "logging.getLogger('hpbandster').setLevel(logging.DEBUG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-c7cfa20db472>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPyTorchWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWorker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mload_mnist_minibatched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-217-c7cfa20db472>\u001b[0m in \u001b[0;36mPyTorchWorker\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfiguration\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \"\"\" Define a configurable convolution model.\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CS' is not defined"
     ]
    }
   ],
   "source": [
    "class PyTorchWorker(Worker):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_loader, self.validation_loader, self.test_loader =\\\n",
    "            load_mnist_minibatched(batch_size=32, n_train=4096, n_valid=512)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_model(config: CS.Configuration) -> nn.Module:\n",
    "        \"\"\" Define a configurable convolution model.\n",
    "            \n",
    "        See description of get_conv_model above for more details on the model.\n",
    "        \"\"\"\n",
    "        # START TODO ################ (1point)\n",
    "        # raise NotImplementedError  \n",
    "        \n",
    "        hidden_dim = config['hidden_dim']\n",
    "        num_layers = config['num_layers']\n",
    "        \n",
    "        model, optimizer, criterion = create_model(hidden_dim, num_layers, \n",
    "                                                   input_size = 10, lr= 0.001, relative_size = 0.75, weight_decay = 0.001)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        # END TODO ################\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_configspace() -> CS.Configuration:\n",
    "        \"\"\" Define a conditional hyperparameter search-space.\n",
    "    \n",
    "        hyperparameters:\n",
    "          num_filters_1   from    4 to   32 (int)\n",
    "          num_filters_2   from    4 to   32 (int)\n",
    "          num_filters_3   from    4 to   32 (int)\n",
    "          num_conv_layers from    1 to    3 (int)\n",
    "          lr              from 1e-6 to 1e-1 (float, log)\n",
    "          sgd_momentum    from 0.00 to 0.99 (float)\n",
    "          optimizer            Adam or  SGD (categoric)\n",
    "          \n",
    "        conditions: \n",
    "          include num_filters_2 only if num_conv_layers > 1\n",
    "          include num_filters_3 only if num_conv_layers > 2\n",
    "          include sgd_momentum  only if       optimizer = SGD\n",
    "        \"\"\"\n",
    "        # START TODO ################ (2points)\n",
    "        #raise NotImplementedError\n",
    "        cs = CS.ConfigurationSpace()\n",
    "        \n",
    "        \n",
    "        num_layers_cs = CSH.UniformIntegerHyperparameter(\"num_layers\",lower=1,upper=5)\n",
    "        hidden_dim_cs = CSH.UniformIntegerHyperparameter(\"num_layers\",lower=5,upper=70)\n",
    "        \n",
    "        lr = CSH.UniformFloatHyperparameter(\"lr\",lower=1e-6,upper=1e-1,log=True)\n",
    "        weight_decay = CSH.UniformFloatHyperparameter(\"weight_decay\",lower=1e-6,upper=1e-1,log=True)\n",
    "        relative_size = CSH.UniformFloatHyperparameter(\"relative_size\",lower=0.1,upper=3.0)\n",
    "        sgd_momentum = CSH.UniformFloatHyperparameter(\"sgd_momentum\",lower=0.00,upper=0.99)\n",
    "        optimizer = CSH.CategoricalHyperparameter('optimizer', choices=['Adam', 'SGD'])\n",
    "        \n",
    "        \n",
    "        cs.add_hyperparameters([num_layers_cs, hidden_dim_cs, weight_decay, lr, relative_size, sgd_momentum, optimizer])\n",
    "    \n",
    "        condition3 = CS.EqualsCondition(sgd_momentum,optimizer,'SGD')\n",
    "        cs.add_condition(condition3)\n",
    "        return cs\n",
    "        # END TODO ################\n",
    "\n",
    "    def compute(self, config: CS.Configuration, budget: float, working_directory: str,\n",
    "                *args, **kwargs) -> dict:\n",
    "        \"\"\"Evaluate a function with the given config and budget and return a loss.\n",
    "        \n",
    "        Bohb tries to minimize the returned loss.\n",
    "        \n",
    "        In our case the function is the training and validation of a model,\n",
    "        the budget is the number of epochs and the loss is the validation error.\n",
    "        \"\"\"\n",
    "        model = self.get_model(config)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # START TODO ################ (3points)\n",
    "        #raise NotImplementedError\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        if config['optimizer'] == 'Adam':\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "        else:\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=config['sgd_momentum'])\n",
    "\n",
    "        for epoch in range(int(budget)):\n",
    "          loss = 0\n",
    "          model.train()\n",
    "          for i, (x, y) in enumerate(self.train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = F.nll_loss(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # END TODO ################\n",
    "        \n",
    "        train_accuracy = evaluate_accuracy(model, self.train_loader)\n",
    "        validation_accuracy = evaluate_accuracy(model, self.validation_loader)\n",
    "        test_accuracy = evaluate_accuracy(model, self.test_loader)\n",
    "        \n",
    "        return ({\n",
    "                'loss': 1 - validation_accuracy,  # remember: HpBandSter minimizes the loss!\n",
    "                'info': {'test_accuracy': test_accuracy,\n",
    "                         'train_accuracy': train_accuracy,\n",
    "                         'valid_accuracy': validation_accuracy,\n",
    "                         'model': str(model)}\n",
    "                })\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLearningCurvePredictor():\n",
    "    \"\"\"A learning curve predictor that predicts the last observed epoch of the validation accuracy as final performance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for datapoint in X:\n",
    "            predictions.append(datapoint[\"Train/val_accuracy\"][-1])\n",
    "        return predictions\n",
    "    \n",
    "def score(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & tuning\n",
    "predictor = SimpleLearningCurvePredictor()\n",
    "for data in train_data:\n",
    "    data['Train/val_accuracy']=data['Train/val_accuracy'][0:10]\n",
    "\n",
    "predictor.fit(train_data, train_targets)\n",
    "preds = predictor.predict(val_data)\n",
    "mse = score(val_targets, preds)\n",
    "print(\"Score on validation set:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation (after tuning)\n",
    "final_preds = predictor.predict(test_data)\n",
    "final_score = score(test_targets, final_preds)\n",
    "print(\"Final test score:\", final_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
