{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task A: Creating a Performance Predictor\n",
    "\n",
    "In this task, you will use training data from 2000 configurations on a single OpenML dataset to train a performance predictor. The data will be splitted into train, test and validation set and we will only use the first 10 epochs of the learning curves for predicitons. You are provided with the full benchmark logs for Fashion-MNIST, that is learning curves, config parameters and gradient statistics, and you can use them freely.\n",
    "\n",
    "For questions, you can contact zimmerl@informatik.uni-freiburg.\n",
    "\n",
    "__Note: Please use the dataloading and splits you are provided with in this notebook.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifications:\n",
    "\n",
    "* Data: fashion_mnist.json\n",
    "* Number of datasets: 1\n",
    "* Number of configurations: 2000\n",
    "* Number of epochs seed during prediction: 10\n",
    "* Available data: Learning curves, architecture parameters and hyperparameters, gradient statistics \n",
    "* Target: Final validation accuracy\n",
    "* Evaluation metric: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and splitting data\n",
    "\n",
    "__Note__: There are 51 steps logged, 50 epochs plus the 0th epoch, prior to any weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd ..\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils_prep\n",
    "from api import Benchmark\n",
    "import utils_prep2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading data...\n",
      "==> No cached data found or cache set to False.\n",
      "==> Reading json data...\n",
      "==> Done.\n"
     ]
    }
   ],
   "source": [
    "bench_dir = \"/home/sven/LCBench/data/11604705/fashion_mnist.json\"\n",
    "bench = Benchmark(bench_dir, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1000\n",
      "Validation: 500\n",
      "Test: 500\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "def cut_data(data, cut_position):\n",
    "    targets = []\n",
    "    for dp in data:\n",
    "        targets.append(dp[\"Train/val_accuracy\"][50])\n",
    "        for tag in dp:\n",
    "            if tag.startswith(\"Train/\"):\n",
    "                dp[tag] = dp[tag][0:cut_position]\n",
    "    return data, targets\n",
    "\n",
    "def read_data():\n",
    "    dataset_name = 'Fashion-MNIST'\n",
    "    n_configs = bench.get_number_of_configs(dataset_name)\n",
    "    \n",
    "    # Query API\n",
    "    data = []\n",
    "    for config_id in range(n_configs):\n",
    "        data_point = dict()\n",
    "        data_point[\"config\"] = bench.query(dataset_name=dataset_name, tag=\"config\", config_id=config_id)\n",
    "        for tag in bench.get_queriable_tags(dataset_name=dataset_name, config_id=config_id):\n",
    "            if tag.startswith(\"Train/\"):\n",
    "                data_point[tag] = bench.query(dataset_name=dataset_name, tag=tag, config_id=config_id)    \n",
    "        data.append(data_point)\n",
    "        \n",
    "    # Split: 50% train, 25% validation, 25% test (the data is already shuffled)\n",
    "    indices = np.arange(n_configs)\n",
    "    ind_train = indices[0:int(np.floor(0.5*n_configs))]\n",
    "    ind_val = indices[int(np.floor(0.5*n_configs)):int(np.floor(0.75*n_configs))]\n",
    "    ind_test = indices[int(np.floor(0.75*n_configs)):]\n",
    "\n",
    "    array_data = np.array(data)\n",
    "    train_data = array_data[ind_train]\n",
    "    val_data = array_data[ind_val]\n",
    "    test_data = array_data[ind_test]\n",
    "    \n",
    "    # Cut curves for validation and test\n",
    "    cut_position = 11\n",
    "    val_data, val_targets = cut_data(val_data, cut_position)\n",
    "    test_data, test_targets = cut_data(test_data, cut_position)\n",
    "    train_data, train_targets = cut_data(train_data, 51)   # Cut last value as it is repeated\n",
    "    \n",
    "    return train_data, val_data, test_data, train_targets, val_targets, test_targets\n",
    "    \n",
    "train_data, val_data, test_data, train_targets, val_targets, test_targets = read_data()\n",
    "\n",
    "print(\"Train:\", len(train_data))\n",
    "print(\"Validation:\", len(val_data))\n",
    "print(\"Test:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains the configuration of the trained model and learning curves as well as global and layer-wise gradient statistics.\n",
    "\n",
    "__Note__: Not all parameters vary across different configurations. The varying parameters are batch_size, max_dropout, max_units, num_layers, learning_rate, momentum, weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config example: {'batch_size': 93, 'imputation_strategy': 'mean', 'learning_rate_scheduler': 'cosine_annealing', 'loss': 'cross_entropy_weighted', 'network': 'shapedmlpnet', 'max_dropout': 0.06145480624960298, 'normalization_strategy': 'standardize', 'optimizer': 'sgd', 'cosine_annealing_T_max': 50, 'cosine_annealing_eta_min': 1e-08, 'activation': 'relu', 'max_units': 402, 'mlp_shape': 'funnel', 'num_layers': True, 'learning_rate': 0.07306153347321286, 'momentum': 0.5844418984083981, 'weight_decay': 0.05967268273584057}\n",
      "\n",
      "\n",
      "DATA Keys: dict_keys(['config', 'Train/loss', 'Train/train_accuracy', 'Train/val_accuracy', 'Train/train_cross_entropy', 'Train/val_cross_entropy', 'Train/train_balanced_accuracy', 'Train/val_balanced_accuracy', 'Train/test_result', 'Train/test_cross_entropy', 'Train/test_balanced_accuracy', 'Train/gradient_max', 'Train/gradient_mean', 'Train/gradient_median', 'Train/gradient_std', 'Train/gradient_q10', 'Train/gradient_q25', 'Train/gradient_q75', 'Train/gradient_q90', 'Train/layer_wise_gradient_max_layer_0', 'Train/layer_wise_gradient_mean_layer_0', 'Train/layer_wise_gradient_median_layer_0', 'Train/layer_wise_gradient_std_layer_0', 'Train/layer_wise_gradient_q10_layer_0', 'Train/layer_wise_gradient_q25_layer_0', 'Train/layer_wise_gradient_q75_layer_0', 'Train/layer_wise_gradient_q90_layer_0', 'Train/gradient_norm', 'Train/lr'])\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "print(\"Config example:\", test_data[0][\"config\"])\n",
    "print(\"\\n\")\n",
    "print(\"DATA Keys:\", test_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe79f097b00>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Learning curve\n",
    "plt.plot(train_data[10][\"Train/val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe79257ecf8>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gradient statistics\n",
    "plt.plot(train_data[10][\"Train/layer_wise_gradient_mean_layer_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = utils_prep2.check_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_classes, embedding_dim, hidden_size, seq_length, \n",
    "                 num_layers, config_size, bidirectional = False, drop_prob=0.5, relative_size = 0.2):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.config_size = config_size\n",
    "        \n",
    "        # self.forecaster = torch.nn.Embedding(100, embedding_dim)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(drop_prob)\n",
    "        self.lstm = torch.nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_dim, \n",
    "                            num_layers = self.num_layers, dropout =drop_prob, bidirectional = bidirectional)\n",
    "        \n",
    " \n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(self.config_size, int(self.hidden_dim*relative_size))\n",
    "                                       \n",
    "                                       \n",
    "        self.linear2 = torch.nn.Linear(int(self.hidden_dim*relative_size),\n",
    "                                       int(self.hidden_dim*relative_size))\n",
    "                                       \n",
    "                                       \n",
    "        self.linear3 = torch.nn.Linear(self.hidden_dim + int(self.hidden_dim*relative_size),\n",
    "                                       self.hidden_dim + int(self.hidden_dim*relative_size))\n",
    "\n",
    "        \n",
    "        self.linear4 = torch.nn.Linear(self.hidden_dim + int(self.hidden_dim*relative_size), self.hidden_dim//2)\n",
    "        \n",
    "        self.linear5 = torch.nn.Linear(self.hidden_dim//2, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x, configs, hidden):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x=x.unsqueeze(0)\n",
    "        \n",
    "        x = x.float()\n",
    "        \n",
    "        #forecast = self.forecaster(x)\n",
    "\n",
    "        lstm_x, hidden = self.lstm(x, hidden)\n",
    "        lstm_x = lstm_x.contiguous().view(-1, self.hidden_dim)\n",
    "        x = self.dropout(lstm_x)\n",
    "\n",
    "                                       \n",
    "        x_config = torch.nn.functional.relu(self.linear1(configs))\n",
    "        x_config = torch.nn.functional.relu(self.linear2(x_config))\n",
    "\n",
    "        x_cat =  torch.cat([x, x_config], dim =1)\n",
    "\n",
    "        \n",
    "        x_cat = torch.nn.functional.relu(self.linear3(x_cat))\n",
    "        x_cat = torch.nn.functional.relu(self.linear4(x_cat))\n",
    "        x_cat = self.linear5(x_cat)\n",
    "\n",
    "        \n",
    "        x_cat = x_cat.view(batch_size, -1)\n",
    "        x_cat = x_cat[:,-1]\n",
    "        \n",
    "        return x_cat, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size = 32):\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_size = 10):\n",
    "\n",
    "    input_size = input_size\n",
    "    outcome_dim = 1\n",
    "    embedding_dim = 400\n",
    "    hidden_dim = 50\n",
    "    seq_length = 1\n",
    "    num_layers = 3\n",
    "    config_size = 7\n",
    "\n",
    "    model = LSTM_Net(input_size, outcome_dim, embedding_dim, hidden_dim, seq_length, num_layers, config_size,\n",
    "                     relative_size = 0.75)\n",
    "    model.to(device)\n",
    "\n",
    "    lr=0.001\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = 0.001)\n",
    "    \n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_model(model, optimizer, criterion):\n",
    "    \n",
    "    epochs = 2000\n",
    "    counter = 0\n",
    "    print_every = 200\n",
    "\n",
    "    clip = 5\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "\n",
    "        for batches, configs , labels in train_data_loader:\n",
    "            counter += 1\n",
    "\n",
    "            batch_size_calc = len(labels)\n",
    "            hidden = model.init_hidden(batch_size=batch_size_calc)\n",
    "\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            output, hidden = model(batches, configs, hidden)\n",
    "\n",
    "\n",
    "            loss = abs(criterion(output.squeeze(), labels.float()))\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            if counter%print_every == 0:\n",
    "\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "\n",
    "                for inp,configs, lab in val_data_loader:\n",
    "\n",
    "                    batch_size_calc = len(lab)\n",
    "\n",
    "                    val_h = model.init_hidden(batch_size = batch_size_calc)\n",
    "\n",
    "\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    inp, lab = inp.to(device), lab.to(device)\n",
    "                    out, val_h = model(inp, configs,val_h)\n",
    "                    val_loss = abs(criterion(out.squeeze(), lab.float()))\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                    print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "\n",
    "                if np.mean(val_losses) <= valid_loss_min:\n",
    "                    torch.save(model.state_dict(), '/home/sven/LCBench/state_dict.pt')\n",
    "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                    valid_loss_min = np.mean(val_losses)\n",
    "                \n",
    "                model.train()\n",
    "\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion):\n",
    "    \n",
    "    model.load_state_dict(torch.load('/home/sven/LCBench/state_dict.pt'))\n",
    "\n",
    "    test_losses = []\n",
    "    msqrt = 0\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    for inputs,configs, labels in test_data_loader:\n",
    "        counter = counter +1\n",
    "        batch_size_calc = len(labels)\n",
    "        h = model.init_hidden(batch_size_calc)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output, h = model(inputs,configs, h)\n",
    "        test_loss = criterion(output.squeeze(), labels.float())\n",
    "        test_losses.append(test_loss.item())\n",
    "        pred = (output.squeeze())\n",
    "\n",
    "        msqrt = msqrt + mean_squared_error(labels.detach().numpy(), pred.detach().numpy())\n",
    "    \n",
    "        print((labels.detach()))\n",
    "        print((pred.detach()))\n",
    "    print(counter)\n",
    "    total_test_acc = msqrt\n",
    "    print(\"TOTAL Loss: {:.3f}\".format(total_test_acc))\n",
    "    test_acc2 = msqrt/counter\n",
    "    \n",
    "    print(\"Msqrt: {:.3f}\".format(test_acc2))\n",
    "\n",
    "    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "    \n",
    "    return test_acc, np.mean(test_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 36.413151\n",
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 34.874458\n",
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 40.541397\n",
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 37.414825\n",
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 40.839330\n",
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 46.287458\n",
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 41.973221\n",
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 39.351261\n",
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 39.728506\n",
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 38.493452\n",
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 39.455660\n",
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 41.114466\n",
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 41.023530\n",
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 41.266410\n",
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 42.417241\n",
      "Epoch: 7/2000... Step: 200... Loss: 102.066483... Val Loss: 41.832202\n",
      "Validation loss decreased (inf --> 41.832202).  Saving model ...\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 26.663403\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 24.116034\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 26.219138\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 24.165489\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 26.125153\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 32.340118\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 29.014713\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 27.167999\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 26.307415\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 25.483947\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 25.597407\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 26.152374\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 25.860066\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 26.768728\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 27.403055\n",
      "Epoch: 13/2000... Step: 400... Loss: 36.014671... Val Loss: 26.654911\n",
      "Validation loss decreased (41.832202 --> 26.654911).  Saving model ...\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 5.835976\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 5.076019\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 7.160038\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 6.951701\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 7.807542\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 16.428354\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 15.260383\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 14.084916\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 13.122711\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 12.759290\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 12.646793\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 12.940730\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 12.783003\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 13.322757\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 13.487123\n",
      "Epoch: 19/2000... Step: 600... Loss: 20.906872... Val Loss: 13.374578\n",
      "Validation loss decreased (26.654911 --> 13.374578).  Saving model ...\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 11.349758\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 14.188423\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 15.498365\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 14.340166\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 14.389362\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 23.286896\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 21.781901\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 20.631314\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 19.160762\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 18.906071\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 18.622283\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 18.820290\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 18.280614\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 19.434163\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 19.626024\n",
      "Epoch: 25/2000... Step: 800... Loss: 22.113018... Val Loss: 19.093476\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 4.638668\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 4.063798\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 5.653169\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 5.792507\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 6.234249\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 14.506737\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 13.561198\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 12.616644\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 11.851863\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 11.391701\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 11.107020\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 11.243949\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 10.823375\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 11.305810\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 11.444610\n",
      "Epoch: 32/2000... Step: 1000... Loss: 9.163610... Val Loss: 11.018830\n",
      "Validation loss decreased (13.374578 --> 11.018830).  Saving model ...\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 4.559486\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 6.127636\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 6.872335\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 6.487731\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 6.820715\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 14.237596\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 13.218130\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 12.500640\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 11.643238\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 11.372410\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 11.018436\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 10.983922\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 10.652797\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 11.627369\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 11.692670\n",
      "Epoch: 38/2000... Step: 1200... Loss: 9.530072... Val Loss: 11.553502\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 4.831944\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 5.348369\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 6.695235\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 6.431102\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 6.674460\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 13.125140\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 12.304416\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 11.598240\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 10.836658\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 10.484822\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 10.000014\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 10.110977\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 9.697671\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 10.471570\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 10.648356\n",
      "Epoch: 44/2000... Step: 1400... Loss: 14.832035... Val Loss: 10.348281\n",
      "Validation loss decreased (11.018830 --> 10.348281).  Saving model ...\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 6.342470\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 6.633053\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 7.417432\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 7.012752\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 6.897681\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 11.587947\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 11.064629\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 10.902519\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 10.365498\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 10.302765\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 9.952538\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 9.957043\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 9.737100\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 10.967823\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 11.094944\n",
      "Epoch: 50/2000... Step: 1600... Loss: 17.410522... Val Loss: 10.946306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 7.998251\n",
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 10.699664\n",
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 12.047720\n",
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 12.091826\n",
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 12.848121\n",
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 17.472272\n",
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 16.621503\n",
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 15.743052\n",
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 14.903518\n",
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 14.323702\n",
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 13.704627\n",
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 14.148885\n",
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 13.672058\n",
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 14.380869\n",
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 14.972537\n",
      "Epoch: 57/2000... Step: 1800... Loss: 14.042742... Val Loss: 14.652826\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 5.773671\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 6.778724\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 7.722179\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 7.139804\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 7.521527\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 11.167502\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 10.461930\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 10.021040\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 9.449596\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 9.225821\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 8.830509\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 9.161226\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 8.910503\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 10.240958\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 10.460490\n",
      "Epoch: 63/2000... Step: 2000... Loss: 7.265472... Val Loss: 10.218750\n",
      "Validation loss decreased (10.348281 --> 10.218750).  Saving model ...\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 12.204443\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 8.409754\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 10.227438\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 10.591392\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 10.862119\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 12.872175\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 13.018663\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 13.367635\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 13.255216\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 13.298469\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 13.279293\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 13.427987\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 13.422527\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 13.790790\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 14.293482\n",
      "Epoch: 69/2000... Step: 2200... Loss: 16.141088... Val Loss: 14.339159\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 5.181020\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 6.340276\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 7.706914\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 7.413201\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 8.060993\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 10.415460\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 9.863207\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 9.402773\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 8.913497\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 8.729302\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 8.354432\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 8.918378\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 8.636843\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 9.784436\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 10.131934\n",
      "Epoch: 75/2000... Step: 2400... Loss: 17.218410... Val Loss: 9.962911\n",
      "Validation loss decreased (10.218750 --> 9.962911).  Saving model ...\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 3.559591\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 3.566822\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 5.144149\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 5.268894\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 5.914644\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 7.976629\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 7.565844\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 7.240633\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 6.919520\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 6.688287\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 6.396338\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 6.868621\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 6.710213\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 7.556754\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 7.937035\n",
      "Epoch: 82/2000... Step: 2600... Loss: 6.831419... Val Loss: 7.844212\n",
      "Validation loss decreased (9.962911 --> 7.844212).  Saving model ...\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 6.064016\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 7.765539\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 8.282699\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 7.990665\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 8.206211\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 9.597576\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 9.340882\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 9.016714\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 8.537337\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 8.534668\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 8.376040\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 8.737483\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 8.486267\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 9.777368\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 10.081494\n",
      "Epoch: 88/2000... Step: 2800... Loss: 7.775282... Val Loss: 9.821742\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 8.071281\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 5.624985\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 7.577594\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 7.662319\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 8.230901\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 9.896454\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 9.841471\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 9.633996\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 9.518705\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 9.424907\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 9.232927\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 9.582820\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 9.518041\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 10.241685\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 10.399536\n",
      "Epoch: 94/2000... Step: 3000... Loss: 8.746207... Val Loss: 10.308146\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 5.444071\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 5.075650\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 6.239543\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 6.196239\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 6.722336\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 8.315894\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 8.045346\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 7.655952\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 7.277891\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 7.195410\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 6.922029\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 7.361857\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 7.102677\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 8.038376\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 8.288646\n",
      "Epoch: 100/2000... Step: 3200... Loss: 14.252797... Val Loss: 8.101431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 9.019120\n",
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 10.608484\n",
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 11.409442\n",
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 11.702715\n",
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 12.347642\n",
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 14.500039\n",
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 13.847661\n",
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 13.105995\n",
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 12.594054\n",
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 12.297116\n",
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 11.861220\n",
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 12.457303\n",
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 12.069433\n",
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 12.964480\n",
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 13.429798\n",
      "Epoch: 107/2000... Step: 3400... Loss: 11.784443... Val Loss: 13.272281\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 5.052291\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 4.416142\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 5.623193\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 5.674893\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 6.102639\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 7.099298\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 6.953547\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 6.791582\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 6.540037\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 6.448932\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 6.356782\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 6.672697\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 6.506352\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 7.445291\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 7.673353\n",
      "Epoch: 113/2000... Step: 3600... Loss: 5.547881... Val Loss: 7.576906\n",
      "Validation loss decreased (7.844212 --> 7.576906).  Saving model ...\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 4.512756\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 3.667841\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 4.502469\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 5.029682\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 5.343466\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 6.414480\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 6.238146\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 6.616439\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 6.331762\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 6.135003\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 6.013803\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 6.349692\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 6.186179\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 6.831253\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 7.256365\n",
      "Epoch: 119/2000... Step: 3800... Loss: 9.153867... Val Loss: 7.291296\n",
      "Validation loss decreased (7.576906 --> 7.291296).  Saving model ...\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 4.478733\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 4.900628\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 5.838723\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 5.826488\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 6.114832\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 7.412492\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 6.943709\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 6.820188\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 6.481747\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 6.309027\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 6.103603\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 6.451476\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 6.255193\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 7.230198\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 7.480562\n",
      "Epoch: 125/2000... Step: 4000... Loss: 14.965926... Val Loss: 7.356335\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 4.905317\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 4.173475\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 5.351622\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 5.589890\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 5.916656\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 7.090073\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 6.618942\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 6.380151\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 6.068645\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 5.961005\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 5.717649\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 6.076234\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 5.840272\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 6.669242\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 6.865688\n",
      "Epoch: 132/2000... Step: 4200... Loss: 5.637675... Val Loss: 6.747289\n",
      "Validation loss decreased (7.291296 --> 6.747289).  Saving model ...\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 4.509046\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 4.705125\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 5.231206\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 5.630581\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 5.915184\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 7.073581\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 6.668507\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 6.466780\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 6.171008\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 6.036476\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 5.832797\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 6.188564\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 5.980723\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 6.978345\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 7.197504\n",
      "Epoch: 138/2000... Step: 4400... Loss: 2.929851... Val Loss: 7.219011\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 4.983770\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 4.756620\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 5.602794\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 6.066368\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 6.412788\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 7.901471\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 7.493754\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 7.124440\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 6.774406\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 6.561399\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 6.261862\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 6.647439\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 6.382154\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 7.223841\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 7.441662\n",
      "Epoch: 144/2000... Step: 4600... Loss: 9.229654... Val Loss: 7.438083\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 5.274695\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 4.079630\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 4.858379\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 5.040618\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 5.280481\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 6.368851\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 6.234588\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 6.307707\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 6.174428\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 6.109071\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 6.030109\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 6.270130\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 6.176969\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 7.042154\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 7.154851\n",
      "Epoch: 150/2000... Step: 4800... Loss: 14.852641... Val Loss: 7.283483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 13.679989\n",
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 16.206313\n",
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 16.007258\n",
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 16.601400\n",
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 16.656551\n",
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 18.222574\n",
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 17.724900\n",
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 17.119441\n",
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 16.616356\n",
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 16.264163\n",
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 15.850940\n",
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 16.217655\n",
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 15.882960\n",
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 17.074367\n",
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 17.312572\n",
      "Epoch: 157/2000... Step: 5000... Loss: 14.655961... Val Loss: 17.252421\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 4.572978\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 3.999081\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 4.906787\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 5.644948\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 5.808688\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 7.152615\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 6.694019\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 6.752985\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 6.388548\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 6.122062\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 5.798916\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 6.081451\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 5.849324\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 6.537571\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 6.803517\n",
      "Epoch: 163/2000... Step: 5200... Loss: 5.095799... Val Loss: 6.771148\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 4.707311\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 3.553202\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 4.379552\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 4.848147\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 4.979767\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 5.737852\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 5.452587\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 5.595881\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 5.421869\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 5.183188\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 4.957526\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 5.200258\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 5.032672\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 5.731022\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 5.853294\n",
      "Epoch: 169/2000... Step: 5400... Loss: 5.346134... Val Loss: 5.844820\n",
      "Validation loss decreased (6.747289 --> 5.844820).  Saving model ...\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 11.610650\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 9.206984\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 8.734833\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 8.961617\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 8.388811\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 8.743737\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 8.451133\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 10.535057\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 10.103585\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 9.394995\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 8.995731\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 9.228302\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 9.102094\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 9.328720\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 9.580003\n",
      "Epoch: 175/2000... Step: 5600... Loss: 95.895569... Val Loss: 9.531502\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 5.966480\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 5.459071\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 6.272130\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 6.579703\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 6.649906\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 7.490308\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 7.242571\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 6.984810\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 6.721827\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 6.529110\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 6.243662\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 6.566941\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 6.284680\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 7.166675\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 7.315447\n",
      "Epoch: 182/2000... Step: 5800... Loss: 5.173922... Val Loss: 7.344440\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 5.311877\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 4.949847\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 5.618067\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 5.758323\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 5.787824\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 7.107951\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 6.622968\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 6.341707\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 6.043053\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 5.861828\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 5.594371\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 5.833283\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 5.588906\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 6.594929\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 6.681225\n",
      "Epoch: 188/2000... Step: 6000... Loss: 5.156885... Val Loss: 6.682878\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 5.087023\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 5.233191\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 5.446101\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 5.542211\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 5.385827\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 6.546816\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 6.235108\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 6.137913\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 5.901591\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 5.697080\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 5.524371\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 5.734085\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 5.535782\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 6.587584\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 6.642990\n",
      "Epoch: 194/2000... Step: 6200... Loss: 5.069579... Val Loss: 6.750004\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 5.433666\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 4.130983\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 4.635228\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 5.041352\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 4.921536\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 5.647681\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 5.466737\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 5.710176\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 5.599429\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 5.435388\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 5.311559\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 5.496448\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 5.327791\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 6.077958\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 6.224452\n",
      "Epoch: 200/2000... Step: 6400... Loss: 5.126642... Val Loss: 6.340618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 6.737236\n",
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 5.641028\n",
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 6.220694\n",
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 7.096809\n",
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 7.099712\n",
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 8.508186\n",
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 8.088697\n",
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 7.760632\n",
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 7.465474\n",
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 7.169807\n",
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 6.772610\n",
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 7.101295\n",
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 6.787712\n",
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 7.416989\n",
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 7.577459\n",
      "Epoch: 207/2000... Step: 6600... Loss: 4.771071... Val Loss: 7.643470\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 7.721927\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 5.866455\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 6.989070\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 6.833292\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 7.004691\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 7.923930\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 7.699202\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 7.660893\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 7.630815\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 7.386118\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 7.261134\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 7.409204\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 7.231319\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 8.040648\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 8.015062\n",
      "Epoch: 213/2000... Step: 6800... Loss: 7.382618... Val Loss: 8.088873\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 5.169792\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 3.868982\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 4.436709\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 4.821293\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 4.778766\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 5.479455\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 5.195512\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 5.251191\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 5.140437\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 4.855069\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 4.652380\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 4.821283\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 4.644644\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 5.468074\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 5.485134\n",
      "Epoch: 219/2000... Step: 7000... Loss: 6.319895... Val Loss: 5.576203\n",
      "Validation loss decreased (5.844820 --> 5.576203).  Saving model ...\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 3.636157\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 3.195376\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 3.648869\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 4.279639\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 4.417582\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 5.498646\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 5.088452\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 5.077723\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 5.017483\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 4.698140\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 4.490348\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 4.676741\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 4.548764\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 5.407193\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 5.436275\n",
      "Epoch: 225/2000... Step: 7200... Loss: 4.260890... Val Loss: 5.605288\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 6.397402\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 6.795534\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 7.345476\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 8.086389\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 8.085516\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 9.010908\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 8.412992\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 8.115053\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 7.837781\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 7.632588\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 7.300013\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 7.595765\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 7.382427\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 8.162480\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 8.264879\n",
      "Epoch: 232/2000... Step: 7400... Loss: 6.290227... Val Loss: 8.336455\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 6.799100\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 4.937186\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 5.467740\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 6.399404\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 6.177500\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 6.536420\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 6.210399\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 6.264499\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 6.021153\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 5.672772\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 5.369291\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 5.603570\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 5.347934\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 5.953813\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 6.094378\n",
      "Epoch: 238/2000... Step: 7600... Loss: 4.246335... Val Loss: 6.139954\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 4.340451\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 3.777301\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 4.272295\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 4.855412\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 4.736415\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 5.462679\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 5.035595\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 5.068410\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 4.905424\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 4.613815\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 4.451954\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 4.595741\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 4.439030\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 5.367657\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 5.477452\n",
      "Epoch: 244/2000... Step: 7800... Loss: 3.645974... Val Loss: 5.642242\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 6.914696\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 5.962485\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 6.262400\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 7.512365\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 7.357736\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 8.446555\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 8.184431\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 7.869764\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 7.556580\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 7.310488\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 6.960457\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 7.171527\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 6.880383\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 7.577193\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 7.690222\n",
      "Epoch: 250/2000... Step: 8000... Loss: 4.413382... Val Loss: 7.789223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 8.065260\n",
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 7.872734\n",
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 8.260605\n",
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 9.366263\n",
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 9.182207\n",
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 10.068866\n",
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 9.737538\n",
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 9.226895\n",
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 8.904032\n",
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 8.713850\n",
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 8.355701\n",
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 8.607700\n",
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 8.299019\n",
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 9.182986\n",
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 9.278742\n",
      "Epoch: 257/2000... Step: 8200... Loss: 7.362272... Val Loss: 9.379398\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 5.986186\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 4.274292\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 5.024675\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 5.580258\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 5.560501\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 6.397777\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 6.185446\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 6.013139\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 5.848886\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 5.605884\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 5.350333\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 5.511316\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 5.318150\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 6.144397\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 6.217848\n",
      "Epoch: 263/2000... Step: 8400... Loss: 7.476990... Val Loss: 6.442257\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 7.429657\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 8.232718\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 8.172513\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 8.716275\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 8.399760\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 9.328639\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 8.937303\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 8.433772\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 8.059311\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 7.895254\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 7.625590\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 7.778573\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 7.503305\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 8.920644\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 8.892454\n",
      "Epoch: 269/2000... Step: 8600... Loss: 9.379829... Val Loss: 8.945232\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 3.955317\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 3.211294\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 3.773947\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 4.509629\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 4.579087\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 5.667531\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 5.245129\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 5.117683\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 4.954672\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 4.644802\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 4.443160\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 4.648138\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 4.471139\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 5.280242\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 5.320365\n",
      "Epoch: 275/2000... Step: 8800... Loss: 2.742025... Val Loss: 5.544270\n",
      "Validation loss decreased (5.576203 --> 5.544270).  Saving model ...\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 6.304297\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 6.269771\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 6.771224\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 7.631007\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 7.486713\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 8.388723\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 8.177491\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 7.691618\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 7.411824\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 7.235459\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 6.957125\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 7.255660\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 6.939770\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 7.910874\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 7.954262\n",
      "Epoch: 282/2000... Step: 9000... Loss: 6.567185... Val Loss: 8.107517\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 6.047319\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 4.891984\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 5.317288\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 6.615248\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 6.450918\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 6.680539\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 6.484567\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 6.313822\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 6.064378\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 5.795355\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 5.528235\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 5.730484\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 5.589741\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 6.434779\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 6.583016\n",
      "Epoch: 288/2000... Step: 9200... Loss: 3.902899... Val Loss: 6.785119\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 6.251467\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 4.323783\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 5.173323\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 5.489760\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 5.340147\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 6.070665\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 5.783439\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 5.876795\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 5.745900\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 5.407270\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 5.099482\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 5.180702\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 4.980039\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 5.715257\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 5.710650\n",
      "Epoch: 294/2000... Step: 9400... Loss: 12.767406... Val Loss: 5.718997\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 5.680602\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 4.080417\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 4.416669\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 4.954565\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 4.784218\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 5.550602\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 5.248497\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 5.313389\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 5.234899\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 4.937028\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 4.882418\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 5.038880\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 4.822181\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 5.526300\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 5.572724\n",
      "Epoch: 300/2000... Step: 9600... Loss: 11.981211... Val Loss: 5.691909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 7.672942\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 7.313700\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 7.621595\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 8.526948\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 8.208915\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 8.529266\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 8.239487\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 8.083605\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 7.850798\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 7.608333\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 7.352901\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 7.552223\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 7.309186\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 8.148170\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 8.197139\n",
      "Epoch: 307/2000... Step: 9800... Loss: 7.863524... Val Loss: 8.335841\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 4.699386\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 3.734569\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 4.584922\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 5.102728\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 5.083031\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 6.398637\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 5.949569\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 5.684149\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 5.465967\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 5.138751\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 4.887753\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 5.053051\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 4.813140\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 5.666285\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 5.684849\n",
      "Epoch: 313/2000... Step: 10000... Loss: 3.026561... Val Loss: 5.772810\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 6.023812\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 4.246514\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 4.762824\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 5.074865\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 5.027213\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 6.070456\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 5.856178\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 5.902179\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 5.845706\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 5.598840\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 5.368653\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 5.490339\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 5.267106\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 5.952769\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 5.938338\n",
      "Epoch: 319/2000... Step: 10200... Loss: 5.523058... Val Loss: 6.002152\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 4.051886\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 3.418687\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 3.723472\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 4.212638\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 4.425322\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 5.579811\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 5.168565\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 5.240472\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 5.167775\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 4.856346\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 4.707300\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 4.857157\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 4.707087\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 5.560504\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 5.595600\n",
      "Epoch: 325/2000... Step: 10400... Loss: 5.420800... Val Loss: 5.858855\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 7.859514\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 7.715529\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 8.061373\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 8.881163\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 8.652314\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 9.449743\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 8.879946\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 8.415832\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 8.134917\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 7.929447\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 7.609696\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 7.840364\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 7.602105\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 8.480767\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 8.539341\n",
      "Epoch: 332/2000... Step: 10600... Loss: 6.294926... Val Loss: 8.688213\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 5.795246\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 4.174308\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 4.847932\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 5.429288\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 5.432091\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 6.344300\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 6.139789\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 5.875568\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 5.732371\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 5.527929\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 5.252121\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 5.409116\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 5.133946\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 5.877053\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 5.878787\n",
      "Epoch: 338/2000... Step: 10800... Loss: 3.608751... Val Loss: 5.930119\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 6.612963\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 4.471444\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 4.891315\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 5.307433\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 5.004685\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 5.723042\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 5.400000\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 5.806406\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 5.617924\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 5.262518\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 5.054053\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 5.140498\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 4.906898\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 5.537140\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 5.644643\n",
      "Epoch: 344/2000... Step: 11000... Loss: 5.418597... Val Loss: 5.751847\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 5.410082\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 4.863729\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 5.072885\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 5.871424\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 5.706346\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 6.689686\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 6.350185\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 6.164302\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 5.973187\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 5.699851\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 5.491738\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 5.700666\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 5.465571\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 6.255014\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 6.321957\n",
      "Epoch: 350/2000... Step: 11200... Loss: 6.555067... Val Loss: 6.482542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 7.210240\n",
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 6.207309\n",
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 6.723917\n",
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 7.340023\n",
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 7.348238\n",
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 8.497944\n",
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 8.010478\n",
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 7.583223\n",
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 7.326884\n",
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 7.082133\n",
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 6.849709\n",
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 7.083983\n",
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 6.814274\n",
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 7.557369\n",
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 7.578467\n",
      "Epoch: 357/2000... Step: 11400... Loss: 7.834246... Val Loss: 7.712095\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 5.884738\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 4.333856\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 5.089091\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 5.350691\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 5.330679\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 5.803121\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 5.597185\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 5.530590\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 5.495349\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 5.307520\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 5.212172\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 5.373709\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 5.181774\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 6.078102\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 6.123212\n",
      "Epoch: 363/2000... Step: 11600... Loss: 10.406517... Val Loss: 6.315670\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 4.998578\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 3.807211\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 4.288758\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 4.552019\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 4.512493\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 5.527713\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 5.276747\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 5.195096\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 5.036528\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 4.826698\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 4.655354\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 4.771281\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 4.547634\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 5.450517\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 5.481660\n",
      "Epoch: 369/2000... Step: 11800... Loss: 7.527850... Val Loss: 5.643702\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 8.415238\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 8.187061\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 8.674724\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 9.844622\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 9.920990\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 10.741307\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 10.282957\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 9.723062\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 9.385496\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 9.135153\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 8.847920\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 9.052457\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 8.782702\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 9.680132\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 9.826685\n",
      "Epoch: 375/2000... Step: 12000... Loss: 4.321398... Val Loss: 9.955179\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 6.808620\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 4.931988\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 5.168606\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 6.323521\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 6.209448\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 7.007102\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 6.596559\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 6.706710\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 6.438168\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 5.953328\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 5.628117\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 5.865034\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 5.631121\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 6.206902\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 6.443181\n",
      "Epoch: 382/2000... Step: 12200... Loss: 10.378190... Val Loss: 6.573735\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 5.632174\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 3.959186\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 4.566807\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 4.903017\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 4.906653\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 5.725300\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 5.477752\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 5.464982\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 5.353320\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 5.104933\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 4.952992\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 5.099029\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 4.943245\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 5.721125\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 5.737768\n",
      "Epoch: 388/2000... Step: 12400... Loss: 6.097319... Val Loss: 5.964155\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 4.819507\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 3.515798\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 4.048400\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 4.730563\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 4.608642\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 5.287211\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 5.046836\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 5.164001\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 5.073661\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 4.747365\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 4.594357\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 4.730288\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 4.581925\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 5.351632\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 5.364607\n",
      "Epoch: 394/2000... Step: 12600... Loss: 3.794972... Val Loss: 5.486454\n",
      "Validation loss decreased (5.544270 --> 5.486454).  Saving model ...\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 6.917023\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 4.723257\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 4.895142\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 5.868865\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 5.680604\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 6.611750\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 6.372172\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 6.658534\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 6.477405\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 6.058944\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 5.869409\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 5.933369\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 5.770693\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 6.420847\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 6.466894\n",
      "Epoch: 400/2000... Step: 12800... Loss: 2.437214... Val Loss: 6.589629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 7.509754\n",
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 7.236155\n",
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 7.565908\n",
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 7.985934\n",
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 7.826970\n",
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 9.235878\n",
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 8.589409\n",
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 8.194443\n",
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 7.960668\n",
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 7.644518\n",
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 7.361289\n",
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 7.572826\n",
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 7.348466\n",
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 8.289262\n",
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 8.273293\n",
      "Epoch: 407/2000... Step: 13000... Loss: 8.469484... Val Loss: 8.470417\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 6.268977\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 4.516711\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 5.130831\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 5.574251\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 5.359798\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 5.906803\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 5.547984\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 5.522454\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 5.346507\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 4.986164\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 4.799244\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 4.912385\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 4.755753\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 5.467610\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 5.471675\n",
      "Epoch: 413/2000... Step: 13200... Loss: 4.258590... Val Loss: 5.559376\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 5.097828\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 3.816896\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 3.980855\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 5.386592\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 5.301633\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 6.052949\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 5.766773\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 5.840485\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 5.576804\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 5.166868\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 4.996863\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 5.115602\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 4.946053\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 5.644667\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 5.629881\n",
      "Epoch: 419/2000... Step: 13400... Loss: 5.546502... Val Loss: 5.733533\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 4.819677\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 3.462477\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 3.638669\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 4.470323\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 4.287921\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 4.948018\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 4.615777\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 4.808014\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 4.624087\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 4.306591\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 4.152833\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 4.234336\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 4.079637\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 4.799205\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 4.826734\n",
      "Epoch: 425/2000... Step: 13600... Loss: 5.058334... Val Loss: 5.043491\n",
      "Validation loss decreased (5.486454 --> 5.043491).  Saving model ...\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 6.211468\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 4.998679\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 5.652953\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 6.294553\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 6.305953\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 7.049092\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 6.580714\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 6.235579\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 6.078282\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 5.757624\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 5.480276\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 5.731748\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 5.507166\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 6.249851\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 6.297843\n",
      "Epoch: 432/2000... Step: 13800... Loss: 3.644161... Val Loss: 6.469977\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 4.796685\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 3.430740\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 3.821323\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 4.723925\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 4.577312\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 5.417144\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 5.076269\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 5.096576\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 4.875258\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 4.530984\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 4.346556\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 4.490126\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 4.327264\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 5.098449\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 5.181609\n",
      "Epoch: 438/2000... Step: 14000... Loss: 3.895640... Val Loss: 5.338085\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 4.230165\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 3.097656\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 3.418368\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 4.133882\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 4.051729\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 4.877472\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 4.535385\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 4.705273\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 4.532877\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 4.198512\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 4.067595\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 4.179295\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 4.013776\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 4.748241\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 4.773945\n",
      "Epoch: 444/2000... Step: 14200... Loss: 3.885540... Val Loss: 4.937858\n",
      "Validation loss decreased (5.043491 --> 4.937858).  Saving model ...\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 5.825954\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 4.000383\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 4.255556\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 4.626049\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 4.469446\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 4.986897\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 4.736447\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 4.980220\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 4.875572\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 4.551673\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 4.409589\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 4.569105\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 4.410022\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 5.082092\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 5.083547\n",
      "Epoch: 450/2000... Step: 14400... Loss: 1.723297... Val Loss: 5.279505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 8.687585\n",
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 7.923843\n",
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 8.089191\n",
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 8.673348\n",
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 8.572717\n",
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 9.427940\n",
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 8.897593\n",
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 8.621037\n",
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 8.374860\n",
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 8.044864\n",
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 7.784018\n",
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 8.016617\n",
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 7.819348\n",
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 8.550651\n",
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 8.640456\n",
      "Epoch: 457/2000... Step: 14600... Loss: 6.081038... Val Loss: 8.822651\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 3.887136\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 3.154254\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 3.580063\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 4.642635\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 4.540451\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 5.747107\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 5.301051\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 5.295970\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 5.022925\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 4.692739\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 4.521820\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 4.656403\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 4.569552\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 5.297374\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 5.357692\n",
      "Epoch: 463/2000... Step: 14800... Loss: 3.148059... Val Loss: 5.781643\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 4.602403\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 3.533516\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 3.698665\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 4.339994\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 4.217692\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 5.038070\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 4.729187\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 4.752328\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 4.598012\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 4.311510\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 4.170472\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 4.328560\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 4.146345\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 4.888017\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 4.955166\n",
      "Epoch: 469/2000... Step: 15000... Loss: 6.238837... Val Loss: 5.158732\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 3.647195\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 3.064880\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 3.304036\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 3.847509\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 3.719450\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 4.625259\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 4.270211\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 4.272895\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 4.141101\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 3.897779\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 3.792093\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 3.876708\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 3.751641\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 4.635169\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 4.665736\n",
      "Epoch: 475/2000... Step: 15200... Loss: 0.738244... Val Loss: 4.854747\n",
      "Validation loss decreased (4.937858 --> 4.854747).  Saving model ...\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 3.600217\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 2.973278\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 3.340841\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 3.842098\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 3.875115\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 4.630071\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 4.299078\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 4.186073\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 4.037666\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 3.779718\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 3.659788\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 3.819150\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 3.683520\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 4.530060\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 4.567394\n",
      "Epoch: 482/2000... Step: 15400... Loss: 6.553493... Val Loss: 4.817203\n",
      "Validation loss decreased (4.854747 --> 4.817203).  Saving model ...\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 4.444715\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 3.580723\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 3.978351\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 4.345202\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 4.322923\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 5.271876\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 5.062826\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 4.855475\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 4.732573\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 4.584261\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 4.455311\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 4.624849\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 4.418189\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 5.260853\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 5.235290\n",
      "Epoch: 488/2000... Step: 15600... Loss: 5.104742... Val Loss: 5.470538\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 3.412599\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 2.875188\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 3.167256\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 3.837858\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 3.846706\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 4.702610\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 4.376171\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 4.303135\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 4.195017\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 3.893980\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 3.795639\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 3.967752\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 3.824615\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 4.656324\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 4.685099\n",
      "Epoch: 494/2000... Step: 15800... Loss: 5.386911... Val Loss: 4.966980\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 5.511724\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 5.424011\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 5.204988\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 5.605707\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 5.235773\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 5.757986\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 5.440052\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 5.259628\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 5.054447\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 4.876146\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 4.776014\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 4.914842\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 4.765513\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 5.903803\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 5.927882\n",
      "Epoch: 500/2000... Step: 16000... Loss: 2.010466... Val Loss: 6.121971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 5.822745\n",
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 4.542908\n",
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 4.953272\n",
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 5.411101\n",
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 5.283418\n",
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 5.989133\n",
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 5.480060\n",
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 5.200876\n",
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 5.020788\n",
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 4.820095\n",
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 4.639272\n",
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 4.822067\n",
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 4.640699\n",
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 5.373264\n",
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 5.390021\n",
      "Epoch: 507/2000... Step: 16200... Loss: 3.064079... Val Loss: 5.516525\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 4.794816\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 3.527095\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 3.846652\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 4.301176\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 4.171064\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 4.490633\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 4.228600\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 4.387022\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 4.239523\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 3.989977\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 3.904648\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 4.044500\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 3.881487\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 4.664879\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 4.816348\n",
      "Epoch: 513/2000... Step: 16400... Loss: 4.799236... Val Loss: 5.118516\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 4.571215\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 3.265570\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 3.586318\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 4.094806\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 4.001675\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 4.707896\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 4.480407\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 4.651763\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 4.544288\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 4.245666\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 4.172875\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 4.274535\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 4.154088\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 4.875473\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 4.861701\n",
      "Epoch: 519/2000... Step: 16600... Loss: 7.836946... Val Loss: 5.059579\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 5.551534\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 3.798475\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 4.010866\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 4.523214\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 4.377839\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 5.235365\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 5.005096\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 4.829303\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 4.670889\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 4.492615\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 4.352762\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 4.461254\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 4.273512\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 4.969238\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 4.899202\n",
      "Epoch: 525/2000... Step: 16800... Loss: 1.974937... Val Loss: 4.991451\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 6.534787\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 5.493196\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 5.883525\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 6.347153\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 6.218743\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 7.345176\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 6.853185\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 6.406952\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 6.152990\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 5.954090\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 5.666570\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 5.843546\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 5.654363\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 6.472706\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 6.465285\n",
      "Epoch: 532/2000... Step: 17000... Loss: 4.686301... Val Loss: 6.641582\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 5.021542\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 3.861013\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 4.424968\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 4.857795\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 4.729625\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 5.488001\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 5.327596\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 5.189503\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 5.080284\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 4.926822\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 4.812780\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 4.962534\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 4.779593\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 5.648563\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 5.662788\n",
      "Epoch: 538/2000... Step: 17200... Loss: 5.432800... Val Loss: 5.858193\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 5.259697\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 3.515972\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 3.798276\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 4.436881\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 4.197975\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 4.801227\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 4.545881\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 4.821166\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 4.641615\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 4.302762\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 4.177727\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 4.312730\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 4.142726\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 4.789296\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 4.991012\n",
      "Epoch: 544/2000... Step: 17400... Loss: 4.991956... Val Loss: 5.261547\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 4.846930\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 3.355933\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 3.709507\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 4.125232\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 4.054718\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 4.613666\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 4.492691\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 4.624504\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 4.512073\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 4.364435\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 4.373251\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 4.536795\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 4.347280\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 5.005548\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 5.144206\n",
      "Epoch: 550/2000... Step: 17600... Loss: 3.403653... Val Loss: 5.483629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 7.359720\n",
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 6.988101\n",
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 6.883606\n",
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 7.801647\n",
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 7.594713\n",
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 8.551545\n",
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 7.964305\n",
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 7.782087\n",
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 7.529717\n",
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 7.233967\n",
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 7.062284\n",
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 7.197962\n",
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 7.110151\n",
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 7.868748\n",
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 7.978258\n",
      "Epoch: 557/2000... Step: 17800... Loss: 7.035603... Val Loss: 8.196510\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 4.839854\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 3.440293\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 3.631948\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 4.070821\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 4.028458\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 4.944437\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 4.616718\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 4.616758\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 4.460965\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 4.210564\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 4.107514\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 4.239129\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 4.084284\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 4.818384\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 4.771040\n",
      "Epoch: 563/2000... Step: 18000... Loss: 5.548865... Val Loss: 4.917607\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 3.648657\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 2.972277\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 3.075952\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 3.962318\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 3.940391\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 4.728443\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 4.327032\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 4.329302\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 4.126626\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 3.840627\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 3.725389\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 3.823717\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 3.717106\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 4.487393\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 4.502880\n",
      "Epoch: 569/2000... Step: 18200... Loss: 3.566336... Val Loss: 4.749131\n",
      "Validation loss decreased (4.817203 --> 4.749131).  Saving model ...\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 5.593298\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 4.000825\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 4.397580\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 4.840766\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 4.746201\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 5.319231\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 5.120242\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 5.499062\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 5.428189\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 5.146832\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 5.138277\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 5.252365\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 5.113475\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 5.824321\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 5.954751\n",
      "Epoch: 575/2000... Step: 18400... Loss: 10.165364... Val Loss: 6.170928\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 9.729685\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 9.625998\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 9.390759\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 10.070449\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 9.728053\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 10.710330\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 10.488897\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 10.305068\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 10.060134\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 9.874932\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 9.727285\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 9.928699\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 9.783190\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 10.566273\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 10.622862\n",
      "Epoch: 582/2000... Step: 18600... Loss: 7.706347... Val Loss: 11.061248\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 4.943657\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 3.368158\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 3.669991\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 4.291528\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 4.213519\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 5.256173\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 4.830405\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 4.884839\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 4.638632\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 4.292945\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 4.120215\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 4.243493\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 4.071149\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 4.748627\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 4.788167\n",
      "Epoch: 588/2000... Step: 18800... Loss: 3.194994... Val Loss: 4.913187\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 4.577148\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 3.292043\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 3.784432\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 4.020212\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 4.051139\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 4.601919\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 4.311924\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 4.258236\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 4.153061\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 3.902845\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 3.824267\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 4.000222\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 3.831122\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 4.576318\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 4.524974\n",
      "Epoch: 594/2000... Step: 19000... Loss: 4.456389... Val Loss: 4.701855\n",
      "Validation loss decreased (4.749131 --> 4.701855).  Saving model ...\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 4.782723\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 3.517780\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 3.576880\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 3.934991\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 3.742246\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 4.552872\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 4.269020\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 4.498645\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 4.368268\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 4.160514\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 4.114521\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 4.303022\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 4.187677\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 4.866133\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 4.993198\n",
      "Epoch: 600/2000... Step: 19200... Loss: 1.425014... Val Loss: 5.532081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 5.291804\n",
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 4.342574\n",
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 4.437302\n",
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 4.995416\n",
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 4.911935\n",
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 5.898828\n",
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 5.404521\n",
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 5.296633\n",
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 5.096863\n",
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 4.808902\n",
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 4.683511\n",
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 4.831724\n",
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 4.716663\n",
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 5.427550\n",
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 5.490864\n",
      "Epoch: 607/2000... Step: 19400... Loss: 4.100084... Val Loss: 5.725174\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 5.036929\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 3.767076\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 3.963076\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 4.735110\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 4.688736\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 5.477177\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 5.086827\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 5.102493\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 4.904494\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 4.570041\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 4.399158\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 4.555675\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 4.423063\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 5.082097\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 5.151330\n",
      "Epoch: 613/2000... Step: 19600... Loss: 2.340087... Val Loss: 5.385208\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 4.647987\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 3.661170\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 3.645569\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 4.021794\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 3.947902\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 4.606656\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 4.304367\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 4.428678\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 4.273655\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 3.972807\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 3.889813\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 3.996837\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 3.908439\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 4.642365\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 4.683193\n",
      "Epoch: 619/2000... Step: 19800... Loss: 4.686992... Val Loss: 4.977547\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 4.003093\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 3.117074\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 3.713656\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 4.454896\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 4.455577\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 5.650177\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 5.199731\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 5.165769\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 4.894229\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 4.527665\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 4.373751\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 4.505443\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 4.379311\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 5.022733\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 5.146748\n",
      "Epoch: 625/2000... Step: 20000... Loss: 29.127895... Val Loss: 5.375066\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 4.418367\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 3.434811\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 3.708089\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 4.053947\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 3.991803\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 5.006068\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 4.611540\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 4.463299\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 4.284167\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 4.059310\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 3.923861\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 4.044886\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 3.907333\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 4.697705\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 4.641187\n",
      "Epoch: 632/2000... Step: 20200... Loss: 3.555370... Val Loss: 4.798621\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 4.760597\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 3.286583\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 3.440953\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 4.184305\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 4.024636\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 4.834437\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 4.503434\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 4.564293\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 4.355751\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 4.032117\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 3.926729\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 4.027872\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 3.915260\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 4.583657\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 4.609799\n",
      "Epoch: 638/2000... Step: 20400... Loss: 3.029778... Val Loss: 4.812431\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 3.788024\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 3.016346\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 3.197592\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 3.789315\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 3.852893\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 4.947013\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 4.502866\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 4.488687\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 4.330354\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 4.022632\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 3.902542\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 4.045654\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 3.923427\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 4.655774\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 4.694202\n",
      "Epoch: 644/2000... Step: 20600... Loss: 2.575044... Val Loss: 4.951441\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 4.008782\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 2.859913\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 3.025440\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 3.648285\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 3.631237\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 4.378608\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 4.103748\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 4.317321\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 4.186523\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 3.860759\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 3.797710\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 3.897736\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 3.763867\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 4.475873\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 4.479660\n",
      "Epoch: 650/2000... Step: 20800... Loss: 1.273006... Val Loss: 4.798177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 6.550604\n",
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 5.279285\n",
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 5.217004\n",
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 5.594659\n",
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 5.283033\n",
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 6.044566\n",
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 5.774250\n",
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 5.670285\n",
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 5.461788\n",
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 5.312874\n",
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 5.179718\n",
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 5.281536\n",
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 5.155424\n",
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 5.829811\n",
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 5.884056\n",
      "Epoch: 657/2000... Step: 21000... Loss: 4.465413... Val Loss: 6.073493\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 6.159717\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 5.422591\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 5.459940\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 5.837062\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 5.598872\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 6.471819\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 6.013626\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 5.794256\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 5.525145\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 5.323061\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 5.173331\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 5.258332\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 5.184444\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 6.061695\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 6.028229\n",
      "Epoch: 663/2000... Step: 21200... Loss: 4.524830... Val Loss: 6.238631\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 4.143734\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 3.073021\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 3.440747\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 4.145271\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 4.199111\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 5.246822\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 4.977803\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 5.233832\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 5.094869\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 4.779650\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 4.692461\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 4.792544\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 4.645732\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 5.310378\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 5.326600\n",
      "Epoch: 669/2000... Step: 21400... Loss: 3.962955... Val Loss: 5.515094\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 4.376478\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 3.161062\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 3.187760\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 3.631459\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 3.495093\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 4.440123\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 4.183584\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 4.265759\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 4.123496\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 3.873343\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 3.794219\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 3.892633\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 3.751262\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 4.470915\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 4.529981\n",
      "Epoch: 675/2000... Step: 21600... Loss: 3.771834... Val Loss: 4.815885\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 5.178931\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 3.907963\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 4.166695\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 4.404200\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 4.295850\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 4.958673\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 4.751878\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 4.644951\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 4.542040\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 4.387827\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 4.309611\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 4.449670\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 4.284335\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 4.989193\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 4.950331\n",
      "Epoch: 682/2000... Step: 21800... Loss: 3.607910... Val Loss: 5.093765\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 5.242606\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 3.707059\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 3.932792\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 4.332849\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 4.326243\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 4.924855\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 4.668536\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 4.602286\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 4.443000\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 4.216897\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 4.101868\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 4.233614\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 4.114719\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 4.755607\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 4.754246\n",
      "Epoch: 688/2000... Step: 22000... Loss: 2.631645... Val Loss: 5.027076\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 4.922794\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 3.236304\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 3.302331\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 3.509278\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 3.482784\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 3.887193\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 3.680418\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 3.879134\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 3.758642\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 3.503514\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 3.443781\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 3.592146\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 3.448358\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 4.099520\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 4.152987\n",
      "Epoch: 694/2000... Step: 22200... Loss: 4.174513... Val Loss: 4.461572\n",
      "Validation loss decreased (4.701855 --> 4.461572).  Saving model ...\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 4.940662\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 3.365729\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 3.358440\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 3.630422\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 3.539727\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 4.230206\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 3.985838\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 4.208195\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 4.099905\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 3.889874\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 3.840546\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 3.933199\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 3.802591\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 4.485707\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 4.485335\n",
      "Epoch: 700/2000... Step: 22400... Loss: 2.094883... Val Loss: 4.748069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 4.821787\n",
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 3.222967\n",
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 3.452239\n",
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 3.871682\n",
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 3.825149\n",
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 4.461049\n",
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 4.241753\n",
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 4.244695\n",
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 4.105849\n",
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 3.899342\n",
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 3.776339\n",
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 3.926905\n",
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 3.794187\n",
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 4.484550\n",
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 4.497040\n",
      "Epoch: 707/2000... Step: 22600... Loss: 4.340904... Val Loss: 4.823535\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 4.714475\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 3.311259\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 3.227692\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 3.507994\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 3.380659\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 3.938743\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 3.672750\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 3.725937\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 3.625709\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 3.439579\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 3.370183\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 3.479111\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 3.366189\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 4.072259\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 4.065990\n",
      "Epoch: 713/2000... Step: 22800... Loss: 3.072670... Val Loss: 4.344639\n",
      "Validation loss decreased (4.461572 --> 4.344639).  Saving model ...\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 4.155934\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 2.831843\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 3.158380\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 3.505803\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 3.343351\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 4.232965\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 3.943656\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 4.034251\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 3.893047\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 3.671589\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 3.600384\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 3.697168\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 3.608631\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 4.414269\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 4.474636\n",
      "Epoch: 719/2000... Step: 23000... Loss: 3.584336... Val Loss: 4.876482\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 3.918504\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 2.490227\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 2.625474\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 2.952693\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 2.807792\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 3.456612\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 3.226345\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 3.400085\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 3.341580\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 3.151423\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 3.098513\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 3.196070\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 3.112772\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 3.861985\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 3.926158\n",
      "Epoch: 725/2000... Step: 23200... Loss: 0.715979... Val Loss: 4.257443\n",
      "Validation loss decreased (4.344639 --> 4.257443).  Saving model ...\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 4.553640\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 3.111810\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 3.098417\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 3.384164\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 3.233201\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 3.939228\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 3.615761\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 3.731948\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 3.633061\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 3.451757\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 3.380031\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 3.421376\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 3.334563\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 4.074038\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 4.068277\n",
      "Epoch: 732/2000... Step: 23400... Loss: 2.492530... Val Loss: 4.251275\n",
      "Validation loss decreased (4.257443 --> 4.251275).  Saving model ...\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 4.633067\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 3.135282\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 3.165098\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 3.372491\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 3.215546\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 4.208404\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 3.873243\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 3.986516\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 3.810609\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 3.555118\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 3.455873\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 3.512873\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 3.396069\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 4.170123\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 4.170354\n",
      "Epoch: 738/2000... Step: 23600... Loss: 1.952261... Val Loss: 4.452683\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 4.150030\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 2.808699\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 3.013734\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 3.435504\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 3.504817\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 4.058742\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 3.789042\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 4.110552\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 4.002628\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 3.699245\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 3.635324\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 3.768518\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 3.637256\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 4.310535\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 4.335591\n",
      "Epoch: 744/2000... Step: 23800... Loss: 2.175956... Val Loss: 4.571525\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 3.898881\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 2.496155\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 2.635711\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 3.036976\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 2.955204\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 3.725452\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 3.451359\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 3.674853\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 3.527669\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 3.269469\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 3.194463\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 3.301631\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 3.198659\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 3.926111\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 3.979885\n",
      "Epoch: 750/2000... Step: 24000... Loss: 1.578695... Val Loss: 4.260369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 7.702932\n",
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 5.942956\n",
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 5.879688\n",
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 6.583072\n",
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 6.477188\n",
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 7.370298\n",
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 7.160391\n",
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 7.045560\n",
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 6.871651\n",
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 6.646489\n",
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 6.579517\n",
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 6.682335\n",
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 6.568952\n",
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 7.134080\n",
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 7.143967\n",
      "Epoch: 757/2000... Step: 24200... Loss: 5.320041... Val Loss: 7.384551\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 5.258517\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 3.277264\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 3.694017\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 4.108439\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 4.005433\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 4.877505\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 4.660615\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 4.599739\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 4.489924\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 4.204487\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 4.050338\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 4.206510\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 4.035230\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 4.701637\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 4.725313\n",
      "Epoch: 763/2000... Step: 24400... Loss: 1.739455... Val Loss: 4.949145\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 3.624972\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 2.449635\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 2.723768\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 3.142289\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 3.150460\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 3.824454\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 3.582601\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 3.767556\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 3.651441\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 3.390495\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 3.309483\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 3.423827\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 3.345792\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 4.084926\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 4.111404\n",
      "Epoch: 769/2000... Step: 24600... Loss: 3.229141... Val Loss: 4.481008\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 4.590527\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 3.011839\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 3.211803\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 3.448359\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 3.394956\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 4.589932\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 4.285808\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 4.494694\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 4.339659\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 4.018912\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 3.928889\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 3.992148\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 3.875200\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 4.560612\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 4.601879\n",
      "Epoch: 775/2000... Step: 24800... Loss: 1.996083... Val Loss: 4.867829\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 6.312457\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 3.679014\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 3.625270\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 4.102769\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 3.867124\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 4.384630\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 4.274701\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 4.429777\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 4.320921\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 4.087486\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 3.936458\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 4.032301\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 3.841407\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 4.360330\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 4.420108\n",
      "Epoch: 782/2000... Step: 25000... Loss: 2.588706... Val Loss: 4.583215\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 4.569421\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 2.763258\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 3.032325\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 3.334559\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 3.250683\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 4.005976\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 3.827870\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 3.865904\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 3.730659\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 3.520405\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 3.394536\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 3.452216\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 3.327853\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 3.992404\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 4.028685\n",
      "Epoch: 788/2000... Step: 25200... Loss: 2.646380... Val Loss: 4.196761\n",
      "Validation loss decreased (4.251275 --> 4.196761).  Saving model ...\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 3.915592\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 2.646135\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 2.894890\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 3.267280\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 3.229797\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 3.930843\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 3.706523\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 3.774601\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 3.663551\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 3.453131\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 3.394961\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 3.515634\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 3.433991\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 4.166630\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 4.212949\n",
      "Epoch: 794/2000... Step: 25400... Loss: 3.904519... Val Loss: 4.597093\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 4.659548\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 2.936233\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 2.849534\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 3.111561\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 2.966333\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 3.475142\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 3.298929\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 3.596314\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 3.483488\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 3.269109\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 3.295897\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 3.345977\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 3.226023\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 3.859128\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 3.847128\n",
      "Epoch: 800/2000... Step: 25600... Loss: 0.440592... Val Loss: 4.092719\n",
      "Validation loss decreased (4.196761 --> 4.092719).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 5.049743\n",
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 3.653914\n",
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 3.564104\n",
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 3.583624\n",
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 3.432362\n",
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 4.279948\n",
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 4.094867\n",
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 4.151811\n",
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 4.024253\n",
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 3.853980\n",
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 3.829087\n",
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 3.855512\n",
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 3.712067\n",
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 4.470343\n",
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 4.420348\n",
      "Epoch: 807/2000... Step: 25800... Loss: 3.063945... Val Loss: 4.624826\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 4.113093\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 2.762304\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 3.158128\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 3.555311\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 3.510602\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 4.564381\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 4.215444\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 4.237803\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 4.057788\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 3.755922\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 3.659689\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 3.768918\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 3.641145\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 4.360578\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 4.422379\n",
      "Epoch: 813/2000... Step: 26000... Loss: 1.685546... Val Loss: 4.595655\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 4.395661\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 2.822442\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 2.866758\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 3.429838\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 3.426603\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 3.927908\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 3.640206\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 3.861594\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 3.757000\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 3.508543\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 3.398463\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 3.494977\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 3.385097\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 3.984752\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 3.989603\n",
      "Epoch: 819/2000... Step: 26200... Loss: 2.466838... Val Loss: 4.150687\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 4.234797\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 2.679191\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 2.818512\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 3.050398\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 2.945158\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 3.621132\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 3.427862\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 3.681262\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 3.588737\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 3.387954\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 3.359114\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 3.397068\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 3.278855\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 3.941755\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 3.938907\n",
      "Epoch: 825/2000... Step: 26400... Loss: 3.281663... Val Loss: 4.147047\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 4.723524\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 3.197798\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 3.108746\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 3.436363\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 3.289343\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 4.203857\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 3.878621\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 3.937291\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 3.841637\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 3.692331\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 3.571765\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 3.622137\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 3.511410\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 4.176225\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 4.143955\n",
      "Epoch: 832/2000... Step: 26600... Loss: 7.949517... Val Loss: 4.285544\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 4.113368\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 2.803407\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 2.952124\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 3.207441\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 3.203016\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 3.754965\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 3.514432\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 3.592837\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 3.460889\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 3.246619\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 3.169989\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 3.256673\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 3.144480\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 3.849256\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 3.810424\n",
      "Epoch: 838/2000... Step: 26800... Loss: 1.656582... Val Loss: 4.009478\n",
      "Validation loss decreased (4.092719 --> 4.009478).  Saving model ...\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 4.768008\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 3.024675\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 3.216324\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 3.401826\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 3.419661\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 4.009081\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 3.775134\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 3.766890\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 3.716937\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 3.538694\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 3.456926\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 3.540491\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 3.412568\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 4.125513\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 4.081419\n",
      "Epoch: 844/2000... Step: 27000... Loss: 2.286877... Val Loss: 4.214251\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 4.503063\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 2.787400\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 2.710468\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 2.951807\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 2.868725\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 3.381408\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 3.222059\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 3.461456\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 3.339634\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 3.125725\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 3.100718\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 3.157631\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 3.050228\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 3.698750\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 3.708530\n",
      "Epoch: 850/2000... Step: 27200... Loss: 3.282326... Val Loss: 3.943565\n",
      "Validation loss decreased (4.009478 --> 3.943565).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 4.061287\n",
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 2.989347\n",
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 2.982050\n",
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 3.305250\n",
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 3.288973\n",
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 3.666264\n",
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 3.397681\n",
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 3.412575\n",
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 3.326819\n",
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 3.147740\n",
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 3.070458\n",
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 3.186604\n",
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 3.120584\n",
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 3.968264\n",
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 3.986588\n",
      "Epoch: 857/2000... Step: 27400... Loss: 1.878682... Val Loss: 4.309182\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 4.791907\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 3.110135\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 3.066020\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 3.395152\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 3.358683\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 3.938072\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 3.672070\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 3.616869\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 3.505909\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 3.342284\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 3.235931\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 3.327726\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 3.227781\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 4.008830\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 3.955678\n",
      "Epoch: 863/2000... Step: 27600... Loss: 2.382051... Val Loss: 4.171464\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 4.043120\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 2.459239\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 2.682216\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 3.054152\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 3.032827\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 3.496227\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 3.262541\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 3.301486\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 3.238483\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 3.031598\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 2.942799\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 3.063364\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 2.945965\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 3.634547\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 3.676368\n",
      "Epoch: 869/2000... Step: 27800... Loss: 3.763385... Val Loss: 3.906815\n",
      "Validation loss decreased (3.943565 --> 3.906815).  Saving model ...\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 3.730778\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 2.226861\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 2.366904\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 2.862209\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 2.775913\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 3.529691\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 3.247986\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 3.354230\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 3.263854\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 3.041106\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 2.970770\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 3.070852\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 2.992233\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 3.684033\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 3.708578\n",
      "Epoch: 875/2000... Step: 28000... Loss: 1.337471... Val Loss: 4.024085\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 4.930245\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 3.630071\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 3.386646\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 3.632815\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 3.493872\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 4.031703\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 3.795729\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 3.899311\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 3.799593\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 3.614206\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 3.537813\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 3.624233\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 3.526013\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 4.302832\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 4.309614\n",
      "Epoch: 882/2000... Step: 28200... Loss: 4.486750... Val Loss: 4.591328\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 4.631578\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 2.912111\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 2.797899\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 3.044800\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 2.920768\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 3.596480\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 3.452485\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 3.578452\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 3.463189\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 3.289200\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 3.212722\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 3.244522\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 3.142920\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 3.723863\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 3.707537\n",
      "Epoch: 888/2000... Step: 28400... Loss: 1.251661... Val Loss: 3.923736\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 4.482869\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 2.903999\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 3.058586\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 3.194067\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 3.180433\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 3.923902\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 3.862465\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 4.083530\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 4.009581\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 3.810603\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 3.798715\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 3.790071\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 3.657185\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 4.332721\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 4.359517\n",
      "Epoch: 894/2000... Step: 28600... Loss: 2.705378... Val Loss: 4.544112\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 4.765411\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 3.225833\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 3.226498\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 3.426363\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 3.420361\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 3.813015\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 3.697889\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 3.868505\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 3.841573\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 3.660309\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 3.643010\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 3.702660\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 3.587694\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 4.298620\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 4.354155\n",
      "Epoch: 900/2000... Step: 28800... Loss: 2.911976... Val Loss: 4.666125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 5.090771\n",
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 3.167114\n",
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 3.076070\n",
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 3.226715\n",
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 3.089849\n",
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 3.142769\n",
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 2.941905\n",
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 3.002980\n",
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 2.935289\n",
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 2.861669\n",
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 2.800522\n",
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 2.858249\n",
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 2.803765\n",
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 3.473382\n",
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 3.464074\n",
      "Epoch: 907/2000... Step: 29000... Loss: 2.031256... Val Loss: 3.587333\n",
      "Validation loss decreased (3.906815 --> 3.587333).  Saving model ...\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 4.619925\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 2.986208\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 2.951322\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 3.233091\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 3.254136\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 3.638014\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 3.397510\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 3.383002\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 3.311232\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 3.129759\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 3.043260\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 3.159692\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 3.052694\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 3.732520\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 3.712194\n",
      "Epoch: 913/2000... Step: 29200... Loss: 1.947605... Val Loss: 3.885830\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 4.704480\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 3.100419\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 3.244070\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 3.423299\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 3.448795\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 3.965872\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 3.754999\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 3.883447\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 3.818972\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 3.637210\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 3.589727\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 3.679642\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 3.535851\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 4.142507\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 4.111962\n",
      "Epoch: 919/2000... Step: 29400... Loss: 2.864461... Val Loss: 4.190213\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 4.652919\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 3.180695\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 3.382375\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 3.503795\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 3.513689\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 3.923068\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 3.875646\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 4.152222\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 4.094983\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 3.892154\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 3.899401\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 3.952072\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 3.835301\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 4.502702\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 4.554570\n",
      "Epoch: 925/2000... Step: 29600... Loss: 2.116581... Val Loss: 4.794431\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 6.783082\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 4.820160\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 4.455938\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 4.615682\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 4.465230\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 4.853089\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 4.713901\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 4.787813\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 4.730596\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 4.625997\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 4.538228\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 4.654696\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 4.626029\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 5.224181\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 5.163791\n",
      "Epoch: 932/2000... Step: 29800... Loss: 2.977008... Val Loss: 5.418706\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 5.005070\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 3.329930\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 3.138772\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 3.464088\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 3.445601\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 3.893279\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 3.686746\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 3.589523\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 3.487453\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 3.380387\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 3.309719\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 3.376091\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 3.313011\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 3.958345\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 3.882832\n",
      "Epoch: 938/2000... Step: 30000... Loss: 2.212311... Val Loss: 4.019410\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 3.588459\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 2.343192\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 2.680031\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 2.730713\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 2.733260\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 3.126509\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 3.059364\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 3.292647\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 3.181605\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 3.020886\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 3.043336\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 3.078041\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 2.984449\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 3.759148\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 3.772780\n",
      "Epoch: 944/2000... Step: 30200... Loss: 2.178969... Val Loss: 4.001937\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 4.099822\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 2.549732\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 2.941503\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 3.206966\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 3.122558\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 3.369925\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 3.298604\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 3.538210\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 3.527228\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 3.343238\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 3.329855\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 3.426257\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 3.327613\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 3.951725\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 3.977689\n",
      "Epoch: 950/2000... Step: 30400... Loss: 4.228241... Val Loss: 4.197734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 5.694009\n",
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 4.364589\n",
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 4.134852\n",
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 4.250238\n",
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 4.086120\n",
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 4.491442\n",
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 4.215926\n",
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 4.185836\n",
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 4.132776\n",
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 4.055914\n",
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 3.988761\n",
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 4.057343\n",
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 4.051179\n",
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 4.830284\n",
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 4.778037\n",
      "Epoch: 957/2000... Step: 30600... Loss: 4.799028... Val Loss: 4.936890\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 4.029826\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 2.557433\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 2.774348\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 3.399348\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 3.480902\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 3.755153\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 3.620138\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 3.513563\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 3.471695\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 3.327107\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 3.238204\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 3.357050\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 3.287960\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 3.988845\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 3.940335\n",
      "Epoch: 963/2000... Step: 30800... Loss: 1.330819... Val Loss: 4.131916\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 4.352030\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 2.909267\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 3.093123\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 3.321638\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 3.308769\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 3.672363\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 3.517580\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 3.732164\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 3.661135\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 3.461003\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 3.471025\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 3.510821\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 3.402021\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 4.104019\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 4.131463\n",
      "Epoch: 969/2000... Step: 31000... Loss: 1.862045... Val Loss: 4.287711\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 4.213727\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 2.697346\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 2.542821\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 2.842120\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 2.826938\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 3.197344\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 3.058644\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 3.216406\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 3.146309\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 2.952065\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 2.948956\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 3.007427\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 2.911980\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 3.507761\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 3.486336\n",
      "Epoch: 975/2000... Step: 31200... Loss: 5.039143... Val Loss: 3.661908\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 4.566245\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 2.775281\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 2.916507\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 3.250748\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 3.279638\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 3.963741\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 3.753022\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 3.768366\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 3.691076\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 3.457457\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 3.364178\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 3.412405\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 3.284102\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 3.902091\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 3.944044\n",
      "Epoch: 982/2000... Step: 31400... Loss: 2.908750... Val Loss: 4.133665\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 4.607165\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 3.068384\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 2.884530\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 3.157011\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 3.289862\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 3.653566\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 3.374889\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 3.335078\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 3.238947\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 3.056715\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 2.962983\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 3.058877\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 2.983171\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 3.687432\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 3.632862\n",
      "Epoch: 988/2000... Step: 31600... Loss: 2.057822... Val Loss: 3.781706\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 6.001119\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 4.049786\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 4.132567\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 4.357489\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 4.277192\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 4.646655\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 4.496975\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 4.840214\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 4.788674\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 4.524665\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 4.504552\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 4.535554\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 4.389408\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 4.957801\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 5.047968\n",
      "Epoch: 994/2000... Step: 31800... Loss: 2.626828... Val Loss: 5.276717\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 4.326504\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 2.783715\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 3.086990\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 3.478212\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 3.280178\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 3.526599\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 3.349527\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 3.625446\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 3.545836\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 3.353984\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 3.332098\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 3.444531\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 3.395381\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 4.032152\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 4.097502\n",
      "Epoch: 1000/2000... Step: 32000... Loss: 0.293005... Val Loss: 4.441233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 6.051786\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 3.963417\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 3.601394\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 3.744745\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 3.771377\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 3.899761\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 3.778000\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 3.707257\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 3.663891\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 3.601661\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 3.517498\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 3.593895\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 3.527896\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 4.140187\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 4.045253\n",
      "Epoch: 1007/2000... Step: 32200... Loss: 2.368666... Val Loss: 4.161760\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 4.347991\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 2.663679\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 2.674115\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 2.986307\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 2.983354\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 3.274868\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 3.144535\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 3.283242\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 3.221279\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 3.050346\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 3.022125\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 3.070692\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 2.961704\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 3.601349\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 3.606610\n",
      "Epoch: 1013/2000... Step: 32400... Loss: 1.496324... Val Loss: 3.753587\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 3.952839\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 2.610040\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 2.522362\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 2.907642\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 3.016014\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 3.245465\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 3.051696\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 3.059886\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 3.000200\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 2.844048\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 2.796101\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 2.867877\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 2.815021\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 3.575994\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 3.505308\n",
      "Epoch: 1019/2000... Step: 32600... Loss: 3.485663... Val Loss: 3.700564\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 4.733604\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 2.891171\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 2.742508\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 2.834734\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 2.916671\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 3.024417\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 3.016029\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 3.173033\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 3.121166\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 2.977841\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 3.024687\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 3.099609\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 2.965071\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 3.546922\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 3.524961\n",
      "Epoch: 1025/2000... Step: 32800... Loss: 8.132734... Val Loss: 3.711832\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 4.483891\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 3.196319\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 3.089834\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 3.535044\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 3.693977\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 4.170092\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 3.882617\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 3.872057\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 3.745629\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 3.555318\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 3.455811\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 3.540878\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 3.524231\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 4.174355\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 4.142921\n",
      "Epoch: 1032/2000... Step: 33000... Loss: 2.438662... Val Loss: 4.310684\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 3.467361\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 2.234324\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 2.204133\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 2.481881\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 2.468605\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 2.886992\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 2.678271\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 2.785771\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 2.682843\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 2.525143\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 2.499501\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 2.538690\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 2.478799\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 3.222451\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 3.197688\n",
      "Epoch: 1038/2000... Step: 33200... Loss: 1.519361... Val Loss: 3.439484\n",
      "Validation loss decreased (3.587333 --> 3.439484).  Saving model ...\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 3.714045\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 2.389937\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 2.443491\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 2.931221\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 3.057731\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 3.281120\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 3.115785\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 3.211051\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 3.131067\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 2.977877\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 2.954700\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 3.054895\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 2.946382\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 3.490956\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 3.438839\n",
      "Epoch: 1044/2000... Step: 33400... Loss: 4.735800... Val Loss: 3.599851\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 4.133424\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 2.771589\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 2.728811\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 2.955247\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 3.012502\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 3.488941\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 3.293397\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 3.257438\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 3.172775\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 3.022749\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 2.952346\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 3.022834\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 2.964122\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 3.740107\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 3.681503\n",
      "Epoch: 1050/2000... Step: 33600... Loss: 6.584918... Val Loss: 3.855278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 4.747328\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 3.367197\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 3.258877\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 3.474715\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 3.677669\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 4.119630\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 3.824763\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 3.736877\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 3.634834\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 3.508784\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 3.401500\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 3.486153\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 3.448349\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 4.141828\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 4.058936\n",
      "Epoch: 1057/2000... Step: 33800... Loss: 2.077533... Val Loss: 4.201962\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 4.235993\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 2.838099\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 2.626492\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 2.877204\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 2.974080\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 3.269639\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 3.056092\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 3.038938\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 2.960362\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 2.840829\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 2.807577\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 2.842981\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 2.785927\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 3.479205\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 3.389889\n",
      "Epoch: 1063/2000... Step: 34000... Loss: 2.158202... Val Loss: 3.539611\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 3.958142\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 2.548426\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 2.644852\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 2.999181\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 3.295208\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 3.417605\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 3.198863\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 3.145274\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 3.113476\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 2.975118\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 2.913615\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 3.010826\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 2.901838\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 3.537657\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 3.460663\n",
      "Epoch: 1069/2000... Step: 34200... Loss: 2.774281... Val Loss: 3.604667\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 5.083492\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 3.611256\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 3.531114\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 3.878460\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 3.936871\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 4.200644\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 4.149874\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 4.596348\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 4.473846\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 4.222421\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 4.201718\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 4.178829\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 4.026151\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 4.566981\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 4.641174\n",
      "Epoch: 1075/2000... Step: 34400... Loss: 6.246987... Val Loss: 4.785570\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 6.562467\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 4.834062\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 4.546520\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 4.681708\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 4.792304\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 5.172015\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 4.804385\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 4.646080\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 4.529125\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 4.516671\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 4.369133\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 4.422680\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 4.442458\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 5.150173\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 5.030549\n",
      "Epoch: 1082/2000... Step: 34600... Loss: 2.407151... Val Loss: 5.121453\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 4.128435\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 2.782191\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 2.783399\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 3.057240\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 3.196547\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 3.749355\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 3.481881\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 3.373817\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 3.288969\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 3.131218\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 3.035576\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 3.123243\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 3.063748\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 3.875488\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 3.832275\n",
      "Epoch: 1088/2000... Step: 34800... Loss: 1.295789... Val Loss: 4.042700\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 3.437838\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 2.112215\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 2.210531\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 2.617781\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 2.650124\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 2.921404\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 2.739635\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 2.842161\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 2.787983\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 2.635164\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 2.601754\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 2.665924\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 2.591509\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 3.312543\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 3.312110\n",
      "Epoch: 1094/2000... Step: 35000... Loss: 4.354206... Val Loss: 3.512253\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 4.033220\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 2.551217\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 2.494595\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 2.742280\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 2.870256\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 3.287890\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 3.086690\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 3.310233\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 3.140174\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 2.992114\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 2.929365\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 2.981252\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 2.887452\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 3.485869\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 3.498861\n",
      "Epoch: 1100/2000... Step: 35200... Loss: 49.271317... Val Loss: 3.652221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 4.857456\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 3.192833\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 2.811211\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 2.885897\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 3.009988\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 3.334154\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 3.117173\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 3.123997\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 2.978231\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 2.909167\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 2.862454\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 2.879013\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 2.803958\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 3.382502\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 3.300553\n",
      "Epoch: 1107/2000... Step: 35400... Loss: 1.283634... Val Loss: 3.346577\n",
      "Validation loss decreased (3.439484 --> 3.346577).  Saving model ...\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 3.741867\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 2.518648\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 2.417250\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 2.644800\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 2.736953\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 3.096787\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 2.887464\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 2.986580\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 2.881434\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 2.727424\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 2.697893\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 2.746138\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 2.666841\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 3.300372\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 3.213086\n",
      "Epoch: 1113/2000... Step: 35600... Loss: 1.395578... Val Loss: 3.421154\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 4.146425\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 2.694028\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 2.627580\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 2.769777\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 3.031868\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 3.145987\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 2.990549\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 2.973390\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 2.907308\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 2.812774\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 2.813281\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 2.873979\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 2.797993\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 3.517249\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 3.427542\n",
      "Epoch: 1119/2000... Step: 35800... Loss: 3.218407... Val Loss: 3.546521\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 4.605324\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 3.182298\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 2.967464\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 3.383260\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 3.387276\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 3.731327\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 3.576936\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 3.913888\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 3.771686\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 3.538203\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 3.478608\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 3.454251\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 3.328004\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 3.833302\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 3.825881\n",
      "Epoch: 1125/2000... Step: 36000... Loss: 1.482233... Val Loss: 3.982707\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 5.699892\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 3.976427\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 3.838056\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 4.069893\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 4.313761\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 4.600993\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 4.328875\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 4.179826\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 4.100936\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 3.978865\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 3.844304\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 3.912343\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 3.912838\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 4.549574\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 4.444966\n",
      "Epoch: 1132/2000... Step: 36200... Loss: 2.493178... Val Loss: 4.477044\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 5.074135\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 3.527180\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 3.337362\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 3.710437\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 3.965895\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 4.158610\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 3.851283\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 3.799620\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 3.732515\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 3.616089\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 3.496879\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 3.579349\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 3.559662\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 4.171132\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 4.073278\n",
      "Epoch: 1138/2000... Step: 36400... Loss: 1.841651... Val Loss: 4.186929\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 4.401781\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 2.680356\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 2.576956\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 2.870747\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 2.961070\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 3.217178\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 3.031396\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 3.151793\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 3.070985\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 2.916190\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 2.923850\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 2.929733\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 2.820609\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 3.409828\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 3.340577\n",
      "Epoch: 1144/2000... Step: 36600... Loss: 1.762006... Val Loss: 3.420395\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 4.361294\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 2.752170\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 2.630920\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 3.005843\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 2.901792\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 3.116046\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 2.978176\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 3.365197\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 3.246360\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 3.076319\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 3.082949\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 3.092639\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 2.979762\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 3.627197\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 3.657566\n",
      "Epoch: 1150/2000... Step: 36800... Loss: 1.256791... Val Loss: 3.827368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 5.036551\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 3.144021\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 2.784119\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 2.930527\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 3.000753\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 3.303354\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 3.200816\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 3.277993\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 3.205041\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 3.103936\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 3.075985\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 3.135127\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 3.058540\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 3.575476\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 3.498891\n",
      "Epoch: 1157/2000... Step: 37000... Loss: 1.688323... Val Loss: 3.692781\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 4.013437\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 2.630572\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 2.420562\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 2.906922\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 2.897166\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 3.336000\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 3.153268\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 3.412451\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 3.294219\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 3.068895\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 2.983923\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 3.005843\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 2.899961\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 3.436435\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 3.392452\n",
      "Epoch: 1163/2000... Step: 37200... Loss: 1.471330... Val Loss: 3.531509\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 4.254292\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 2.838017\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 2.961638\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 3.395673\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 3.407018\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 3.574219\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 3.390148\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 3.569294\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 3.485635\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 3.283517\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 3.247489\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 3.268168\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 3.163414\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 3.803403\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 3.826252\n",
      "Epoch: 1169/2000... Step: 37400... Loss: 2.249953... Val Loss: 4.003922\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 3.997309\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 2.433548\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 2.341092\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 2.784152\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 2.896868\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 3.347304\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 3.121020\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 3.207165\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 3.104678\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 2.943819\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 2.873340\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 2.886008\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 2.792622\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 3.345198\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 3.272380\n",
      "Epoch: 1175/2000... Step: 37600... Loss: 0.630598... Val Loss: 3.356029\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 4.590415\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 2.786940\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 2.516960\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 2.929631\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 2.823822\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 3.371822\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 3.153127\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 3.195292\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 3.128513\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 2.972200\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 2.926912\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 2.975278\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 2.891305\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 3.413731\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 3.358005\n",
      "Epoch: 1182/2000... Step: 37800... Loss: 2.908728... Val Loss: 3.510070\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 4.123174\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 2.843934\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 2.752226\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 3.171389\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 3.252398\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 3.175121\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 2.999441\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 2.943818\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 2.891863\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 2.810495\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 2.726233\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 2.756211\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 2.737507\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 3.605610\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 3.519136\n",
      "Epoch: 1188/2000... Step: 38000... Loss: 0.982017... Val Loss: 3.538202\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 3.640817\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 2.479603\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 2.607540\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 3.057961\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 3.151611\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 3.361936\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 3.183466\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 3.132295\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 3.088302\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 2.971600\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 2.930658\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 2.975177\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 2.900334\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 3.757184\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 3.681391\n",
      "Epoch: 1194/2000... Step: 38200... Loss: 1.709048... Val Loss: 3.729799\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 4.034431\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 2.644685\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 2.591406\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 2.966852\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 2.974676\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 3.150237\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 3.001914\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 3.182737\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 3.113811\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 2.967839\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 2.982530\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 2.969088\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 2.863717\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 3.454279\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 3.425659\n",
      "Epoch: 1200/2000... Step: 38400... Loss: 4.272272... Val Loss: 3.558711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 4.242970\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 2.882180\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 2.703514\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 3.123970\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 3.283917\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 3.885294\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 3.560372\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 3.440606\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 3.390382\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 3.260887\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 3.171989\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 3.258997\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 3.216158\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 3.736794\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 3.662133\n",
      "Epoch: 1207/2000... Step: 38600... Loss: 1.563044... Val Loss: 3.860927\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 4.022741\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 2.388147\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 2.420756\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 2.981070\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 2.996020\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 3.223367\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 3.052264\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 3.130831\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 3.062335\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 2.880452\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 2.839272\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 2.902856\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 2.815098\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 3.379618\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 3.346455\n",
      "Epoch: 1213/2000... Step: 38800... Loss: 1.395274... Val Loss: 3.462812\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 4.183626\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 2.619937\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 2.521175\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 2.954628\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 2.942627\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 2.937035\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 2.781282\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 2.897200\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 2.815166\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 2.648863\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 2.642363\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 2.660055\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 2.584042\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 3.160098\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 3.135342\n",
      "Epoch: 1219/2000... Step: 39000... Loss: 2.818319... Val Loss: 3.265242\n",
      "Validation loss decreased (3.346577 --> 3.265242).  Saving model ...\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 5.628836\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 4.159237\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 3.994976\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 4.370176\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 4.268578\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 4.349060\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 4.251128\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 4.632858\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 4.539950\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 4.368415\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 4.464957\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 4.443195\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 4.299516\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 4.845168\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 4.841105\n",
      "Epoch: 1225/2000... Step: 39200... Loss: 3.315368... Val Loss: 4.992798\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 6.884701\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 5.142486\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 4.652625\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 5.076977\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 5.057732\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 5.344901\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 5.052392\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 4.829578\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 4.798682\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 4.752150\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 4.610555\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 4.627980\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 4.695651\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 5.441155\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 5.280301\n",
      "Epoch: 1232/2000... Step: 39400... Loss: 2.862692... Val Loss: 5.375836\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 4.316300\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 2.693668\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 2.609004\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 2.928470\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 2.916399\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 3.324732\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 3.147218\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 3.525926\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 3.423881\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 3.211783\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 3.158450\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 3.166275\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 3.045541\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 3.546955\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 3.502433\n",
      "Epoch: 1238/2000... Step: 39600... Loss: 1.146494... Val Loss: 3.665255\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 3.767045\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 2.312273\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 2.530455\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 2.948610\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 3.042126\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 3.102043\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 2.929446\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 3.021189\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 2.998723\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 2.815847\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 2.788374\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 2.843455\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 2.778642\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 3.507619\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 3.469993\n",
      "Epoch: 1244/2000... Step: 39800... Loss: 1.868282... Val Loss: 3.559023\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 4.722438\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 3.645822\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 3.920365\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 4.427750\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 4.423557\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 4.709041\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 4.507404\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 4.768275\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 4.703256\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 4.518151\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 4.530197\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 4.555558\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 4.453216\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 5.063974\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 5.081717\n",
      "Epoch: 1250/2000... Step: 40000... Loss: 2.381089... Val Loss: 5.250586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 4.034806\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 2.576093\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 2.601616\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 3.120401\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 3.137778\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 3.333218\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 3.099631\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 3.054868\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 3.042637\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 2.925247\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 2.844375\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 2.935499\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 2.920207\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 3.530881\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 3.513256\n",
      "Epoch: 1257/2000... Step: 40200... Loss: 2.209723... Val Loss: 3.713734\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 4.479583\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 2.818000\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 2.613335\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 3.008328\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 2.954851\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 3.223273\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 3.041527\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 3.322416\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 3.213850\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 3.015695\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 2.966047\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 2.977701\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 2.865420\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 3.325325\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 3.319583\n",
      "Epoch: 1263/2000... Step: 40400... Loss: 2.029428... Val Loss: 3.504302\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 4.221728\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 2.597025\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 2.608504\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 3.073272\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 3.133215\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 3.338879\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 3.218429\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 3.492204\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 3.420424\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 3.250820\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 3.218646\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 3.240264\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 3.134234\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 3.657503\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 3.605617\n",
      "Epoch: 1269/2000... Step: 40600... Loss: 2.542914... Val Loss: 3.654658\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 4.249598\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 2.663481\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 2.800930\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 3.188493\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 3.136672\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 3.411489\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 3.186324\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 3.364321\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 3.300177\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 3.115622\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 3.116287\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 3.147992\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 3.057823\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 3.742536\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 3.719501\n",
      "Epoch: 1275/2000... Step: 40800... Loss: 2.363618... Val Loss: 3.801714\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 6.221791\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 4.264295\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 3.868058\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 4.274478\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 4.362489\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 4.675921\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 4.394125\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 4.216998\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 4.184055\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 4.120031\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 4.000998\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 4.055125\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 4.050394\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 4.622384\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 4.500156\n",
      "Epoch: 1282/2000... Step: 41000... Loss: 3.422025... Val Loss: 4.547964\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 4.791041\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 3.050541\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 2.842579\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 3.337679\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 3.461196\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 4.027505\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 3.704438\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 3.587235\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 3.507508\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 3.362674\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 3.247422\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 3.266447\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 3.238469\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 3.852908\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 3.765363\n",
      "Epoch: 1288/2000... Step: 41200... Loss: 1.401729... Val Loss: 3.826661\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 3.767301\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 2.353660\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 2.321308\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 2.874414\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 2.841309\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 3.241848\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 3.062698\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 3.151622\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 3.081840\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 2.901761\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 2.897948\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 2.911987\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 2.844481\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 3.482545\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 3.421693\n",
      "Epoch: 1294/2000... Step: 41400... Loss: 2.740724... Val Loss: 3.536379\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 3.750945\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 2.449979\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 2.272113\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 2.820955\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 2.851899\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 3.093962\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 2.878552\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 2.991554\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 2.925283\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 2.768383\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 2.711379\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 2.718314\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 2.656346\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 3.151734\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 3.115079\n",
      "Epoch: 1300/2000... Step: 41600... Loss: 7.134443... Val Loss: 3.301670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 5.822588\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 3.966311\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 3.490056\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 4.014554\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 3.977359\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 3.986269\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 3.760792\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 3.729728\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 3.702297\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 3.649031\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 3.535684\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 3.524100\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 3.576064\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 4.161556\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 4.060895\n",
      "Epoch: 1307/2000... Step: 41800... Loss: 1.637690... Val Loss: 4.155244\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 3.787732\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 2.374087\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 2.335768\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 2.779599\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 2.809702\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 3.235034\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 3.022730\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 3.079453\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 2.995031\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 2.857388\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 2.780221\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 2.799967\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 2.760266\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 3.312301\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 3.272632\n",
      "Epoch: 1313/2000... Step: 42000... Loss: 1.099244... Val Loss: 3.513859\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 3.717315\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 2.447619\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 2.446231\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 2.836251\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 2.883516\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 3.165362\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 2.996085\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 3.030157\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 2.984706\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 2.844418\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 2.810500\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 2.824511\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 2.772060\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 3.509304\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 3.450488\n",
      "Epoch: 1319/2000... Step: 42200... Loss: 1.768142... Val Loss: 3.552610\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 7.057853\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 5.513279\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 5.215378\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 5.504545\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 5.592493\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 5.747561\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 5.453861\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 5.182418\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 5.158291\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 5.095577\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 4.965833\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 5.043076\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 5.126856\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 5.950852\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 5.806826\n",
      "Epoch: 1325/2000... Step: 42400... Loss: 4.945853... Val Loss: 5.937016\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 6.458558\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 4.640455\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 4.222379\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 4.565907\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 4.487022\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 5.043622\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 4.774824\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 4.628292\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 4.610494\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 4.517737\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 4.422744\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 4.447320\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 4.474296\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 5.120287\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 5.011757\n",
      "Epoch: 1332/2000... Step: 42600... Loss: 3.302590... Val Loss: 5.224849\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 4.369143\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 2.620964\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 2.774953\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 3.406152\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 3.631998\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 3.933554\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 3.597576\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 3.608018\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 3.539706\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 3.347520\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 3.226699\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 3.284601\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 3.269098\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 3.781580\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 3.708228\n",
      "Epoch: 1338/2000... Step: 42800... Loss: 1.719407... Val Loss: 3.889140\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 4.240632\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 2.831610\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 2.785012\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 3.409318\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 3.398477\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 3.735641\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 3.506149\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 3.522175\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 3.455727\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 3.307203\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 3.304511\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 3.269681\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 3.182972\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 3.661211\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 3.610897\n",
      "Epoch: 1344/2000... Step: 43000... Loss: 2.142635... Val Loss: 3.715156\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 4.937652\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 2.975420\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 2.502058\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 2.960111\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 2.876052\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 3.385869\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 3.178220\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 3.277477\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 3.210006\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 3.066593\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 3.007416\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 3.013963\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 2.926091\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 3.357866\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 3.314187\n",
      "Epoch: 1350/2000... Step: 43200... Loss: 1.638845... Val Loss: 3.471924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 4.109782\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 2.588642\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 2.315984\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 2.751172\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 2.713089\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 2.947053\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 2.806814\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 2.898031\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 2.823399\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 2.730128\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 2.757524\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 2.717003\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 2.642185\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 3.141516\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 3.118233\n",
      "Epoch: 1357/2000... Step: 43400... Loss: 2.347775... Val Loss: 3.289771\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 4.117202\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 2.638302\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 2.460260\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 2.903105\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 2.984510\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 3.402889\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 3.132910\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 3.075785\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 3.048814\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 2.920675\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 2.848668\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 2.892316\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 2.849807\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 3.408038\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 3.354618\n",
      "Epoch: 1363/2000... Step: 43600... Loss: 1.544088... Val Loss: 3.553099\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 3.933442\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 2.380641\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 2.271167\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 2.792645\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 2.689267\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 2.921984\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 2.730869\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 2.923639\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 2.880912\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 2.718747\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 2.714240\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 2.717989\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 2.647296\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 3.209958\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 3.183150\n",
      "Epoch: 1369/2000... Step: 43800... Loss: 2.493461... Val Loss: 3.394115\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 4.198948\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 2.670548\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 2.563012\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 3.043140\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 3.188414\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 3.652320\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 3.426865\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 3.241097\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 3.224469\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 3.125383\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 3.034067\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 3.085964\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 3.029441\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 3.545410\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 3.458809\n",
      "Epoch: 1375/2000... Step: 44000... Loss: 2.626918... Val Loss: 3.620256\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 5.730512\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 3.673762\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 3.264406\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 3.763600\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 3.661227\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 4.003404\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 3.787416\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 3.698038\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 3.735603\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 3.663143\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 3.601758\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 3.622396\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 3.583220\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 4.041764\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 3.963633\n",
      "Epoch: 1382/2000... Step: 44200... Loss: 2.645585... Val Loss: 4.121403\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 4.657022\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 3.000365\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 2.681799\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 3.116847\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 3.094929\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 3.304414\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 3.110981\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 3.347027\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 3.258626\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 3.117200\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 3.126383\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 3.058245\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 2.957194\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 3.478092\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 3.446503\n",
      "Epoch: 1388/2000... Step: 44400... Loss: 1.886194... Val Loss: 3.513215\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 4.650284\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 3.450942\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 3.412496\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 3.985426\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 4.017531\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 4.323535\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 4.077235\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 4.322859\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 4.242897\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 4.002267\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 3.947007\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 3.945102\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 3.838178\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 4.386415\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 4.333167\n",
      "Epoch: 1394/2000... Step: 44600... Loss: 4.241096... Val Loss: 4.429913\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 3.473400\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 2.301697\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 2.148754\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 2.567864\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 2.667597\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 2.831549\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 2.616346\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 2.594655\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 2.555932\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 2.436007\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 2.384896\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 2.425814\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 2.397126\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 3.000624\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 2.956154\n",
      "Epoch: 1400/2000... Step: 44800... Loss: 8.470669... Val Loss: 3.114017\n",
      "Validation loss decreased (3.265242 --> 3.114017).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 6.910289\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 4.987378\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 4.279202\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 4.681904\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 4.528012\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 4.932688\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 4.674592\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 4.588956\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 4.594891\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 4.534749\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 4.470465\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 4.424600\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 4.508004\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 5.066877\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 4.949240\n",
      "Epoch: 1407/2000... Step: 45000... Loss: 3.167730... Val Loss: 5.082343\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 4.586620\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 3.063209\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 3.094046\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 3.633965\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 3.606473\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 3.631026\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 3.636825\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 3.762964\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 3.767282\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 3.627719\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 3.683710\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 3.653214\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 3.519626\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 4.149377\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 4.179847\n",
      "Epoch: 1413/2000... Step: 45200... Loss: 3.811138... Val Loss: 4.410497\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 3.900574\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 2.362519\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 2.298059\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 2.867100\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 2.915728\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 3.300442\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 3.062195\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 3.035684\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 2.996531\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 2.841565\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 2.827069\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 2.838604\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 2.754564\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 3.273072\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 3.236257\n",
      "Epoch: 1419/2000... Step: 45400... Loss: 2.208660... Val Loss: 3.378416\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 4.250345\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 2.700917\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 2.376190\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 3.011381\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 2.965272\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 3.248278\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 3.151816\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 3.293218\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 3.213097\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 3.080214\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 3.092759\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 3.040025\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 2.936600\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 3.455437\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 3.479723\n",
      "Epoch: 1425/2000... Step: 45600... Loss: 2.705763... Val Loss: 3.757687\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 4.347883\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 2.882695\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 2.500701\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 3.097640\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 3.225076\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 3.422588\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 3.136791\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 3.034994\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 2.977161\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 2.879327\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 2.803236\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 2.808898\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 2.815720\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 3.362469\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 3.286381\n",
      "Epoch: 1432/2000... Step: 45800... Loss: 2.407408... Val Loss: 3.354443\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 3.444992\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 2.120953\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 2.178747\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 2.727341\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 2.783368\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 3.223946\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 2.973014\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 3.053690\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 3.013345\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 2.824867\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 2.749695\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 2.763577\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 2.701391\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 3.276650\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 3.222987\n",
      "Epoch: 1438/2000... Step: 46000... Loss: 1.444939... Val Loss: 3.466360\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 4.082904\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 2.510978\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 2.441241\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 2.867536\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 2.802583\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 3.107576\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 2.930973\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 3.094170\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 3.083462\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 2.891071\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 2.815558\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 2.770228\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 2.695337\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 3.256729\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 3.215328\n",
      "Epoch: 1444/2000... Step: 46200... Loss: 1.921535... Val Loss: 3.392591\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 4.155394\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 3.134032\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 2.981460\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 3.607571\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 3.618882\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 3.735910\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 3.735505\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 3.900053\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 3.799093\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 3.687806\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 3.787938\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 3.727351\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 3.628560\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 4.183703\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 4.228384\n",
      "Epoch: 1450/2000... Step: 46400... Loss: 2.219440... Val Loss: 4.520472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 5.087811\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 3.293461\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 2.836521\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 3.254225\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 3.152535\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 3.802205\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 3.593187\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 3.524240\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 3.464672\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 3.356733\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 3.328779\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 3.304785\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 3.249871\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 3.893032\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 3.793530\n",
      "Epoch: 1457/2000... Step: 46600... Loss: 2.997713... Val Loss: 3.978008\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 3.364188\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 2.078143\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 2.073101\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 2.751328\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 2.701416\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 2.968747\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 2.804726\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 2.860874\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 2.822692\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 2.720672\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 2.720815\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 2.722130\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 2.686258\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 3.474905\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 3.423200\n",
      "Epoch: 1463/2000... Step: 46800... Loss: 0.893258... Val Loss: 3.524255\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 3.454315\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 2.054580\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 2.446832\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 3.153296\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 3.151475\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 3.305792\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 3.033637\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 3.104156\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 3.055262\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 2.928633\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 2.887971\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 2.872124\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 2.872635\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 3.510412\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 3.438808\n",
      "Epoch: 1469/2000... Step: 47000... Loss: 2.231238... Val Loss: 3.569219\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 4.396391\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 3.045152\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 2.821884\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 3.501468\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 3.477210\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 3.797555\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 3.640672\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 3.966229\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 3.873319\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 3.700183\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 3.709005\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 3.667685\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 3.545571\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 3.937393\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 3.960246\n",
      "Epoch: 1475/2000... Step: 47200... Loss: 4.212811... Val Loss: 4.232155\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 5.029119\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 3.360598\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 3.098731\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 3.568108\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 3.627801\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 3.967076\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 3.709320\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 3.566547\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 3.547155\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 3.471648\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 3.420634\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 3.449465\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 3.424320\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 3.962289\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 3.879515\n",
      "Epoch: 1482/2000... Step: 47400... Loss: 2.312728... Val Loss: 4.013335\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 3.593670\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 2.323361\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 2.184814\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 2.748306\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 2.739839\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 3.439235\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 3.194932\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 3.155347\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 3.091523\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 2.946799\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 2.961582\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 2.908133\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 2.817930\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 3.410276\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 3.357475\n",
      "Epoch: 1488/2000... Step: 47600... Loss: 0.759383... Val Loss: 3.553752\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 3.374598\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 2.116421\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 1.981603\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 2.732862\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 2.618701\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 3.008973\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 2.749360\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 2.718458\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 2.674245\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 2.522790\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 2.448649\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 2.421297\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 2.388171\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 3.181786\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 3.141723\n",
      "Epoch: 1494/2000... Step: 47800... Loss: 3.240966... Val Loss: 3.399757\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 9.575816\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 8.301308\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 7.723465\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 8.212384\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 8.253559\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 8.669710\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 8.483452\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 8.121994\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 8.190494\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 8.172004\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 8.054473\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 8.053538\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 8.144202\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 8.929574\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 8.755468\n",
      "Epoch: 1500/2000... Step: 48000... Loss: 49.508659... Val Loss: 8.953046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 5.496129\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 3.799947\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 3.345652\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 4.129559\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 4.108517\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 4.752933\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 4.419149\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 4.274215\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 4.209423\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 4.109288\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 4.009556\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 3.964620\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 3.969058\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 4.567344\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 4.473537\n",
      "Epoch: 1507/2000... Step: 48200... Loss: 1.602241... Val Loss: 4.584310\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 3.314792\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 1.994115\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 1.893288\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 2.706642\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 2.691814\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 3.182337\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 2.912740\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 2.856282\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 2.801911\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 2.708619\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 2.690561\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 2.654463\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 2.622825\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 3.287792\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 3.242616\n",
      "Epoch: 1513/2000... Step: 48400... Loss: 1.419760... Val Loss: 3.366891\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 3.670044\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 2.108409\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 2.099231\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 2.699804\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 2.722475\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 3.126972\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 2.876506\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 2.883590\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 2.848446\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 2.711223\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 2.661513\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 2.664907\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 2.598821\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 3.140726\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 3.117655\n",
      "Epoch: 1519/2000... Step: 48600... Loss: 2.702757... Val Loss: 3.294887\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 6.063581\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 4.641548\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 4.668632\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 5.231389\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 5.130831\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 5.361665\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 5.090679\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 5.352835\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 5.261894\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 5.025046\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 5.026287\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 4.977803\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 4.816333\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 5.140404\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 5.216464\n",
      "Epoch: 1525/2000... Step: 48800... Loss: 2.154684... Val Loss: 5.379037\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 4.578940\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 2.833963\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 2.686837\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 3.426379\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 3.451156\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 3.960097\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 3.608065\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 3.574277\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 3.549072\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 3.439132\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 3.340564\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 3.363264\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 3.280398\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 3.608758\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 3.591102\n",
      "Epoch: 1532/2000... Step: 49000... Loss: 1.037795... Val Loss: 3.799334\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 3.185662\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 2.088132\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 1.972278\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 2.520288\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 2.504904\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 2.925089\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 2.704326\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 2.651293\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 2.614360\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 2.537818\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 2.532771\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 2.465261\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 2.448335\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 3.290793\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 3.257234\n",
      "Epoch: 1538/2000... Step: 49200... Loss: 0.897362... Val Loss: 3.614788\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 4.075698\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 2.977893\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 3.192076\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 3.592809\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 3.606826\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 3.950352\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 3.667540\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 3.785333\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 3.755655\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 3.587965\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 3.602285\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 3.578935\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 3.497773\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 4.128874\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 4.109316\n",
      "Epoch: 1544/2000... Step: 49400... Loss: 2.156251... Val Loss: 4.206743\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 3.972614\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 2.708083\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 2.835817\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 3.542512\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 3.479969\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 3.859531\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 3.618750\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 3.753617\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 3.668207\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 3.480485\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 3.478383\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 3.486183\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 3.431904\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 3.971376\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 4.028155\n",
      "Epoch: 1550/2000... Step: 49600... Loss: 3.345783... Val Loss: 4.303851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 7.658030\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 5.861648\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 5.264729\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 5.730213\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 5.601543\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 6.005233\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 5.813368\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 5.652932\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 5.639508\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 5.630614\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 5.579928\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 5.529292\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 5.609388\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 6.243625\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 6.146273\n",
      "Epoch: 1557/2000... Step: 49800... Loss: 3.040096... Val Loss: 6.317696\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 4.010319\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 2.458891\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 2.355517\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 3.057797\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 3.094536\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 3.540200\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 3.322277\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 3.185450\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 3.145764\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 3.034659\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 2.893981\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 2.899116\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 2.856314\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 3.448210\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 3.378856\n",
      "Epoch: 1563/2000... Step: 50000... Loss: 0.616177... Val Loss: 3.548845\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 4.086362\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 2.558023\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 2.559008\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 3.024841\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 3.053765\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 3.391485\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 3.206760\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 3.157805\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 3.110880\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 3.025454\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 3.095256\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 3.028180\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 2.951183\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 3.741156\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 3.686878\n",
      "Epoch: 1569/2000... Step: 50200... Loss: 3.023782... Val Loss: 3.933620\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 4.446246\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 2.760143\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 2.363018\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 2.817772\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 2.753200\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 3.274667\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 3.143168\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 3.346529\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 3.269902\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 3.188500\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 3.243749\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 3.180119\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 3.070726\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 3.539525\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 3.567761\n",
      "Epoch: 1575/2000... Step: 50400... Loss: 1.559016... Val Loss: 3.803341\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 5.022652\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 3.515659\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 3.083201\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 3.697615\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 3.686459\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 4.113266\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 3.855182\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 3.662287\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 3.621305\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 3.588896\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 3.577438\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 3.538188\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 3.541475\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 4.117064\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 4.053810\n",
      "Epoch: 1582/2000... Step: 50600... Loss: 2.510201... Val Loss: 4.279531\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 3.513062\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 2.311900\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 2.280709\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 2.897952\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 2.919074\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 3.254258\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 3.012599\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 2.837629\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 2.773982\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 2.733106\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 2.763020\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 2.724398\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 2.681523\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 3.452028\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 3.364301\n",
      "Epoch: 1588/2000... Step: 50800... Loss: 1.255881... Val Loss: 3.545656\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 3.752928\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 2.555842\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 2.671164\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 3.336660\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 3.414572\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 3.710382\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 3.412136\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 3.481937\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 3.420443\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 3.257723\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 3.244121\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 3.200263\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 3.137952\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 3.667924\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 3.664323\n",
      "Epoch: 1594/2000... Step: 51000... Loss: 4.124461... Val Loss: 3.894115\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 4.078604\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 2.578772\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 2.434725\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 2.848990\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 2.932517\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 3.175824\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 3.042177\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 3.110840\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 3.082092\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 2.977140\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 3.030228\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 3.004667\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 2.903490\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 3.383780\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 3.387141\n",
      "Epoch: 1600/2000... Step: 51200... Loss: 5.442889... Val Loss: 3.581958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 5.780314\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 4.075555\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 3.545990\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 4.265632\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 4.116984\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 4.588852\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 4.247702\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 4.113955\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 4.088033\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 4.045777\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 3.987867\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 3.949628\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 3.969879\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 4.465454\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 4.413679\n",
      "Epoch: 1607/2000... Step: 51400... Loss: 2.503285... Val Loss: 4.537798\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 4.757698\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 2.852241\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 2.577304\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 3.138600\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 3.098036\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 3.155814\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 2.938441\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 2.903057\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 2.855888\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 2.828428\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 2.870130\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 2.823157\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 2.728210\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 3.098381\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 3.061521\n",
      "Epoch: 1613/2000... Step: 51600... Loss: 1.271380... Val Loss: 3.210351\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 3.887551\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 2.319403\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 2.448660\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 3.114494\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 3.179333\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 3.876389\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 3.511541\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 3.449367\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 3.436453\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 3.291744\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 3.252138\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 3.223396\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 3.121881\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 3.576953\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 3.545665\n",
      "Epoch: 1619/2000... Step: 51800... Loss: 2.679266... Val Loss: 3.715277\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 4.272125\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 3.110080\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 3.332774\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 3.974257\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 4.033463\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 4.516458\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 4.274156\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 4.320918\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 4.206608\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 4.028412\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 4.063174\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 4.052590\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 3.941205\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 4.490012\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 4.538545\n",
      "Epoch: 1625/2000... Step: 52000... Loss: 3.024685... Val Loss: 4.787803\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 4.296711\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 2.797038\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 2.642032\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 3.125648\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 3.077826\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 3.825905\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 3.543159\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 3.450928\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 3.408761\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 3.275252\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 3.224210\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 3.195156\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 3.167323\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 4.005055\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 3.986132\n",
      "Epoch: 1632/2000... Step: 52200... Loss: 1.221220... Val Loss: 4.227840\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 4.375206\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 2.514032\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 2.209597\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 3.052317\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 2.957004\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 3.347306\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 3.070868\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 3.197501\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 3.100046\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 2.986245\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 2.999800\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 2.908248\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 2.804447\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 3.217686\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 3.216966\n",
      "Epoch: 1638/2000... Step: 52400... Loss: 1.135945... Val Loss: 3.508380\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 4.168880\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 2.537758\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 2.410841\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 2.968424\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 2.972551\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 3.384498\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 3.103462\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 3.133916\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 3.076782\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 2.976909\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 2.994474\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 2.948227\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 2.857893\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 3.422649\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 3.444122\n",
      "Epoch: 1644/2000... Step: 52600... Loss: 1.795645... Val Loss: 3.742902\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 3.616994\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 2.627768\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 2.712774\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 3.333599\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 3.342107\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 3.765198\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 3.490130\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 3.536975\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 3.462423\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 3.402450\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 3.487709\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 3.423524\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 3.349600\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 4.157960\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 4.213070\n",
      "Epoch: 1650/2000... Step: 52800... Loss: 4.825349... Val Loss: 4.515657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 4.210163\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 2.612241\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 2.342711\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 2.820959\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 2.829291\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 3.198919\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 3.024319\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 3.002359\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 2.933171\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 2.847987\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 2.884835\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 2.817607\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 2.719982\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 3.246762\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 3.243829\n",
      "Epoch: 1657/2000... Step: 53000... Loss: 1.163773... Val Loss: 3.478264\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 3.724577\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 2.160869\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 2.215446\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 2.818427\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 2.850754\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 3.458194\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 3.151889\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 3.030387\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 2.997858\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 2.855359\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 2.755609\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 2.737093\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 2.669397\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 3.148287\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 3.139670\n",
      "Epoch: 1663/2000... Step: 53200... Loss: 1.024992... Val Loss: 3.451271\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 3.216519\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 1.988901\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 2.426674\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 3.128969\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 3.352216\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 3.898317\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 3.456805\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 3.286744\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 3.224343\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 3.124614\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 3.064528\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 3.074858\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 3.026669\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 3.532368\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 3.487896\n",
      "Epoch: 1669/2000... Step: 53400... Loss: 4.716918... Val Loss: 3.684610\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 3.852067\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 2.421499\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 2.236989\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 2.765573\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 2.769061\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 2.989237\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 2.863165\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 2.853750\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 2.765280\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 2.738912\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 2.869543\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 2.791306\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 2.715102\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 3.242637\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 3.252649\n",
      "Epoch: 1675/2000... Step: 53600... Loss: 3.339397... Val Loss: 3.472685\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 3.518518\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 2.213769\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 2.053180\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 2.558885\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 2.584404\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 3.242090\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 3.019764\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 2.943398\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 2.874463\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 2.806485\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 2.862030\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 2.768159\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 2.693939\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 3.329286\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 3.380375\n",
      "Epoch: 1682/2000... Step: 53800... Loss: 1.376354... Val Loss: 3.592150\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 3.537627\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 2.139347\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 1.962574\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 2.579599\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 2.587450\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 3.097480\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 2.769942\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 2.640284\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 2.588775\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 2.517119\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 2.493125\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 2.432723\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 2.385613\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 3.110585\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 3.102925\n",
      "Epoch: 1688/2000... Step: 54000... Loss: 0.758869... Val Loss: 3.331351\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 3.234661\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 2.023975\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 2.417383\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 3.019991\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 3.093223\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 3.563615\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 3.255294\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 3.178541\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 3.185528\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 3.074478\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 3.044502\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 3.027242\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 2.952761\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 3.618707\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 3.594980\n",
      "Epoch: 1694/2000... Step: 54200... Loss: 4.658917... Val Loss: 3.813502\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 3.612987\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 2.442859\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 2.473504\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 2.962022\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 3.004012\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 3.233960\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 3.103654\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 3.076128\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 2.972103\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 2.978890\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 3.158635\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 3.092392\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 3.001313\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 3.477627\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 3.487867\n",
      "Epoch: 1700/2000... Step: 54400... Loss: 3.674083... Val Loss: 3.659402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 4.682803\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 3.422622\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 3.514662\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 4.092138\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 4.234271\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 4.599572\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 4.266232\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 4.019844\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 3.936659\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 3.915592\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 3.891277\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 3.918559\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 3.884091\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 4.489618\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 4.430546\n",
      "Epoch: 1707/2000... Step: 54600... Loss: 3.130327... Val Loss: 4.560728\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 3.158190\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 1.987304\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 2.299841\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 2.834676\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 2.897770\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 3.280142\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 3.005575\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 2.852204\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 2.766955\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 2.704035\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 2.737371\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 2.744184\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 2.706688\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 3.413047\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 3.384103\n",
      "Epoch: 1713/2000... Step: 54800... Loss: 2.111705... Val Loss: 3.598940\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 4.242817\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 3.078753\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 3.374117\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 3.609021\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 3.669367\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 3.998160\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 3.848466\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 3.958149\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 3.956174\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 3.888529\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 4.022645\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 3.949360\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 3.874372\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 4.667471\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 4.788869\n",
      "Epoch: 1719/2000... Step: 55000... Loss: 2.165691... Val Loss: 5.102123\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 4.059879\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 2.648144\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 2.311525\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 2.826433\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 2.724382\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 3.197396\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 3.006538\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 2.970158\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 2.876906\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 2.850707\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 2.904998\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 2.806786\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 2.758413\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 3.625415\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 3.596596\n",
      "Epoch: 1725/2000... Step: 55200... Loss: 4.593831... Val Loss: 3.806956\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 4.610033\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 3.213943\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 3.172364\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 3.688495\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 3.695270\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 4.156296\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 3.889898\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 3.713979\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 3.662605\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 3.628504\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 3.637424\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 3.621483\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 3.558909\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 4.126039\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 4.048780\n",
      "Epoch: 1732/2000... Step: 55400... Loss: 3.058368... Val Loss: 4.280117\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 3.215993\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 2.070186\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 2.224297\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 2.647037\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 2.637096\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 3.373947\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 3.053439\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 2.940064\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 2.872722\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 2.786565\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 2.833618\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 2.765020\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 2.689357\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 3.536330\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 3.495167\n",
      "Epoch: 1738/2000... Step: 55600... Loss: 1.089833... Val Loss: 3.684366\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 5.010181\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 3.037769\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 2.832473\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 3.578310\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 3.381306\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 3.950288\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 3.640014\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 3.819093\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 3.699313\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 3.526722\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 3.528599\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 3.404930\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 3.274876\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 3.669818\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 3.725697\n",
      "Epoch: 1744/2000... Step: 55800... Loss: 2.243326... Val Loss: 3.953095\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 3.709526\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 2.315442\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 2.239506\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 2.580588\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 2.660445\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 3.492174\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 3.153126\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 2.990894\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 2.897636\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 2.807661\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 2.817542\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 2.753012\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 2.670357\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 3.318013\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 3.258860\n",
      "Epoch: 1750/2000... Step: 56000... Loss: 3.653988... Val Loss: 3.478453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 4.050462\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 2.575968\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 2.433266\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 2.963611\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 2.990099\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 3.724909\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 3.327856\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 3.214644\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 3.131659\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 3.024102\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 2.977818\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 2.940644\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 2.870553\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 3.438987\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 3.422463\n",
      "Epoch: 1757/2000... Step: 56200... Loss: 1.324206... Val Loss: 3.618287\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 3.113243\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 2.174226\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 2.119914\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 2.563775\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 2.561916\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 2.796105\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 2.515508\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 2.467990\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 2.404428\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 2.340316\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 2.352667\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 2.307431\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 2.293274\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 3.329496\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 3.276617\n",
      "Epoch: 1763/2000... Step: 56400... Loss: 0.726431... Val Loss: 3.592360\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 3.590841\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 2.250104\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 2.308666\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 2.663167\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 2.695740\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 3.069512\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 2.794972\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 2.763995\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 2.741998\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 2.685500\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 2.755384\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 2.701381\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 2.606701\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 3.433427\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 3.370806\n",
      "Epoch: 1769/2000... Step: 56600... Loss: 1.898695... Val Loss: 3.607143\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 3.592611\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 2.305696\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 2.257991\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 2.822957\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 2.862456\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 3.350405\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 3.120381\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 3.123424\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 3.050927\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 2.982628\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 3.071889\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 2.978840\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 2.882044\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 3.628929\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 3.607052\n",
      "Epoch: 1775/2000... Step: 56800... Loss: 3.056916... Val Loss: 3.902121\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 5.144644\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 3.318217\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 3.120024\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 3.445851\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 3.510775\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 4.100507\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 3.735519\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 3.520595\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 3.442784\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 3.344355\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 3.340151\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 3.311059\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 3.219679\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 3.663110\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 3.594651\n",
      "Epoch: 1782/2000... Step: 57000... Loss: 15.588361... Val Loss: 3.846371\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 3.478098\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 2.163506\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 2.046446\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 2.653629\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 2.601429\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 3.155176\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 2.898393\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 2.819779\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 2.730494\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 2.682194\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 2.782873\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 2.683274\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 2.591799\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 3.514315\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 3.460661\n",
      "Epoch: 1788/2000... Step: 57200... Loss: 0.728731... Val Loss: 3.774678\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 3.518041\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 2.153904\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 2.068399\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 2.663351\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 2.662966\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 3.119326\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 2.830858\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 2.794020\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 2.754065\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 2.664986\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 2.743737\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 2.673724\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 2.581984\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 3.181279\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 3.135979\n",
      "Epoch: 1794/2000... Step: 57400... Loss: 1.336248... Val Loss: 3.386551\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 3.516522\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 2.223736\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 2.443652\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 2.789770\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 2.822123\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 3.157011\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 2.978134\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 2.966927\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 2.944777\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 2.910717\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 3.076341\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 3.037332\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 2.959792\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 3.766873\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 3.778936\n",
      "Epoch: 1800/2000... Step: 57600... Loss: 1.350878... Val Loss: 3.995929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 6.636251\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 5.262954\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 5.015971\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 5.565628\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 5.622080\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 6.481589\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 6.238243\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 5.890269\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 5.815666\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 5.728620\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 5.538086\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 5.503417\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 5.431004\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 6.231585\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 6.138548\n",
      "Epoch: 1807/2000... Step: 57800... Loss: 2.541652... Val Loss: 6.251807\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 4.205177\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 2.786102\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 2.685705\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 3.164942\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 3.148497\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 3.667672\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 3.548016\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 3.639139\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 3.597471\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 3.639367\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 3.854206\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 3.666565\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 3.539541\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 4.392128\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 4.374676\n",
      "Epoch: 1813/2000... Step: 58000... Loss: 2.174688... Val Loss: 4.875801\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 3.736432\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 2.381305\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 2.331586\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 2.727358\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 2.675128\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 3.024412\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 2.712152\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 2.744198\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 2.737278\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 2.706949\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 2.802288\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 2.738455\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 2.642299\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 3.424483\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 3.346097\n",
      "Epoch: 1819/2000... Step: 58200... Loss: 1.904826... Val Loss: 3.641313\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 3.875514\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 2.399088\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 2.216894\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 2.720340\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 2.744254\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 3.290231\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 2.951271\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 2.864963\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 2.774680\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 2.730037\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 2.802322\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 2.710334\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 2.615478\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 3.268335\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 3.243133\n",
      "Epoch: 1825/2000... Step: 58400... Loss: 1.748963... Val Loss: 3.458124\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 3.940930\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 2.532009\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 2.241884\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 2.521766\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 2.564684\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 3.030972\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 2.720251\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 2.726065\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 2.685762\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 2.657649\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 2.706535\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 2.634169\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 2.566930\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 3.127360\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 3.092602\n",
      "Epoch: 1832/2000... Step: 58600... Loss: 2.323159... Val Loss: 3.264970\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 4.144989\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 2.648377\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 2.388962\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 2.804528\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 2.777993\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 3.414013\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 3.057638\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 3.019820\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 2.973552\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 2.904559\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 2.976342\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 2.910230\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 2.812123\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 3.478625\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 3.407209\n",
      "Epoch: 1838/2000... Step: 58800... Loss: 0.694475... Val Loss: 3.611656\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 3.279082\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 2.093099\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 2.321454\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 2.656638\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 2.759917\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 3.550823\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 3.172259\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 3.098858\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 3.095439\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 2.956913\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 2.959734\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 2.935375\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 2.840480\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 3.653847\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 3.569915\n",
      "Epoch: 1844/2000... Step: 59000... Loss: 4.541822... Val Loss: 3.788973\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 5.172378\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 3.598509\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 3.623783\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 3.992600\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 4.047481\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 4.333904\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 4.115486\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 4.324904\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 4.324506\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 4.273344\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 4.431590\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 4.366637\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 4.245609\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 4.864497\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 4.948372\n",
      "Epoch: 1850/2000... Step: 59200... Loss: 1.606652... Val Loss: 5.348970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 4.142529\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 2.616430\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 2.215180\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 2.627540\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 2.561015\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 3.004407\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 2.855141\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 2.837031\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 2.765586\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 2.773598\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 2.957705\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 2.848633\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 2.750662\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 3.321947\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 3.266264\n",
      "Epoch: 1857/2000... Step: 59400... Loss: 0.884074... Val Loss: 3.608259\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 3.980263\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 2.575194\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 2.464313\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 2.939825\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 2.977528\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 3.662074\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 3.281204\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 3.097089\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 3.027287\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 2.932877\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 2.985925\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 2.935835\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 2.848539\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 3.512641\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 3.416103\n",
      "Epoch: 1863/2000... Step: 59600... Loss: 0.811817... Val Loss: 3.678139\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 3.859999\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 2.521679\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 2.653748\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 2.934602\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 3.004391\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 3.626948\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 3.325535\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 3.201037\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 3.142716\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 3.016427\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 3.051473\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 3.061773\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 2.955541\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 3.636852\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 3.552220\n",
      "Epoch: 1869/2000... Step: 59800... Loss: 1.740976... Val Loss: 3.745646\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 3.582014\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 2.342286\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 2.506389\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 2.753490\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 2.833663\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 3.254228\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 2.946961\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 2.981807\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 2.949954\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 2.949261\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 3.115053\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 3.071543\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 2.967008\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 3.722056\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 3.692201\n",
      "Epoch: 1875/2000... Step: 60000... Loss: 2.235545... Val Loss: 3.986484\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 4.141273\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 2.589555\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 2.469002\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 2.755005\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 2.866965\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 3.606954\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 3.264258\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 3.091650\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 3.061479\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 2.922536\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 2.842818\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 2.843967\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 2.710325\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 3.389674\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 3.301308\n",
      "Epoch: 1882/2000... Step: 60200... Loss: 1.524362... Val Loss: 3.577746\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 4.376265\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 2.821017\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 2.508463\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 2.931626\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 2.916030\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 3.749829\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 3.442425\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 3.495323\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 3.431418\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 3.380786\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 3.510092\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 3.388262\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 3.242243\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 3.877511\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 3.861405\n",
      "Epoch: 1888/2000... Step: 60400... Loss: 1.103960... Val Loss: 4.247953\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 3.611932\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 2.387993\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 2.455725\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 2.766814\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 2.871685\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 3.543878\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 3.161820\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 3.075232\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 3.010134\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 2.940559\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 2.977779\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 2.976746\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 2.846830\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 3.563105\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 3.460249\n",
      "Epoch: 1894/2000... Step: 60600... Loss: 1.705661... Val Loss: 3.665332\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 3.755008\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 2.456328\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 2.458916\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 2.864801\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 2.930138\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 3.708817\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 3.329756\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 3.355243\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 3.250789\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 3.161570\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 3.269949\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 3.198670\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 3.065713\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 3.691942\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 3.628302\n",
      "Epoch: 1900/2000... Step: 60800... Loss: 1.416809... Val Loss: 3.919619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 3.730152\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 2.436547\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 2.580333\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 3.116854\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 3.162583\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 3.766206\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 3.492018\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 3.400780\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 3.338169\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 3.261185\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 3.381002\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 3.316688\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 3.178299\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 3.877897\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 3.871190\n",
      "Epoch: 1907/2000... Step: 61000... Loss: 1.041762... Val Loss: 4.091553\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 3.816566\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 2.365648\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 2.017171\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 2.650688\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 2.573904\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 4.060571\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 3.643485\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 3.615570\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 3.509173\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 3.318561\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 3.267821\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 3.121679\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 3.012887\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 3.931736\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 3.907988\n",
      "Epoch: 1913/2000... Step: 61200... Loss: 0.972349... Val Loss: 4.402382\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 3.600614\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 2.401490\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 2.224477\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 2.746557\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 2.674104\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 3.285942\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 3.001097\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 2.912646\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 2.886556\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 2.867210\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 2.931175\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 2.874355\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 2.787430\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 3.750051\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 3.672695\n",
      "Epoch: 1919/2000... Step: 61400... Loss: 1.910821... Val Loss: 3.958741\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 3.000213\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 1.943464\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 2.089817\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 2.355998\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 2.378202\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 2.977480\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 2.694801\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 2.763380\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 2.723914\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 2.665474\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 2.755404\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 2.658027\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 2.616124\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 3.596055\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 3.607013\n",
      "Epoch: 1925/2000... Step: 61600... Loss: 3.065566... Val Loss: 3.882204\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 3.746683\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 2.393391\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 2.303731\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 2.498691\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 2.613329\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 3.173821\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 2.961978\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 2.843138\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 2.790037\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 2.772631\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 2.898481\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 2.843985\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 2.715675\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 3.436058\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 3.353369\n",
      "Epoch: 1932/2000... Step: 61800... Loss: 0.735358... Val Loss: 3.602916\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 3.603256\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 2.361600\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 1.943747\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 2.644485\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 2.582536\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 3.713016\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 3.379742\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 3.234420\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 3.165989\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 3.102639\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 3.114169\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 3.011096\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 2.908425\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 3.908518\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 3.854134\n",
      "Epoch: 1938/2000... Step: 62000... Loss: 0.738246... Val Loss: 4.245927\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 3.846481\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 2.434876\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 2.412941\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 2.662436\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 2.785346\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 3.316754\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 3.045520\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 2.964972\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 2.948865\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 2.916704\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 3.036310\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 2.993506\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 2.872012\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 3.538373\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 3.465645\n",
      "Epoch: 1944/2000... Step: 62200... Loss: 2.042153... Val Loss: 3.684772\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 3.710535\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 2.449818\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 2.687681\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 2.759548\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 2.865091\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 3.574879\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 3.221595\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 3.401297\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 3.313956\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 3.332451\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 3.461495\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 3.385484\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 3.311976\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 4.056361\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 4.029673\n",
      "Epoch: 1950/2000... Step: 62400... Loss: 1.021779... Val Loss: 4.268470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 3.678904\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 2.287741\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 2.014901\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 2.573304\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 2.627633\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 3.322940\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 2.975675\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 2.951722\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 2.903049\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 2.794886\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 2.785517\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 2.702682\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 2.610937\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 3.400304\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 3.340672\n",
      "Epoch: 1957/2000... Step: 62600... Loss: 0.886027... Val Loss: 3.619412\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 4.750381\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 3.220804\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 3.221140\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 3.552800\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 3.629431\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 4.338832\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 3.972848\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 3.713530\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 3.636537\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 3.576513\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 3.552697\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 3.542778\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 3.456737\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 4.123586\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 4.001756\n",
      "Epoch: 1963/2000... Step: 62800... Loss: 0.765928... Val Loss: 4.169519\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 3.750657\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 2.451427\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 2.499313\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 2.869256\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 3.004973\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 3.617811\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 3.234638\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 3.064150\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 3.007323\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 2.927076\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 2.946333\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 2.916002\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 2.822254\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 3.653373\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 3.551067\n",
      "Epoch: 1969/2000... Step: 63000... Loss: 1.142106... Val Loss: 3.788515\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 3.498929\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 2.188449\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 2.354987\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 2.523763\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 2.583212\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 3.294369\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 3.041672\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 3.081741\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 3.001658\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 2.969775\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 3.143471\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 3.079151\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 2.991511\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 3.728668\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 3.752460\n",
      "Epoch: 1975/2000... Step: 63200... Loss: 0.903688... Val Loss: 3.957096\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 4.381062\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 2.972720\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 3.330832\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 3.632150\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 3.821212\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 4.815055\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 4.399835\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 4.172473\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 4.126871\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 4.007925\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 4.006398\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 4.013369\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 3.907032\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 4.668695\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 4.581239\n",
      "Epoch: 1982/2000... Step: 63400... Loss: 2.256347... Val Loss: 4.670146\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 3.382611\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 2.062347\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 2.049961\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 2.416113\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 2.534250\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 3.480193\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 3.080598\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 2.940331\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 2.901676\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 2.837050\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 2.905699\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 2.825753\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 2.714209\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 3.514045\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 3.452443\n",
      "Epoch: 1988/2000... Step: 63600... Loss: 0.695304... Val Loss: 3.690979\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 3.858506\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 2.467516\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 2.605371\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 2.610238\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 2.688619\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 3.471257\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 3.104222\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 3.150881\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 3.082551\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 3.048216\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 3.122148\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 3.088855\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 2.968343\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 3.564370\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 3.512986\n",
      "Epoch: 1994/2000... Step: 63800... Loss: 4.229533... Val Loss: 3.715956\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 3.537111\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 2.168185\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 2.173914\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 2.525079\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 2.680763\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 3.291442\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 2.936893\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 2.896432\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 2.819019\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 2.765522\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 2.805992\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 2.730699\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 2.660721\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 3.503600\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 3.497310\n",
      "Epoch: 2000/2000... Step: 64000... Loss: 1.930574... Val Loss: 3.718506\n"
     ]
    }
   ],
   "source": [
    "from utils import prep_data\n",
    "\n",
    "\n",
    "\n",
    "train_data_loader = prep_data(train_data, train_targets, batch_size = 32,first_n_epochs = 10 )\n",
    "test_data_loader = prep_data(test_data, test_targets, batch_size = 32,first_n_epochs = 10 )\n",
    "val_data_loader = prep_data(val_data, val_targets, batch_size = 32, first_n_epochs=10)\n",
    "\n",
    "\n",
    "model, optimizer, criterion = create_model(input_size = 10)\n",
    "train_validate_model(model, optimizer, criterion)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([83.5056, 81.3736, 80.0233, 81.2056, 83.1180, 81.3025, 84.1388, 80.9148,\n",
      "        84.1646, 70.7779, 59.4650, 70.4419, 81.4640, 80.5401, 82.1489, 83.8610,\n",
      "        81.8969, 59.4327, 79.8294, 83.2860, 83.6607, 78.5890, 71.3012, 75.5847,\n",
      "        51.6346, 73.3687, 78.2272, 81.9421, 81.5157, 82.1166, 76.8510, 81.7095])\n",
      "tensor([83.4315, 81.3518, 78.6776, 80.6873, 82.8128, 81.6355, 85.0174, 81.0411,\n",
      "        84.3208, 69.2617, 55.6905, 72.7859, 80.0380, 81.4923, 82.2792, 84.6914,\n",
      "        80.9904, 64.5527, 80.2922, 83.1278, 84.3212, 77.3441, 69.3791, 73.6370,\n",
      "        53.0603, 73.7312, 79.4034, 82.7655, 81.2402, 82.9738, 77.8426, 81.9449])\n",
      "tensor([82.3298, 83.9643, 59.0322, 83.9191, 84.3455, 84.1840, 80.6177, 84.4812,\n",
      "        80.9665, 82.7756, 84.0871, 82.8983, 83.1890, 78.0204, 88.0346, 84.0806,\n",
      "        83.8351, 82.6916, 89.1717, 80.6952, 68.0062, 77.3679, 72.8195, 84.2938,\n",
      "        83.0727, 84.6233, 74.5768, 83.2472, 84.1194, 82.5688, 82.0390, 83.3247])\n",
      "tensor([82.3615, 84.0750, 62.4047, 84.2383, 83.9864, 84.4283, 82.5339, 84.8774,\n",
      "        80.3488, 82.2999, 84.6934, 82.3845, 82.8562, 75.8585, 87.3866, 85.2086,\n",
      "        83.4109, 81.4122, 88.2305, 80.2750, 68.5040, 82.1329, 77.2057, 84.7274,\n",
      "        83.0824, 84.3383, 73.6811, 84.0440, 83.5101, 83.3856, 80.5272, 83.3051])\n",
      "tensor([83.7124, 84.2680, 84.9787, 79.2415, 57.7465, 84.3197, 83.4022, 84.5393,\n",
      "        76.3212, 60.0078, 84.8947, 73.8726, 57.7852, 76.7606, 89.1782, 75.0549,\n",
      "        83.3699, 84.6169, 83.9385, 84.5329, 73.2265, 81.4511, 78.6794, 85.0627,\n",
      "        85.2436, 84.7913, 82.9888, 84.2680, 78.8474, 81.4317, 81.2379, 83.9837])\n",
      "tensor([83.8764, 84.0216, 86.1782, 81.4337, 54.9716, 84.8204, 83.3683, 82.8692,\n",
      "        76.1110, 62.1092, 85.2328, 73.3381, 60.5693, 78.9977, 87.9457, 75.6056,\n",
      "        83.2223, 84.2727, 83.4156, 84.8750, 73.9274, 81.4885, 76.9047, 85.1893,\n",
      "        84.4444, 84.4131, 82.7581, 84.4156, 79.3397, 81.4013, 81.1764, 84.4514])\n",
      "tensor([60.8218, 45.2901, 83.2536, 57.2490, 79.4870, 79.8747, 84.3391, 58.6316,\n",
      "        72.1669, 83.6671, 83.5379, 83.4346, 79.7842, 79.2092, 75.8108, 24.5962,\n",
      "        76.5603, 77.1224, 85.4116, 83.9256, 81.3090, 82.0843, 79.2609, 79.6162,\n",
      "        76.8316, 82.3298, 78.7763, 82.8918, 62.3853, 83.4798, 84.2874, 76.7541])\n",
      "tensor([65.2743, 43.9269, 82.9211, 54.8850, 80.7320, 79.0574, 84.3655, 64.0034,\n",
      "        76.4285, 83.9898, 84.6368, 83.3491, 77.9677, 80.8200, 76.3623, 25.2773,\n",
      "        75.4138, 80.1214, 86.8702, 83.6475, 80.3975, 82.4182, 80.8353, 78.4376,\n",
      "        74.6684, 83.0696, 78.0852, 83.1095, 65.0014, 83.5319, 84.7131, 76.0598])\n",
      "tensor([77.1288, 77.8524, 81.8840, 82.9565, 82.8466, 71.7987, 83.2536, 63.3674,\n",
      "        83.1309, 81.6126, 71.1138, 83.6607, 79.9910, 60.4406, 83.5508, 83.6413,\n",
      "        63.4643, 81.1281, 30.0943, 77.8137, 85.0239, 67.1469, 85.0110, 81.7871,\n",
      "        73.0004, 84.7655, 81.3800, 80.7469, 82.4913, 82.6463, 82.0067, 77.3291])\n",
      "tensor([76.9921, 76.5520, 80.7002, 82.4386, 82.3910, 69.8959, 83.9279, 65.3376,\n",
      "        81.7273, 81.8556, 71.9127, 82.8548, 79.0829, 59.0606, 83.7648, 85.0552,\n",
      "        63.9469, 81.3110, 33.5304, 77.9104, 84.8448, 62.1383, 84.8890, 82.1616,\n",
      "        72.4467, 84.0282, 80.1367, 80.0203, 83.4542, 82.5450, 81.0927, 77.1075])\n",
      "tensor([74.9128, 74.6737, 83.2278, 81.3800, 85.7475, 83.6090, 78.8280, 81.6708,\n",
      "        82.4331, 81.1668, 80.4884, 83.5379, 69.1175, 87.7568, 83.5250, 82.0067,\n",
      "        83.9256, 83.4152, 85.5925, 84.1969, 57.7077, 68.0256, 83.6994, 76.3600,\n",
      "        84.2357, 54.1155, 81.5997, 76.6830, 79.6033, 87.7116, 83.2278, 86.1352])\n",
      "tensor([74.4781, 79.1257, 82.8057, 81.6371, 85.3736, 82.2767, 79.3600, 82.7035,\n",
      "        83.0824, 80.4534, 79.4377, 83.9015, 68.5411, 87.5342, 83.8650, 82.3537,\n",
      "        83.1394, 83.4965, 86.2297, 83.8581, 52.9882, 67.7212, 83.9631, 77.7209,\n",
      "        84.2913, 56.0040, 81.5074, 77.9392, 78.6279, 87.8193, 84.0362, 84.9545])\n",
      "tensor([82.3556, 55.3883, 75.8948, 78.8086, 76.2631, 78.2788, 80.8309, 60.7507,\n",
      "        82.8079, 73.5754, 80.5530, 77.9041, 70.6164, 81.3154, 59.1872, 78.5308,\n",
      "        66.0421, 82.1618, 79.2350, 83.9514, 82.9694, 75.7591, 84.2421, 70.4484,\n",
      "        79.7519, 81.2120, 81.2185, 73.2976, 83.7124, 74.2538, 79.9199, 83.3053])\n",
      "tensor([82.8822, 55.7612, 77.9279, 77.9218, 76.9316, 77.0669, 80.2906, 58.7659,\n",
      "        82.1892, 72.7888, 81.8649, 78.2857, 67.9764, 80.8696, 54.4663, 78.7594,\n",
      "        64.0426, 82.6620, 78.9792, 84.8834, 82.8098, 77.8647, 83.7241, 72.5638,\n",
      "        79.8692, 81.4006, 82.2559, 73.3862, 82.5394, 75.4928, 81.2387, 82.9329])\n",
      "tensor([70.2416, 84.2486, 60.2274, 83.2407, 83.5056, 83.9450, 79.2092, 82.9758,\n",
      "        83.7641, 84.1646, 86.1481, 58.2569, 83.1890, 82.7432, 88.9521, 79.3255,\n",
      "        76.0822, 75.2229, 79.9199, 83.1438, 69.6279, 81.8775, 77.6909, 80.1654,\n",
      "        83.8287, 84.1065, 80.3592, 82.4848, 70.8425, 82.8595, 79.9070, 70.9717])\n",
      "tensor([70.3207, 84.5723, 60.5633, 83.3856, 83.3508, 82.6969, 79.8002, 83.4381,\n",
      "        83.7713, 83.9360, 86.5266, 57.6323, 82.9408, 82.4277, 89.2569, 79.8095,\n",
      "        74.5054, 75.3879, 80.3999, 83.4681, 70.3408, 82.2288, 77.0954, 81.8715,\n",
      "        84.8866, 84.4332, 79.4771, 80.2689, 69.5892, 82.3659, 79.9982, 70.2424])\n",
      "tensor([82.3039, 77.3485, 83.3312, 83.2020, 76.7283, 84.1969, 72.4835, 82.9306,\n",
      "        83.2795, 84.1775, 79.4612, 85.6829, 74.3959, 77.6715, 79.4353, 69.8217,\n",
      "        70.9329, 84.0742, 84.6104, 82.6528, 71.6372, 83.0469, 83.5702, 78.9508,\n",
      "        75.9917, 85.4439, 79.5322, 83.1503, 83.6155, 81.5609, 80.9924, 83.8028])\n",
      "tensor([81.5689, 80.4431, 83.1734, 83.1327, 76.6855, 84.1420, 70.9174, 82.5854,\n",
      "        80.6962, 84.2724, 78.8101, 84.9104, 77.1193, 77.0329, 81.0102, 73.6867,\n",
      "        73.8818, 84.0591, 84.4840, 82.3146, 66.4102, 82.7266, 83.2720, 76.5564,\n",
      "        75.9287, 83.9626, 80.8055, 83.9462, 83.9666, 80.3918, 81.2321, 83.4770])\n",
      "tensor([84.2745, 83.1761, 83.0275, 58.9094, 81.3477, 68.8203, 82.2135, 80.2429,\n",
      "        84.2228, 84.4618, 83.4152, 61.1255, 75.9400, 76.2049, 80.7533, 79.5775,\n",
      "        67.2438, 82.9500, 80.7921, 77.0384, 78.3241, 83.9062, 77.5552, 67.3278,\n",
      "        60.7249, 81.0053, 83.2536, 79.5258, 82.9435, 84.1323, 81.6061, 84.5329])\n",
      "tensor([84.5170, 82.9735, 82.7501, 59.8585, 82.1469, 67.2253, 81.5675, 79.5980,\n",
      "        84.6766, 84.8123, 83.3350, 58.3070, 76.3509, 75.8193, 79.7276, 78.1518,\n",
      "        66.1357, 82.7970, 79.6833, 76.8557, 77.0309, 83.9644, 77.2081, 66.1562,\n",
      "        64.1016, 80.1548, 83.2149, 79.0884, 82.7617, 84.1380, 81.5986, 85.7274])\n",
      "tensor([83.5185, 76.2825, 81.9098, 83.3699, 80.2429, 80.9472, 86.9944, 83.6413,\n",
      "        81.7418, 81.2831, 80.9665, 76.1533, 76.8898, 81.1474, 82.1424, 80.4820,\n",
      "        82.8725, 83.3829, 84.9464, 80.2623, 79.6938, 81.9873, 72.0119, 83.8545,\n",
      "        73.2394, 84.1711, 84.2615, 84.1775, 73.3234, 77.6974, 66.0163, 59.4457])\n",
      "tensor([83.6173, 78.0273, 82.5070, 83.6272, 79.1917, 78.2399, 86.6525, 83.8395,\n",
      "        81.7452, 81.3828, 81.7106, 75.8467, 78.1243, 81.6849, 82.9847, 79.3449,\n",
      "        83.0985, 84.1777, 84.4262, 79.4958, 78.7366, 82.1207, 71.1392, 82.5952,\n",
      "        73.7803, 84.1129, 84.9302, 84.3588, 70.5862, 77.9818, 64.0808, 58.9979])\n",
      "tensor([78.7763, 83.2084, 84.6427, 75.6428, 81.5480, 76.3342, 84.2098, 72.6580,\n",
      "        70.6487, 82.1166, 83.0340, 84.6556, 76.3212, 46.4078, 71.8891, 83.1180,\n",
      "        83.7253, 83.5315, 82.3298, 82.1166, 75.2746, 85.0174, 80.5078, 82.8725,\n",
      "        81.3929, 75.5330, 73.4074, 84.1646, 78.6730, 82.1166, 77.9558, 73.0069])\n",
      "tensor([78.3245, 83.0032, 84.1184, 73.8460, 81.9305, 75.7699, 83.9919, 74.1901,\n",
      "        64.0646, 82.3280, 84.0860, 85.0481, 76.5459, 53.2522, 68.8131, 82.9910,\n",
      "        85.6324, 83.6811, 83.3728, 82.2458, 75.4778, 84.0525, 81.2192, 82.7531,\n",
      "        81.1285, 72.4165, 69.2802, 83.9237, 81.9054, 80.5553, 77.6173, 71.8623])\n",
      "tensor([46.7955, 84.7332, 83.6025, 75.7850, 84.6556, 84.4230, 74.4088, 84.1905,\n",
      "        80.2042, 84.9658, 75.8754, 87.0784, 82.1812, 85.8767, 85.7152, 84.1840,\n",
      "        84.0483, 85.3276, 78.0656, 59.8979, 82.8725, 83.7770, 84.7655, 74.9645,\n",
      "        83.6607, 82.3491, 73.8274, 75.8884, 72.6450, 89.2945, 84.2745, 80.4626])\n",
      "tensor([50.5336, 84.6943, 83.4733, 74.5864, 85.4217, 84.6611, 75.1497, 84.1303,\n",
      "        80.4009, 85.2842, 75.0154, 88.0579, 81.1618, 85.0731, 84.1353, 84.6135,\n",
      "        83.1863, 84.6378, 79.1817, 57.5088, 84.5179, 84.1308, 84.4654, 73.4442,\n",
      "        83.4794, 81.8788, 75.2431, 75.2232, 74.6078, 89.3965, 84.4475, 76.4827])\n",
      "tensor([79.5839, 88.2995, 83.4087, 82.7949, 79.9716, 83.0275, 72.0571, 81.7354,\n",
      "        78.8668, 77.7168, 83.4669, 87.1883, 81.1345, 83.2795, 53.7020, 81.3865,\n",
      "        56.8743, 60.7701, 79.0025, 75.7398, 82.1101, 82.8466, 82.8660, 55.9698,\n",
      "        82.6011, 86.7037, 61.8555, 76.3342, 84.5264, 76.5861, 65.1570, 84.0677])\n",
      "tensor([79.2951, 86.9173, 83.2625, 83.3845, 78.0815, 83.8660, 72.9334, 81.0293,\n",
      "        78.5160, 78.1579, 83.6355, 87.3597, 80.6370, 80.4425, 61.5954, 81.0621,\n",
      "        58.0301, 61.1885, 81.0432, 76.2840, 82.6004, 82.4573, 81.8823, 54.9502,\n",
      "        82.5073, 86.2921, 63.8733, 77.5015, 83.8477, 76.8451, 62.8424, 84.2435])\n",
      "tensor([83.9450, 83.0663, 80.7792, 79.6292, 84.5200, 76.2049, 84.9851, 77.9817,\n",
      "        87.3239, 81.5092, 68.1354, 79.2738, 76.0951, 80.8696, 82.1036, 80.9665,\n",
      "        63.3480, 79.9328, 80.1912, 80.1008, 81.2250, 75.1971, 82.9435, 83.3441,\n",
      "        84.2809, 84.5716, 76.9738, 77.3808, 77.1159, 76.7153, 66.6688, 81.2379])\n",
      "tensor([84.2313, 82.8400, 81.6481, 79.9772, 85.3312, 77.1963, 85.1222, 77.7597,\n",
      "        86.2322, 82.1565, 70.9061, 81.0144, 82.6668, 80.5332, 82.5290, 81.0710,\n",
      "        62.5052, 80.6305, 79.6747, 81.7960, 79.8570, 72.3893, 82.6061, 83.5639,\n",
      "        84.7931, 84.4540, 79.9029, 77.9072, 76.0099, 76.2128, 67.9098, 81.1246])\n",
      "tensor([82.6205, 84.3326, 81.6061, 69.6925, 84.0871, 75.9207, 82.0196, 84.4037,\n",
      "        85.1854, 83.2536, 70.0478, 82.9371, 83.4087, 69.6343, 85.1854, 75.6105,\n",
      "        74.0276, 85.8444, 75.5976, 81.5092])\n",
      "tensor([82.7670, 84.7846, 82.0197, 68.9059, 84.0727, 73.4447, 82.8514, 84.6212,\n",
      "        84.6829, 83.2345, 68.1190, 83.7089, 83.7322, 71.6767, 84.4821, 76.7049,\n",
      "        74.1232, 85.6309, 73.5327, 82.2495])\n",
      "16\n",
      "TOTAL Loss: 34.563\n",
      "Msqrt: 0.069\n",
      "Msqrt2: 2.160\n",
      "Test loss: 2.160\n"
     ]
    }
   ],
   "source": [
    "test_acc, msqrt = test_model(model, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLearningCurvePredictor():\n",
    "    \"\"\"A learning curve predictor that predicts the last observed epoch of the validation accuracy as final performance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for datapoint in X:\n",
    "            predictions.append(datapoint[\"Train/val_accuracy\"][-1])\n",
    "        return predictions\n",
    "    \n",
    "def score(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on validation set: 31.921338670622784\n"
     ]
    }
   ],
   "source": [
    "# Training & tuning\n",
    "predictor = SimpleLearningCurvePredictor()\n",
    "for data in train_data:\n",
    "    data['Train/val_accuracy']=data['Train/val_accuracy'][0:10]\n",
    "\n",
    "predictor.fit(train_data, train_targets)\n",
    "preds = predictor.predict(val_data)\n",
    "mse = score(val_targets, preds)\n",
    "print(\"Score on validation set:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test score: 24.199496266785523\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation (after tuning)\n",
    "final_preds = predictor.predict(test_data)\n",
    "final_score = score(test_targets, final_preds)\n",
    "print(\"Final test score:\", final_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
