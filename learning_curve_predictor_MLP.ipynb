{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learning_curve_predictor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe547VaI7tXS",
        "colab_type": "text"
      },
      "source": [
        "Some data read applications - already provided\n",
        "https://github.com/automl/LCBench/blob/master/notebooks/Task%20A.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cd-C4iPDtS6",
        "colab_type": "text"
      },
      "source": [
        "The current version provides 2000 configurations, each evaluated on 35 datasets over 50 epochs. Logs include for each epoch:\n",
        "\n",
        "Training, test and validation losses\n",
        "\n",
        "*   Training, test and validation accuracy\n",
        "*   Global gradient statistics (max, mean, median, norm, std, q10, q25, q75, q90)\n",
        "\n",
        "\n",
        "*   Training, test and validation balanced accuracy\n",
        "\n",
        "*   Layer-wise gradient statistics (max, mean, median, norm, std, q10, q25, q75, q90)\n",
        "*   Learning rate\n",
        "\n",
        "\n",
        "*   Runtime\n",
        "\n",
        "\n",
        "And additionally:\n",
        "\n",
        "Configuration (architecture, hyperparameters)\n",
        "\n",
        "\n",
        "*   Number of model parameters\n",
        "*   Dataset statistics (number of classes, instances and features)\n",
        "\n",
        "\n",
        "> The data was created using Auto-PyTorch. All runs feature funnel-shaped MLP nets and use SGD with cosine annealing without restarts. Overall, 7 parameters were sampled at random (4 float, 3 integer). These are:\n",
        "\n",
        "* Batch size: [16, 512], on log-scale\n",
        "* Learning rate: [1e-4, 1e-1], on log-scale\n",
        "* Momentum: [0.1, 0.99]\n",
        "* Weight decay: [1e-5, 1e-1]\n",
        "* Number of layers: [1, 5]\n",
        "* Maximum number of units per layer: [64, 1024], on log-scale\n",
        "* Dropout: [0.0, 1.0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTpjw8xbdnOD",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "# copy the API part as is\n",
        "import os as os\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Benchmark():\n",
        "    \"\"\"API for TabularBench.\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir, cache=False, cache_dir=\"cached/\"):\n",
        "        \"\"\"Initialize dataset (will take a few seconds-minutes).\n",
        "        \n",
        "        Keyword arguments:\n",
        "        bench_data -- str, the raw benchmark data directory\n",
        "        \"\"\"\n",
        "        if not os.path.isfile(data_dir) or not data_dir.endswith(\".json\"):\n",
        "            raise ValueError(\"Please specify path to the bench json file.\")\n",
        "            \n",
        "        self.data_dir = data_dir\n",
        "        self.cache_dir = cache_dir\n",
        "        self.cache = cache\n",
        "        \n",
        "        print(\"==> Loading data...\")\n",
        "        self.data = self._read_data(data_dir)\n",
        "        self.dataset_names = list(self.data.keys())\n",
        "        print(\"==> Done.\")\n",
        "        \n",
        "    def query(self, dataset_name, tag, config_id):\n",
        "        \"\"\"Query a run.\n",
        "        \n",
        "        Keyword arguments:\n",
        "        dataset_name -- str, the name of the dataset in the benchmark\n",
        "        tag -- str, the tag you want to query\n",
        "        config_id -- int, an identifier for which run you want to query, if too large will query the last run\n",
        "        \"\"\"\n",
        "        config_id = str(config_id)\n",
        "        if dataset_name not in self.dataset_names:\n",
        "            raise ValueError(\"Dataset name not found.\")\n",
        "        \n",
        "        if config_id not in self.data[dataset_name].keys():\n",
        "            raise ValueError(\"Config nr %s not found for dataset %s.\" % (config_id, dataset_name))\n",
        "            \n",
        "        if tag in self.data[dataset_name][config_id][\"log\"].keys():\n",
        "            return self.data[dataset_name][config_id][\"log\"][tag]\n",
        "        \n",
        "        if tag in self.data[dataset_name][config_id][\"results\"].keys():\n",
        "            return self.data[dataset_name][config_id][\"results\"][tag]\n",
        "        \n",
        "        if tag in self.data[dataset_name][config_id][\"config\"].keys():\n",
        "            return self.data[dataset_name][config_id][\"config\"][tag]\n",
        "        \n",
        "        if tag == \"config\":\n",
        "            return self.data[dataset_name][config_id][\"config\"]\n",
        "            \n",
        "        raise ValueError(\"Tag %s not found for config %s for dataset %s\" % (tag, config_id, dataset_name))\n",
        "        \n",
        "    def query_best(self, dataset_name, tag, criterion, position=0):\n",
        "        \"\"\"Query the n-th best run. \"Best\" here means achieving the largest value at any epoch/step,\n",
        "        \n",
        "        Keyword arguments:\n",
        "        dataset_name -- str, the name of the dataset in the benchmark\n",
        "        tag -- str, the tag you want to query\n",
        "        criterion -- str, the tag you want to use for the ranking\n",
        "        position -- int, an identifier for which position in the ranking you want to query\n",
        "        \"\"\"\n",
        "        performances = []\n",
        "        for config_id in self.data[dataset_name].keys():\n",
        "            performances.append((config_id, max(self.query(dataset_name, criterion, config_id))))\n",
        "\n",
        "        performances.sort(key=lambda x: x[1]*1000, reverse=True)\n",
        "        desired_position = performances[position][0]\n",
        "\n",
        "        return self.query(dataset_name, tag, desired_position)\n",
        "        \n",
        "    def get_queriable_tags(self, dataset_name=None, config_id=None):\n",
        "        \"\"\"Returns a list of all queriable tags\"\"\"\n",
        "        if dataset_name is None or config_id is None:\n",
        "            dataset_name = list(self.data.keys())[0]\n",
        "            config_id = list(self.data[dataset_name].keys())[0]\n",
        "        else:\n",
        "            config_id = str(config_id)\n",
        "        log_tags = list(self.data[dataset_name][config_id][\"log\"].keys())\n",
        "        result_tags = list(self.data[dataset_name][config_id][\"results\"].keys())\n",
        "        config_tags = list(self.data[dataset_name][config_id][\"config\"].keys())\n",
        "        additional = [\"config\"]\n",
        "        return log_tags+result_tags+config_tags+additional\n",
        "    \n",
        "    def get_dataset_names(self):\n",
        "        \"\"\"Returns a list of all availabe dataset names like defined on openml\"\"\"\n",
        "        return self.dataset_names\n",
        "    \n",
        "    def get_openml_task_ids(self):\n",
        "        \"\"\"Returns a list of openml task ids\"\"\"\n",
        "        task_ids = []\n",
        "        for dataset_name in self.dataset_names:\n",
        "            task_ids.append(self.query(dataset_name, \"OpenML_task_id\", 1))\n",
        "        return task_ids\n",
        "    \n",
        "    def get_number_of_configs(self, dataset_name):\n",
        "        \"\"\"Returns the number of configurations for a dataset\"\"\"\n",
        "        if dataset_name not in self.dataset_names:\n",
        "            raise ValueError(\"Dataset name not found.\")\n",
        "        return len(self.data[dataset_name].keys())\n",
        "    \n",
        "    def get_config(self, dataset_name, config_id):\n",
        "        \"\"\"Returns the configuration of a run specified by dataset name and config id\"\"\"\n",
        "        if dataset_name not in self.dataset_names:\n",
        "            raise ValueError(\"Dataset name not found.\")\n",
        "        return self.data[dataset_name][config_id][\"config\"]\n",
        "        \n",
        "    def plot_by_name(self, dataset_names, x_col, y_col, n_configs=10, show_best=False, xscale='linear', yscale='linear', criterion=None):\n",
        "        \"\"\"Plot multiple datasets and multiple runs.\n",
        "        \n",
        "        Keyword arguments:\n",
        "        dataset_names -- list\n",
        "        x_col -- str, tag to plot on x-axis\n",
        "        y_col -- str, tag to plot on y-axis\n",
        "        n_configs -- int, number of configs to plot for each dataset\n",
        "        show_best -- bool, weather to show the n_configs best (according to query_best())\n",
        "        xscale -- str, set xscale, options as in matplotlib: \"linear\", \"log\", \"symlog\", \"logit\", ...\n",
        "        yscale -- str, set yscale, options as in matplotlib: \"linear\", \"log\", \"symlog\", \"logit\", ...\n",
        "        criterion -- str, tag used as criterion for query_best()    \n",
        "        \"\"\"\n",
        "        if isinstance(dataset_names, str):\n",
        "            dataset_names = [dataset_names]\n",
        "        if not isinstance(dataset_names, (list, np.ndarray)):\n",
        "            raise ValueError(\"Please specify a dataset name or a list list of dataset names.\")\n",
        "    \n",
        "        n_rows = len(dataset_names)\n",
        "        fig, axes = plt.subplots(n_rows, 1, sharex=False, sharey=False, figsize=(10,7*n_rows))\n",
        "    \n",
        "        if criterion is None:\n",
        "            criterion = y_col\n",
        "            \n",
        "        loop_arg = enumerate(axes.flatten()) if len(dataset_names)>1 else [(0,axes)]\n",
        "    \n",
        "        for ind_ax, ax in loop_arg:\n",
        "            for ind in range(n_configs):\n",
        "                try:\n",
        "                    if ind==0:\n",
        "                        instances = int(self.query(dataset_names[ind_ax], \"instances\", 0))\n",
        "                        classes = int(self.query(dataset_names[ind_ax], \"classes\", 0))\n",
        "                        features = int(self.query(dataset_names[ind_ax], \"features\", 0))\n",
        "            \n",
        "                    if show_best:\n",
        "                        x = self.query_best(dataset_names[ind_ax], x_col, criterion, ind)\n",
        "                        y = self.query_best(dataset_names[ind_ax], y_col, criterion, ind)\n",
        "                    else:\n",
        "                        x = self.query(dataset_names[ind_ax], x_col, ind+1)\n",
        "                        y = self.query(dataset_names[ind_ax], y_col, ind+1)\n",
        "                        \n",
        "                    ax.plot(x, y, 'p-')\n",
        "                    ax.set_xscale(xscale)\n",
        "                    ax.set_yscale(yscale)\n",
        "                    ax.set(xlabel=\"step\", ylabel=y_col)\n",
        "                    title_str = \", \".join([dataset_names[ind_ax],\n",
        "                                          \"features: \" + str(features),\n",
        "                                          \"classes: \" + str(classes),\n",
        "                                          \"instances: \" + str(instances)])\n",
        "                    ax.title.set_text(title_str)\n",
        "                except ValueError:\n",
        "                    print(\"Run %i not found for dataset %s\" %(ind, dataset_names[ind_ax]))\n",
        "                except Exception as e:\n",
        "                    raise e\n",
        "                    \n",
        "    def _cache_data(self, data, cache_file):\n",
        "        os.makedirs(self.cache_dir, exist_ok=True)\n",
        "        with gzip.open(cache_file, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "    \n",
        "    def _read_cached_data(self, cache_file):\n",
        "        with gzip.open(cache_file, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        return data\n",
        "                    \n",
        "    def _read_file_string(self, path):\n",
        "        \"\"\"Reads a large json string from path. Python file handler has issues with large files so it has to be chunked.\"\"\"\n",
        "        # Shoutout to https://stackoverflow.com/questions/48122798/oserror-errno-22-invalid-argument-when-reading-a-huge-file\n",
        "        file_str = ''\n",
        "        with open(path, 'r') as f:\n",
        "            while True:\n",
        "                block = f.read(64 * (1 << 20)) # Read 64 MB at a time\n",
        "                if not block:                  # Reached EOF\n",
        "                    break\n",
        "                file_str += block\n",
        "        return file_str\n",
        "        \n",
        "    def _read_data(self, path):\n",
        "        \"\"\"Reads cached data if available. If not, reads json and caches the data as .pkl.gz\"\"\"\n",
        "        cache_file = os.path.join(self.cache_dir, os.path.basename(self.data_dir).replace(\".json\", \".pkl.gz\"))\n",
        "        if os.path.exists(cache_file) and self.cache:\n",
        "            print(\"==> Found cached data, loading...\")\n",
        "            data = self._read_cached_data(cache_file)\n",
        "        else:\n",
        "            print(\"==> No cached data found or cache set to False.\")\n",
        "            print(\"==> Reading json data...\")\n",
        "            data = json.loads(self._read_file_string(path))\n",
        "            if self.cache:\n",
        "                print(\"==> Caching data...\")\n",
        "                self._cache_data(data, cache_file)\n",
        "        return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaO_igFU7rRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Read data\n",
        "def cut_data(data, cut_position):\n",
        "    targets = []\n",
        "    for dp in data:\n",
        "        targets.append(dp[\"Train/val_accuracy\"][50])\n",
        "        for tag in dp:\n",
        "            if tag.startswith(\"Train/\"):\n",
        "                dp[tag] = dp[tag][0:cut_position]\n",
        "    return data, targets\n",
        "\n",
        "def read_data(bench=None):\n",
        "    dataset_name = 'Fashion-MNIST'\n",
        "    n_configs = bench.get_number_of_configs(dataset_name)\n",
        "    \n",
        "    # Query API\n",
        "    data = []\n",
        "    for config_id in range(n_configs):\n",
        "        data_point = dict()\n",
        "        data_point[\"config\"] = bench.query(dataset_name=dataset_name, tag=\"config\", config_id=config_id)\n",
        "        for tag in bench.get_queriable_tags(dataset_name=dataset_name, config_id=config_id):\n",
        "            if tag.startswith(\"Train/\"):\n",
        "                data_point[tag] = bench.query(dataset_name=dataset_name, tag=tag, config_id=config_id)    \n",
        "        data.append(data_point)\n",
        "        \n",
        "    # Split: 50% train, 25% validation, 25% test (the data is already shuffled)\n",
        "    indices = np.arange(n_configs)\n",
        "    ind_train = indices[0:int(np.floor(0.5*n_configs))]\n",
        "    ind_val = indices[int(np.floor(0.5*n_configs)):int(np.floor(0.75*n_configs))]\n",
        "    ind_test = indices[int(np.floor(0.75*n_configs)):]\n",
        "\n",
        "    array_data = np.array(data)\n",
        "    train_data = array_data[ind_train]\n",
        "    val_data = array_data[ind_val]\n",
        "    test_data = array_data[ind_test]\n",
        "    \n",
        "    # Cut curves for validation and test\n",
        "    cut_position = 11\n",
        "    val_data, val_targets = cut_data(val_data, cut_position)\n",
        "    test_data, test_targets = cut_data(test_data, cut_position)\n",
        "    train_data, train_targets = cut_data(train_data, 51)   # Cut last value as it is repeated\n",
        "    \n",
        "    return train_data, val_data, test_data, train_targets, val_targets, test_targets\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIr3ZHe2BXEl",
        "colab_type": "text"
      },
      "source": [
        "Utils by Sven - some data preprocessing and normalizing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSGFWVT0BRao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import typing\n",
        "\n",
        "def check_cuda():\n",
        "    \"\"\"Returns device\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def tt(ndarray:np.ndarray):\n",
        "  if torch.cuda.is_available():\n",
        "    return Variable(torch.from_numpy(ndarray).float().cuda(), requires_grad=True)\n",
        "  return Variable(torch.from_numpy(ndarray).float(), requires_grad=True)\n",
        "\n",
        "def make_torch_dataset(data:np.ndarray, targets:np.ndarray)->torch.utils.data.dataset:\n",
        "    return TensorDataset(*data,targets)\n",
        "\n",
        "def make_torch_dataloader(torch_dataset:torch.utils.data.dataset,batch_size)->torch.utils.data.dataset:\n",
        "    return DataLoader(dataset=torch_dataset,batch_size=batch_size)\n",
        "\n",
        "def remove_config_entry(configs:np.ndarray,keys=['activation',\n",
        "                                                 'cosine_annealing_T_max',\n",
        "                                                 'cosine_annealing_eta_min',\n",
        "                                                 'imputation_strategy',\n",
        "                                                 'learning_rate_scheduler',\n",
        "                                                 'loss',\n",
        "                                                 'mlp_shape',\n",
        "                                                 'normalization_strategy',\n",
        "                                                 'optimizer',\n",
        "                                                 'network'])->np.ndarray:\n",
        "    for c in configs:\n",
        "        for key in keys:\n",
        "            if key in c.keys():\n",
        "                del c[key]\n",
        "    return configs\n",
        "\n",
        "def get_first_n_epochs(temporal_data:np.ndarray,n=10)->np.ndarray:\n",
        "    return temporal_data[:,:n]\n",
        "\n",
        "def get_last_n_epochs(temporal_data:np.ndarray,n=41)->np.ndarray:\n",
        "    return temporal_data[:,-n:]\n",
        "\n",
        "def extract_from_data(data:np.ndarray, key)->np.ndarray:\n",
        "    output = []\n",
        "    for d in data:\n",
        "        output.append(d[key])\n",
        "    return np.array(output)\n",
        "\n",
        "def normalize_configs(configs:np.ndarray)->np.ndarray:\n",
        "    output = []\n",
        "    for config in configs:\n",
        "        #config['activation'] = 0 if \"relu\" else 1\n",
        "        config['batch_size'] /= 511\n",
        "        #config['cosine_annealing_T_max'] = 1\n",
        "        #config['cosine_annealing_eta_min'] = config['cosine_annealing_eta_min']\n",
        "        #config['imputation_strategy'] = 0\n",
        "        config['learning_rate'] = config['learning_rate']\n",
        "        #config['learning_rate_scheduler'] = 0\n",
        "        #config['loss'] = 0\n",
        "        config['max_dropout'] = config['max_dropout']\n",
        "        config['max_units'] /= 1024\n",
        "        #config['mlp_shape'] = 0\n",
        "        config['momentum'] = config['momentum']\n",
        "        #config['normalization_strategy'] = 0\n",
        "        config['num_layers'] /= 4\n",
        "        #config['optimizer'] = 0\n",
        "        config['weight_decay'] = config['weight_decay']\n",
        "\n",
        "        list_values = [float(v) for v in config.values()]\n",
        "        output.append(list_values)\n",
        "\n",
        "    return np.array(output)\n",
        "\n",
        "def normalize_temporal_data(temporal_data:np.ndarray,\n",
        "                            normalization_factor)->np.ndarray:\n",
        "    return temporal_data/normalization_factor\n",
        "\n",
        "def prep_data(data:np.ndarray, target_data:np.ndarray, batch_size,\n",
        "              temporal_keys=['Train/val_accuracy'], first_n_epochs=10,\n",
        "              normalization_factor_temporal_data=[1],\n",
        "              one_shot=False)->torch.utils.data.dataset:\n",
        "\n",
        "    assert batch_size > 0\n",
        "    assert first_n_epochs > 0\n",
        "    assert len(temporal_keys) == len(normalization_factor_temporal_data)\n",
        "    assert all([normalization_factor > 0 for normalization_factor in normalization_factor_temporal_data])\n",
        "\n",
        "    configs = extract_from_data(data,\"config\")\n",
        "    configs = remove_config_entry(configs)\n",
        "    configs = normalize_configs(configs)\n",
        "    configs = torch.FloatTensor(configs)\n",
        "\n",
        "    data_list = []\n",
        "    for k, normalization_factor in zip(temporal_keys, normalization_factor_temporal_data):\n",
        "        d = extract_from_data(data,key=k)\n",
        "        d = get_first_n_epochs(d, first_n_epochs)\n",
        "        d = normalize_temporal_data(d, normalization_factor)\n",
        "        d = torch.FloatTensor(d)\n",
        "        data_list.append(d)\n",
        "\n",
        "    data_list.append(configs)\n",
        "\n",
        "    if one_shot:\n",
        "        val_acc = extract_from_data(data, key='Train/val_accuracy')\n",
        "        target_data = val_acc\n",
        "\n",
        "    target_data = torch.FloatTensor(target_data)\n",
        "    dataset = make_torch_dataset(data_list,target_data)\n",
        "    data_loader = make_torch_dataloader(dataset,batch_size)\n",
        "    return data_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68QRUjH9235T",
        "colab_type": "code",
        "outputId": "8b8b71e3-a38a-45d8-8226-ced8dcaf4bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  data_path = '/content/drive/My Drive/ColabWorks/DL_project/DATA/fashion_mnist.json'\n",
        "  data_root = Benchmark(data_dir=data_path)\n",
        "\n",
        "  train_data, val_data, test_data, train_targets, val_targets, test_targets = read_data(data_root)\n",
        "\n",
        "  print(\"Train:\", len(train_data))\n",
        "  print(\"Validation:\", len(val_data))\n",
        "  print(\"Test:\", len(test_data))\n",
        "\n",
        "  \n",
        "  # we have 1000 configurations, each having 50 epochs\n",
        "  print(train_data[0])\n",
        "  print(train_targets)\n",
        "  print(test_data[0].keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Loading data...\n",
            "==> No cached data found or cache set to False.\n",
            "==> Reading json data...\n",
            "==> Done.\n",
            "Train: 1000\n",
            "Validation: 500\n",
            "Test: 500\n",
            "{'config': {'batch_size': 71, 'imputation_strategy': 'mean', 'learning_rate_scheduler': 'cosine_annealing', 'loss': 'cross_entropy_weighted', 'network': 'shapedmlpnet', 'max_dropout': 0.025926231827891333, 'normalization_strategy': 'standardize', 'optimizer': 'sgd', 'cosine_annealing_T_max': 50, 'cosine_annealing_eta_min': 1e-08, 'activation': 'relu', 'max_units': 293, 'mlp_shape': 'funnel', 'num_layers': 3, 'learning_rate': 0.0018243300267253295, 'momentum': 0.21325193168301043, 'weight_decay': 0.020472816917443872}, 'Train/loss': [2.314540386199951, 1.9220430850982664, 1.201086401939392, 0.8936665654182434, 0.7658511996269226, 0.6972703337669373, 0.6540151834487915, 0.622721254825592, 0.5984394550323486, 0.5788958072662354, 0.5626271367073059, 0.5487494468688965, 0.5370408892631531, 0.52688068151474, 0.5176886320114136, 0.5100060701370239, 0.5032476782798767, 0.4971102178096771, 0.4917032718658447, 0.4868072271347046, 0.4824835956096649, 0.478643923997879, 0.4751310348510742, 0.4720841944217682, 0.4692725241184234, 0.4667946696281433, 0.4644564986228943, 0.4624367654323578, 0.4604667127132416, 0.4588868916034698, 0.4573012888431549, 0.4560083150863648, 0.4547891616821289, 0.4537324607372284, 0.4526824653148651, 0.4518616795539856, 0.4510762691497803, 0.4503895342350006, 0.4498150646686554, 0.4493193626403809, 0.4488559663295746, 0.4484851360321045, 0.4481396079063416, 0.4478870332241057, 0.4476811587810517, 0.4475265145301819, 0.4473650753498077, 0.4472693502902985, 0.4472173452377319, 0.4471725225448608, 0.4471440017223358], 'Train/train_accuracy': [11.106867790222168, 50.709693908691406, 66.00152587890625, 71.7586441040039, 74.17733001708984, 75.60944366455078, 76.52281951904297, 77.54121398925781, 78.31774139404297, 79.0401611328125, 79.7498550415039, 80.4404525756836, 80.90827941894531, 81.39202117919922, 81.68798828125, 82.047607421875, 82.22901153564453, 82.55680847167969, 82.70001983642578, 82.91642761230469, 83.11055755615234, 83.2060317993164, 83.37152099609375, 83.47972869873047, 83.64839935302734, 83.70568084716797, 83.81388854980469, 83.87117004394531, 83.9857406616211, 84.0653076171875, 84.04621124267578, 84.14168548583984, 84.17032623291016, 84.2467041015625, 84.29444122314453, 84.3358154296875, 84.35491180419922, 84.34217834472656, 84.42174530029297, 84.40901184082031, 84.45356750488281, 84.44720458984375, 84.4790267944336, 84.48220825195312, 84.50130462646484, 84.51721954345703, 84.51721954345703, 84.5140380859375, 84.52040100097656, 84.52676391601562, 84.52040100097656], 'Train/val_accuracy': [10.808890342712402, 62.20441818237305, 69.59555816650389, 73.38803100585938, 74.7641830444336, 75.93358612060547, 76.85101318359375, 77.67799377441406, 78.53727722167969, 79.16397094726562, 79.90696716308594, 80.24292755126953, 80.73394775390625, 80.97945404052734, 81.4963150024414, 81.54154205322266, 81.98087310791016, 82.20054626464844, 82.3814468383789, 82.58818817138672, 82.60757446289062, 82.7174072265625, 82.9952163696289, 82.9952163696289, 83.20842742919922, 83.22134399414062, 83.24073028564453, 83.20196533203125, 83.28595733642578, 83.37640380859375, 83.47977447509766, 83.47332000732422, 83.46039581298828, 83.5250015258789, 83.48623657226562, 83.49916076660156, 83.64775848388672, 83.64129638671875, 83.5831527709961, 83.60899353027344, 83.57022857666016, 83.5831527709961, 83.58961486816406, 83.62837219238281, 83.60899353027344, 83.6154556274414, 83.6154556274414, 83.60899353027344, 83.60899353027344, 83.60899353027344, 83.60899353027344], 'Train/train_cross_entropy': [2.31477952003479, 1.925082087516785, 1.2073423862457275, 0.898890495300293, 0.7705944776535034, 0.701798677444458, 0.6584690809249878, 0.6270896792411804, 0.6027058362960815, 0.5831239223480225, 0.5667710304260254, 0.552855908870697, 0.5410635471343994, 0.5308622717857361, 0.5216435790061951, 0.5139043927192688, 0.5071302652359009, 0.5009735822677612, 0.4955162703990936, 0.4906153082847595, 0.4863073825836182, 0.4823884069919586, 0.4788355529308319, 0.4758119583129883, 0.4729432463645935, 0.4704678356647492, 0.4681459963321686, 0.4660601615905762, 0.4641269445419312, 0.4624836444854736, 0.4609126150608063, 0.4596169590950012, 0.4583995640277863, 0.4573152959346771, 0.4562482535839081, 0.4554460048675537, 0.4546229243278504, 0.4539612531661987, 0.4533813297748566, 0.45289254188537603, 0.4524313509464264, 0.4520545899868012, 0.4517109394073486, 0.4514327645301819, 0.4512335658073425, 0.4510596990585327, 0.450927346944809, 0.4508289694786072, 0.4507613778114319, 0.4507176578044891, 0.4506911337375641], 'Train/val_cross_entropy': [2.315356969833374, 1.5003297328948977, 1.0052359104156494, 0.8254659175872803, 0.7370596528053284, 0.686074435710907, 0.6500141620635986, 0.6234857439994812, 0.6025150418281555, 0.5845781564712524, 0.5705317258834839, 0.558441162109375, 0.5472085475921631, 0.5379854440689087, 0.5302418470382689, 0.5230669379234314, 0.5169710516929626, 0.511444091796875, 0.5065515041351318, 0.5024760961532593, 0.4987019896507263, 0.4952479600906372, 0.4922080934047699, 0.489131212234497, 0.4866599440574646, 0.4844854474067688, 0.4825488030910492, 0.4808264672756195, 0.4793540835380554, 0.4775918424129486, 0.4764044284820557, 0.4753933846950531, 0.4743318259716034, 0.4732537865638733, 0.4727434515953064, 0.4719882011413574, 0.471136748790741, 0.4705747067928314, 0.4701181054115296, 0.4696892499923706, 0.4694279730319977, 0.4691015779972077, 0.468923807144165, 0.4686501324176788, 0.4684690833091736, 0.4683493375778198, 0.4682632982730866, 0.4682066738605499, 0.4681692123413086, 0.4681477546691895, 0.46813818812370295], 'Train/train_balanced_accuracy': [0.11164481192827223, 0.5111237168312073, 0.663524866104126, 0.7199824452400208, 0.7438814640045166, 0.7581242322921753, 0.7672160863876343, 0.7773272395133972, 0.7850455045700073, 0.7922106385231018, 0.7992759943008423, 0.8061105012893677, 0.8107619285583496, 0.8155531287193298, 0.8185028433799744, 0.8220609426498413, 0.8238718509674072, 0.8271278738975525, 0.8285375833511353, 0.8306913971900941, 0.8326320648193359, 0.8335700035095215, 0.835210919380188, 0.8362869620323181, 0.8379560112953186, 0.8385252356529236, 0.8396113514900208, 0.8401509523391724, 0.8413178324699402, 0.8420843482017517, 0.8419018983840942, 0.8428568840026855, 0.8431277871131897, 0.8438923954963684, 0.8443647027015686, 0.8447757363319397, 0.8449568152427673, 0.844839334487915, 0.8456266522407532, 0.8455063700675964, 0.8459436297416687, 0.8458757996559143, 0.8461952209472656, 0.8462176322937012, 0.84641432762146, 0.8465741276741028, 0.8465726971626282, 0.8465458154678345, 0.8466061353683472, 0.8466677069664001, 0.8466043472290039], 'Train/val_balanced_accuracy': [0.10835736244916916, 0.6229134798049927, 0.6968868374824524, 0.7348409295082092, 0.7483322620391846, 0.7604489922523499, 0.7694666981697083, 0.7778470516204834, 0.7863079905509949, 0.7924979329109192, 0.7999365329742432, 0.8033849596977234, 0.8081265091896057, 0.8106713891029358, 0.8156009912490845, 0.8162862658500671, 0.8205859661102295, 0.8227128386497498, 0.8245580792427063, 0.8266464471817017, 0.8267289400100708, 0.8279160261154175, 0.8306246399879456, 0.8306862711906433, 0.8327447772026062, 0.8328984379768372, 0.833074688911438, 0.8327571153640747, 0.8335115313529968, 0.8344327807426453, 0.8354799151420593, 0.8354057073593141, 0.8353031277656555, 0.8358986377716064, 0.835580587387085, 0.8356673717498779, 0.8371261954307556, 0.837026059627533, 0.8365148901939392, 0.8367510437965393, 0.8363507390022278, 0.8364783525466919, 0.8365402817726135, 0.8369189500808716, 0.8367278575897217, 0.8367916941642761, 0.8367916941642761, 0.8367284536361694, 0.8367284536361694, 0.8367284536361694, 0.8367284536361694], 'Train/test_result': [11.298701286315918, 63.40692520141602, 70.80086517333984, 74.26839447021484, 75.69696807861328, 76.79653930664062, 77.85281372070312, 78.4372329711914, 79.20779418945312, 80.01298522949219, 80.6277084350586, 81.19047546386719, 81.58441925048828, 81.9177474975586, 82.33766174316406, 82.37662506103516, 82.66233825683594, 82.89177703857422, 83.14286041259766, 83.36796569824219, 83.38961029052734, 83.58441925048828, 83.67532348632812, 83.76190185546875, 83.91342163085938, 83.99134063720703, 84.04762268066406, 84.08657836914062, 84.18614959716797, 84.2164535522461, 84.16450500488281, 84.3160171508789, 84.32467651367188, 84.34632110595703, 84.35064697265625, 84.4112548828125, 84.47186279296875, 84.48052215576172, 84.51515197753906, 84.54545593261719, 84.5497817993164, 84.5627670288086, 84.58008575439453, 84.60173034667969, 84.59740447998047, 84.60173034667969, 84.60173034667969, 84.61038970947266, 84.60606384277344, 84.61038970947266, 84.61038970947266], 'Train/test_cross_entropy': [2.3033690452575684, 2.1482305526733403, 1.9666275978088381, 1.8846628665924072, 1.8396514654159544, 1.8117018938064573, 1.795209288597107, 1.7807185649871826, 1.7705466747283936, 1.7626832723617554, 1.7560901641845703, 1.7497503757476809, 1.7445058822631836, 1.7387540340423584, 1.7362792491912842, 1.7308284044265747, 1.7287636995315552, 1.7258812189102173, 1.723011493682861, 1.720811128616333, 1.7192497253417969, 1.7169904708862305, 1.7154482603073118, 1.7135871648788452, 1.71268892288208, 1.7111783027648926, 1.7100942134857178, 1.709059476852417, 1.7085707187652588, 1.7074123620986938, 1.7069252729415894, 1.7060939073562622, 1.7054632902145386, 1.7050728797912598, 1.704724669456482, 1.7045077085494995, 1.7038688659667969, 1.7039333581924438, 1.7033045291900637, 1.7032833099365234, 1.7031219005584717, 1.7029352188110352, 1.7027705907821655, 1.7026275396347046, 1.7025402784347534, 1.7024644613265991, 1.702415943145752, 1.702390432357788, 1.7023699283599854, 1.702359318733215, 1.702353596687317], 'Train/test_balanced_accuracy': [0.11198260635137558, 0.6276976466178894, 0.7036504745483398, 0.7391173243522644, 0.7536818981170654, 0.7644315361976624, 0.7752931118011475, 0.7810603380203247, 0.7889361381530762, 0.7971172332763672, 0.80345219373703, 0.8090721368789673, 0.8130962252616882, 0.8163279294967651, 0.8208298087120056, 0.8209488391876221, 0.8239546418190002, 0.8263662457466125, 0.8288120031356812, 0.8310720324516296, 0.8314934372901917, 0.8333563804626465, 0.8342950344085693, 0.835132896900177, 0.8366842269897461, 0.8374318480491638, 0.8380611538887024, 0.8383867740631104, 0.8394902348518372, 0.8397294878959656, 0.8391944169998169, 0.8407616615295409, 0.8408417701721191, 0.8410762548446655, 0.8410933017730713, 0.8417477607727051, 0.8423286676406859, 0.8424581289291382, 0.8427663445472717, 0.8431017398834229, 0.843153715133667, 0.84327632188797, 0.8434633612632751, 0.843668520450592, 0.8436223864555359, 0.8436602950096129, 0.8436611890792847, 0.8437498211860657, 0.8437056541442871, 0.8437510132789612, 0.8437510132789612], 'Train/gradient_max': [0.0, 0.0, 0.10586243122816086, 0.1372951865196228, 0.060096394270658486, 0.12831450998783112, 0.07224087417125702, 0.10895182192325592, 0.038571696728467934, 0.11969753354787828, 0.08576388657093048, 0.19101792573928836, 0.04423476010560989, 0.05116775631904602, 0.08508592844009401, 0.06636020541191101, 0.12620417773723602, 0.08887721598148346, 0.04745351150631905, 0.0675930306315422, 0.035544462502002716, 0.060606077313423164, 0.035128045827150345, 0.0988294705748558, 0.10556405782699584, 0.03621358796954155, 0.033543430268764496, 0.110090933740139, 0.04895811900496483, 0.0743754431605339, 0.14462310075759888, 0.0599600076675415, 0.07811316847801207, 0.11659039556980133, 0.09219434857368468, 0.09934605658054352, 0.03157912194728851, 0.057568054646253586, 0.05027781799435616, 0.0254621934145689, 0.03828880935907364, 0.05076158791780472, 0.06936022639274597, 0.07100971788167952, 0.07659123092889786, 0.05108122527599335, 0.06981608271598816, 0.09405843168497086, 0.06168945878744125, 0.09513851255178453, 0.0842503160238266], 'Train/gradient_mean': [0.0, 0.0, -0.00014559304690919816, 3.464809560682625e-05, 7.102977542672306e-05, 7.978617941262199e-05, -3.648757774499245e-05, 5.4346874094335356e-05, -1.7948208551388234e-05, -2.0157405742793344e-05, 2.948513611045201e-06, -5.0230566557729624e-05, 2.715408663789276e-05, 1.2231323125888592e-05, 9.319127798335104e-07, -1.6989893993013542e-05, 5.703762508346699e-05, 0.0001227392494911328, 5.495534787769429e-05, 2.4339708033949137e-05, 3.6305295907368418e-06, 2.603233042464126e-06, -1.2715433513221795e-05, -4.6759527322137735e-05, -1.7048618246917613e-05, -2.029118695645593e-05, 4.198414444545052e-06, -7.335015652643051e-06, -2.0118810425628905e-05, 8.201002492569387e-05, 6.144759390735999e-05, 1.1377592272765469e-06, -3.1278443202609196e-05, -1.1186142728547566e-05, 4.044498564326205e-05, 2.9918272048234943e-05, -4.7591734073648695e-06, 2.317938378837425e-05, -1.7166403267765418e-05, -4.545447882264853e-05, -2.5250512408092615e-05, -6.725797902618068e-06, -1.2444980711734388e-05, -3.512934927130118e-05, 2.9816363166901286e-05, 1.5863315638853234e-05, -1.2432899893610738e-05, -2.7994208721793257e-05, -3.55812767338648e-06, -6.13318698015064e-05, 4.389401146909222e-05], 'Train/gradient_median': [0.0, 0.0, -8.711432747077197e-05, 4.657680619857274e-05, 4.6458426368189976e-05, 5.198971848585643e-05, -2.445553036523052e-05, 3.048463622690178e-05, -1.6678004612913355e-05, -3.184920387866441e-06, 1.0874144891204196e-06, -3.6123623431194574e-05, 1.2323990631557535e-05, 1.516591783001786e-05, -4.103385435882956e-06, -1.1928643289138565e-05, 1.841806442826055e-05, 6.323226989479735e-05, 1.896826142910868e-05, 1.3519214917323552e-05, -6.334213367154007e-07, -2.0056710070548434e-07, 1.9626522771432064e-06, -3.4519114706199616e-05, -1.1136859029647894e-05, -1.965713636309374e-05, 3.5713680972548896e-06, -3.7081788377690827e-06, -9.460519322601613e-06, 3.416834806557745e-05, 1.3168204532121308e-05, -6.9224206526996585e-06, 4.479394647205481e-06, -2.008880073844921e-05, 1.1047938642150257e-05, 1.2343450180196667e-05, 1.7635917174629867e-05, -8.61050523326412e-07, -9.503944966127163e-06, -2.4130920792231336e-05, -2.1377194570959546e-05, -6.1767159422743126e-06, -6.332355042104609e-06, -2.0598470655386333e-05, 1.2376055565255228e-05, -4.8942206376523245e-06, -1.1662696124403736e-05, -2.6110263206646774e-05, 3.875160928146215e-06, -4.590328899212182e-05, 2.085893902403768e-05], 'Train/gradient_std': [0.0, 0.0, 0.0016678317915648222, 0.0017902028048411012, 0.0015404667938128116, 0.001956119667738676, 0.0012916972627863288, 0.0022860511671751733, 0.001120465574786067, 0.0020147189497947693, 0.0013948471751064062, 0.002713550813496113, 0.0014641875168308616, 0.0017461240058764815, 0.001688468735665083, 0.0012095230631530285, 0.0019407949876040218, 0.0015583720523864029, 0.0013217029627412558, 0.0015215162420645356, 0.0011193165555596352, 0.001380746136419475, 0.0012006119359284642, 0.0019091195426881315, 0.0012276875786483288, 0.0011980239069089293, 0.0009683744865469637, 0.0021090442314744, 0.001206308137625456, 0.0020342259667813782, 0.002048986265435815, 0.0014594786334782839, 0.0020065223798155785, 0.0017949147149920466, 0.0020116211380809546, 0.001774360192939639, 0.0013683560537174346, 0.0018030786886811254, 0.0012920990120619535, 0.0012378428364172578, 0.001192210242152214, 0.001257391762919724, 0.001212912262417376, 0.0015738991787657142, 0.001327879261225462, 0.001206951565109193, 0.0017600822029635308, 0.001829562243074179, 0.0015667254338040948, 0.001895847264677286, 0.001988691044971347], 'Train/gradient_q10': [0.0, 0.0, -0.001655328320339322, -0.0015535787679255009, -0.0013263424625620244, -0.001469781738705933, -0.0012163768988102677, -0.0016644453862681985, -0.0010792494285851717, -0.0015117943985387685, -0.0012938219588249922, -0.0023767438251525164, -0.0013535261387005448, -0.001550738699734211, -0.0016069767298176885, -0.001228363486006856, -0.0016575595363974571, -0.0013709006598219275, -0.0011729507241398096, -0.0013794052647426724, -0.0010996733326464894, -0.001183271873742342, -0.0011447267606854439, -0.001928391749970615, -0.0011099885450676084, -0.0011710908729583025, -0.0009354890207760037, -0.0018837057286873462, -0.0012565316865220664, -0.0017112658824771645, -0.0015889883507043125, -0.001445969333872199, -0.0018286831909790637, -0.0017138152616098525, -0.001690921955741942, -0.0014665480703115465, -0.0013824739726260304, -0.001610029139555991, -0.0012613643193617465, -0.001240058452822268, -0.001210712012834847, -0.0012670876458287241, -0.001075850450433791, -0.0016127006383612752, -0.0011950486805289984, -0.0011153483064845202, -0.001755397068336606, -0.001736583188176155, -0.001383887603878975, -0.0018509328365325926, -0.0019108881242573261], 'Train/gradient_q25': [0.0, 0.0, -0.0007790295057930052, -0.0006591492565348744, -0.0005883371341042222, -0.0006320884567685425, -0.0005672548431903124, -0.0006770956679247321, -0.0005163089372217655, -0.0006374684744514525, -0.0005625080084428191, -0.0009755723876878619, -0.0005779994535259902, -0.0006573633290827273, -0.0006925340858288108, -0.0005332148866727948, -0.0006889985525049268, -0.0005594699177891017, -0.0004952503368258476, -0.0005845548002980651, -0.00046019404544495046, -0.000503815186675638, -0.00047691600047983235, -0.0007909719133749603, -0.00048138713464140887, -0.0005212535616010426, -0.00040538728353567416, -0.0007699949201196432, -0.0005248198285698892, -0.0006261324742808938, -0.0006046072230674326, -0.0005916753434576094, -0.0007234196527861059, -0.0006974731804803014, -0.0006338701350614429, -0.0005595810944214463, -0.0005447031580843031, -0.0006774419452995063, -0.0005177145940251648, -0.0005128192133270204, -0.0005081759300082922, -0.0005420618108473718, -0.00043040807940997183, -0.0006713051116093992, -0.00047645001905038953, -0.0004623349814210087, -0.0006979355821385981, -0.0007006531814113259, -0.0005460051470436156, -0.0007915744790807366, -0.000724690908100456], 'Train/gradient_q75': [0.0, 0.0, 0.0005752102006226777, 0.0007538322824984789, 0.0007062561926431954, 0.0007828341913409531, 0.000523556605912745, 0.0007765215123072268, 0.0004860768385697156, 0.0006320016691461205, 0.0005693635903298856, 0.0008549467311240733, 0.0006226915284059943, 0.0007011937559582293, 0.0007058639894239603, 0.0005104136653244495, 0.0007799242157489061, 0.0007667322643101215, 0.0005541546270251273, 0.0006367117166519165, 0.0004633292846847325, 0.0005169963696971536, 0.00048010132741183037, 0.0007061926880851388, 0.0004562386893667281, 0.00048134409007616336, 0.0004176564689259976, 0.0007553861942142247, 0.0004995182389393449, 0.0007568360888399184, 0.0006800103583373129, 0.0005827235290780662, 0.0007081566727720201, 0.0006473975954577327, 0.0006948210648261012, 0.0006133784772828221, 0.0005798580241389573, 0.0006841923459433018, 0.0004960401565767826, 0.00045088841579854494, 0.00045396218774840247, 0.0005299477488733828, 0.00041422061622142786, 0.0006167868850752711, 0.0005135033861733973, 0.0004697838739957661, 0.0006820042617619038, 0.0006331279873847961, 0.0005552906659431756, 0.0006610053242184223, 0.0008026358555071057], 'Train/gradient_q90': [0.0, 0.0, 0.0012909946963191032, 0.0016132111195474865, 0.0015292580937966704, 0.0017053090268746018, 0.0011341475183144214, 0.0018386808224022386, 0.0010338259162381294, 0.0014896783977746966, 0.0013030725531280039, 0.0022236069198697805, 0.0014366877730935812, 0.001595565234310925, 0.001635239226743579, 0.0011846928391605616, 0.0018417524406686425, 0.0017302848864346745, 0.001320590847171843, 0.00146336923353374, 0.0011047973530367015, 0.0012130895629525185, 0.0011150046484544873, 0.0018045156029984355, 0.001055571250617504, 0.0011469119926914573, 0.0009550058166496457, 0.0018653407460078601, 0.0011960952542722225, 0.001969842240214348, 0.0018014785600826144, 0.0014443326508626342, 0.0017255202401429415, 0.001671735430136323, 0.0018049387726932766, 0.0015732066240161655, 0.0013598110526800153, 0.0016742816660553215, 0.0012253538006916642, 0.0011119727278128266, 0.001139759086072445, 0.0012609270634129646, 0.0010228282772004604, 0.001515519921667874, 0.0012589916586875913, 0.0011841050582006574, 0.001714737038128078, 0.0016611663158982992, 0.001361953211016953, 0.0016831390094012024, 0.0019764658063650127], 'Train/layer_wise_gradient_max_layer_0': [0.0, 0.0, 0.01651715114712715, 0.011683543212711813, 0.018144059926271442, 0.031675852835178375, 0.012453529983758926, 0.01840977370738983, 0.019656496122479442, 0.00968546885997057, 0.011793262325227259, 0.01728774420917034, 0.029392698779702187, 0.034151941537857056, 0.016034087166190147, 0.010077035054564476, 0.0414479561150074, 0.01638725958764553, 0.021935852244496342, 0.04216143488883972, 0.016186965629458427, 0.060606077313423164, 0.035128045827150345, 0.015821924433112144, 0.01114327646791935, 0.011421244591474531, 0.01139084529131651, 0.0196295902132988, 0.01308115106076002, 0.05526401475071907, 0.024634674191474915, 0.021945815533399585, 0.047110531479120255, 0.017008326947689056, 0.022532595321536064, 0.028154958039522168, 0.025919906795024872, 0.057568054646253586, 0.020132994279265404, 0.0177955012768507, 0.014238499104976652, 0.012037170119583608, 0.014772284775972366, 0.0133784469217062, 0.027433283627033237, 0.011457303538918495, 0.020326515659689903, 0.016401715576648712, 0.015031388960778713, 0.0329357348382473, 0.03786109387874603], 'Train/layer_wise_gradient_max_layer_1': [0.0, 0.0, 0.01432336773723364, 0.014678011648356916, 0.013536152429878712, 0.017029820010066032, 0.011806017719209194, 0.01732623390853405, 0.010365849360823631, 0.022634290158748627, 0.015509989112615584, 0.03706558048725128, 0.01398603618144989, 0.018847979605197903, 0.015541759319603443, 0.010555312037467957, 0.01990571804344654, 0.014268189668655397, 0.011073307134211063, 0.013621670193970205, 0.009257458150386809, 0.013682144694030285, 0.008342934772372246, 0.021904274821281437, 0.012697702273726463, 0.00924509484320879, 0.00889637041836977, 0.020852787420153614, 0.01092276256531477, 0.01427292078733444, 0.020824339240789413, 0.012655532918870449, 0.01903964951634407, 0.018061874434351918, 0.023554326966404915, 0.0184819120913744, 0.01017659343779087, 0.014793711714446543, 0.012360796332359314, 0.010573630221188068, 0.010109646245837213, 0.014950762502849104, 0.01324383169412613, 0.015280272811651232, 0.012506408616900444, 0.012529610656201841, 0.01914080232381821, 0.025855101644992832, 0.016351990401744843, 0.028825003653764725, 0.016847684979438782], 'Train/layer_wise_gradient_max_layer_2': [0.0, 0.0, 0.10586243122816086, 0.1372951865196228, 0.060096394270658486, 0.12831450998783112, 0.07224087417125702, 0.10895182192325592, 0.038571696728467934, 0.11969753354787828, 0.08576388657093048, 0.19101792573928836, 0.04423476010560989, 0.05116775631904602, 0.08508592844009401, 0.06636020541191101, 0.12620417773723602, 0.08887721598148346, 0.04745351150631905, 0.0675930306315422, 0.035544462502002716, 0.035209104418754585, 0.03122202306985855, 0.0988294705748558, 0.10556405782699584, 0.03621358796954155, 0.033543430268764496, 0.110090933740139, 0.04895811900496483, 0.0743754431605339, 0.14462310075759888, 0.0599600076675415, 0.07811316847801207, 0.11659039556980133, 0.09219434857368468, 0.09934605658054352, 0.03157912194728851, 0.04080936312675476, 0.05027781799435616, 0.0254621934145689, 0.03828880935907364, 0.05076158791780472, 0.06936022639274597, 0.07100971788167952, 0.07659123092889786, 0.05108122527599335, 0.06981608271598816, 0.09405843168497086, 0.06168945878744125, 0.09513851255178453, 0.0842503160238266], 'Train/layer_wise_gradient_mean_layer_0': [0.0, 0.0, -6.361035048030317e-05, 0.0001156383877969347, 0.00012787788000423458, 0.00013594090705737474, -9.854789823293686e-06, 6.254554318729788e-05, -7.968754289322533e-06, 4.799211819772609e-05, 7.1881186158861965e-06, -0.0001304136822000146, 4.163588164374232e-05, 5.7507015299052e-05, 2.187249447160866e-05, 1.3365144695853814e-05, 8.006364805623889e-05, 0.0001737966522341594, 0.000106607360066846, 3.5853612644132233e-05, 1.0831647159648128e-05, 3.0344097012857674e-06, -9.405282980878836e-06, -0.00010152237518923356, -2.457105802022852e-05, -7.74995714891702e-06, 2.0430941731319766e-05, -4.085845284862444e-05, -7.55111204853165e-06, 9.860772115644069e-05, 8.968883048510179e-05, -1.9098317352472804e-05, 2.161348857043777e-05, -3.18556631100364e-05, 3.269565422669985e-05, 3.4683409467106685e-05, 4.245009222358931e-06, 4.920683932141401e-05, -2.6571495254756886e-05, -7.432992424583063e-05, -9.540360224491453e-06, -4.12899089496932e-06, -1.854225956776645e-05, -6.683899846393615e-05, 3.031572123290971e-05, 8.88626618689159e-06, -4.133391121285968e-05, -6.389545887941496e-05, -4.663637355406536e-06, -0.00011613908282015471, 4.212308340356685e-05], 'Train/layer_wise_gradient_mean_layer_1': [0.0, 0.0, -0.00030941411387175316, -0.00012491006054915488, -4.038627957925201e-05, -3.014757203345653e-05, -8.96047495189123e-05, 3.879481300828047e-05, -3.7939178582746536e-05, -0.0001550503948237747, -5.438353127829032e-06, 0.0001074233659892343, -1.1480942703201436e-06, -7.70726110204123e-05, -4.0458202420268215e-05, -7.719982386333868e-05, 1.2247374797880186e-05, 2.3432196030626073e-05, -4.639093458536081e-05, 1.868283220574085e-06, -1.0587755241431296e-05, 1.7418163906768314e-06, -1.9457318558124825e-05, 6.074754855944776e-05, -2.452434500810341e-06, -4.535791595117189e-05, -2.784469870675821e-05, 5.873360714758746e-05, -4.523434836301021e-05, 5.024064739700407e-05, 6.422165370167932e-06, 4.107595668756403e-05, -0.0001361679605906829, 2.9451190130203028e-05, 5.62269997317344e-05, 2.0852323359576985e-05, -2.2639098460786045e-05, -2.796132321236655e-05, 1.1503791483846728e-06, 1.0951443073281553e-05, -5.6634620705153786e-05, -1.197685742226895e-05, -5.998383016958542e-07, 2.7005771698895842e-05, 2.9174245355534367e-05, 2.980751196446363e-05, 4.444566366146319e-05, 4.250976417097263e-05, -1.4570767916666227e-06, 4.608209928846918e-05, 4.791513492818922e-05], 'Train/layer_wise_gradient_mean_layer_2': [0.0, 0.0, 4.806530341738835e-06, 4.707149400928756e-06, 4.609505594999064e-06, 4.514377906161826e-06, 4.421406629262491e-06, 4.332269327278482e-06, 4.245258423907217e-06, 4.161182914685924e-06, 4.0802524381433605e-06, 4.0012387216847856e-06, 3.926358658645768e-06, 3.852796453429619e-06, 3.783966349146794e-06, 3.7172931115492247e-06, 3.654453621493303e-06, 3.5934112929680846e-06, 3.53650239048875e-06, 3.4810311717592417e-06, 3.429753178352257e-06, 3.380751422810136e-06, 3.333906533953268e-06, 3.2911348171182912e-06, 3.2501604891876927e-06, 3.212061301383073e-06, 3.1760889669385506e-06, 3.142452669635532e-06, 3.1126203339226777e-06, 3.083267301917658e-06, 3.0573887670470867e-06, 3.033906295968336e-06, 3.011622084159171e-06, 2.99167390949151e-06, 2.973043820020393e-06, 2.956929620268056e-06, 2.9427922072500223e-06, 2.9301522772584576e-06, 2.918051677625044e-06, 2.9091559099470032e-06, 2.9010388971073553e-06, 2.894269755415735e-06, 2.8885788196930666e-06, 2.8831875624746317e-06, 2.8801323423977014e-06, 2.8771971756214043e-06, 2.8736028525599977e-06, 2.8734530133078806e-06, 2.872045342883212e-06, 2.8717158784274943e-06, 2.8714462132484182e-06], 'Train/layer_wise_gradient_median_layer_0': [0.0, 0.0, -3.0334416805999354e-05, 0.00010953540186164899, 8.363397500943394e-05, 9.137202141573653e-05, -4.704594175564125e-06, 3.532965638441965e-05, -6.0378520174708675e-06, 2.593731005617883e-05, 1.2198801414342595e-05, -5.750009950133972e-05, 1.839704054873437e-05, 5.0098435167456046e-05, 5.196017809794284e-06, 9.13111216505058e-06, 1.8516191630624235e-05, 9.167226380668579e-05, 4.984650149708614e-05, 2.0717787265311927e-05, 8.548171535949221e-06, 6.681948434561491e-07, 1.0478089279786218e-05, -5.445216083899141e-05, -8.920648724597411e-06, -2.226651122327894e-05, 1.8336560970055867e-05, -2.2988084310782145e-05, 2.8742560971295465e-06, 3.372103674337268e-05, 3.6353410450828964e-06, -2.6472072931937877e-05, 4.602372791850939e-05, -2.6440118745085783e-05, -2.2266399355430617e-06, 1.0331783414585516e-05, 3.188883783877827e-05, 5.0675025704549626e-06, -1.0882068636419716e-05, -4.6342120185727254e-05, -8.048232302826364e-06, -2.8714998734358232e-06, -8.408464054809883e-06, -4.4269450881984085e-05, 1.618132409930695e-05, -1.3534265235648489e-05, -4.314054240239784e-05, -3.8225058233365417e-05, 2.223122464783956e-06, -8.690998947713524e-05, 1.1918409654754214e-05], 'Train/layer_wise_gradient_median_layer_1': [0.0, 0.0, -0.0002199928421759978, -0.00010050532000605016, -3.69221561413724e-05, -3.515977005008608e-05, -6.660259532509373e-05, 1.5776013242430054e-05, -4.126521889702417e-05, -6.713482434861362e-05, -2.0960191250196655e-05, 3.686975787786651e-06, -1.6700626019883205e-06, -5.636695641442202e-05, -2.269626929773949e-05, -5.014042835682631e-05, 1.692004116193857e-05, 1.134807098424062e-05, -3.569938780856319e-05, -6.996630759203981e-07, -1.7231208403245546e-05, -3.1937522635416826e-06, -1.5720430383225906e-05, 6.788606583540968e-07, -1.5205226191028489e-05, -1.6872256310307417e-05, -2.3401162252412178e-05, 3.286742139607668e-05, -3.4081433113897226e-05, 3.400737114134245e-05, 2.664979547262192e-05, 2.4812914489302788e-05, -7.081452349666506e-05, -1.0193696653004736e-05, 3.4006458008661866e-05, 1.364368017675588e-05, -7.756358172628097e-06, -1.1972699212492445e-05, -7.109087164280935e-06, 1.0494837624719366e-05, -4.404581704875454e-05, -1.2445837455743456e-05, -3.797782028414076e-06, 1.5471468941541392e-05, 5.859048542333768e-06, 7.737947271380108e-06, 3.825866588158533e-05, -7.1870927058625975e-06, 4.550568064587424e-06, 1.472575604566373e-05, 3.329160608700477e-05], 'Train/layer_wise_gradient_median_layer_2': [0.0, 0.0, 0.0011979166883975267, 0.0004914893652312458, 0.0008687566150911152, 0.0006103944033384322, 0.0003560659824870527, 0.0009584116050973537, 0.0005714092403650284, 0.0007534477626904845, 0.00017226435011252758, -0.0001759632432367653, 0.00026179343694821, 0.0006942853215150535, 0.00038878421764820814, 0.00016210388275794685, 0.0003270838060416281, 3.055139313801192e-05, 0.00026729470118880266, -2.449397288728505e-05, -3.300508251413703e-05, 0.0003259498917032033, 0.0002994808601215482, 5.43312489753589e-05, -9.019745630212128e-05, 0.0005656525027006866, 0.00012851148494519293, -9.051338565768674e-05, 0.0003868445346597582, 0.00021581328473985195, 0.0004552284081000834, 8.088530012173578e-05, 0.0006905521731823683, 0.00030229391995817423, -1.6684116417309273e-05, 0.0005563283339142798, 0.0005074987420812249, 0.00027927797054871917, 3.7849611544515944e-05, 0.0005050235777162014, 0.0001892155851237476, 0.0001698922278592363, 0.00022334587993100288, 0.0003206765977665782, 5.394600884756074e-05, 0.0005147581687197089, 0.0005046167643740773, 0.00031980645144358283, 0.0006687059649266303, 0.0003849103231914341, 0.00042826469871215517], 'Train/layer_wise_gradient_std_layer_0': [0.0, 0.0, 0.0011192118981853127, 0.001231759088113904, 0.0011565370950847864, 0.0012928502401337028, 0.0010279243579134343, 0.0013815489364787936, 0.000942583312280476, 0.0012657075421884654, 0.0011168158380314708, 0.002060707425698638, 0.0012777921510860324, 0.0014370345743373034, 0.0014489339664578438, 0.0011035494972020388, 0.0017109918408095837, 0.0013790064258500936, 0.0012388213071972132, 0.0013843298656865954, 0.0011234902776777744, 0.0013078466290608046, 0.0011938577517867088, 0.0016509300330653787, 0.0009746956056915225, 0.0010307971388101578, 0.0009614889859221877, 0.0016114985337480905, 0.0011373297311365604, 0.002049378352239728, 0.0018123789923265574, 0.001342527917586267, 0.0017453542677685616, 0.001559462514705956, 0.0018173233838751914, 0.00161092693451792, 0.001385789248161018, 0.0019148524152114987, 0.0012338964734226465, 0.001231969567015767, 0.0012001009890809655, 0.0011689569801092148, 0.0010975340846925974, 0.0014143545413389802, 0.001220358768478036, 0.0011129195336252449, 0.0016112768789753318, 0.0015797954984009266, 0.0012534427223727107, 0.001663331873714924, 0.0019204670097678898], 'Train/layer_wise_gradient_std_layer_1': [0.0, 0.0, 0.0015989135717973113, 0.0016537591582164168, 0.0014405453111976385, 0.001762772095389664, 0.0011263496708124876, 0.0020447324495762587, 0.0010258085094392302, 0.0019301425199955704, 0.00128776126075536, 0.002541605848819017, 0.0013096106704324484, 0.0015650397399440408, 0.0015072913374751804, 0.001047372817993164, 0.0016467206878587606, 0.0013559773797169328, 0.0011085760779678822, 0.0013013428542762995, 0.0009115479770116508, 0.0011287780944257975, 0.0009642087388783693, 0.0017107489984482527, 0.0011084761936217546, 0.0010761466110125184, 0.0007938544149510562, 0.0019436229486018421, 0.0010474566370248797, 0.001546952873468399, 0.001709262258373201, 0.0012237379560247064, 0.001780832652002573, 0.0015749558806419373, 0.00159099989105016, 0.0014189351350069044, 0.0010760853765532372, 0.0012650188291445374, 0.0010348851792514324, 0.0009429770871065557, 0.0009284793050028383, 0.0010485340608283877, 0.0010164943523705006, 0.0013453153660520911, 0.0011442112736403944, 0.0010237139649689198, 0.0014971854398027062, 0.0016233042115345595, 0.001394404796883464, 0.0016312646912410855, 0.0015674990136176348], 'Train/layer_wise_gradient_std_layer_2': [0.0, 0.0, 0.01592751033604145, 0.017261216416954994, 0.013577082194387913, 0.01979684270918369, 0.011280772276222706, 0.02449491992592812, 0.008594386279582977, 0.02019866555929184, 0.011507168412208555, 0.023603852838277817, 0.010707633569836615, 0.014184435829520226, 0.012813889421522616, 0.008221673779189587, 0.014693561010062696, 0.011353258974850178, 0.008596585132181644, 0.010609634220600128, 0.005728991236537695, 0.009040260687470436, 0.0066339876502752295, 0.014164554886519909, 0.01046173181384802, 0.008983262814581394, 0.005191621370613575, 0.01851145178079605, 0.0073764724656939515, 0.011449076235294342, 0.015655217692255974, 0.01010665949434042, 0.01487219240516424, 0.01353496126830578, 0.015417910180985928, 0.013325292617082596, 0.007095969747751951, 0.008242825977504253, 0.008439788594841957, 0.0073312600143253795, 0.00649464363232255, 0.0085128890350461, 0.008772908709943295, 0.011313319206237793, 0.00892653036862612, 0.00819582212716341, 0.012138509191572666, 0.013791318982839584, 0.013380173593759537, 0.014257072471082209, 0.012746637687087059], 'Train/layer_wise_gradient_q10_layer_0': [0.0, 0.0, -0.0014165082247927785, -0.0013102823868393898, -0.0011752079008147118, -0.0012793580535799265, -0.001129022683016956, -0.0014852841850370169, -0.001038984628394246, -0.0013244410511106253, -0.0012566236546263099, -0.002435738686472177, -0.0013297026744112372, -0.0014576504472643135, -0.0015794963110238314, -0.0012310881866142154, -0.0016146452398970725, -0.0013215286890044806, -0.0011518362443894148, -0.0013406104408204556, -0.0011651853565126655, -0.001201034989207983, -0.001182003179565072, -0.00205303356051445, -0.0011049374006688595, -0.0011541654821485279, -0.0009686842677183447, -0.0018591041443869474, -0.001295106136240065, -0.0017833905294537546, -0.0015921058366075158, -0.0015245797112584116, -0.0017800970235839484, -0.0017918818630278108, -0.0017710400279611351, -0.0015522153116762636, -0.0014750993577763438, -0.0016939237248152494, -0.001349711325019598, -0.001366340322420001, -0.0012818800751119852, -0.0013226867886260152, -0.001129843876697123, -0.0017197285778820515, -0.0012226331746205688, -0.0011657369323074818, -0.0018646286334842446, -0.0018270468572154641, -0.0013672622153535485, -0.0020231225062161684, -0.002051446586847305], 'Train/layer_wise_gradient_q10_layer_1': [0.0, 0.0, -0.002147065242752433, -0.002016443526372313, -0.0016412697732448578, -0.0019139457726851106, -0.0013768455246463418, -0.002080110600218177, -0.0011471158359199762, -0.0019645423162728552, -0.0013546391855925322, -0.002180999144911766, -0.0013816782739013431, -0.0017157692927867176, -0.0016404903726652265, -0.0012015829561278224, -0.0017237995052710176, -0.0014405410038307311, -0.0011978207621723411, -0.0014273343840613961, -0.000961155048571527, -0.0011267535155639052, -0.0010650118347257378, -0.0016146752750501034, -0.0010978694772347806, -0.0011960993288084865, -0.0008608100470155478, -0.001896214671432972, -0.001161704189144075, -0.001544949016533792, -0.001557531068101525, -0.0012404731241986153, -0.0018863586010411386, -0.0015024828026071189, -0.00148144515696913, -0.0012697610072791574, -0.0011926728766411545, -0.0014310991391539574, -0.0010691301431506872, -0.0009657855262048542, -0.0010587226133793592, -0.0011197607964277267, -0.0009439711575396359, -0.0013438649475574491, -0.001108553959056735, -0.0009890258079394698, -0.0014886726858094337, -0.0015044563915580511, -0.0013974995817989109, -0.0013769129291176796, -0.001575447036884725], 'Train/layer_wise_gradient_q10_layer_2': [0.0, 0.0, -0.016049999743700027, -0.01696678064763546, -0.0157957561314106, -0.02154836617410183, -0.009741456247866155, -0.020818721503019333, -0.0074826390482485294, -0.015296441502869131, -0.010409560985863207, -0.020744843408465385, -0.010997645556926727, -0.011661305092275144, -0.01371172908693552, -0.007870321162045002, -0.014017543755471706, -0.011459341272711754, -0.007944433018565178, -0.011350171640515327, -0.005561030935496092, -0.008265005424618721, -0.00681994343176484, -0.013759942725300787, -0.008812625892460344, -0.00806285999715328, -0.0039314040914177895, -0.01772136241197586, -0.008569748140871525, -0.012229708954691887, -0.012181643396615982, -0.010191277600824831, -0.012876045890152454, -0.012668661773204805, -0.013529769144952295, -0.009406242519617079, -0.008420734666287899, -0.00972511526197195, -0.008453136309981346, -0.007052640896290541, -0.0074851876124739656, -0.009422194212675095, -0.007520553190261125, -0.011024912819266321, -0.008917295373976229, -0.00862199254333973, -0.01371201407164335, -0.011450767517089844, -0.012977717444300652, -0.007430535275489092, -0.012450747191905977], 'Train/layer_wise_gradient_q25_layer_0': [0.0, 0.0, -0.0006693067261949183, -0.0005484311259351671, -0.0005287929088808597, -0.0005673283594660461, -0.0005330302519723773, -0.0006339436513371767, -0.0004974065814167262, -0.000581696629524231, -0.0005549644120037557, -0.0010443473001942039, -0.0005767229595221579, -0.0006108802626840769, -0.0006996314041316509, -0.0005293002468533814, -0.0006974812713451684, -0.0005374003667384387, -0.0004805448697879911, -0.0005760333733633161, -0.0004690404748544097, -0.0005173749523237348, -0.0004798412846866995, -0.000873718352522701, -0.0004859072214458138, -0.0005278489552438259, -0.0004064694803673774, -0.0008096919045783579, -0.0005354699678719044, -0.0006584014045074582, -0.0006356515805236995, -0.0006560254842042923, -0.0006884015747345983, -0.0007554745534434915, -0.0006861847941763699, -0.0005922388518229128, -0.0005736096063628793, -0.0007122991373762487, -0.0005528331967070699, -0.0005850176094099879, -0.0005270449910312892, -0.0005806502304039897, -0.00046106366789899766, -0.000754455802962184, -0.000495894404593855, -0.0004945968394167721, -0.0007780161686241628, -0.0007644764264114201, -0.0005540799465961754, -0.0009381565614603461, -0.0008117032703012228], 'Train/layer_wise_gradient_q25_layer_1': [0.0, 0.0, -0.001037187408655882, -0.0009168186807073653, -0.0007133079925552009, -0.0007871446432545781, -0.0006354819051921368, -0.0007862390484660863, -0.000550263619516045, -0.0007741837180219591, -0.0005736266612075268, -0.0008313274593092501, -0.0005766709800809622, -0.0007469608099199831, -0.0006730420282110572, -0.0005361975054256618, -0.0006645038956776261, -0.0005962066352367401, -0.0005193800898268819, -0.0005955709493719041, -0.0004409534158185125, -0.00047661471762694413, -0.00046942383050918585, -0.0006412810762412847, -0.00046819946146570157, -0.000506591226439923, -0.0004013259604107589, -0.0006751851178705692, -0.0005045915022492409, -0.0005595103139057754, -0.0005370121216401458, -0.0004700510471593589, -0.0007829290698282421, -0.000591268646530807, -0.0005332768196240066, -0.0005027545848861337, -0.0004947506240569055, -0.0006126154330559075, -0.00045478841639123857, -0.0003995433798991144, -0.00047491380246356135, -0.00047160251415334637, -0.0003803435829468072, -0.000521001173183322, -0.000438079732703045, -0.00040581406210549176, -0.0005444721900857985, -0.0005805417895317078, -0.0005274707218632102, -0.0005501408013515173, -0.0005767831462435424], 'Train/layer_wise_gradient_q25_layer_2': [0.0, 0.0, -0.0036593822296708814, -0.005568626336753368, -0.0042440895922482005, -0.0050867972895503035, -0.0025266176089644428, -0.004108816385269165, -0.0020566696766763926, -0.0020998013205826287, -0.0023455137852579355, -0.007131560239940881, -0.002833819482475519, -0.003121383255347609, -0.003389157587662339, -0.0023208935745060444, -0.0042017195373773575, -0.004386916756629944, -0.0016116858460009098, -0.004318605177104472, -0.0021993895061314106, -0.002468953607603908, -0.001549839274957776, -0.004417697433382273, -0.002661022823303938, -0.001160691026598215, -0.0012432399671524765, -0.005852122791111468, -0.001971871126443148, -0.002882602624595165, -0.0014566193567588925, -0.003248205874115229, -0.004101424477994442, -0.004187889862805605, -0.00432182801887393, -0.002026428934186697, -0.002011825330555439, -0.00335250748321414, -0.0029151334892958407, -0.0006580722983926535, -0.0022840469609946012, -0.0030210402328521013, -0.001245797611773014, -0.003208597656339407, -0.0027658098842948675, -0.001589965308085084, -0.003344052471220493, -0.003224147949367761, -0.001998028950765729, -0.002264609094709158, -0.003338085487484932], 'Train/layer_wise_gradient_q75_layer_0': [0.0, 0.0, 0.0005937642999924718, 0.0007751743542030454, 0.000724887300748378, 0.0007936856709420682, 0.0005297214956954122, 0.0007312297821044923, 0.00048548213089816267, 0.0006463786703534424, 0.0005752770812250674, 0.0008173536043614148, 0.0006332156481221317, 0.0007366106146946548, 0.000737461494281888, 0.0005479989922605455, 0.0008049672469496728, 0.0008287096861749887, 0.0006168235559016466, 0.0006459021824412048, 0.0004891809658147395, 0.000527349126059562, 0.0004936220357194543, 0.0006977982702665031, 0.00045682460768148303, 0.00048436905490234494, 0.0004441257915459573, 0.0007249413756653666, 0.000526625954080373, 0.0007958877831697464, 0.0007312167435884477, 0.0005949174519628286, 0.0007835705764591694, 0.0006589852273464202, 0.0007152365287765862, 0.0006398756522685289, 0.0006392823997884989, 0.0007533454336225986, 0.0005161049193702638, 0.00045677239540964365, 0.000496510649099946, 0.0005702114431187509, 0.00042932061478495603, 0.0006342885317280889, 0.0005338451592251658, 0.0004795909044332802, 0.0006950570386834443, 0.0006469398504123093, 0.0005478914245031773, 0.0006777310045436026, 0.0008679163875058293], 'Train/layer_wise_gradient_q75_layer_1': [0.0, 0.0, 0.0005125949974171816, 0.0006859257118776442, 0.0006529050297103822, 0.0007428635144606233, 0.0005033423658460379, 0.000875418190844357, 0.0004790483217220754, 0.0005890887696295978, 0.0005506656016223133, 0.000929616973735392, 0.0005932032945565879, 0.0006142588099464773, 0.0006342597771435976, 0.00043605946120806033, 0.0007232868229039012, 0.0006464655743911862, 0.00044721245649270713, 0.0006108519155532122, 0.00041440394124947483, 0.0004895735182799399, 0.00044825291843153536, 0.0007128117140382527, 0.0004510635626502335, 0.0004671613569371402, 0.000362797814887017, 0.0008148993365466595, 0.0004417812742758542, 0.0006819949485361576, 0.0005891364417038858, 0.0005548518383875488, 0.0005498392856679857, 0.0006151520647108555, 0.0006518098525702952, 0.0005595037946477532, 0.00047308340435847635, 0.0005710259429179133, 0.0004541704256553203, 0.00043305748840793967, 0.0003795239608734846, 0.00045366660924628377, 0.00038296432467177516, 0.000577976112253964, 0.0004698029661085457, 0.0004434621660038829, 0.0006502270116470754, 0.0005964097799733281, 0.000559723237529397, 0.0006247831624932588, 0.0006864263559691608], 'Train/layer_wise_gradient_q75_layer_2': [0.0, 0.0, 0.0059115211479365826, 0.004812880884855986, 0.0055368710309267035, 0.005643642973154782, 0.0029143569990992546, 0.007283185143023729, 0.00300983851775527, 0.004898415878415108, 0.002961382269859314, 0.002912136260420084, 0.0035903132520616046, 0.00502056535333395, 0.004472564905881882, 0.002585073933005333, 0.004573605488985778, 0.003599371062591672, 0.002335170516744256, 0.003284540260210633, 0.0016104720998555422, 0.0035396795719861984, 0.00274779018945992, 0.003477979684248566, 0.0014083106070756912, 0.0030696624889969826, 0.0014791297726333141, 0.0051737953908741465, 0.0031218384392559533, 0.0034941453486680984, 0.003036473412066698, 0.002071373164653778, 0.005255076102912426, 0.0031479026656597853, 0.003920219838619232, 0.0028537116013467308, 0.003402130678296089, 0.003204914042726159, 0.002532976912334561, 0.0030165035277605057, 0.002296098042279482, 0.0025028542149811983, 0.0020724069327116013, 0.003157516242936253, 0.0017389309359714387, 0.0027873525395989422, 0.003842162201181054, 0.0030436990782618523, 0.004722122102975845, 0.003482045605778694, 0.003422729903832078], 'Train/layer_wise_gradient_q90_layer_0': [0.0, 0.0, 0.0012401961721479893, 0.00155903201084584, 0.0015023078303784132, 0.0016275544185191393, 0.001113403937779367, 0.0016590325394645331, 0.0009989700047299266, 0.0014425694243982436, 0.001265758415684104, 0.0020775070879608393, 0.0014384743990376592, 0.0016020003240555525, 0.001655251020565629, 0.0012595239095389845, 0.0018712257733568551, 0.0018159449100494387, 0.0014329762198030946, 0.0014682960463687778, 0.0011806824477389455, 0.0012236735783517359, 0.001150441006757319, 0.0017639403231441977, 0.0010240040719509123, 0.0011624500621110199, 0.0010210752952843904, 0.0017558690160512926, 0.001240314682945609, 0.002117752330377698, 0.001935566542670131, 0.0014716676669195294, 0.0017912798793986442, 0.0016803269973024724, 0.0018763149855658405, 0.0016625364078208804, 0.0014628826174885035, 0.001834849012084305, 0.001285670790821314, 0.001158052124083042, 0.0012513014953583481, 0.0013259248808026314, 0.0010483622318133712, 0.0015551027609035373, 0.0012802158016711473, 0.0012152174022048714, 0.001765766995958984, 0.0016517241019755602, 0.0013122882228344679, 0.0017286652000620961, 0.002091707196086645], 'Train/layer_wise_gradient_q90_layer_1': [0.0, 0.0, 0.001376026775687933, 0.0017102591227740052, 0.0015467943158000708, 0.0018570363754406567, 0.0011525306617841125, 0.0022125928662717342, 0.0010833541164174676, 0.0015569539973512292, 0.0013649924658238888, 0.0025343250017613173, 0.0014027634169906378, 0.0015336284413933756, 0.0015558279119431973, 0.001011849381029606, 0.0017396738985553384, 0.0015177015447989106, 0.0010630286997184155, 0.001425628666765988, 0.0009458845597691834, 0.0011608616914600134, 0.0010237109381705523, 0.0018502826569601891, 0.0011078756069764497, 0.0010887999087572098, 0.0008156677940860392, 0.002056279219686985, 0.0010789987863972783, 0.0016726836329326036, 0.0015000952407717705, 0.0013661477714776993, 0.0015180136542767284, 0.0016162298852577806, 0.0016342729795724154, 0.0013636479852721095, 0.0011166376061737537, 0.0013456344604492188, 0.001087142387405038, 0.001002310891635716, 0.0009160638437606394, 0.0010939867934212089, 0.0009457357227802276, 0.001401651301421225, 0.0011930932523682716, 0.0010911027202382684, 0.0015699636423960328, 0.0016408548690378664, 0.0014226423809304831, 0.0015448547201231122, 0.0017016405472531915], 'Train/layer_wise_gradient_q90_layer_2': [0.0, 0.0, 0.013378499075770378, 0.013861083425581457, 0.013529323041439056, 0.018041053786873814, 0.009523051790893078, 0.022131368517875668, 0.00775986025109887, 0.015262754634022713, 0.00914633832871914, 0.019326046109199524, 0.011523733846843244, 0.013410183601081371, 0.011991414241492748, 0.0069628478959202775, 0.01340365968644619, 0.0102730393409729, 0.008161910809576511, 0.011226281523704529, 0.005714334547519685, 0.009125287644565105, 0.006284750998020172, 0.011376316659152508, 0.006774649024009705, 0.008569350466132164, 0.004184992518275976, 0.019464025273919102, 0.006981173995882273, 0.01104680635035038, 0.00944607425481081, 0.009856666438281536, 0.012963959015905857, 0.010830330662429331, 0.014890099875628946, 0.007014961447566748, 0.007239398080855608, 0.009941651485860348, 0.00901041179895401, 0.006713080685585736, 0.006500914692878722, 0.00833029393106699, 0.0055519924499094495, 0.009481973014771938, 0.00827321782708168, 0.007343575824052095, 0.0117113022133708, 0.010518965311348438, 0.012195204384624958, 0.009022073820233343, 0.01061573065817356], 'Train/gradient_norm': [0.0, 0.4172365069389343, 0.5076378583908081, 0.534052848815918, 0.4604084491729736, 0.5795320868492126, 0.3821776211261749, 0.6740172505378723, 0.3298194408416748, 0.5976179242134094, 0.4097511470317841, 0.803475558757782, 0.4270030558109283, 0.5127139091491699, 0.4915972948074341, 0.3482203781604767, 0.5650042891502379, 0.4537647366523743, 0.3800636529922485, 0.4367767870426178, 0.3066085875034332, 0.3945742249488831, 0.3335249125957489, 0.558509886264801, 0.36813947558403015, 0.3510926365852356, 0.26622599363327026, 0.6266355514526367, 0.34085747599601746, 0.5629625916481018, 0.6004955768585205, 0.4210423529148102, 0.5859913229942322, 0.523945689201355, 0.5785903930664062, 0.5113118886947632, 0.3737640678882599, 0.4728278517723084, 0.3667045831680298, 0.346596360206604, 0.32667097449302673, 0.3615325093269348, 0.3608074188232422, 0.457619309425354, 0.3843853175640106, 0.3476897180080414, 0.5091668963432312, 0.5356152653694153, 0.4671671092510224, 0.5508825182914734, 0.5588887929916382], 'Train/lr': [0.0, 0.0018243300728499892, 0.0018243300728499892, 0.0018225300591439009, 0.0018171373521909122, 0.0018081731395795941, 0.0017956728115677836, 0.001779685728251934, 0.0017602749867364766, 0.0017375170718878508, 0.0017115019727498293, 0.0016823321348056195, 0.0016501229256391525, 0.001615001354366541, 0.0015771060716360807, 0.0015365865547209976, 0.0014936026418581605, 0.0014483241830021145, 0.0014009297592565415, 0.0013516064500436187, 0.0013005489017814398, 0.0012479585129767654, 0.0011940429685637355, 0.0011390149593353274, 0.0010830917162820695, 0.0010264939628541472, 0.000969445041846484, 0.000912170042283833, 0.0008548949845135213, 0.0007978460635058582, 0.0007412482518702745, 0.000685325067024678, 0.0006302970577962698, 0.0005763815133832394, 0.0005237911827862263, 0.0004727335763163865, 0.00042341023799963295, 0.00037601581425406033, 0.00033073732629418373, 0.00028775352984666824, 0.0002472339838277549, 0.00020933864288963377, 0.0001742170570651069, 0.00014200784789863974, 0.0001128380972659215, 8.682295447215438e-05, 6.40650832792744e-05, 4.465428719413467e-05, 2.866718386940192e-05, 1.6166861314559352e-05, 7.202654160209932e-06]}\n",
            "[83.60899353027344, 66.82387542724611, 76.01111602783203, 59.65240859985352, 83.46039581298828, 78.14317321777344, 87.91187286376953, 75.54593658447266, 83.7769775390625, 77.36141967773438, 83.3182601928711, 83.43455505371094, 83.32472229003906, 83.36994171142578, 83.71236419677734, 78.67295837402344, 77.87828063964844, 81.57384490966797, 85.75397491455078, 67.52164459228516, 82.85308074951172, 89.28156280517578, 77.16759490966797, 82.7303237915039, 84.42304992675781, 83.7898941040039, 83.90618896484375, 83.10504913330078, 82.72386932373047, 68.21940612792969, 82.31683349609375, 80.1395492553711, 81.73536682128906, 80.03617858886719, 80.72748565673828, 84.10647583007812, 83.2471923828125, 57.96614456176758, 81.13451385498047, 76.59258270263672, 79.12521362304688, 81.86457824707031, 81.8193588256836, 84.24214935302734, 83.37640380859375, 83.01460266113281, 78.52436065673828, 81.61260986328125, 72.54167175292969, 82.91768646240234, 83.28595733642578, 73.18775177001953, 74.20209503173828, 83.19550323486328, 85.10789489746094, 83.27949523925781, 82.10363006591797, 78.25946807861328, 81.83873748779297, 83.13735961914062, 77.09652709960938, 83.20196533203125, 74.20855712890625, 67.02416229248047, 76.2760009765625, 82.8207778930664, 84.39720916748047, 83.421630859375, 60.951026916503906, 77.25804138183594, 79.57746124267578, 78.6600341796875, 75.06784057617188, 84.21630859375, 84.21630859375, 84.63626098632812, 83.44100952148438, 84.20338439941406, 83.56376647949219, 83.05982971191406, 84.15170288085938, 75.28104400634766, 73.78860473632812, 85.07559204101562, 60.466468811035156, 81.16035461425781, 81.14097595214844, 82.21346282958984, 77.72322082519531, 83.62191772460938, 80.79855346679688, 77.85889434814453, 79.7002182006836, 81.77413177490234, 83.5250015258789, 80.99237823486328, 79.93927001953125, 83.02106475830078, 84.15170288085938, 70.86833190917969, 84.30030059814453, 83.71236419677734, 63.58702850341797, 68.18064117431639, 84.88822937011719, 87.42085266113281, 83.91911315917969, 83.53146362304688, 83.81573486328125, 84.31321716308594, 73.22651672363281, 76.4698257446289, 84.32614135742188, 47.59658813476562, 83.18904113769531, 84.98513793945312, 80.41090393066406, 75.02261352539062, 84.43597412109375, 78.98953247070312, 81.8516616821289, 83.5120849609375, 82.07132720947266, 84.62333679199219, 79.88758087158203, 84.57164764404297, 82.63987731933594, 81.218505859375, 58.77374267578125, 82.85954284667969, 81.17974090576172, 55.64026260375977, 84.20338439941406, 73.53663635253906, 76.366455078125, 60.82827377319336, 83.57669067382812, 75.92066192626953, 82.56234741210938, 84.830078125, 85.04328918457031, 81.75474548339844, 83.74466705322266, 83.51853942871094, 83.7898941040039, 81.06344604492188, 83.19550323486328, 76.03695678710938, 82.79493713378906, 82.93061065673828, 80.84378051757812, 85.91548919677734, 84.38428497314453, 71.73407745361328, 75.05491638183594, 82.97583770751953, 84.03540802001953, 81.96795654296875, 80.4948959350586, 83.82219696044922, 81.4640121459961, 80.99884033203125, 84.01602172851562, 83.97079467773438, 80.3333740234375, 53.12055969238281, 52.91381454467773, 82.23931121826172, 82.84661865234375, 75.03553771972656, 78.50497436523438, 82.34268188476562, 84.15170288085938, 71.9408187866211, 81.48985290527344, 82.07132720947266, 85.01744079589844, 74.88047790527344, 81.832275390625, 77.18697357177734, 82.97583770751953, 59.94960403442383, 83.09859466552734, 81.06344604492188, 83.55730438232422, 84.4101333618164, 74.51221466064453, 78.03334045410156, 84.10001373291016, 82.43959045410156, 79.9844970703125, 57.61726379394531, 83.01460266113281, 51.8607063293457, 83.53146362304688, 84.66856384277344, 78.10440826416016, 85.06266784667969, 79.71314239501953, 84.51996612548828, 83.18904113769531, 83.00167846679688, 82.97583770751953, 61.7457046508789, 70.6163558959961, 47.68703842163086, 82.86600494384766, 84.45535278320312, 84.61041259765625, 80.04910278320312, 86.4905014038086, 56.260498046875, 84.35198211669922, 82.50419616699219, 82.891845703125, 80.34629821777344, 80.7016372680664, 87.45315551757812, 84.18400573730469, 79.04768371582031, 83.60899353027344, 83.3182601928711, 73.1231460571289, 80.7920913696289, 78.53727722167969, 73.97596740722656, 76.9866943359375, 77.29680633544922, 60.60214614868164, 73.33634948730469, 76.14678955078125, 83.95787811279297, 76.2630844116211, 78.98307037353516, 87.51130676269531, 79.8229751586914, 77.02545928955078, 83.421630859375, 83.89972686767578, 88.03462982177734, 81.302490234375, 79.83589935302734, 85.30818176269531, 84.31321716308594, 88.42227935791016, 83.44100952148438, 83.02751922607422, 82.23284912109375, 83.20196533203125, 74.20209503173828, 81.832275390625, 83.55730438232422, 87.67928314208984, 87.5500717163086, 82.30391693115234, 78.19485473632812, 82.82723999023438, 63.40612411499024, 89.17819213867188, 81.38648223876953, 83.50562286376953, 84.77193450927734, 88.05400848388672, 83.44747161865234, 83.7898941040039, 77.5875473022461, 83.99664306640625, 76.19847869873047, 82.6463394165039, 82.47835540771484, 70.76495361328125, 75.20997619628906, 59.64595031738281, 83.5250015258789, 80.33983612060547, 81.20558166503906, 83.23426818847656, 76.37937927246094, 75.97880554199219, 83.22134399414062, 84.20338439941406, 75.42317962646484, 85.17896270751953, 73.75630187988281, 83.0533676147461, 84.02894592285156, 77.51647186279297, 83.5831527709961, 83.87388610839844, 82.87246704101562, 88.16384887695312, 76.23078155517578, 76.92854309082031, 81.57384490966797, 87.53068542480469, 82.57527160644531, 74.18270874023438, 83.48623657226562, 81.90980529785156, 78.5889663696289, 72.79364013671875, 84.08062744140625, 75.41026306152344, 82.06486511230469, 60.70551681518555, 79.66145324707031, 82.76908874511719, 84.22277069091797, 85.30171966552734, 84.15170288085938, 76.30831146240234, 85.56015014648438, 83.46039581298828, 83.62837219238281, 84.72024536132812, 60.69905853271485, 53.06241226196289, 83.42809295654297, 83.0856704711914, 83.4022445678711, 77.6198501586914, 83.91265106201172, 84.15170288085938, 83.57022857666016, 76.896240234375, 26.695955276489254, 89.15880584716797, 84.30675506591797, 83.40870666503906, 53.9669189453125, 49.53482437133789, 79.26734924316406, 77.25157928466797, 84.17754364013672, 79.9844970703125, 79.95864868164062, 59.83331298828125, 71.2688980102539, 39.391395568847656, 76.67657470703125, 72.65796661376953, 82.32975769042969, 82.9952163696289, 72.87117004394531, 79.51285552978516, 52.2935791015625, 83.23426818847656, 76.56674194335938, 85.15312194824219, 44.96059036254883, 79.42886352539062, 62.56622314453125, 82.71094512939453, 76.11448669433594, 84.26153564453125, 50.34888076782226, 84.5522689819336, 63.01847839355469, 76.25016021728516, 83.20196533203125, 82.7884750366211, 78.80216979980469, 83.48623657226562, 82.66571807861328, 80.71456146240234, 84.69440460205078, 77.65861511230469, 61.62940979003906, 83.91265106201172, 83.46685791015625, 84.63626098632812, 83.09213256835938, 81.4833984375, 84.62333679199219, 83.57669067382812, 82.42667388916016, 82.53004455566406, 86.28376007080078, 80.80501556396484, 83.71882629394531, 78.94430541992188, 82.56234741210938, 81.40586853027344, 83.71236419677734, 85.32109832763672, 87.78912353515625, 77.7619857788086, 81.58030700683594, 23.33634757995605, 79.5386962890625, 70.09303283691406, 66.84326171875, 83.71236419677734, 82.9822998046875, 83.66068267822266, 79.41594696044922, 83.1761245727539, 70.11888122558594, 74.0922622680664, 83.8480453491211, 83.2601089477539, 80.95361328125, 81.80643463134766, 84.830078125, 85.65705871582031, 86.12223815917969, 74.52513122558594, 64.77581024169922, 84.1258544921875, 68.15480041503906, 76.63134765625, 83.55084991455078, 81.25080871582031, 82.54942321777344, 79.97803497314453, 82.53004455566406, 82.26515197753906, 73.68523406982422, 84.08708953857422, 83.57669067382812, 83.13735961914062, 79.33841705322266, 56.984107971191406, 82.5235824584961, 84.00955963134766, 76.84455108642578, 77.27742767333984, 83.02751922607422, 83.26657104492188, 74.99031066894531, 84.55873107910156, 77.71675872802734, 79.37071990966797, 83.02106475830078, 85.01744079589844, 74.95154571533203, 79.67437744140625, 84.18400573730469, 83.94495391845703, 84.06124877929688, 72.00542449951172, 61.08024215698242, 77.46479034423828, 83.13089752197266, 80.97945404052734, 81.9937973022461, 76.366455078125, 72.10234069824219, 87.00090789794922, 74.49282836914062, 87.31101989746094, 84.15815734863281, 83.42809295654297, 81.21204376220703, 81.45109558105469, 83.09213256835938, 81.67076110839844, 84.42304992675781, 83.81573486328125, 83.84158325195312, 64.22664642333984, 87.62760162353516, 76.21785736083984, 76.93500518798828, 79.22212219238281, 71.34642791748047, 86.29667663574219, 84.15170288085938, 79.01537322998047, 82.87892150878906, 71.91497802734375, 73.5430908203125, 83.28595733642578, 77.50355529785156, 81.61260986328125, 82.88538360595703, 83.18904113769531, 77.34203338623047, 83.92557525634766, 79.82943725585938, 79.66791534423828, 81.06344604492188, 77.72322082519531, 84.481201171875, 75.51363372802734, 81.72244262695312, 83.8480453491211, 76.896240234375, 58.10828399658203, 84.35198211669922, 83.90618896484375, 79.93927001953125, 72.18633270263672, 82.47189331054688, 74.40237426757812, 42.57009887695313, 83.60899353027344, 79.86174011230469, 82.96937561035156, 77.00607299804688, 78.21424102783203, 84.37136840820312, 81.06990814208984, 76.28892517089844, 83.82865905761719, 83.3505630493164, 84.90115356445312, 83.0856704711914, 81.34771728515625, 83.46039581298828, 83.23426818847656, 81.52216339111328, 67.67024230957031, 70.39669036865234, 69.75707244873047, 83.46039581298828, 74.20855712890625, 81.24434661865234, 81.17327880859375, 79.81005096435547, 74.13748168945312, 82.05194091796875, 81.36064147949219, 81.8193588256836, 77.52293395996094, 52.76521682739258, 74.0599594116211, 72.95516204833984, 74.24085998535156, 82.51065826416016, 85.76689147949219, 83.13089752197266, 79.15751647949219, 82.49127960205078, 63.91006469726562, 78.31114959716797, 58.85127258300781, 82.74971008300781, 75.4684066772461, 84.10001373291016, 82.29745483398438, 78.80216979980469, 84.22277069091797, 80.39798736572266, 83.14381408691406, 82.79493713378906, 84.59749603271484, 83.46039581298828, 66.06797027587889, 48.34603881835938, 29.157514572143555, 84.01602172851562, 81.67721557617188, 86.49696350097656, 84.87530517578125, 83.55084991455078, 81.66429901123047, 75.39087677001953, 81.56092834472656, 79.18335723876953, 82.09070587158203, 71.78575897216797, 83.21488189697266, 83.09213256835938, 80.21707916259766, 80.423828125, 80.0684814453125, 82.2586898803711, 81.04405975341797, 82.68510437011719, 82.39436340332031, 62.12688827514648, 84.27445220947266, 83.00814056396484, 22.58043670654297, 56.37033081054688, 75.0419921875, 80.8631591796875, 82.9822998046875, 79.70668029785156, 85.15958404541016, 83.82865905761719, 83.3505630493164, 83.15027618408203, 84.59749603271484, 84.61687469482422, 80.86962127685547, 82.1165542602539, 76.50212860107422, 84.88822937011719, 83.5250015258789, 80.87608337402344, 63.88422393798828, 82.98876190185547, 83.57669067382812, 77.2645034790039, 60.44708633422852, 85.1143569946289, 81.38002014160156, 83.45393371582031, 76.91561889648438, 79.67437744140625, 77.21927642822266, 76.366455078125, 83.21488189697266, 76.41168212890625, 83.76405334472656, 68.43907165527344, 77.69091796875, 71.72761535644531, 81.86457824707031, 85.60537719726562, 76.05633544921875, 71.92789459228516, 83.87388610839844, 83.91265106201172, 83.91265106201172, 83.66068267822266, 87.59529876708984, 82.3491439819336, 82.26515197753906, 84.1452407836914, 81.302490234375, 68.68458557128906, 80.65641784667969, 79.01537322998047, 59.29060745239258, 83.97079467773438, 83.57669067382812, 83.09213256835938, 60.388938903808594, 84.48766326904297, 81.73536682128906, 81.77413177490234, 70.66158294677734, 56.10543823242188, 84.88176727294922, 83.29887390136719, 83.53792572021484, 78.039794921875, 73.97596740722656, 76.18555450439453, 81.2314224243164, 81.4833984375, 75.39087677001953, 75.44256591796875, 81.09574890136719, 83.60899353027344, 78.54373931884766, 79.00891876220703, 75.306884765625, 83.53792572021484, 76.61196899414062, 87.82788848876953, 85.98656463623047, 81.93565368652344, 66.85618591308594, 78.84739685058594, 82.77555084228516, 83.46039581298828, 82.94353485107422, 74.04703521728516, 82.93707275390625, 87.13658142089844, 84.37782287597656, 68.73626708984375, 84.30675506591797, 62.55976104736328, 77.45186614990234, 86.43235778808594, 84.1969223022461, 82.91768646240234, 77.2322006225586, 82.76908874511719, 69.56324768066406, 84.46827697753906, 69.94443511962889, 84.24861145019531, 84.92699432373047, 58.59930038452149, 85.36632537841797, 77.87828063964844, 78.31761169433594, 60.10466384887695, 71.25597381591797, 85.7152099609375, 83.94495391845703, 62.785888671875, 84.82362365722656, 81.5544662475586, 83.5831527709961, 73.8984375, 82.94999694824219, 87.1107406616211, 74.75125885009766, 81.52861785888672, 84.18400573730469, 85.79920196533203, 82.87892150878906, 70.99754333496094, 83.43455505371094, 69.19498443603516, 77.92996215820312, 80.43029022216797, 86.29022216796875, 84.60395050048828, 72.42537689208984, 83.2601089477539, 74.4088363647461, 81.13451385498047, 82.94999694824219, 82.96937561035156, 83.47332000732422, 85.10789489746094, 83.50562286376953, 77.6198501586914, 83.54438781738281, 46.43364715576172, 59.18077087402344, 78.82801055908203, 84.00309753417969, 83.38932800292969, 83.7898941040039, 83.13735961914062, 81.75474548339844, 46.28504943847656, 88.77115631103516, 70.4419174194336, 78.5889663696289, 89.41077423095703, 74.9709243774414, 41.5428352355957, 83.60899353027344, 83.24073028564453, 83.4022445678711, 76.74764251708984, 83.2278060913086, 76.92208099365234, 75.40380096435547, 76.52151489257812, 75.35857391357422, 68.36800384521484, 81.63199615478516, 83.33763885498047, 74.65435028076172, 76.5408935546875, 83.3182601928711, 82.78201293945312, 45.01873779296875, 76.94146728515625, 76.4698257446289, 66.90786743164061, 78.05271911621094, 74.63496398925781, 78.66649627685547, 76.41168212890625, 84.62979888916016, 74.22793579101562, 83.01460266113281, 81.83873748779297, 85.46969604492188, 83.01460266113281, 83.91911315917969, 83.02106475830078, 80.80501556396484, 79.24150085449219, 81.81289672851562, 77.19989776611328, 89.0489730834961, 82.39436340332031, 74.66081237792969, 81.31541442871094, 78.93785095214844, 78.65357208251953, 81.75474548339844, 84.2292251586914, 76.37937927246094, 82.09716796875, 88.9585189819336, 56.84843063354492, 73.84674835205078, 58.9869499206543, 66.30701446533203, 80.37860107421875, 83.46685791015625, 74.08580017089844, 83.29887390136719, 82.76908874511719, 80.83731842041016, 79.84881591796875, 82.67218017578125, 83.80281829833984, 80.44320678710938, 75.90127563476562, 82.96937561035156, 84.43597412109375, 55.93099975585938, 74.20209503173828, 66.048583984375, 75.8560562133789, 82.7303237915039, 84.1258544921875, 84.32614135742188, 83.45393371582031, 76.67011260986328, 79.3642578125, 85.54076385498047, 77.63276672363281, 61.15130996704102, 84.0224838256836, 75.41671752929688, 61.54541778564453, 79.9974136352539, 86.89752960205078, 61.42912673950195, 78.2917709350586, 84.16461944580078, 83.72528839111328, 80.17831420898438, 83.1761245727539, 74.0599594116211, 81.41878509521484, 82.56234741210938, 75.84959411621094, 80.11370849609375, 84.78485870361328, 85.14665985107422, 74.49282836914062, 83.4926986694336, 81.13451385498047, 84.29383850097656, 78.34991455078125, 79.33195495605469, 80.56596374511719, 83.68006134033203, 78.27884674072266, 81.25080871582031, 84.2873764038086, 84.28091430664062, 85.42447662353516, 36.82000350952149, 79.85527801513672, 84.08708953857422, 82.84015655517578, 84.08062744140625, 82.54942321777344, 54.43855667114258, 76.74118041992188, 83.33763885498047, 80.48197174072266, 82.88538360595703, 82.12947082519531, 79.75836944580078, 79.89404296875, 83.8803482055664, 75.93358612060547, 82.9952163696289, 83.53792572021484, 67.68316650390625, 83.97079467773438, 83.92557525634766, 84.1258544921875, 84.9916000366211, 80.60472869873047, 80.7597885131836, 82.9952163696289, 69.15621948242189, 78.07855987548828, 63.871299743652344, 81.50277709960938, 71.6759262084961, 78.02041625976562, 76.39875793457031, 62.33363342285156, 79.40948486328125, 46.621009826660156, 84.97867584228516, 87.41439819335938, 84.90115356445312, 82.37498474121094, 81.59323120117188, 83.62837219238281, 82.95645141601562, 81.61907196044922, 83.53792572021484, 76.32122802734375, 84.31321716308594, 84.34552001953125, 76.56674194335938, 81.23788452148438, 83.53792572021484, 75.21643829345703, 78.9249267578125, 80.07494354248047, 83.14381408691406, 80.92131042480469, 73.69168853759766, 84.24861145019531, 74.4088363647461, 78.91846466064453, 80.45613098144531, 76.85747528076172, 79.22858428955078, 78.20777893066406, 82.34268188476562, 80.20416259765625, 84.61687469482422, 83.76405334472656, 82.7174072265625, 78.24654388427734, 65.8741455078125, 75.91419982910156, 83.05982971191406, 83.58961486816406, 67.15338134765625, 74.1762466430664, 78.20777893066406, 85.04975128173828, 76.03695678710938, 83.36348724365234, 81.93565368652344, 77.94288635253906, 83.44747161865234, 83.40870666503906, 84.15815734863281, 83.82865905761719, 83.63483428955078, 64.38816070556639, 84.565185546875, 68.25817108154297, 83.36994171142578, 82.88538360595703, 83.24073028564453, 83.11797332763672, 81.94210815429688, 74.91278076171875, 84.18400573730469, 83.50562286376953, 84.06771087646484, 89.01667022705078, 51.162940979003906, 83.951416015625, 82.41374969482422, 83.56376647949219, 86.7876968383789, 88.38351440429688, 81.83873748779297, 84.42951202392578, 81.20558166503906, 84.50057983398438, 87.1947250366211, 80.59180450439453, 83.13089752197266, 76.86393737792969, 76.53443908691406, 82.20054626464844, 81.17974090576172, 83.16966247558594, 76.896240234375, 84.1969223022461, 84.97867584228516, 54.34164810180664, 83.12443542480469, 49.25701141357422, 82.59465026855469, 52.43571472167969, 83.00814056396484, 83.28595733642578, 81.21204376220703, 76.48921203613281, 82.7174072265625, 78.9249267578125, 84.18400573730469, 87.62113952636719, 84.26153564453125, 82.74971008300781, 85.37278747558594, 80.71456146240234, 84.34552001953125, 73.98888397216797, 83.11151123046875, 86.0834732055664, 77.34849548339844, 84.04186248779297, 84.83654022216797, 85.24356842041016, 53.77309799194336, 83.01460266113281, 80.79855346679688, 83.53146362304688, 84.85592651367188, 80.45613098144531, 70.52590942382812, 82.07778930664062, 83.63483428955078, 84.2292251586914, 63.70331954956055, 83.8609619140625, 69.38234710693361, 83.38932800292969]\n",
            "dict_keys(['config', 'Train/loss', 'Train/train_accuracy', 'Train/val_accuracy', 'Train/train_cross_entropy', 'Train/val_cross_entropy', 'Train/train_balanced_accuracy', 'Train/val_balanced_accuracy', 'Train/test_result', 'Train/test_cross_entropy', 'Train/test_balanced_accuracy', 'Train/gradient_max', 'Train/gradient_mean', 'Train/gradient_median', 'Train/gradient_std', 'Train/gradient_q10', 'Train/gradient_q25', 'Train/gradient_q75', 'Train/gradient_q90', 'Train/layer_wise_gradient_max_layer_0', 'Train/layer_wise_gradient_mean_layer_0', 'Train/layer_wise_gradient_median_layer_0', 'Train/layer_wise_gradient_std_layer_0', 'Train/layer_wise_gradient_q10_layer_0', 'Train/layer_wise_gradient_q25_layer_0', 'Train/layer_wise_gradient_q75_layer_0', 'Train/layer_wise_gradient_q90_layer_0', 'Train/gradient_norm', 'Train/lr'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJk2sHMGCRK8",
        "colab_type": "code",
        "outputId": "70ce56ff-f785-4c94-e148-344d611c5e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# get the data for config 0 across first 10 epochs\n",
        "config_0 = train_data[0]\n",
        "config_0_keys = config_0.keys()\n",
        "\n",
        "# each config has a static part which is the network config parameters and dynamic parts - loss, accuracy, cross_entropy, test_result, gradient_max etc\n",
        "print(\"============ static =============\")\n",
        "for stat_keys in config_0[\"config\"].keys():\n",
        "  print(stat_keys)\n",
        "\n",
        "for key in train_data[0].keys():\n",
        "  print(key)\n",
        "\n",
        "for key in val_data[0].keys():\n",
        "  print(key)\n",
        "\n",
        "for key in test_data[0].keys():\n",
        "  print(key)\n",
        "  # print(\"number of values in {0} = {1}\".format(key, len(train_data[0][key])))\n",
        "\n",
        "# trainig dataset structure - [1000, feature, values across 50 epochs]\n",
        "# features to use batch_size, max_dropout, max_units, num_layers, learning_rate, momentum, weight_decay"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============ static =============\n",
            "batch_size\n",
            "imputation_strategy\n",
            "learning_rate_scheduler\n",
            "loss\n",
            "network\n",
            "max_dropout\n",
            "normalization_strategy\n",
            "optimizer\n",
            "cosine_annealing_T_max\n",
            "cosine_annealing_eta_min\n",
            "activation\n",
            "max_units\n",
            "mlp_shape\n",
            "num_layers\n",
            "learning_rate\n",
            "momentum\n",
            "weight_decay\n",
            "config\n",
            "Train/loss\n",
            "Train/train_accuracy\n",
            "Train/val_accuracy\n",
            "Train/train_cross_entropy\n",
            "Train/val_cross_entropy\n",
            "Train/train_balanced_accuracy\n",
            "Train/val_balanced_accuracy\n",
            "Train/test_result\n",
            "Train/test_cross_entropy\n",
            "Train/test_balanced_accuracy\n",
            "Train/gradient_max\n",
            "Train/gradient_mean\n",
            "Train/gradient_median\n",
            "Train/gradient_std\n",
            "Train/gradient_q10\n",
            "Train/gradient_q25\n",
            "Train/gradient_q75\n",
            "Train/gradient_q90\n",
            "Train/layer_wise_gradient_max_layer_0\n",
            "Train/layer_wise_gradient_max_layer_1\n",
            "Train/layer_wise_gradient_max_layer_2\n",
            "Train/layer_wise_gradient_mean_layer_0\n",
            "Train/layer_wise_gradient_mean_layer_1\n",
            "Train/layer_wise_gradient_mean_layer_2\n",
            "Train/layer_wise_gradient_median_layer_0\n",
            "Train/layer_wise_gradient_median_layer_1\n",
            "Train/layer_wise_gradient_median_layer_2\n",
            "Train/layer_wise_gradient_std_layer_0\n",
            "Train/layer_wise_gradient_std_layer_1\n",
            "Train/layer_wise_gradient_std_layer_2\n",
            "Train/layer_wise_gradient_q10_layer_0\n",
            "Train/layer_wise_gradient_q10_layer_1\n",
            "Train/layer_wise_gradient_q10_layer_2\n",
            "Train/layer_wise_gradient_q25_layer_0\n",
            "Train/layer_wise_gradient_q25_layer_1\n",
            "Train/layer_wise_gradient_q25_layer_2\n",
            "Train/layer_wise_gradient_q75_layer_0\n",
            "Train/layer_wise_gradient_q75_layer_1\n",
            "Train/layer_wise_gradient_q75_layer_2\n",
            "Train/layer_wise_gradient_q90_layer_0\n",
            "Train/layer_wise_gradient_q90_layer_1\n",
            "Train/layer_wise_gradient_q90_layer_2\n",
            "Train/gradient_norm\n",
            "Train/lr\n",
            "config\n",
            "Train/loss\n",
            "Train/train_accuracy\n",
            "Train/val_accuracy\n",
            "Train/train_cross_entropy\n",
            "Train/val_cross_entropy\n",
            "Train/train_balanced_accuracy\n",
            "Train/val_balanced_accuracy\n",
            "Train/test_result\n",
            "Train/test_cross_entropy\n",
            "Train/test_balanced_accuracy\n",
            "Train/gradient_max\n",
            "Train/gradient_mean\n",
            "Train/gradient_median\n",
            "Train/gradient_std\n",
            "Train/gradient_q10\n",
            "Train/gradient_q25\n",
            "Train/gradient_q75\n",
            "Train/gradient_q90\n",
            "Train/layer_wise_gradient_max_layer_0\n",
            "Train/layer_wise_gradient_max_layer_1\n",
            "Train/layer_wise_gradient_mean_layer_0\n",
            "Train/layer_wise_gradient_mean_layer_1\n",
            "Train/layer_wise_gradient_median_layer_0\n",
            "Train/layer_wise_gradient_median_layer_1\n",
            "Train/layer_wise_gradient_std_layer_0\n",
            "Train/layer_wise_gradient_std_layer_1\n",
            "Train/layer_wise_gradient_q10_layer_0\n",
            "Train/layer_wise_gradient_q10_layer_1\n",
            "Train/layer_wise_gradient_q25_layer_0\n",
            "Train/layer_wise_gradient_q25_layer_1\n",
            "Train/layer_wise_gradient_q75_layer_0\n",
            "Train/layer_wise_gradient_q75_layer_1\n",
            "Train/layer_wise_gradient_q90_layer_0\n",
            "Train/layer_wise_gradient_q90_layer_1\n",
            "Train/gradient_norm\n",
            "Train/lr\n",
            "config\n",
            "Train/loss\n",
            "Train/train_accuracy\n",
            "Train/val_accuracy\n",
            "Train/train_cross_entropy\n",
            "Train/val_cross_entropy\n",
            "Train/train_balanced_accuracy\n",
            "Train/val_balanced_accuracy\n",
            "Train/test_result\n",
            "Train/test_cross_entropy\n",
            "Train/test_balanced_accuracy\n",
            "Train/gradient_max\n",
            "Train/gradient_mean\n",
            "Train/gradient_median\n",
            "Train/gradient_std\n",
            "Train/gradient_q10\n",
            "Train/gradient_q25\n",
            "Train/gradient_q75\n",
            "Train/gradient_q90\n",
            "Train/layer_wise_gradient_max_layer_0\n",
            "Train/layer_wise_gradient_mean_layer_0\n",
            "Train/layer_wise_gradient_median_layer_0\n",
            "Train/layer_wise_gradient_std_layer_0\n",
            "Train/layer_wise_gradient_q10_layer_0\n",
            "Train/layer_wise_gradient_q25_layer_0\n",
            "Train/layer_wise_gradient_q75_layer_0\n",
            "Train/layer_wise_gradient_q90_layer_0\n",
            "Train/gradient_norm\n",
            "Train/lr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7erGNAzO_ki5",
        "colab_type": "code",
        "outputId": "19885d28-95d7-4145-a791-84f35ad9d2a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#------ old data prep code --------#\n",
        "\"\"\"\n",
        "  #-------------------------- training data preparation ----------------------------------------------------------------------------------#\n",
        "\n",
        "  # normalization : https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0\n",
        "\n",
        "\n",
        "  # let's build the input data, each column is a feature, each row is an epoch, 10 rows = 10 epochs, 10 columns = 10 features\n",
        "  # features : [batch_size, max_dropout, max_units, num_layers, learning_rate, momentum, weight_decay] -> static config params\n",
        "  # featuers : [Train/loss, Train/train_accuracy, Train/val_accuracy, Train/gradient_std]\n",
        "  for index, data in enumerate(train_data):\n",
        "    \n",
        "    # col 0 is batch_size\n",
        "    batch_size = np.full((10, 1), data[\"config\"][\"batch_size\"])  # parameters from config are same for all 10 epochs\n",
        "    train_data_X[index, :, 0] = batch_size.T\n",
        "    feat_min = np.min(train_data_X[index, :, 0])  # get feature min\n",
        "    feat_max = np.max(train_data_X[index, :, 0])  # get feature max\n",
        "    train_data_X[index, :, 0] = (train_data_X[index, :, 0] - feat_min) / (feat_max - feat_min) + 0.00000001\n",
        "    \n",
        "    # col1 is max_dropout\n",
        "    max_dropout = np.full((10, 1), data[\"config\"][\"max_dropout\"])\n",
        "    train_data_X[index, :, 1] = max_dropout.T\n",
        "    feat_min = np.min(train_data_X[index, :, 1])  # get feature min\n",
        "    feat_max = np.max(train_data_X[index, :, 1])  # get feature max\n",
        "    train_data_X[index, :, 1] = (train_data_X[index, :, 1] - feat_min) / (feat_max - feat_min)+ 0.00000001\n",
        "\n",
        "    # col 2 is max_units\n",
        "    max_units = np.full((10, 1), data[\"config\"][\"max_units\"])\n",
        "    train_data_X[index, :, 2] = max_units.T\n",
        "    feat_min = np.min(train_data_X[index, :, 2])  # get feature min\n",
        "    feat_max = np.max(train_data_X[index, :, 2])  # get feature max\n",
        "    train_data_X[index, :, 2] = (train_data_X[index, :, 2] - feat_min) / (feat_max - feat_min)+ 0.00000001\n",
        "\n",
        "    # col 3 is num_layers\n",
        "    num_layers = np.full((10, 1), data[\"config\"][\"num_layers\"])\n",
        "    train_data_X[index, :, 3] = num_layers.T\n",
        "    feat_min = np.min(train_data_X[index, :, 3])  # get feature min\n",
        "    feat_max = np.max(train_data_X[index, :, 3])  # get feature max\n",
        "    train_data_X[index, :, 3] = (train_data_X[index, :, 3] - feat_min) / (feat_max - feat_min)+ 0.00000001\n",
        "\n",
        "    # col 4 is learning_rate\n",
        "    learning_rate = np.full((10, 1), data[\"config\"][\"learning_rate\"])\n",
        "    train_data_X[index, :, 4] = learning_rate.T\n",
        "    feat_min = np.min(train_data_X[index, :, 4])  # get feature min\n",
        "    feat_max = np.max(train_data_X[index, :, 4])  # get feature max\n",
        "    train_data_X[index, :, 5] = (train_data_X[index, :, 4] - feat_min) / (feat_max - feat_min)+ 0.00000001\n",
        "\n",
        "    # col 5 is momentum\n",
        "    momentum = np.full((10, 1), data[\"config\"][\"momentum\"])\n",
        "    train_data_X[index, :, 5] = momentum.T\n",
        "    feat_min = np.min(train_data_X[index, :, 5])  # get feature min\n",
        "    feat_max = np.max(train_data_X[index, :, 5])  # get feature max\n",
        "    train_data_X[index, :, 5] = (train_data_X[index, :, 5] - feat_min) / (feat_max - feat_min)+ 0.00000001\n",
        "\n",
        "    # col 6 is weight_decay\n",
        "    weight_decay = np.full((10, 1), data[\"config\"][\"weight_decay\"])\n",
        "    train_data_X[index, :, 6] = weight_decay.T\n",
        "    feat_min = np.min(train_data_X[index, :, 6])  # get feature min\n",
        "    feat_max = np.max(train_data_X[index, :, 6])  # get feature max\n",
        "    train_data_X[index, :, 6] = (train_data_X[index, :, 6] - feat_min) / (feat_max - feat_min)+ 0.00000001\n",
        "\n",
        "    # col7 = Train/loss\n",
        "    train_loss = np.asarray(data[\"Train/loss\"][0:10])\n",
        "    train_data_X[index, :, 7] = train_loss.T\n",
        "    feat_min = np.min(train_data_X[index, :, 7])  # get feature min\n",
        "    feat_max = np.max(train_data_X[index, :, 7])  # get feature max\n",
        "    train_data_X[index, :, 7] = (train_data_X[index, :, 7] - feat_min) / (feat_max - feat_min)+ 0.00000001\n",
        "\n",
        "    # col 8 = train_accuracy\n",
        "    train_accuracy = np.asarray(data[\"Train/train_accuracy\"][0:10])\n",
        "    train_data_X[index, :, 8] = train_accuracy.T\n",
        "    feat_min = np.min(train_data_X[index, :, 8])  # get feature min\n",
        "    feat_max = np.max(train_data_X[index, :, 8])  # get feature max\n",
        "    train_data_X[index, :, 8] = (train_data_X[index, :, 8] - feat_min) / (feat_max - feat_min)+ 0.00000001\n",
        "\n",
        "    # col 9 = Train/val_accuracy\n",
        "    val_acc = np.asarray(data[\"Train/val_accuracy\"][0:10])\n",
        "    train_data_X[index, :, 9] = val_acc.T\n",
        "    feat_min = np.min(train_data_X[index, :, 9])  # get feature min\n",
        "    feat_max = np.max(train_data_X[index, :, 9])  # get feature max\n",
        "    train_data_X[index, :, 9] = (train_data_X[index, :, 9] - feat_min) / (feat_max - feat_min)+ 0.00000001\n",
        "\n",
        "    # get the label for this config, it is the Train/val_accuracy for the 51st epoch of this config\n",
        "    train_data_Y[index] = train_targets[index]\n",
        "    \n",
        "\n",
        "    # Y = data[\"Train/val_accuracy\"][50] # same as the above statement\n",
        "\n",
        "  # normalize the targets\n",
        "  feat_min = np.min(train_data_Y)  # get feature min\n",
        "  feat_max = np.max(train_data_Y)  # get feature max\n",
        "  train_data_Y = (train_data_Y - feat_min) / (feat_max - feat_min)+ 0.00000001\n",
        "\n",
        "\n",
        "  # do column wise/ feature wise normalization\n",
        "\n",
        "\n",
        "  # add a channel dimension\n",
        "  # np.expand_dims(train_data_X, axis=-1)  \n",
        "  # print a summary\n",
        "  print(\"========= training data summary =========\")\n",
        "  print(\"train_X shape : \", train_data_X.shape)\n",
        "  print(\"Unique values in train_X : \", np.unique(train_data_X))\n",
        "  print(\"train_Y_shape : \", train_data_Y.shape)\n",
        "\n",
        "\n",
        "  #---------------------------------------------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "  # -------------- validation data preparation -------------------------------------------------------------------------------------------------#\n",
        "  validation_data_X = np.zeros((500, 10, 10))\n",
        "  validation_data_Y = np.zeros((500, 1))\n",
        "\n",
        "  for index, data in enumerate(val_data):\n",
        "    # col 0 is batch_size\n",
        "    batch_size = np.full((10, 1), data[\"config\"][\"batch_size\"])  # parameters from config are same for all 10 epochs\n",
        "    validation_data_X[index, :, 0] = batch_size.T\n",
        "    validation_data_X[index, :, 0] /= np.max(validation_data_X[index, :, 0])\n",
        "    \n",
        "    # col1 is max_dropout\n",
        "    max_dropout = np.full((10, 1), data[\"config\"][\"max_dropout\"])\n",
        "    validation_data_X[index, :, 1] = max_dropout.T\n",
        "    validation_data_X[index, :, 1] /= np.max(validation_data_X[index, :, 1])\n",
        "\n",
        "    # col 2 is max_units\n",
        "    max_units = np.full((10, 1), data[\"config\"][\"max_units\"])\n",
        "    validation_data_X[index, :, 2] = max_units.T\n",
        "    validation_data_X[index, :, 2] /= np.max(validation_data_X[index, :, 2])\n",
        "\n",
        "    # col 3 is num_layers\n",
        "    num_layers = np.full((10, 1), data[\"config\"][\"num_layers\"])\n",
        "    validation_data_X[index, :, 3] = num_layers.T\n",
        "    validation_data_X[index, :, 3] /= np.max(validation_data_X[index, :, 3])\n",
        "\n",
        "    # col 4 is learning_rate\n",
        "    learning_rate = np.full((10, 1), data[\"config\"][\"learning_rate\"])\n",
        "    validation_data_X[index, :, 4] = learning_rate.T\n",
        "    validation_data_X[index, :, 4] /= np.max(validation_data_X[index, :, 4])\n",
        "\n",
        "    # col 5 is momentum\n",
        "    momentum = np.full((10, 1), data[\"config\"][\"momentum\"])\n",
        "    validation_data_X[index, :, 5] = momentum.T\n",
        "    validation_data_X[index, :, 5] /= np.max(validation_data_X[index, :, 5])\n",
        "\n",
        "    # col 6 is weight_decay\n",
        "    weight_decay = np.full((10, 1), data[\"config\"][\"weight_decay\"])\n",
        "    validation_data_X[index, :, 6] = weight_decay.T\n",
        "    validation_data_X[index, :, 6] /= np.max(validation_data_X[index, :, 6])\n",
        "\n",
        "    # col7 = Train/loss\n",
        "    train_loss = np.asarray(data[\"Train/loss\"][0:10])\n",
        "    validation_data_X[index, :, 7] = train_loss.T\n",
        "    validation_data_X[index, :, 7] /= np.max(validation_data_X[index, :, 7])\n",
        "\n",
        "    # col 8 = train_accuracy\n",
        "    train_accuracy = np.asarray(data[\"Train/train_accuracy\"][0:10])\n",
        "    validation_data_X[index, :, 8] = train_accuracy.T\n",
        "    validation_data_X[index, :, 8] /= np.max(validation_data_X[index, :, 8])\n",
        "\n",
        "    # col 9 = Train/val_accuracy\n",
        "    val_acc = np.asarray(data[\"Train/val_accuracy\"][0:10])\n",
        "    validation_data_X[index, :, 9] = val_acc.T\n",
        "    validation_data_X[index, :, 9] /= np.max(validation_data_X[index, :, 9])\n",
        "\n",
        "    # get the label for this config, it is the Train/val_accuracy for the 51st epoch of this config\n",
        "    validation_data_Y[index] = val_targets[index]\n",
        "\n",
        "  validation_data_Y /= np.max(validation_data_Y)\n",
        "\n",
        "\n",
        "  return train_data_X, train_data_Y, validation_data_X, validation_data_Y\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "# train_data_Y = train_data[:][\"Train/val_accuracy\"][-1]\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  #-------------------------- training data preparation ----------------------------------------------------------------------------------#\\n\\n  # normalization : https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0\\n\\n\\n  # let\\'s build the input data, each column is a feature, each row is an epoch, 10 rows = 10 epochs, 10 columns = 10 features\\n  # features : [batch_size, max_dropout, max_units, num_layers, learning_rate, momentum, weight_decay] -> static config params\\n  # featuers : [Train/loss, Train/train_accuracy, Train/val_accuracy, Train/gradient_std]\\n  for index, data in enumerate(train_data):\\n    \\n    # col 0 is batch_size\\n    batch_size = np.full((10, 1), data[\"config\"][\"batch_size\"])  # parameters from config are same for all 10 epochs\\n    train_data_X[index, :, 0] = batch_size.T\\n    feat_min = np.min(train_data_X[index, :, 0])  # get feature min\\n    feat_max = np.max(train_data_X[index, :, 0])  # get feature max\\n    train_data_X[index, :, 0] = (train_data_X[index, :, 0] - feat_min) / (feat_max - feat_min) + 0.00000001\\n    \\n    # col1 is max_dropout\\n    max_dropout = np.full((10, 1), data[\"config\"][\"max_dropout\"])\\n    train_data_X[index, :, 1] = max_dropout.T\\n    feat_min = np.min(train_data_X[index, :, 1])  # get feature min\\n    feat_max = np.max(train_data_X[index, :, 1])  # get feature max\\n    train_data_X[index, :, 1] = (train_data_X[index, :, 1] - feat_min) / (feat_max - feat_min)+ 0.00000001\\n\\n    # col 2 is max_units\\n    max_units = np.full((10, 1), data[\"config\"][\"max_units\"])\\n    train_data_X[index, :, 2] = max_units.T\\n    feat_min = np.min(train_data_X[index, :, 2])  # get feature min\\n    feat_max = np.max(train_data_X[index, :, 2])  # get feature max\\n    train_data_X[index, :, 2] = (train_data_X[index, :, 2] - feat_min) / (feat_max - feat_min)+ 0.00000001\\n\\n    # col 3 is num_layers\\n    num_layers = np.full((10, 1), data[\"config\"][\"num_layers\"])\\n    train_data_X[index, :, 3] = num_layers.T\\n    feat_min = np.min(train_data_X[index, :, 3])  # get feature min\\n    feat_max = np.max(train_data_X[index, :, 3])  # get feature max\\n    train_data_X[index, :, 3] = (train_data_X[index, :, 3] - feat_min) / (feat_max - feat_min)+ 0.00000001\\n\\n    # col 4 is learning_rate\\n    learning_rate = np.full((10, 1), data[\"config\"][\"learning_rate\"])\\n    train_data_X[index, :, 4] = learning_rate.T\\n    feat_min = np.min(train_data_X[index, :, 4])  # get feature min\\n    feat_max = np.max(train_data_X[index, :, 4])  # get feature max\\n    train_data_X[index, :, 5] = (train_data_X[index, :, 4] - feat_min) / (feat_max - feat_min)+ 0.00000001\\n\\n    # col 5 is momentum\\n    momentum = np.full((10, 1), data[\"config\"][\"momentum\"])\\n    train_data_X[index, :, 5] = momentum.T\\n    feat_min = np.min(train_data_X[index, :, 5])  # get feature min\\n    feat_max = np.max(train_data_X[index, :, 5])  # get feature max\\n    train_data_X[index, :, 5] = (train_data_X[index, :, 5] - feat_min) / (feat_max - feat_min)+ 0.00000001\\n\\n    # col 6 is weight_decay\\n    weight_decay = np.full((10, 1), data[\"config\"][\"weight_decay\"])\\n    train_data_X[index, :, 6] = weight_decay.T\\n    feat_min = np.min(train_data_X[index, :, 6])  # get feature min\\n    feat_max = np.max(train_data_X[index, :, 6])  # get feature max\\n    train_data_X[index, :, 6] = (train_data_X[index, :, 6] - feat_min) / (feat_max - feat_min)+ 0.00000001\\n\\n    # col7 = Train/loss\\n    train_loss = np.asarray(data[\"Train/loss\"][0:10])\\n    train_data_X[index, :, 7] = train_loss.T\\n    feat_min = np.min(train_data_X[index, :, 7])  # get feature min\\n    feat_max = np.max(train_data_X[index, :, 7])  # get feature max\\n    train_data_X[index, :, 7] = (train_data_X[index, :, 7] - feat_min) / (feat_max - feat_min)+ 0.00000001\\n\\n    # col 8 = train_accuracy\\n    train_accuracy = np.asarray(data[\"Train/train_accuracy\"][0:10])\\n    train_data_X[index, :, 8] = train_accuracy.T\\n    feat_min = np.min(train_data_X[index, :, 8])  # get feature min\\n    feat_max = np.max(train_data_X[index, :, 8])  # get feature max\\n    train_data_X[index, :, 8] = (train_data_X[index, :, 8] - feat_min) / (feat_max - feat_min)+ 0.00000001\\n\\n    # col 9 = Train/val_accuracy\\n    val_acc = np.asarray(data[\"Train/val_accuracy\"][0:10])\\n    train_data_X[index, :, 9] = val_acc.T\\n    feat_min = np.min(train_data_X[index, :, 9])  # get feature min\\n    feat_max = np.max(train_data_X[index, :, 9])  # get feature max\\n    train_data_X[index, :, 9] = (train_data_X[index, :, 9] - feat_min) / (feat_max - feat_min)+ 0.00000001\\n\\n    # get the label for this config, it is the Train/val_accuracy for the 51st epoch of this config\\n    train_data_Y[index] = train_targets[index]\\n    \\n\\n    # Y = data[\"Train/val_accuracy\"][50] # same as the above statement\\n\\n  # normalize the targets\\n  feat_min = np.min(train_data_Y)  # get feature min\\n  feat_max = np.max(train_data_Y)  # get feature max\\n  train_data_Y = (train_data_Y - feat_min) / (feat_max - feat_min)+ 0.00000001\\n\\n\\n  # do column wise/ feature wise normalization\\n\\n\\n  # add a channel dimension\\n  # np.expand_dims(train_data_X, axis=-1)  \\n  # print a summary\\n  print(\"========= training data summary =========\")\\n  print(\"train_X shape : \", train_data_X.shape)\\n  print(\"Unique values in train_X : \", np.unique(train_data_X))\\n  print(\"train_Y_shape : \", train_data_Y.shape)\\n\\n\\n  #---------------------------------------------------------------------------------------------------------------------------------------------#\\n\\n  # -------------- validation data preparation -------------------------------------------------------------------------------------------------#\\n  validation_data_X = np.zeros((500, 10, 10))\\n  validation_data_Y = np.zeros((500, 1))\\n\\n  for index, data in enumerate(val_data):\\n    # col 0 is batch_size\\n    batch_size = np.full((10, 1), data[\"config\"][\"batch_size\"])  # parameters from config are same for all 10 epochs\\n    validation_data_X[index, :, 0] = batch_size.T\\n    validation_data_X[index, :, 0] /= np.max(validation_data_X[index, :, 0])\\n    \\n    # col1 is max_dropout\\n    max_dropout = np.full((10, 1), data[\"config\"][\"max_dropout\"])\\n    validation_data_X[index, :, 1] = max_dropout.T\\n    validation_data_X[index, :, 1] /= np.max(validation_data_X[index, :, 1])\\n\\n    # col 2 is max_units\\n    max_units = np.full((10, 1), data[\"config\"][\"max_units\"])\\n    validation_data_X[index, :, 2] = max_units.T\\n    validation_data_X[index, :, 2] /= np.max(validation_data_X[index, :, 2])\\n\\n    # col 3 is num_layers\\n    num_layers = np.full((10, 1), data[\"config\"][\"num_layers\"])\\n    validation_data_X[index, :, 3] = num_layers.T\\n    validation_data_X[index, :, 3] /= np.max(validation_data_X[index, :, 3])\\n\\n    # col 4 is learning_rate\\n    learning_rate = np.full((10, 1), data[\"config\"][\"learning_rate\"])\\n    validation_data_X[index, :, 4] = learning_rate.T\\n    validation_data_X[index, :, 4] /= np.max(validation_data_X[index, :, 4])\\n\\n    # col 5 is momentum\\n    momentum = np.full((10, 1), data[\"config\"][\"momentum\"])\\n    validation_data_X[index, :, 5] = momentum.T\\n    validation_data_X[index, :, 5] /= np.max(validation_data_X[index, :, 5])\\n\\n    # col 6 is weight_decay\\n    weight_decay = np.full((10, 1), data[\"config\"][\"weight_decay\"])\\n    validation_data_X[index, :, 6] = weight_decay.T\\n    validation_data_X[index, :, 6] /= np.max(validation_data_X[index, :, 6])\\n\\n    # col7 = Train/loss\\n    train_loss = np.asarray(data[\"Train/loss\"][0:10])\\n    validation_data_X[index, :, 7] = train_loss.T\\n    validation_data_X[index, :, 7] /= np.max(validation_data_X[index, :, 7])\\n\\n    # col 8 = train_accuracy\\n    train_accuracy = np.asarray(data[\"Train/train_accuracy\"][0:10])\\n    validation_data_X[index, :, 8] = train_accuracy.T\\n    validation_data_X[index, :, 8] /= np.max(validation_data_X[index, :, 8])\\n\\n    # col 9 = Train/val_accuracy\\n    val_acc = np.asarray(data[\"Train/val_accuracy\"][0:10])\\n    validation_data_X[index, :, 9] = val_acc.T\\n    validation_data_X[index, :, 9] /= np.max(validation_data_X[index, :, 9])\\n\\n    # get the label for this config, it is the Train/val_accuracy for the 51st epoch of this config\\n    validation_data_Y[index] = val_targets[index]\\n\\n  validation_data_Y /= np.max(validation_data_Y)\\n\\n\\n  return train_data_X, train_data_Y, validation_data_X, validation_data_Y\\n\\n  \\n\\n\\n# train_data_Y = train_data[:][\"Train/val_accuracy\"][-1]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-aaj5XDRNc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_data():\n",
        "  \"\"\"\n",
        "  Function returns training and validation data and labels normalized [0-1]\n",
        "  \"\"\"\n",
        "\n",
        "  # lets see how the targets look likes\n",
        "  # print(type(train_targets))\n",
        "  # print(train_targets)\n",
        "  # print(len(train_targets))\n",
        "\n",
        "  # we have to predict the validation accuracy\n",
        "  # in the data, for each config, we have 50 epochs, each epoch has a validation accuracy\n",
        "  # use only 10 epochs for training and predict the 51st epoch val/accuracy\n",
        "\n",
        "  # print(config_0[\"Train/val_accuracy\"])\n",
        "  # print(len(config_0[\"Train/val_accuracy\"]))\n",
        "\n",
        "\n",
        "  # ------------ new code - just make column vectors for each config, so (1000, 10, 1) is the shape of training data ---------------#\n",
        "  train_data_new_X = np.zeros((1000, 14, 1))\n",
        "  train_data_new_Y = np.zeros((1000, 1))\n",
        "\n",
        "  val_data_X = np.zeros((500, 14, 1))\n",
        "  val_data_Y = np.zeros((500, 1))\n",
        "\n",
        "  test_data_X = np.zeros((500, 14, 1))\n",
        "  test_data_Y = np.zeros((500, 1))\n",
        "\n",
        "  for index, data in enumerate(train_data):\n",
        "    # print(index, data)\n",
        "    train_data_new_X[index, 0] = data[\"config\"][\"batch_size\"]  # config params are single value scalars\n",
        "    train_data_new_X[index, 1] = data[\"config\"][\"max_dropout\"]\n",
        "    train_data_new_X[index, 2] = data[\"config\"][\"max_units\"]\n",
        "    train_data_new_X[index, 3] = data[\"config\"][\"num_layers\"]\n",
        "    train_data_new_X[index, 4] = data[\"config\"][\"learning_rate\"]\n",
        "    train_data_new_X[index, 5] = data[\"config\"][\"momentum\"]\n",
        "    train_data_new_X[index, 6] = data[\"config\"][\"weight_decay\"]\n",
        "\n",
        "    #------- dynamic params -----------#\n",
        "    train_data_new_X[index, 7] = np.sum(np.asarray(data[\"Train/loss\"][0:10])) / 10  # these we use only first 10 epochs, average loss\n",
        "    train_data_new_X[index, 8] = np.sum(np.asarray(data[\"Train/train_accuracy\"][0:10])) / 10 # average accuracy across 10 epochs\n",
        "    train_data_new_X[index, 9] = np.sum(np.asarray(data[\"Train/val_accuracy\"][0:10])) / 10 # average val_acc across 10 epochs\n",
        "    train_data_new_X[index, 10] = np.sum(np.asarray(data[\"Train/train_cross_entropy\"][0:10])) / 10\n",
        "    train_data_new_X[index, 11] = np.sum(np.asarray(data[\"Train/val_cross_entropy\"][0:10])) / 10\n",
        "    train_data_new_X[index, 12] = np.sum(np.asarray(data[\"Train/gradient_mean\"][0:10])) / 10\n",
        "    train_data_new_X[index, 13] = np.sum(np.asarray(data[\"Train/lr\"][0:10])) / 10\n",
        "\n",
        "\n",
        "    train_data_new_Y[index] = train_targets[index]  # validation accuracy for 51st epoch, for each\n",
        "\n",
        "    # print(\"target for config 0 : {0}, from dataset : {1}\".format(train_data_new_Y[index], data[\"Train/val_accuracy\"][50])) \n",
        "\n",
        "    # print(train_data_new_X[0])\n",
        "\n",
        "\n",
        "  # normalize each feature across all batches, so each feature is in range [0-1]\n",
        "  for feat_index in range(14):\n",
        "    feat_min = np.min(train_data_new_X[:, feat_index, 0])  # min of feature 0 across all batches\n",
        "    feat_max = np.max(train_data_new_X[:, feat_index, 0]) \n",
        "    # print(feat_min, feat_max)\n",
        "\n",
        "    # normalize input features\n",
        "    train_data_new_X[:, feat_index, 0] = (train_data_new_X[:, feat_index, 0] - feat_min) / (feat_max - feat_min)\n",
        "    # print(train_data_new_X[:, feat_index], train_data_new_X[:, feat_index].shape)\n",
        "\n",
        "  # normalize labels\n",
        "  feat_min = np.min(train_data_new_Y)\n",
        "  feat_max = np.max(train_data_new_Y)\n",
        "\n",
        "  train_data_new_Y = (train_data_new_Y - feat_min) / (feat_max - feat_min)  # normalize labels\n",
        "\n",
        "  \n",
        "\n",
        "  #------- for validation set --------------#\n",
        "  \n",
        "  for index, data in enumerate(val_data):\n",
        "    # print(index, data)\n",
        "    val_data_X[index, 0] = data[\"config\"][\"batch_size\"]  # config params are single value scalars\n",
        "    val_data_X[index, 1] = data[\"config\"][\"max_dropout\"]\n",
        "    val_data_X[index, 2] = data[\"config\"][\"max_units\"]\n",
        "    val_data_X[index, 3] = data[\"config\"][\"num_layers\"]\n",
        "    val_data_X[index, 4] = data[\"config\"][\"learning_rate\"]\n",
        "    val_data_X[index, 5] = data[\"config\"][\"momentum\"]\n",
        "    val_data_X[index, 6] = data[\"config\"][\"weight_decay\"]\n",
        "\n",
        "    #------- dynamic params -----------#\n",
        "    # val_data_X[index, 7] = np.sum(np.asarray(data[\"Train/loss\"][0:10])) / 10  # these we use only first 10 epochs, average loss\n",
        "    # val_data_X[index, 8] = np.sum(np.asarray(data[\"Train/train_accuracy\"][0:10])) / 10 # average accuracy across 10 epochs\n",
        "    # val_data_X[index, 9] = np.sum(np.asarray(data[\"Train/val_accuracy\"][0:10])) / 10 # average val_acc across 10 epochs\n",
        "\n",
        "    val_data_X[index, 7] = np.sum(np.asarray(data[\"Train/loss\"][0:10])) / 10  # these we use only first 10 epochs, average loss\n",
        "    val_data_X[index, 8] = np.sum(np.asarray(data[\"Train/train_accuracy\"][0:10])) / 10 # average accuracy across 10 epochs\n",
        "    val_data_X[index, 9] = np.sum(np.asarray(data[\"Train/val_accuracy\"][0:10])) / 10 # average val_acc across 10 epochs\n",
        "    val_data_X[index, 10] = np.sum(np.asarray(data[\"Train/train_cross_entropy\"][0:10])) / 10\n",
        "    val_data_X[index, 11] = np.sum(np.asarray(data[\"Train/val_cross_entropy\"][0:10])) / 10\n",
        "    val_data_X[index, 12] = np.sum(np.asarray(data[\"Train/gradient_mean\"][0:10])) / 10\n",
        "    val_data_X[index, 13] = np.sum(np.asarray(data[\"Train/lr\"][0:10])) / 10\n",
        "\n",
        "    val_data_Y[index] = val_targets[index]  # validation accuracy for 51st epoch, for each \n",
        "\n",
        "  \n",
        "  \n",
        "  # normalize each feature across all batches, so each feature is in range [0-1]\n",
        "  for feat_index in range(14):\n",
        "    feat_min = np.min(val_data_X[:, feat_index, 0])  # min of feature 0 across all batches\n",
        "    feat_max = np.max(val_data_X[:, feat_index, 0]) \n",
        "    print(feat_min, feat_max)\n",
        "\n",
        "    # normalize input features\n",
        "    val_data_X[:, feat_index, 0] = (val_data_X[:, feat_index, 0] - feat_min) / (feat_max - feat_min)\n",
        "    # print(val_data_X[:, feat_index], val_data_X[:, feat_index].shape)\n",
        "\n",
        "  # normalize labels\n",
        "  feat_min = np.min(val_data_Y)\n",
        "  feat_max = np.max(val_data_Y)\n",
        "\n",
        "  val_data_Y = (val_data_Y - feat_min) / (feat_max - feat_min) # normalize labels\n",
        "\n",
        "\n",
        "  #------- for test set --------------#\n",
        "  \n",
        "  for index, data in enumerate(test_data):\n",
        "    # print(index, data)\n",
        "    test_data_X[index, 0] = data[\"config\"][\"batch_size\"]  # config params are single value scalars\n",
        "    test_data_X[index, 1] = data[\"config\"][\"max_dropout\"]\n",
        "    test_data_X[index, 2] = data[\"config\"][\"max_units\"]\n",
        "    test_data_X[index, 3] = data[\"config\"][\"num_layers\"]\n",
        "    test_data_X[index, 4] = data[\"config\"][\"learning_rate\"]\n",
        "    test_data_X[index, 5] = data[\"config\"][\"momentum\"]\n",
        "    test_data_X[index, 6] = data[\"config\"][\"weight_decay\"]\n",
        "\n",
        "    #------- dynamic params -----------#\n",
        "    test_data_X[index, 7] = np.sum(np.asarray(data[\"Train/loss\"][0:10])) / 10  # these we use only first 10 epochs, average loss\n",
        "    test_data_X[index, 8] = np.sum(np.asarray(data[\"Train/train_accuracy\"][0:10])) / 10 # average accuracy across 10 epochs\n",
        "    test_data_X[index, 9] = np.sum(np.asarray(data[\"Train/val_accuracy\"][0:10])) / 10 # average val_acc across 10 epochs\n",
        "    test_data_X[index, 10] = np.sum(np.asarray(data[\"Train/train_cross_entropy\"][0:10])) / 10\n",
        "    test_data_X[index, 11] = np.sum(np.asarray(data[\"Train/val_cross_entropy\"][0:10])) / 10\n",
        "    test_data_X[index, 12] = np.sum(np.asarray(data[\"Train/gradient_mean\"][0:10])) / 10\n",
        "    test_data_X[index, 13] = np.sum(np.asarray(data[\"Train/lr\"][0:10])) / 10\n",
        "\n",
        "\n",
        "    test_data_Y[index] = test_targets[index]  # validation accuracy for 51st epoch, for each \n",
        "    \n",
        "  \n",
        "  # normalize each feature across all batches, so each feature is in range [0-1]\n",
        "  for feat_index in range(14):\n",
        "    feat_min = np.min(test_data_X[:, feat_index, 0])  # min of feature 0 across all batches\n",
        "    feat_max = np.max(test_data_X[:, feat_index, 0]) \n",
        "    print(feat_min, feat_max)\n",
        "\n",
        "    # normalize input features\n",
        "    test_data_X[:, feat_index, 0] = (test_data_X[:, feat_index, 0] - feat_min) / (feat_max - feat_min)\n",
        "    # print(val_data_X[:, feat_index], val_data_X[:, feat_index].shape)\n",
        "\n",
        "  # normalize labels\n",
        "  feat_min = np.min(test_data_Y)\n",
        "  feat_max = np.max(test_data_Y)\n",
        "\n",
        "  test_data_Y = (test_data_Y - feat_min) / (feat_max - feat_min) # normalize labels\n",
        "\n",
        "\n",
        "  # print(val_data_Y)\n",
        "  \n",
        "\n",
        "  return train_data_new_X, train_data_new_Y, val_data_X, val_data_Y, test_data_X, test_data_Y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcO2keiPchs8",
        "colab_type": "code",
        "outputId": "57710133-bf77-4a84-e49d-3227ab061672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# build a MLP here\n",
        "import tensorflow as tf\n",
        "import keras \n",
        "from keras.activations import relu, sigmoid, tanh\n",
        "from keras.layers import Conv2D, Conv1D, Dense, Flatten, Input, Dropout\n",
        "from keras.optimizers import Adam,SGD\n",
        "from keras.models import Model\n",
        "\n",
        "def build_model(input_shape=(1000, 10, 1)):\n",
        "  \"\"\"\n",
        "  Function builds a keras model\n",
        "  \"\"\"\n",
        "\n",
        "  # optim = SGD(lr=0.001, momentum=0.9)\n",
        "  optim = Adam()\n",
        "  inputs = Input(shape=(14, 1))\n",
        "\n",
        "  dense1_out = Dense(16, activation='sigmoid', kernel_initializer='he_uniform')(inputs)\n",
        "  drop1_out = Dropout(rate=0.3)(dense1_out)\n",
        "  dense2_out = Dense(16, activation='sigmoid', kernel_initializer='he_uniform')(drop1_out)\n",
        "  drop2_out = Dropout(rate=0.2)(dense2_out)\n",
        "  dense3_out = Dense(16, activation='sigmoid', kernel_initializer='he_uniform')(drop2_out)\n",
        "  flat_out = Flatten()(dense3_out)\n",
        "  final_out = Dense(1, activation='sigmoid')(flat_out)  # last layer, no activation\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=final_out)\n",
        "\n",
        "  model.compile(optimizer=optim,\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  model = build_model()\n",
        "\n",
        "  for layer in model.layers:\n",
        "    print(layer.output.shape)\n",
        "\n",
        "  train_X, train_Y, val_X, val_Y, test_X, test_Y = get_data()  # get the prepared data\n",
        "\n",
        "  print(train_X.shape, train_Y.shape, val_X.shape, val_Y.shape)\n",
        "  \n",
        "  # debug code - match that the processed targets and targets read from the database are equal except the normalizing\n",
        "  \n",
        "  # for i in range(0, 10):\n",
        "  #   print(\"processed : {0}, database : {1}\".format(train_Y[i], train_targets[i]))\n",
        "\n",
        "  # # removing normalization\n",
        "  # print(\"without norm\")\n",
        "  # for i in range(0, 10):\n",
        "  #   print(\"processed : {0}, database : {1}\".format(train_Y[i] * np.max(train_targets), train_targets[i]))\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "  history = model.fit(x=train_X, y=train_Y, batch_size=16, epochs=100, verbose=1, validation_data=(val_X, val_Y))\n",
        "  \n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['loss'])\n",
        "\n",
        "  model.save(\"/content/drive/My Drive/ColabWorks/DL_project/LC_predictor.h5\")\n",
        "\n",
        "  res = model.predict(test_X)\n",
        "\n",
        "  print(res.shape, test_Y.shape)\n",
        "\n",
        "  #----------- predictions ------------#\n",
        "  for index, data in enumerate(test_Y):\n",
        "    print(\"test target : {0}, prediction : {1}, error(%) : {2}\".format(data, res[index], abs(res[index] - data)*100))\n",
        "\n",
        "\n",
        "  avg_err = (np.sum(res - test_Y)/1000)*100\n",
        "  print(\"average error over 1000 predictions : \", avg_err)\n",
        "  print(\"maximum error over 1000 predictions : \", np.max(res - test_Y))\n",
        "  print(\"minimum error over 1000 predictions : \", np.min(res - test_Y))\n",
        "\n",
        "\n",
        "  # res = model.evaluate(x=test_X, y=test_Y)\n",
        "  # print(res)\n",
        "\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_88\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_88 (InputLayer)        (None, 14, 1)             0         \n",
            "_________________________________________________________________\n",
            "dense_309 (Dense)            (None, 14, 16)            32        \n",
            "_________________________________________________________________\n",
            "dropout_131 (Dropout)        (None, 14, 16)            0         \n",
            "_________________________________________________________________\n",
            "dense_310 (Dense)            (None, 14, 16)            272       \n",
            "_________________________________________________________________\n",
            "dropout_132 (Dropout)        (None, 14, 16)            0         \n",
            "_________________________________________________________________\n",
            "dense_311 (Dense)            (None, 14, 16)            272       \n",
            "_________________________________________________________________\n",
            "flatten_82 (Flatten)         (None, 224)               0         \n",
            "_________________________________________________________________\n",
            "dense_312 (Dense)            (None, 1)                 225       \n",
            "=================================================================\n",
            "Total params: 801\n",
            "Trainable params: 801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(?, 14, 1)\n",
            "(?, 14, 16)\n",
            "(?, 14, 16)\n",
            "(?, 14, 16)\n",
            "(?, 14, 16)\n",
            "(?, 14, 16)\n",
            "(?, ?)\n",
            "(?, 1)\n",
            "16.0 511.0\n",
            "0.0011812322346441695 0.9928635855422393\n",
            "64.0 1012.0\n",
            "1.0 4.0\n",
            "0.00010025816064755269 0.09902960198487802\n",
            "0.10037276575931453 0.9621438949781691\n",
            "0.0001766854737253736 0.0997793204689283\n",
            "0.5694407224655151 2.534297776222229\n",
            "10.358347702026368 78.85271482467651\n",
            "10.220312690734863 78.7789122581482\n",
            "0.572177705168724 2.505104994773865\n",
            "0.5715352803468704 2.678911364078522\n",
            "-0.00010811567772179842 0.0002224183666839963\n",
            "8.886231371434406e-05 0.08777326121926307\n",
            "16.0 511.0\n",
            "0.001366841190672985 0.9992073059281495\n",
            "64.0 1017.0\n",
            "1.0 4.0\n",
            "0.00010010296604187056 0.09889472649046882\n",
            "0.10000386302346072 0.9792307850916381\n",
            "0.00033380650067516454 0.09999745197542904\n",
            "0.5311092972755432 2.6400418519973754\n",
            "10.622811985015868 80.63331336975098\n",
            "12.190851402282714 79.58263282775879\n",
            "0.5335983544588089 2.5103718280792235\n",
            "0.5585014879703522 2.498201847076416\n",
            "-0.00011287007737337262 0.00015542155383627688\n",
            "8.872475809766911e-05 0.08765371888875961\n",
            "(1000, 14, 1) (1000, 1) (500, 14, 1) (500, 1)\n",
            "Train on 1000 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0518 - acc: 1.0000e-03 - val_loss: 0.0125 - val_acc: 0.0020\n",
            "Epoch 2/100\n",
            "1000/1000 [==============================] - 0s 368us/step - loss: 0.0172 - acc: 1.0000e-03 - val_loss: 0.0124 - val_acc: 0.0020\n",
            "Epoch 3/100\n",
            "1000/1000 [==============================] - 0s 351us/step - loss: 0.0174 - acc: 1.0000e-03 - val_loss: 0.0125 - val_acc: 0.0020\n",
            "Epoch 4/100\n",
            "1000/1000 [==============================] - 0s 336us/step - loss: 0.0172 - acc: 1.0000e-03 - val_loss: 0.0124 - val_acc: 0.0020\n",
            "Epoch 5/100\n",
            "1000/1000 [==============================] - 0s 373us/step - loss: 0.0172 - acc: 1.0000e-03 - val_loss: 0.0125 - val_acc: 0.0020\n",
            "Epoch 6/100\n",
            "1000/1000 [==============================] - 0s 347us/step - loss: 0.0171 - acc: 1.0000e-03 - val_loss: 0.0124 - val_acc: 0.0020\n",
            "Epoch 7/100\n",
            "1000/1000 [==============================] - 0s 363us/step - loss: 0.0171 - acc: 1.0000e-03 - val_loss: 0.0123 - val_acc: 0.0020\n",
            "Epoch 8/100\n",
            "1000/1000 [==============================] - 0s 391us/step - loss: 0.0169 - acc: 1.0000e-03 - val_loss: 0.0121 - val_acc: 0.0020\n",
            "Epoch 9/100\n",
            "1000/1000 [==============================] - 0s 393us/step - loss: 0.0168 - acc: 1.0000e-03 - val_loss: 0.0122 - val_acc: 0.0020\n",
            "Epoch 10/100\n",
            "1000/1000 [==============================] - 0s 353us/step - loss: 0.0169 - acc: 1.0000e-03 - val_loss: 0.0122 - val_acc: 0.0020\n",
            "Epoch 11/100\n",
            "1000/1000 [==============================] - 0s 367us/step - loss: 0.0171 - acc: 1.0000e-03 - val_loss: 0.0118 - val_acc: 0.0020\n",
            "Epoch 12/100\n",
            "1000/1000 [==============================] - 0s 332us/step - loss: 0.0168 - acc: 1.0000e-03 - val_loss: 0.0123 - val_acc: 0.0020\n",
            "Epoch 13/100\n",
            "1000/1000 [==============================] - 0s 328us/step - loss: 0.0164 - acc: 1.0000e-03 - val_loss: 0.0118 - val_acc: 0.0020\n",
            "Epoch 14/100\n",
            "1000/1000 [==============================] - 0s 352us/step - loss: 0.0161 - acc: 1.0000e-03 - val_loss: 0.0129 - val_acc: 0.0020\n",
            "Epoch 15/100\n",
            "1000/1000 [==============================] - 0s 329us/step - loss: 0.0163 - acc: 1.0000e-03 - val_loss: 0.0117 - val_acc: 0.0020\n",
            "Epoch 16/100\n",
            "1000/1000 [==============================] - 0s 348us/step - loss: 0.0163 - acc: 1.0000e-03 - val_loss: 0.0112 - val_acc: 0.0020\n",
            "Epoch 17/100\n",
            "1000/1000 [==============================] - 0s 360us/step - loss: 0.0161 - acc: 1.0000e-03 - val_loss: 0.0109 - val_acc: 0.0020\n",
            "Epoch 18/100\n",
            "1000/1000 [==============================] - 0s 333us/step - loss: 0.0154 - acc: 1.0000e-03 - val_loss: 0.0108 - val_acc: 0.0020\n",
            "Epoch 19/100\n",
            "1000/1000 [==============================] - 0s 336us/step - loss: 0.0156 - acc: 1.0000e-03 - val_loss: 0.0103 - val_acc: 0.0020\n",
            "Epoch 20/100\n",
            "1000/1000 [==============================] - 0s 365us/step - loss: 0.0149 - acc: 1.0000e-03 - val_loss: 0.0109 - val_acc: 0.0020\n",
            "Epoch 21/100\n",
            "1000/1000 [==============================] - 0s 353us/step - loss: 0.0143 - acc: 1.0000e-03 - val_loss: 0.0097 - val_acc: 0.0020\n",
            "Epoch 22/100\n",
            "1000/1000 [==============================] - 0s 331us/step - loss: 0.0135 - acc: 1.0000e-03 - val_loss: 0.0113 - val_acc: 0.0020\n",
            "Epoch 23/100\n",
            "1000/1000 [==============================] - 0s 371us/step - loss: 0.0126 - acc: 1.0000e-03 - val_loss: 0.0078 - val_acc: 0.0020\n",
            "Epoch 24/100\n",
            "1000/1000 [==============================] - 0s 343us/step - loss: 0.0114 - acc: 1.0000e-03 - val_loss: 0.0067 - val_acc: 0.0020\n",
            "Epoch 25/100\n",
            "1000/1000 [==============================] - 0s 361us/step - loss: 0.0102 - acc: 1.0000e-03 - val_loss: 0.0059 - val_acc: 0.0020\n",
            "Epoch 26/100\n",
            "1000/1000 [==============================] - 0s 362us/step - loss: 0.0095 - acc: 1.0000e-03 - val_loss: 0.0046 - val_acc: 0.0020\n",
            "Epoch 27/100\n",
            "1000/1000 [==============================] - 0s 351us/step - loss: 0.0083 - acc: 1.0000e-03 - val_loss: 0.0031 - val_acc: 0.0020\n",
            "Epoch 28/100\n",
            "1000/1000 [==============================] - 0s 356us/step - loss: 0.0067 - acc: 1.0000e-03 - val_loss: 0.0026 - val_acc: 0.0040\n",
            "Epoch 29/100\n",
            "1000/1000 [==============================] - 0s 379us/step - loss: 0.0059 - acc: 1.0000e-03 - val_loss: 0.0024 - val_acc: 0.0040\n",
            "Epoch 30/100\n",
            "1000/1000 [==============================] - 0s 325us/step - loss: 0.0057 - acc: 1.0000e-03 - val_loss: 0.0032 - val_acc: 0.0040\n",
            "Epoch 31/100\n",
            "1000/1000 [==============================] - 0s 383us/step - loss: 0.0058 - acc: 1.0000e-03 - val_loss: 0.0030 - val_acc: 0.0040\n",
            "Epoch 32/100\n",
            "1000/1000 [==============================] - 0s 388us/step - loss: 0.0057 - acc: 1.0000e-03 - val_loss: 0.0024 - val_acc: 0.0040\n",
            "Epoch 33/100\n",
            "1000/1000 [==============================] - 0s 345us/step - loss: 0.0047 - acc: 0.0020 - val_loss: 0.0034 - val_acc: 0.0040\n",
            "Epoch 34/100\n",
            "1000/1000 [==============================] - 0s 358us/step - loss: 0.0043 - acc: 1.0000e-03 - val_loss: 0.0028 - val_acc: 0.0040\n",
            "Epoch 35/100\n",
            "1000/1000 [==============================] - 0s 382us/step - loss: 0.0038 - acc: 0.0020 - val_loss: 0.0026 - val_acc: 0.0040\n",
            "Epoch 36/100\n",
            "1000/1000 [==============================] - 0s 337us/step - loss: 0.0044 - acc: 1.0000e-03 - val_loss: 0.0036 - val_acc: 0.0040\n",
            "Epoch 37/100\n",
            "1000/1000 [==============================] - 0s 352us/step - loss: 0.0043 - acc: 1.0000e-03 - val_loss: 0.0035 - val_acc: 0.0040\n",
            "Epoch 38/100\n",
            "1000/1000 [==============================] - 0s 336us/step - loss: 0.0040 - acc: 0.0020 - val_loss: 0.0026 - val_acc: 0.0040\n",
            "Epoch 39/100\n",
            "1000/1000 [==============================] - 0s 323us/step - loss: 0.0037 - acc: 0.0020 - val_loss: 0.0023 - val_acc: 0.0040\n",
            "Epoch 40/100\n",
            "1000/1000 [==============================] - 0s 328us/step - loss: 0.0042 - acc: 0.0020 - val_loss: 0.0022 - val_acc: 0.0040\n",
            "Epoch 41/100\n",
            "1000/1000 [==============================] - 0s 331us/step - loss: 0.0039 - acc: 1.0000e-03 - val_loss: 0.0025 - val_acc: 0.0040\n",
            "Epoch 42/100\n",
            "1000/1000 [==============================] - 0s 345us/step - loss: 0.0042 - acc: 0.0020 - val_loss: 0.0042 - val_acc: 0.0040\n",
            "Epoch 43/100\n",
            "1000/1000 [==============================] - 0s 333us/step - loss: 0.0037 - acc: 0.0020 - val_loss: 0.0019 - val_acc: 0.0040\n",
            "Epoch 44/100\n",
            "1000/1000 [==============================] - 0s 346us/step - loss: 0.0040 - acc: 0.0020 - val_loss: 0.0019 - val_acc: 0.0040\n",
            "Epoch 45/100\n",
            "1000/1000 [==============================] - 0s 324us/step - loss: 0.0036 - acc: 0.0020 - val_loss: 0.0022 - val_acc: 0.0040\n",
            "Epoch 46/100\n",
            "1000/1000 [==============================] - 0s 351us/step - loss: 0.0036 - acc: 1.0000e-03 - val_loss: 0.0033 - val_acc: 0.0040\n",
            "Epoch 47/100\n",
            "1000/1000 [==============================] - 0s 348us/step - loss: 0.0035 - acc: 0.0020 - val_loss: 0.0030 - val_acc: 0.0040\n",
            "Epoch 48/100\n",
            "1000/1000 [==============================] - 0s 352us/step - loss: 0.0035 - acc: 0.0020 - val_loss: 0.0021 - val_acc: 0.0040\n",
            "Epoch 49/100\n",
            "1000/1000 [==============================] - 0s 385us/step - loss: 0.0035 - acc: 0.0020 - val_loss: 0.0023 - val_acc: 0.0040\n",
            "Epoch 50/100\n",
            "1000/1000 [==============================] - 1s 512us/step - loss: 0.0040 - acc: 1.0000e-03 - val_loss: 0.0023 - val_acc: 0.0040\n",
            "Epoch 51/100\n",
            "1000/1000 [==============================] - 0s 378us/step - loss: 0.0035 - acc: 0.0020 - val_loss: 0.0018 - val_acc: 0.0040\n",
            "Epoch 52/100\n",
            "1000/1000 [==============================] - 0s 345us/step - loss: 0.0033 - acc: 0.0020 - val_loss: 0.0021 - val_acc: 0.0040\n",
            "Epoch 53/100\n",
            "1000/1000 [==============================] - 0s 376us/step - loss: 0.0033 - acc: 0.0020 - val_loss: 0.0026 - val_acc: 0.0040\n",
            "Epoch 54/100\n",
            "1000/1000 [==============================] - 0s 343us/step - loss: 0.0036 - acc: 0.0020 - val_loss: 0.0033 - val_acc: 0.0040\n",
            "Epoch 55/100\n",
            "1000/1000 [==============================] - 0s 335us/step - loss: 0.0032 - acc: 0.0020 - val_loss: 0.0020 - val_acc: 0.0040\n",
            "Epoch 56/100\n",
            "1000/1000 [==============================] - 0s 345us/step - loss: 0.0035 - acc: 1.0000e-03 - val_loss: 0.0032 - val_acc: 0.0040\n",
            "Epoch 57/100\n",
            "1000/1000 [==============================] - 0s 371us/step - loss: 0.0035 - acc: 0.0020 - val_loss: 0.0028 - val_acc: 0.0040\n",
            "Epoch 58/100\n",
            "1000/1000 [==============================] - 0s 391us/step - loss: 0.0033 - acc: 0.0020 - val_loss: 0.0023 - val_acc: 0.0040\n",
            "Epoch 59/100\n",
            "1000/1000 [==============================] - 0s 321us/step - loss: 0.0028 - acc: 0.0020 - val_loss: 0.0029 - val_acc: 0.0040\n",
            "Epoch 60/100\n",
            "1000/1000 [==============================] - 0s 341us/step - loss: 0.0035 - acc: 0.0020 - val_loss: 0.0020 - val_acc: 0.0040\n",
            "Epoch 61/100\n",
            "1000/1000 [==============================] - 0s 340us/step - loss: 0.0030 - acc: 0.0020 - val_loss: 0.0019 - val_acc: 0.0040\n",
            "Epoch 62/100\n",
            "1000/1000 [==============================] - 0s 343us/step - loss: 0.0030 - acc: 0.0020 - val_loss: 0.0017 - val_acc: 0.0040\n",
            "Epoch 63/100\n",
            "1000/1000 [==============================] - 0s 339us/step - loss: 0.0033 - acc: 0.0020 - val_loss: 0.0018 - val_acc: 0.0040\n",
            "Epoch 64/100\n",
            "1000/1000 [==============================] - 0s 347us/step - loss: 0.0031 - acc: 0.0020 - val_loss: 0.0018 - val_acc: 0.0040\n",
            "Epoch 65/100\n",
            "1000/1000 [==============================] - 0s 352us/step - loss: 0.0029 - acc: 0.0020 - val_loss: 0.0023 - val_acc: 0.0040\n",
            "Epoch 66/100\n",
            "1000/1000 [==============================] - 0s 363us/step - loss: 0.0032 - acc: 0.0020 - val_loss: 0.0018 - val_acc: 0.0040\n",
            "Epoch 67/100\n",
            "1000/1000 [==============================] - 0s 335us/step - loss: 0.0030 - acc: 0.0020 - val_loss: 0.0024 - val_acc: 0.0040\n",
            "Epoch 68/100\n",
            "1000/1000 [==============================] - 0s 383us/step - loss: 0.0028 - acc: 0.0020 - val_loss: 0.0019 - val_acc: 0.0040\n",
            "Epoch 69/100\n",
            "1000/1000 [==============================] - 0s 378us/step - loss: 0.0032 - acc: 1.0000e-03 - val_loss: 0.0017 - val_acc: 0.0040\n",
            "Epoch 70/100\n",
            "1000/1000 [==============================] - 0s 348us/step - loss: 0.0030 - acc: 0.0020 - val_loss: 0.0018 - val_acc: 0.0040\n",
            "Epoch 71/100\n",
            "1000/1000 [==============================] - 0s 388us/step - loss: 0.0027 - acc: 0.0020 - val_loss: 0.0023 - val_acc: 0.0040\n",
            "Epoch 72/100\n",
            "1000/1000 [==============================] - 0s 327us/step - loss: 0.0030 - acc: 0.0020 - val_loss: 0.0023 - val_acc: 0.0040\n",
            "Epoch 73/100\n",
            "1000/1000 [==============================] - 0s 340us/step - loss: 0.0027 - acc: 0.0020 - val_loss: 0.0020 - val_acc: 0.0040\n",
            "Epoch 74/100\n",
            "1000/1000 [==============================] - 0s 408us/step - loss: 0.0029 - acc: 0.0020 - val_loss: 0.0019 - val_acc: 0.0040\n",
            "Epoch 75/100\n",
            "1000/1000 [==============================] - 0s 320us/step - loss: 0.0029 - acc: 0.0020 - val_loss: 0.0022 - val_acc: 0.0040\n",
            "Epoch 76/100\n",
            "1000/1000 [==============================] - 0s 326us/step - loss: 0.0028 - acc: 0.0020 - val_loss: 0.0030 - val_acc: 0.0040\n",
            "Epoch 77/100\n",
            "1000/1000 [==============================] - 0s 362us/step - loss: 0.0026 - acc: 0.0020 - val_loss: 0.0023 - val_acc: 0.0040\n",
            "Epoch 78/100\n",
            "1000/1000 [==============================] - 0s 345us/step - loss: 0.0031 - acc: 0.0020 - val_loss: 0.0024 - val_acc: 0.0040\n",
            "Epoch 79/100\n",
            "1000/1000 [==============================] - 0s 328us/step - loss: 0.0027 - acc: 0.0020 - val_loss: 0.0025 - val_acc: 0.0040\n",
            "Epoch 80/100\n",
            "1000/1000 [==============================] - 0s 386us/step - loss: 0.0031 - acc: 0.0020 - val_loss: 0.0020 - val_acc: 0.0040\n",
            "Epoch 81/100\n",
            "1000/1000 [==============================] - 0s 320us/step - loss: 0.0028 - acc: 0.0020 - val_loss: 0.0020 - val_acc: 0.0040\n",
            "Epoch 82/100\n",
            "1000/1000 [==============================] - 0s 326us/step - loss: 0.0026 - acc: 0.0020 - val_loss: 0.0018 - val_acc: 0.0040\n",
            "Epoch 83/100\n",
            "1000/1000 [==============================] - 0s 369us/step - loss: 0.0028 - acc: 1.0000e-03 - val_loss: 0.0019 - val_acc: 0.0040\n",
            "Epoch 84/100\n",
            "1000/1000 [==============================] - 0s 375us/step - loss: 0.0028 - acc: 0.0020 - val_loss: 0.0018 - val_acc: 0.0040\n",
            "Epoch 85/100\n",
            "1000/1000 [==============================] - 0s 338us/step - loss: 0.0027 - acc: 0.0020 - val_loss: 0.0026 - val_acc: 0.0040\n",
            "Epoch 86/100\n",
            "1000/1000 [==============================] - 0s 358us/step - loss: 0.0030 - acc: 1.0000e-03 - val_loss: 0.0018 - val_acc: 0.0040\n",
            "Epoch 87/100\n",
            "1000/1000 [==============================] - 0s 370us/step - loss: 0.0026 - acc: 0.0020 - val_loss: 0.0020 - val_acc: 0.0040\n",
            "Epoch 88/100\n",
            "1000/1000 [==============================] - 0s 354us/step - loss: 0.0027 - acc: 0.0020 - val_loss: 0.0029 - val_acc: 0.0040\n",
            "Epoch 89/100\n",
            "1000/1000 [==============================] - 0s 366us/step - loss: 0.0028 - acc: 1.0000e-03 - val_loss: 0.0016 - val_acc: 0.0040\n",
            "Epoch 90/100\n",
            "1000/1000 [==============================] - 0s 332us/step - loss: 0.0029 - acc: 0.0020 - val_loss: 0.0016 - val_acc: 0.0040\n",
            "Epoch 91/100\n",
            "1000/1000 [==============================] - 0s 339us/step - loss: 0.0029 - acc: 0.0020 - val_loss: 0.0025 - val_acc: 0.0040\n",
            "Epoch 92/100\n",
            "1000/1000 [==============================] - 0s 344us/step - loss: 0.0027 - acc: 0.0020 - val_loss: 0.0017 - val_acc: 0.0040\n",
            "Epoch 93/100\n",
            "1000/1000 [==============================] - 0s 396us/step - loss: 0.0028 - acc: 0.0020 - val_loss: 0.0024 - val_acc: 0.0040\n",
            "Epoch 94/100\n",
            "1000/1000 [==============================] - 1s 515us/step - loss: 0.0027 - acc: 0.0020 - val_loss: 0.0020 - val_acc: 0.0040\n",
            "Epoch 95/100\n",
            "1000/1000 [==============================] - 0s 425us/step - loss: 0.0028 - acc: 0.0020 - val_loss: 0.0024 - val_acc: 0.0040\n",
            "Epoch 96/100\n",
            "1000/1000 [==============================] - 0s 389us/step - loss: 0.0029 - acc: 0.0020 - val_loss: 0.0016 - val_acc: 0.0040\n",
            "Epoch 97/100\n",
            "1000/1000 [==============================] - 0s 355us/step - loss: 0.0028 - acc: 0.0020 - val_loss: 0.0019 - val_acc: 0.0040\n",
            "Epoch 98/100\n",
            "1000/1000 [==============================] - 0s 344us/step - loss: 0.0026 - acc: 0.0020 - val_loss: 0.0016 - val_acc: 0.0040\n",
            "Epoch 99/100\n",
            "1000/1000 [==============================] - 0s 348us/step - loss: 0.0025 - acc: 0.0020 - val_loss: 0.0027 - val_acc: 0.0040\n",
            "Epoch 100/100\n",
            "1000/1000 [==============================] - 0s 359us/step - loss: 0.0028 - acc: 0.0020 - val_loss: 0.0016 - val_acc: 0.0040\n",
            "(500, 1) (500, 1)\n",
            "test target : [0.91052534], prediction : [0.93042696], error(%) : [1.99016159]\n",
            "test target : [0.87757149], prediction : [0.88973427], error(%) : [1.21627762]\n",
            "test target : [0.85670071], prediction : [0.86117125], error(%) : [0.44705353]\n",
            "test target : [0.87497507], prediction : [0.8818891], error(%) : [0.69140344]\n",
            "test target : [0.90453369], prediction : [0.905638], error(%) : [0.11042916]\n",
            "test target : [0.87647292], prediction : [0.896536], error(%) : [2.00630683]\n",
            "test target : [0.92031162], prediction : [0.928802], error(%) : [0.84903886]\n",
            "test target : [0.87048139], prediction : [0.8767965], error(%) : [0.63150932]\n",
            "test target : [0.92071103], prediction : [0.93495274], error(%) : [1.42417069]\n",
            "test target : [0.71380071], prediction : [0.8239404], error(%) : [11.01396896]\n",
            "test target : [0.53894548], prediction : [0.35680026], error(%) : [18.21452259]\n",
            "test target : [0.70860798], prediction : [0.745189], error(%) : [3.65810297]\n",
            "test target : [0.87896947], prediction : [0.8837986], error(%) : [0.48291338]\n",
            "test target : [0.8646895], prediction : [0.87752634], error(%) : [1.28368426]\n",
            "test target : [0.88955468], prediction : [0.907323], error(%) : [1.77683257]\n",
            "test target : [0.91601759], prediction : [0.9057972], error(%) : [1.02204048]\n",
            "test target : [0.88566016], prediction : [0.88823354], error(%) : [0.25733799]\n",
            "test target : [0.5384462], prediction : [0.64868426], error(%) : [11.02380638]\n",
            "test target : [0.85370488], prediction : [0.859246], error(%) : [0.5541131]\n",
            "test target : [0.90713011], prediction : [0.92045486], error(%) : [1.33247502]\n",
            "test target : [0.912922], prediction : [0.91029143], error(%) : [0.26305666]\n",
            "test target : [0.83453172], prediction : [0.87179744], error(%) : [3.72657248]\n",
            "test target : [0.72188938], prediction : [0.72932535], error(%) : [0.74359768]\n",
            "test target : [0.78809671], prediction : [0.71324193], error(%) : [7.48547787]\n",
            "test target : [0.41791495], prediction : [0.49985856], error(%) : [8.19436087]\n",
            "test target : [0.75384466], prediction : [0.7038222], error(%) : [5.00224598]\n",
            "test target : [0.82893947], prediction : [0.851845], error(%) : [2.29055557]\n",
            "test target : [0.88635909], prediction : [0.8874515], error(%) : [0.10924392]\n",
            "test target : [0.87976839], prediction : [0.89175326], error(%) : [1.19848647]\n",
            "test target : [0.88905539], prediction : [0.9037856], error(%) : [1.47301939]\n",
            "test target : [0.80766928], prediction : [0.8393413], error(%) : [3.16719984]\n",
            "test target : [0.88276422], prediction : [0.89427733], error(%) : [1.15131169]\n",
            "test target : [0.89235074], prediction : [0.88933516], error(%) : [0.30155863]\n",
            "test target : [0.91761544], prediction : [0.9280753], error(%) : [1.04598729]\n",
            "test target : [0.53225485], prediction : [0.40105808], error(%) : [13.11967681]\n",
            "test target : [0.91691639], prediction : [0.9315342], error(%) : [1.46177761]\n",
            "test target : [0.92350709], prediction : [0.93295455], error(%) : [0.94474561]\n",
            "test target : [0.92101067], prediction : [0.93883944], error(%) : [1.78287651]\n",
            "test target : [0.86588783], prediction : [0.89504945], error(%) : [2.91616222]\n",
            "test target : [0.92560423], prediction : [0.93693066], error(%) : [1.13264258]\n",
            "test target : [0.8712802], prediction : [0.8770176], error(%) : [0.57374182]\n",
            "test target : [0.89924108], prediction : [0.9080398], error(%) : [0.87987262]\n",
            "test target : [0.9195127], prediction : [0.9378858], error(%) : [1.83731222]\n",
            "test target : [0.90113846], prediction : [0.90239584], error(%) : [0.12573865]\n",
            "test target : [0.90563214], prediction : [0.9052502], error(%) : [0.0381946]\n",
            "test target : [0.825744], prediction : [0.8501663], error(%) : [2.44223193]\n",
            "test target : [0.98052731], prediction : [0.9421536], error(%) : [3.83737365]\n",
            "test target : [0.91941282], prediction : [0.90283525], error(%) : [1.65775683]\n",
            "test target : [0.91561818], prediction : [0.9193964], error(%) : [0.37782163]\n",
            "test target : [0.89794287], prediction : [0.92440724], error(%) : [2.64643725]\n",
            "test target : [0.99810274], prediction : [0.9519739], error(%) : [4.61288269]\n",
            "test target : [0.86708616], prediction : [0.8688804], error(%) : [0.17942302]\n",
            "test target : [0.67096069], prediction : [0.6951366], error(%) : [2.41759133]\n",
            "test target : [0.81565807], prediction : [0.8677341], error(%) : [5.20760001]\n",
            "test target : [0.74535658], prediction : [0.6875935], error(%) : [5.7763061]\n",
            "test target : [0.92270829], prediction : [0.94105303], error(%) : [1.83447474]\n",
            "test target : [0.90383464], prediction : [0.90124345], error(%) : [0.2591194]\n",
            "test target : [0.92780113], prediction : [0.9429119], error(%) : [1.51107927]\n",
            "test target : [0.77251854], prediction : [0.78922665], error(%) : [1.67081137]\n",
            "test target : [0.90653094], prediction : [0.89801884], error(%) : [0.85121074]\n",
            "test target : [0.92001198], prediction : [0.92901516], error(%) : [0.90031763]\n",
            "test target : [0.89604561], prediction : [0.9019186], error(%) : [0.58729769]\n",
            "test target : [0.88785706], prediction : [0.8662623], error(%) : [2.15947454]\n",
            "test target : [0.90772927], prediction : [0.90473384], error(%) : [0.2995438]\n",
            "test target : [0.91372081], prediction : [0.9308142], error(%) : [1.70933984]\n",
            "test target : [0.92230876], prediction : [0.91972136], error(%) : [0.25873984]\n",
            "test target : [0.93329338], prediction : [0.90815455], error(%) : [2.51388319]\n",
            "test target : [0.84461753], prediction : [0.91799754], error(%) : [7.33800123]\n",
            "test target : [0.51238269], prediction : [0.3828467], error(%) : [12.95359793]\n",
            "test target : [0.92310769], prediction : [0.9360273], error(%) : [1.29195989]\n",
            "test target : [0.90892749], prediction : [0.91889286], error(%) : [0.99653734]\n",
            "test target : [0.92650292], prediction : [0.92776495], error(%) : [0.12620328]\n",
            "test target : [0.79948073], prediction : [0.8386618], error(%) : [3.91810565]\n",
            "test target : [0.54733374], prediction : [0.44063982], error(%) : [10.66939135]\n",
            "test target : [0.93199529], prediction : [0.9312793], error(%) : [0.07159846]\n",
            "test target : [0.7616338], prediction : [0.7399902], error(%) : [2.16436275]\n",
            "test target : [0.51298186], prediction : [0.5426369], error(%) : [2.96550138]\n",
            "test target : [0.80627131], prediction : [0.8835374], error(%) : [7.72661007]\n",
            "test target : [0.99820262], prediction : [0.9431645], error(%) : [5.50380951]\n",
            "test target : [0.77990816], prediction : [0.79893756], error(%) : [1.90293967]\n",
            "test target : [0.9084282], prediction : [0.91512907], error(%) : [0.67008631]\n",
            "test target : [0.92770125], prediction : [0.93936205], error(%) : [1.16607994]\n",
            "test target : [0.91721592], prediction : [0.9236177], error(%) : [0.6401802]\n",
            "test target : [0.92640304], prediction : [0.9397477], error(%) : [1.33446522]\n",
            "test target : [0.75164776], prediction : [0.6785928], error(%) : [7.30549547]\n",
            "test target : [0.87876982], prediction : [0.9113468], error(%) : [3.25769709]\n",
            "test target : [0.83592969], prediction : [0.88019407], error(%) : [4.42643769]\n",
            "test target : [0.93459159], prediction : [0.946698], error(%) : [1.210642]\n",
            "test target : [0.93738765], prediction : [0.9285071], error(%) : [0.88805651]\n",
            "test target : [0.93039743], prediction : [0.9414272], error(%) : [1.10297373]\n",
            "test target : [0.90253655], prediction : [0.90574455], error(%) : [0.32080033]\n",
            "test target : [0.92230876], prediction : [0.93871534], error(%) : [1.64065753]\n",
            "test target : [0.83852611], prediction : [0.86542296], error(%) : [2.68968514]\n",
            "test target : [0.87847018], prediction : [0.91208273], error(%) : [3.36125509]\n",
            "test target : [0.87547436], prediction : [0.89236057], error(%) : [1.6886213]\n",
            "test target : [0.91791496], prediction : [0.9298021], error(%) : [1.18871555]\n",
            "test target : [0.55991615], prediction : [0.53320336], error(%) : [2.67127829]\n",
            "test target : [0.31985222], prediction : [0.30366868], error(%) : [1.6183538]\n",
            "test target : [0.90663071], prediction : [0.91029966], error(%) : [0.36689518]\n",
            "test target : [0.50469343], prediction : [0.46554273], error(%) : [3.91506923]\n",
            "test target : [0.84841228], prediction : [0.88438725], error(%) : [3.5974976]\n",
            "test target : [0.85440393], prediction : [0.87373066], error(%) : [1.93267293]\n",
            "test target : [0.92340733], prediction : [0.9230627], error(%) : [0.03446489]\n",
            "test target : [0.52606355], prediction : [0.63341594], error(%) : [10.73523861]\n",
            "test target : [0.73527065], prediction : [0.73797613], error(%) : [0.27054802]\n",
            "test target : [0.91302176], prediction : [0.9315108], error(%) : [1.84890435]\n",
            "test target : [0.91102462], prediction : [0.92125547], error(%) : [1.02308453]\n",
            "test target : [0.90942689], prediction : [0.92109287], error(%) : [1.16659783]\n",
            "test target : [0.85300584], prediction : [0.85465884], error(%) : [0.16530032]\n",
            "test target : [0.84411824], prediction : [0.902535], error(%) : [5.84167792]\n",
            "test target : [0.79159182], prediction : [0.8584459], error(%) : [6.6854059]\n",
            "test target : [0.], prediction : [0.28939426], error(%) : [28.93942595]\n",
            "test target : [0.8031756], prediction : [0.83546805], error(%) : [3.2292449]\n",
            "test target : [0.81186344], prediction : [0.8917491], error(%) : [7.98856435]\n",
            "test target : [0.93998408], prediction : [0.9117398], error(%) : [2.82442501]\n",
            "test target : [0.91701628], prediction : [0.92523026], error(%) : [0.82139892]\n",
            "test target : [0.8765728], prediction : [0.88767827], error(%) : [1.11054607]\n",
            "test target : [0.88855611], prediction : [0.9002383], error(%) : [1.16821677]\n",
            "test target : [0.84491717], prediction : [0.823195], error(%) : [2.17221877]\n",
            "test target : [0.85040942], prediction : [0.82949805], error(%) : [2.09113646]\n",
            "test target : [0.80736976], prediction : [0.7459413], error(%) : [6.14284795]\n",
            "test target : [0.89235074], prediction : [0.9039842], error(%) : [1.16334472]\n",
            "test target : [0.83742766], prediction : [0.84150106], error(%) : [0.40733942]\n",
            "test target : [0.90103858], prediction : [0.9001087], error(%) : [0.09298825]\n",
            "test target : [0.58408234], prediction : [0.6368724], error(%) : [5.27900743]\n",
            "test target : [0.91012582], prediction : [0.9128294], error(%) : [0.27035818]\n",
            "test target : [0.9226084], prediction : [0.93387663], error(%) : [1.12682287]\n",
            "test target : [0.80617143], prediction : [0.8425782], error(%) : [3.64067422]\n",
            "test target : [0.81196332], prediction : [0.8427838], error(%) : [3.08204877]\n",
            "test target : [0.82314758], prediction : [0.83138347], error(%) : [0.82358869]\n",
            "test target : [0.8854604], prediction : [0.9261627], error(%) : [4.07023182]\n",
            "test target : [0.90203715], prediction : [0.8982584], error(%) : [0.37787587]\n",
            "test target : [0.90033953], prediction : [0.8988283], error(%) : [0.15112042]\n",
            "test target : [0.72957864], prediction : [0.6898514], error(%) : [3.97272404]\n",
            "test target : [0.90663071], prediction : [0.90359175], error(%) : [0.30389549]\n",
            "test target : [0.59926105], prediction : [0.6578678], error(%) : [5.860674]\n",
            "test target : [0.90473345], prediction : [0.9100051], error(%) : [0.52716437]\n",
            "test target : [0.88126625], prediction : [0.8954154], error(%) : [1.41491798]\n",
            "test target : [0.71899343], prediction : [0.7165264], error(%) : [0.24670429]\n",
            "test target : [0.912922], prediction : [0.9294637], error(%) : [1.65416846]\n",
            "test target : [0.85620131], prediction : [0.8724485], error(%) : [1.62471963]\n",
            "test target : [0.55402438], prediction : [0.42347425], error(%) : [13.05501232]\n",
            "test target : [0.91122439], prediction : [0.92147815], error(%) : [1.02537673]\n",
            "test target : [0.91262236], prediction : [0.90710735], error(%) : [0.55150052]\n",
            "test target : [0.60075896], prediction : [0.5801032], error(%) : [2.06557436]\n",
            "test target : [0.87377674], prediction : [0.88528967], error(%) : [1.15129289]\n",
            "test target : [0.08498104], prediction : [0.27796596], error(%) : [19.29849229]\n",
            "test target : [0.82254841], prediction : [0.84207535], error(%) : [1.95269332]\n",
            "test target : [0.93399242], prediction : [0.94265515], error(%) : [0.86627213]\n",
            "test target : [0.6576793], prediction : [0.5789271], error(%) : [7.87521978]\n",
            "test target : [0.93379278], prediction : [0.94495755], error(%) : [1.11647726]\n",
            "test target : [0.88396255], prediction : [0.8975101], error(%) : [1.35475637]\n",
            "test target : [0.74815265], prediction : [0.82945836], error(%) : [8.13057105]\n",
            "test target : [0.92999803], prediction : [0.9296839], error(%) : [0.03141061]\n",
            "test target : [0.87767125], prediction : [0.88999736], error(%) : [1.23261085]\n",
            "test target : [0.86788497], prediction : [0.87638915], error(%) : [0.85041767]\n",
            "test target : [0.89484728], prediction : [0.9056436], error(%) : [1.07962994]\n",
            "test target : [0.89724394], prediction : [0.9028339], error(%) : [0.55899355]\n",
            "test target : [0.88735778], prediction : [0.8897002], error(%) : [0.23423968]\n",
            "test target : [0.81505891], prediction : [0.83400965], error(%) : [1.89507381]\n",
            "test target : [0.77771126], prediction : [0.81948495], error(%) : [4.17736863]\n",
            "test target : [0.77401639], prediction : [0.7697551], error(%) : [0.42612664]\n",
            "test target : [0.9062313], prediction : [0.9115324], error(%) : [0.53010992]\n",
            "test target : [0.87767125], prediction : [0.8782246], error(%) : [0.05533567]\n",
            "test target : [0.9451768], prediction : [0.92630243], error(%) : [1.88743686]\n",
            "test target : [0.91212307], prediction : [0.9359392], error(%) : [2.3816119]\n",
            "test target : [0.83822647], prediction : [0.8666188], error(%) : [2.83923409]\n",
            "test target : [0.88216505], prediction : [0.9344904], error(%) : [5.23253305]\n",
            "test target : [0.89394848], prediction : [0.90716565], error(%) : [1.32171703]\n",
            "test target : [0.87437591], prediction : [0.88693655], error(%) : [1.25606401]\n",
            "test target : [0.86389057], prediction : [0.8955858], error(%) : [3.16952013]\n",
            "test target : [0.91102462], prediction : [0.9100587], error(%) : [0.09659468]\n",
            "test target : [0.6881366], prediction : [0.74046236], error(%) : [5.2325759]\n",
            "test target : [0.97623327], prediction : [0.928398], error(%) : [4.78352599]\n",
            "test target : [0.91082486], prediction : [0.9173372], error(%) : [0.65123162]\n",
            "test target : [0.88735778], prediction : [0.8996028], error(%) : [1.22449933]\n",
            "test target : [0.91701628], prediction : [0.9200582], error(%) : [0.30419153]\n",
            "test target : [0.90912725], prediction : [0.8920443], error(%) : [1.70829422]\n",
            "test target : [0.94278014], prediction : [0.9351988], error(%) : [0.75813572]\n",
            "test target : [0.92121031], prediction : [0.93908054], error(%) : [1.78702228]\n",
            "test target : [0.51178353], prediction : [0.34687728], error(%) : [16.49062503]\n",
            "test target : [0.67126022], prediction : [0.7287768], error(%) : [5.75165955]\n",
            "test target : [0.91352117], prediction : [0.9132129], error(%) : [0.03082697]\n",
            "test target : [0.8000799], prediction : [0.77194214], error(%) : [2.81377598]\n",
            "test target : [0.92180948], prediction : [0.92995954], error(%) : [0.81500568]\n",
            "test target : [0.45626128], prediction : [0.5136358], error(%) : [5.7374531]\n",
            "test target : [0.8810666], prediction : [0.88439065], error(%) : [0.33240498]\n",
            "test target : [0.80507298], prediction : [0.8433262], error(%) : [3.82532302]\n",
            "test target : [0.85020977], prediction : [0.8732917], error(%) : [2.30818971]\n",
            "test target : [0.97553435], prediction : [0.9353714], error(%) : [4.01629463]\n",
            "test target : [0.9062313], prediction : [0.9117634], error(%) : [0.55321268]\n",
            "test target : [0.95116845], prediction : [0.9379046], error(%) : [1.32638568]\n",
            "test target : [0.89275015], prediction : [0.9042971], error(%) : [1.15469675]\n",
            "test target : [0.47593373], prediction : [0.4513691], error(%) : [2.45646279]\n",
            "test target : [0.79289003], prediction : [0.71665215], error(%) : [7.62378797]\n",
            "test target : [0.83792695], prediction : [0.86158335], error(%) : [2.36564045]\n",
            "test target : [0.79858204], prediction : [0.8533182], error(%) : [5.47361698]\n",
            "test target : [0.8297384], prediction : [0.8512206], error(%) : [2.14822113]\n",
            "test target : [0.86918318], prediction : [0.88169086], error(%) : [1.25076798]\n",
            "test target : [0.5588177], prediction : [0.38738173], error(%) : [17.14359641]\n",
            "test target : [0.89974037], prediction : [0.89894956], error(%) : [0.07908032]\n",
            "test target : [0.75704012], prediction : [0.7302958], error(%) : [2.67443468]\n",
            "test target : [0.86488914], prediction : [0.90435183], error(%) : [3.94626871]\n",
            "test target : [0.82394651], prediction : [0.8938811], error(%) : [6.99345765]\n",
            "test target : [0.71130417], prediction : [0.71011484], error(%) : [0.11893289]\n",
            "test target : [0.87667269], prediction : [0.8852632], error(%) : [0.85905192]\n",
            "test target : [0.53465151], prediction : [0.48621517], error(%) : [4.84363323]\n",
            "test target : [0.83363291], prediction : [0.8393228], error(%) : [0.56898944]\n",
            "test target : [0.64060315], prediction : [0.6352278], error(%) : [0.53753503]\n",
            "test target : [0.88975444], prediction : [0.8864081], error(%) : [0.33463475]\n",
            "test target : [0.84451776], prediction : [0.85757184], error(%) : [1.30540761]\n",
            "test target : [0.91741568], prediction : [0.93604475], error(%) : [1.8629073]\n",
            "test target : [0.90223691], prediction : [0.9004592], error(%) : [0.17777374]\n",
            "test target : [0.7907929], prediction : [0.8369751], error(%) : [4.61822]\n",
            "test target : [0.92190936], prediction : [0.93481076], error(%) : [1.29013984]\n",
            "test target : [0.70870786], prediction : [0.72471786], error(%) : [1.60099935]\n",
            "test target : [0.85250655], prediction : [0.8724427], error(%) : [1.99361681]\n",
            "test target : [0.87507495], prediction : [0.8854964], error(%) : [1.0421427]\n",
            "test target : [0.87517483], prediction : [0.88687575], error(%) : [1.17009172]\n",
            "test target : [0.75274621], prediction : [0.7936835], error(%) : [4.09373234]\n",
            "test target : [0.91372081], prediction : [0.9202806], error(%) : [0.65597675]\n",
            "test target : [0.76752546], prediction : [0.7990105], error(%) : [3.14850599]\n",
            "test target : [0.85510286], prediction : [0.899752], error(%) : [4.46491629]\n",
            "test target : [0.90742963], prediction : [0.9186254], error(%) : [1.11957813]\n",
            "test target : [0.70551228], prediction : [0.7249922], error(%) : [1.94799405]\n",
            "test target : [0.92200924], prediction : [0.9185499], error(%) : [0.34593445]\n",
            "test target : [0.55072903], prediction : [0.4810465], error(%) : [6.96825282]\n",
            "test target : [0.90643106], prediction : [0.90889543], error(%) : [0.24643691]\n",
            "test target : [0.91052534], prediction : [0.9175228], error(%) : [0.69974487]\n",
            "test target : [0.9173158], prediction : [0.93913233], error(%) : [2.18165337]\n",
            "test target : [0.84411824], prediction : [0.82510626], error(%) : [1.9011979]\n",
            "test target : [0.90233679], prediction : [0.9058372], error(%) : [0.35003899]\n",
            "test target : [0.91451973], prediction : [0.91669035], error(%) : [0.21706152]\n",
            "test target : [0.92071103], prediction : [0.9304925], error(%) : [0.97814914]\n",
            "test target : [0.9513681], prediction : [0.93562824], error(%) : [1.57398609]\n",
            "test target : [0.52027166], prediction : [0.5121367], error(%) : [0.81349631]\n",
            "test target : [0.90563214], prediction : [0.90909004], error(%) : [0.34579045]\n",
            "test target : [0.8987418], prediction : [0.8969152], error(%) : [0.18266]\n",
            "test target : [0.99470751], prediction : [0.9369192], error(%) : [5.77882998]\n",
            "test target : [0.84591574], prediction : [0.8697184], error(%) : [2.38026352]\n",
            "test target : [0.79578598], prediction : [0.8201963], error(%) : [2.44102911]\n",
            "test target : [0.78250458], prediction : [0.7922459], error(%) : [0.97413404]\n",
            "test target : [0.85510286], prediction : [0.9188841], error(%) : [6.37812406]\n",
            "test target : [0.90493309], prediction : [0.91169465], error(%) : [0.67615539]\n",
            "test target : [0.69602563], prediction : [0.6537933], error(%) : [4.22323558]\n",
            "test target : [0.88536052], prediction : [0.895271], error(%) : [0.99104822]\n",
            "test target : [0.82065116], prediction : [0.83045447], error(%) : [0.98033121]\n",
            "test target : [0.85889761], prediction : [0.8754514], error(%) : [1.65537761]\n",
            "test target : [0.9155183], prediction : [0.91974944], error(%) : [0.42311351]\n",
            "test target : [0.91981234], prediction : [0.9380527], error(%) : [1.82403737]\n",
            "test target : [0.86189344], prediction : [0.87490344], error(%) : [1.30100049]\n",
            "test target : [0.8947474], prediction : [0.89395237], error(%) : [0.07950327]\n",
            "test target : [0.71479928], prediction : [0.74153113], error(%) : [2.67318577]\n",
            "test target : [0.90053929], prediction : [0.8927775], error(%) : [0.77617903]\n",
            "test target : [0.85490321], prediction : [0.8661076], error(%) : [1.12043682]\n",
            "test target : [0.71679653], prediction : [0.7664665], error(%) : [4.96699659]\n",
            "test target : [0.89195134], prediction : [0.9012401], error(%) : [0.92887727]\n",
            "test target : [0.81535855], prediction : [0.87853646], error(%) : [6.31779119]\n",
            "test target : [0.90782904], prediction : [0.89864016], error(%) : [0.91888814]\n",
            "test target : [0.9058319], prediction : [0.9072351], error(%) : [0.14031873]\n",
            "test target : [0.80577191], prediction : [0.8349469], error(%) : [2.91749623]\n",
            "test target : [0.92121031], prediction : [0.9386629], error(%) : [1.7452573]\n",
            "test target : [0.74016386], prediction : [0.71895003], error(%) : [2.12138221]\n",
            "test target : [0.90163774], prediction : [0.90183485], error(%) : [0.01971029]\n",
            "test target : [0.90703023], prediction : [0.9048066], error(%) : [0.22236151]\n",
            "test target : [0.92091079], prediction : [0.9308617], error(%) : [0.99509215]\n",
            "test target : [0.84801287], prediction : [0.8604916], error(%) : [1.24786992]\n",
            "test target : [0.94417823], prediction : [0.93020463], error(%) : [1.39736025]\n",
            "test target : [0.76972247], prediction : [0.7804829], error(%) : [1.07604154]\n",
            "test target : [0.82035152], prediction : [0.84344506], error(%) : [2.30935475]\n",
            "test target : [0.84761335], prediction : [0.8762069], error(%) : [2.85935223]\n",
            "test target : [0.69902146], prediction : [0.7975086], error(%) : [9.84871404]\n",
            "test target : [0.71619737], prediction : [0.7911979], error(%) : [7.50005287]\n",
            "test target : [0.91931306], prediction : [0.9369527], error(%) : [1.76396546]\n",
            "test target : [0.92760137], prediction : [0.9422994], error(%) : [1.46980564]\n",
            "test target : [0.89734371], prediction : [0.90296125], error(%) : [0.5617548]\n",
            "test target : [0.7270821], prediction : [0.64660233], error(%) : [8.047977]\n",
            "test target : [0.90343524], prediction : [0.9042819], error(%) : [0.08466761]\n",
            "test target : [0.91152391], prediction : [0.9167986], error(%) : [0.5274683]\n",
            "test target : [0.84012385], prediction : [0.84868294], error(%) : [0.85590929]\n",
            "test target : [0.79438789], prediction : [0.8563644], error(%) : [6.19765404]\n",
            "test target : [0.94048336], prediction : [0.91749126], error(%) : [2.29921038]\n",
            "test target : [0.84911132], prediction : [0.90410036], error(%) : [5.49890341]\n",
            "test target : [0.90503297], prediction : [0.9028213], error(%) : [0.22116701]\n",
            "test target : [0.91222295], prediction : [0.92270315], error(%) : [1.04801926]\n",
            "test target : [0.88046744], prediction : [0.8792187], error(%) : [0.12487397]\n",
            "test target : [0.87167972], prediction : [0.88843036], error(%) : [1.67506359]\n",
            "test target : [0.9151189], prediction : [0.92517126], error(%) : [1.00523565]\n",
            "test target : [0.92240864], prediction : [0.93836987], error(%) : [1.59612263]\n",
            "test target : [0.90543249], prediction : [0.9132583], error(%) : [0.78258196]\n",
            "test target : [0.9031356], prediction : [0.9051645], error(%) : [0.20288837]\n",
            "test target : [0.53035753], prediction : [0.46598116], error(%) : [6.43763733]\n",
            "test target : [0.87717197], prediction : [0.8858818], error(%) : [0.87098116]\n",
            "test target : [0.68354304], prediction : [0.69508916], error(%) : [1.15461178]\n",
            "test target : [0.89055325], prediction : [0.89661795], error(%) : [0.60647027]\n",
            "test target : [0.86009594], prediction : [0.8717768], error(%) : [1.16808791]\n",
            "test target : [0.92160984], prediction : [0.93573], error(%) : [1.41201448]\n",
            "test target : [0.92530459], prediction : [0.9401518], error(%) : [1.48472215]\n",
            "test target : [0.90912725], prediction : [0.9375111], error(%) : [2.83838384]\n",
            "test target : [0.56460959], prediction : [0.6034166], error(%) : [3.88070346]\n",
            "test target : [0.79358896], prediction : [0.83714294], error(%) : [4.35539819]\n",
            "test target : [0.79768324], prediction : [0.8028214], error(%) : [0.51381599]\n",
            "test target : [0.86798485], prediction : [0.89487386], error(%) : [2.68890079]\n",
            "test target : [0.84981025], prediction : [0.85938746], error(%) : [0.95772053]\n",
            "test target : [0.65917715], prediction : [0.71272707], error(%) : [5.35499184]\n",
            "test target : [0.90193738], prediction : [0.9030093], error(%) : [0.10719113]\n",
            "test target : [0.86858401], prediction : [0.8888885], error(%) : [2.03044635]\n",
            "test target : [0.81056523], prediction : [0.8356525], error(%) : [2.50872409]\n",
            "test target : [0.83043744], prediction : [0.8556482], error(%) : [2.52107774]\n",
            "test target : [0.91671663], prediction : [0.9303531], error(%) : [1.36364711]\n",
            "test target : [0.81855402], prediction : [0.84214103], error(%) : [2.35870125]\n",
            "test target : [0.66047536], prediction : [0.6824534], error(%) : [2.19780317]\n",
            "test target : [0.55841823], prediction : [0.6097103], error(%) : [5.12920426]\n",
            "test target : [0.87187936], prediction : [0.88776046], error(%) : [1.58810962]\n",
            "test target : [0.90663071], prediction : [0.91454685], error(%) : [0.79161404]\n",
            "test target : [0.84901144], prediction : [0.879226], error(%) : [3.02145851]\n",
            "test target : [0.9018375], prediction : [0.8990586], error(%) : [0.27789233]\n",
            "test target : [0.92021174], prediction : [0.9271641], error(%) : [0.69523334]\n",
            "test target : [0.88116636], prediction : [0.8897722], error(%) : [0.86058117]\n",
            "test target : [0.92640304], prediction : [0.9246098], error(%) : [0.17932586]\n",
            "test target : [0.91072498], prediction : [0.9184598], error(%) : [0.77347906]\n",
            "test target : [0.79888157], prediction : [0.79596317], error(%) : [0.29184001]\n",
            "test target : [0.88585981], prediction : [0.89333695], error(%) : [0.74771461]\n",
            "test target : [0.9084282], prediction : [0.91796374], error(%) : [0.95355408]\n",
            "test target : [0.86009594], prediction : [0.89267164], error(%) : [3.25757046]\n",
            "test target : [0.87098068], prediction : [0.87326205], error(%) : [0.22813724]\n",
            "test target : [0.96444985], prediction : [0.93584585], error(%) : [2.86039971]\n",
            "test target : [0.91262236], prediction : [0.9218843], error(%) : [0.92619399]\n",
            "test target : [0.8832635], prediction : [0.9158313], error(%) : [3.25678255]\n",
            "test target : [0.8761734], prediction : [0.881126], error(%) : [0.49525859]\n",
            "test target : [0.8712802], prediction : [0.89415693], error(%) : [2.28767338]\n",
            "test target : [0.79688443], prediction : [0.79153264], error(%) : [0.5351794]\n",
            "test target : [0.80826845], prediction : [0.8207543], error(%) : [1.24858401]\n",
            "test target : [0.87407638], prediction : [0.89948654], error(%) : [2.54101601]\n",
            "test target : [0.8894548], prediction : [0.89954716], error(%) : [1.00923631]\n",
            "test target : [0.86379069], prediction : [0.87281936], error(%) : [0.90286705]\n",
            "test target : [0.90073905], prediction : [0.8963717], error(%) : [0.43673317]\n",
            "test target : [0.90862796], prediction : [0.9224368], error(%) : [1.38088104]\n",
            "test target : [0.93279409], prediction : [0.94474113], error(%) : [1.19470353]\n",
            "test target : [0.86039546], prediction : [0.88614124], error(%) : [2.57457769]\n",
            "test target : [0.85160775], prediction : [0.88168436], error(%) : [3.00766154]\n",
            "test target : [0.88705814], prediction : [0.89576995], error(%) : [0.87118178]\n",
            "test target : [0.73287399], prediction : [0.6550636], error(%) : [7.78103639]\n",
            "test target : [0.91591771], prediction : [0.937525], error(%) : [2.16072666]\n",
            "test target : [0.7518474], prediction : [0.831289], error(%) : [7.94415945]\n",
            "test target : [0.92081091], prediction : [0.9301809], error(%) : [0.93699978]\n",
            "test target : [0.922209], prediction : [0.9364269], error(%) : [1.42178772]\n",
            "test target : [0.92091079], prediction : [0.93883264], error(%) : [1.79218507]\n",
            "test target : [0.75314561], prediction : [0.70903313], error(%) : [4.41124781]\n",
            "test target : [0.82075104], prediction : [0.89391375], error(%) : [7.31627087]\n",
            "test target : [0.64020375], prediction : [0.67051154], error(%) : [3.03077981]\n",
            "test target : [0.5386459], prediction : [0.5912215], error(%) : [5.25756099]\n",
            "test target : [0.83742766], prediction : [0.86500907], error(%) : [2.75814065]\n",
            "test target : [0.90593178], prediction : [0.8967968], error(%) : [0.91349566]\n",
            "test target : [0.92810065], prediction : [0.9242854], error(%) : [0.3815242]\n",
            "test target : [0.7889954], prediction : [0.7816688], error(%) : [0.732662]\n",
            "test target : [0.88026768], prediction : [0.91044354], error(%) : [3.01758682]\n",
            "test target : [0.79968049], prediction : [0.858948], error(%) : [5.92674979]\n",
            "test target : [0.92141007], prediction : [0.9393405], error(%) : [1.79303976]\n",
            "test target : [0.74286004], prediction : [0.80974275], error(%) : [6.68827092]\n",
            "test target : [0.71180357], prediction : [0.68425333], error(%) : [2.75502332]\n",
            "test target : [0.88905539], prediction : [0.8933854], error(%) : [0.43300179]\n",
            "test target : [0.90323548], prediction : [0.8918808], error(%) : [1.13546668]\n",
            "test target : [0.92830041], prediction : [0.9484049], error(%) : [2.01044933]\n",
            "test target : [0.79948073], prediction : [0.828285], error(%) : [2.88042455]\n",
            "test target : [0.33712807], prediction : [0.49758086], error(%) : [16.04527905]\n",
            "test target : [0.73097662], prediction : [0.6246913], error(%) : [10.62853095]\n",
            "test target : [0.90453369], prediction : [0.9045284], error(%) : [0.00053084]\n",
            "test target : [0.91392057], prediction : [0.9167503], error(%) : [0.28297426]\n",
            "test target : [0.91092474], prediction : [0.9213468], error(%) : [1.04220402]\n",
            "test target : [0.89235074], prediction : [0.92065066], error(%) : [2.82999192]\n",
            "test target : [0.88905539], prediction : [0.90522164], error(%) : [1.61662486]\n",
            "test target : [0.78330339], prediction : [0.7600008], error(%) : [2.33025673]\n",
            "test target : [0.93389254], prediction : [0.9290023], error(%) : [0.48902593]\n",
            "test target : [0.86419022], prediction : [0.87631285], error(%) : [1.21226363]\n",
            "test target : [0.90073905], prediction : [0.9031751], error(%) : [0.24360617]\n",
            "test target : [0.87787102], prediction : [0.9005695], error(%) : [2.26984829]\n",
            "test target : [0.78729779], prediction : [0.7051567], error(%) : [8.21411034]\n",
            "test target : [0.75444382], prediction : [0.7400542], error(%) : [1.43896303]\n",
            "test target : [0.92071103], prediction : [0.9324521], error(%) : [1.17410537]\n",
            "test target : [0.83582993], prediction : [0.9022306], error(%) : [6.64006919]\n",
            "test target : [0.88905539], prediction : [0.8693062], error(%) : [1.97491857]\n",
            "test target : [0.82474543], prediction : [0.8521972], error(%) : [2.74517381]\n",
            "test target : [0.74825253], prediction : [0.7321557], error(%) : [1.60968452]\n",
            "test target : [0.34311966], prediction : [0.43278345], error(%) : [8.96637965]\n",
            "test target : [0.92949875], prediction : [0.94219863], error(%) : [1.2699889]\n",
            "test target : [0.91202319], prediction : [0.9216489], error(%) : [0.96257263]\n",
            "test target : [0.79119242], prediction : [0.834929], error(%) : [4.37365697]\n",
            "test target : [0.92830041], prediction : [0.9257047], error(%) : [0.25956972]\n",
            "test target : [0.92470542], prediction : [0.9377472], error(%) : [1.30417565]\n",
            "test target : [0.76992212], prediction : [0.77085894], error(%) : [0.09368276]\n",
            "test target : [0.92111043], prediction : [0.9425096], error(%) : [2.13991585]\n",
            "test target : [0.85949677], prediction : [0.87169576], error(%) : [1.21989819]\n",
            "test target : [0.93309374], prediction : [0.9464042], error(%) : [1.33104826]\n",
            "test target : [0.79259039], prediction : [0.8222524], error(%) : [2.96619997]\n",
            "test target : [0.96574794], prediction : [0.9342862], error(%) : [3.14617647]\n",
            "test target : [0.89005396], prediction : [0.894302], error(%) : [0.42480488]\n",
            "test target : [0.94717406], prediction : [0.93376756], error(%) : [1.3406501]\n",
            "test target : [0.94467752], prediction : [0.91590154], error(%) : [2.87759753]\n",
            "test target : [0.92101067], prediction : [0.9313965], error(%) : [1.03858139]\n",
            "test target : [0.91891353], prediction : [0.9300968], error(%) : [1.11832716]\n",
            "test target : [0.93868587], prediction : [0.9077233], error(%) : [3.09625579]\n",
            "test target : [0.82644305], prediction : [0.82730275], error(%) : [0.08597068]\n",
            "test target : [0.54563612], prediction : [0.6235737], error(%) : [7.79375982]\n",
            "test target : [0.90073905], prediction : [0.86416256], error(%) : [3.65764896]\n",
            "test target : [0.9147195], prediction : [0.9237433], error(%) : [0.90238122]\n",
            "test target : [0.92999803], prediction : [0.94233775], error(%) : [1.23397216]\n",
            "test target : [0.77851007], prediction : [0.7946466], error(%) : [1.61365497]\n",
            "test target : [0.912922], prediction : [0.92490834], error(%) : [1.198634]\n",
            "test target : [0.89265038], prediction : [0.86336684], error(%) : [2.92835411]\n",
            "test target : [0.76093476], prediction : [0.7811447], error(%) : [2.0209922]\n",
            "test target : [0.79279015], prediction : [0.8423599], error(%) : [4.95697463]\n",
            "test target : [0.74266028], prediction : [0.7810403], error(%) : [3.83800323]\n",
            "test target : [1.], prediction : [0.9339336], error(%) : [6.60663843]\n",
            "test target : [0.92240864], prediction : [0.9467724], error(%) : [2.43637527]\n",
            "test target : [0.86349117], prediction : [0.8698224], error(%) : [0.63312129]\n",
            "test target : [0.84991013], prediction : [0.8646629], error(%) : [1.47527531]\n",
            "test target : [0.98462159], prediction : [0.9403731], error(%) : [4.42484624]\n",
            "test target : [0.90902737], prediction : [0.9259265], error(%) : [1.6899139]\n",
            "test target : [0.89954072], prediction : [0.8844845], error(%) : [1.50561941]\n",
            "test target : [0.85590178], prediction : [0.8711468], error(%) : [1.5245014]\n",
            "test target : [0.9031356], prediction : [0.90464103], error(%) : [0.15054357]\n",
            "test target : [0.73357304], prediction : [0.8413278], error(%) : [10.77547477]\n",
            "test target : [0.88316362], prediction : [0.90574336], error(%) : [2.25797391]\n",
            "test target : [0.83882564], prediction : [0.86116433], error(%) : [2.23386951]\n",
            "test target : [0.82105056], prediction : [0.8990532], error(%) : [7.80026552]\n",
            "test target : [0.90992617], prediction : [0.9181179], error(%) : [0.81917066]\n",
            "test target : [0.96744567], prediction : [0.9249526], error(%) : [4.24930485]\n",
            "test target : [0.87387662], prediction : [0.87865305], error(%) : [0.47764288]\n",
            "test target : [0.90703023], prediction : [0.8991649], error(%) : [0.78653139]\n",
            "test target : [0.44987023], prediction : [0.59417963], error(%) : [14.43094027]\n",
            "test target : [0.87777114], prediction : [0.8736694], error(%) : [0.41017492]\n",
            "test target : [0.49890154], prediction : [0.48380575], error(%) : [1.50957896]\n",
            "test target : [0.55911728], prediction : [0.5246022], error(%) : [3.45151044]\n",
            "test target : [0.84092277], prediction : [0.9164659], error(%) : [7.55431052]\n",
            "test target : [0.79049337], prediction : [0.8261206], error(%) : [3.56272409]\n",
            "test target : [0.88895551], prediction : [0.8952651], error(%) : [0.63095905]\n",
            "test target : [0.90033953], prediction : [0.9044795], error(%) : [0.41399718]\n",
            "test target : [0.90063917], prediction : [0.9180988], error(%) : [1.7459634]\n",
            "test target : [0.48492115], prediction : [0.43533823], error(%) : [4.9582924]\n",
            "test target : [0.8965449], prediction : [0.9165126], error(%) : [1.99677107]\n",
            "test target : [0.95995605], prediction : [0.93885773], error(%) : [2.10983172]\n",
            "test target : [0.57589379], prediction : [0.5886481], error(%) : [1.27542954]\n",
            "test target : [0.79968049], prediction : [0.81781626], error(%) : [1.81357631]\n",
            "test target : [0.92630328], prediction : [0.9232813], error(%) : [0.30219643]\n",
            "test target : [0.80357501], prediction : [0.8470237], error(%) : [4.34487166]\n",
            "test target : [0.62692235], prediction : [0.58823377], error(%) : [3.86885808]\n",
            "test target : [0.91921318], prediction : [0.91942585], error(%) : [0.02126701]\n",
            "test target : [0.9173158], prediction : [0.928165], error(%) : [1.08492195]\n",
            "test target : [0.90373476], prediction : [0.8941048], error(%) : [0.96299829]\n",
            "test target : [0.86838425], prediction : [0.900067], error(%) : [3.1682718]\n",
            "test target : [0.85060918], prediction : [0.8942199], error(%) : [4.36106971]\n",
            "test target : [0.9262034], prediction : [0.9223571], error(%) : [0.38463134]\n",
            "test target : [0.79768324], prediction : [0.75298816], error(%) : [4.46950783]\n",
            "test target : [0.93339326], prediction : [0.9294666], error(%) : [0.39266545]\n",
            "test target : [0.82514484], prediction : [0.8443203], error(%) : [1.91754609]\n",
            "test target : [0.96954269], prediction : [0.93819267], error(%) : [3.13500282]\n",
            "test target : [0.87966851], prediction : [0.8942835], error(%) : [1.46149624]\n",
            "test target : [0.67295795], prediction : [0.71703535], error(%) : [4.40774033]\n",
            "test target : [0.84511693], prediction : [0.9109971], error(%) : [6.58801634]\n",
            "test target : [0.79598562], prediction : [0.8239691], error(%) : [2.79835028]\n",
            "test target : [0.86978235], prediction : [0.8713033], error(%) : [0.15209748]\n",
            "test target : [0.88885563], prediction : [0.8999811], error(%) : [1.11254501]\n",
            "test target : [0.8712802], prediction : [0.8906379], error(%) : [1.93576756]\n",
            "test target : [0.59896147], prediction : [0.557753], error(%) : [4.12084402]\n",
            "test target : [0.85530262], prediction : [0.89445674], error(%) : [3.91541253]\n",
            "test target : [0.85929701], prediction : [0.88105106], error(%) : [2.17540496]\n",
            "test target : [0.85789904], prediction : [0.90643823], error(%) : [4.85391909]\n",
            "test target : [0.87527471], prediction : [0.89782494], error(%) : [2.25502311]\n",
            "test target : [0.78210506], prediction : [0.7805261], error(%) : [0.15789604]\n",
            "test target : [0.9018375], prediction : [0.90072274], error(%) : [0.11147616]\n",
            "test target : [0.9080288], prediction : [0.91754895], error(%) : [0.95201562]\n",
            "test target : [0.92250852], prediction : [0.91812694], error(%) : [0.43815837]\n",
            "test target : [0.9270022], prediction : [0.9187569], error(%) : [0.82453018]\n",
            "test target : [0.80956666], prediction : [0.840594], error(%) : [3.10273333]\n",
            "test target : [0.81585784], prediction : [0.83101296], error(%) : [1.51551288]\n",
            "test target : [0.81176356], prediction : [0.8140917], error(%) : [0.23281225]\n",
            "test target : [0.80557227], prediction : [0.83348924], error(%) : [2.79169739]\n",
            "test target : [0.65028967], prediction : [0.638301], error(%) : [1.19886577]\n",
            "test target : [0.87547436], prediction : [0.89794374], error(%) : [2.24693801]\n",
            "test target : [0.89684442], prediction : [0.9134334], error(%) : [1.65889515]\n",
            "test target : [0.92330745], prediction : [0.94031596], error(%) : [1.70085113]\n",
            "test target : [0.88116636], prediction : [0.8846526], error(%) : [0.34862496]\n",
            "test target : [0.6970242], prediction : [0.7162394], error(%) : [1.92151923]\n",
            "test target : [0.9195127], prediction : [0.9147614], error(%) : [0.47512746]\n",
            "test target : [0.79328944], prediction : [0.8007292], error(%) : [0.74397764]\n",
            "test target : [0.88755742], prediction : [0.912596], error(%) : [2.50385667]\n",
            "test target : [0.9244059], prediction : [0.94619906], error(%) : [2.17931591]\n",
            "test target : [0.93648897], prediction : [0.93156195], error(%) : [0.49270191]\n",
            "test target : [0.90663071], prediction : [0.90643984], error(%) : [0.01908661]\n",
            "test target : [0.70251657], prediction : [0.7530173], error(%) : [5.0500739]\n",
            "test target : [0.90173762], prediction : [0.90796125], error(%) : [0.62236262]\n",
            "test target : [0.90902737], prediction : [0.9083792], error(%) : [0.06481704]\n",
            "test target : [0.69612551], prediction : [0.73098934], error(%) : [3.48638253]\n",
            "test target : [0.93648897], prediction : [0.9412364], error(%) : [0.47474108]\n",
            "test target : [0.78849612], prediction : [0.82241136], error(%) : [3.39152408]\n",
            "test target : [0.76403034], prediction : [0.6383028], error(%) : [12.57275419]\n",
            "test target : [0.94667466], prediction : [0.92941236], error(%) : [1.72622906]\n",
            "test target : [0.78829647], prediction : [0.84555864], error(%) : [5.72621689]\n",
            "test target : [0.87966851], prediction : [0.905171], error(%) : [2.5502466]\n",
            "average error over 1000 predictions :  0.46937216667835535\n",
            "maximum error over 1000 predictions :  0.2893942594528198\n",
            "minimum error over 1000 predictions :  -0.18214522591901683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBb2ISMQp6EVEIu7/AVmoDxwuybqKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vila-dCkkNS0",
        "colab_type": "code",
        "outputId": "e359697d-d9b2-485d-e88c-f316b6c33023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "err = abs(res - test_Y)\n",
        "# print(err)\n",
        "# print(np.maximum(err))\n",
        "# print(np.minimum(err))\n",
        "\n",
        "print(\"max %: \", np.max(err) * 100)\n",
        "print(\"min %: \",  np.min(err) * 100)\n",
        "\n",
        "print(\"avg error : \", (np.sum(err)/1000)*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max %:  28.939425945281982\n",
            "min %:  0.0005308431500705701\n",
            "avg error :  1.2785721668780932\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}