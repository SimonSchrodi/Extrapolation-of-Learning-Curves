{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "Inter_Conditional_Univariate_LSTM.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJd_VxDKN8pn",
        "colab_type": "text"
      },
      "source": [
        "# Task A: Creating a Performance Predictor\n",
        "\n",
        "In this task, you will use training data from 2000 configurations on a single OpenML dataset to train a performance predictor. The data will be splitted into train, test and validation set and we will only use the first 10 epochs of the learning curves for predicitons. You are provided with the full benchmark logs for Fashion-MNIST, that is learning curves, config parameters and gradient statistics, and you can use them freely.\n",
        "\n",
        "For questions, you can contact zimmerl@informatik.uni-freiburg.\n",
        "\n",
        "__Note: Please use the dataloading and splits you are provided with in this notebook.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqPBWpd5N8pt",
        "colab_type": "text"
      },
      "source": [
        "## Specifications:\n",
        "\n",
        "* Data: fashion_mnist.json\n",
        "* Number of datasets: 1\n",
        "* Number of configurations: 2000\n",
        "* Number of epochs seed during prediction: 10\n",
        "* Available data: Learning curves, architecture parameters and hyperparameters, gradient statistics \n",
        "* Target: Final validation accuracy\n",
        "* Evaluation metric: MSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eYlSG9BN8py",
        "colab_type": "text"
      },
      "source": [
        "## Importing and splitting data\n",
        "\n",
        "__Note__: There are 51 steps logged, 50 epochs plus the 0th epoch, prior to any weight updates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT8tEKUJOqNy",
        "colab_type": "code",
        "outputId": "825772e9-7a48-43c8-9fac-b056e0acc571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "!pip install wget\n",
        "!pip install zipfile36"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=51c083105d35062aa9806b0bc5e838fd88c6a7f81abbcca9b252294e9e71edec\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting zipfile36\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/8a/3b7da0b0bd87d1ef05b74207827c72d348b56a0d6d83242582be18a81e02/zipfile36-0.1.3-py3-none-any.whl\n",
            "Installing collected packages: zipfile36\n",
            "Successfully installed zipfile36-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GSY73FpOPdD",
        "colab_type": "code",
        "outputId": "dded4298-b599-4e86-f676-ab9adbaf498c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import wget\n",
        "import zipfile\n",
        "dir_path = 'content/'\n",
        "filename=wget.download('https://ndownloader.figshare.com/files/21001311')\n",
        "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"\")\n",
        "!rm fashion_mnist.zip\n",
        "wget.download('https://raw.githubusercontent.com/automl/LCBench/master/api.py')\n",
        "wget.download('https://raw.githubusercontent.com/infomon/Extrapolation-of-Learning-Curves/master/utils.py')\n",
        "!mkdir content/models\n",
        "!mkdir models"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘content/models’: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql1OqCfgN8p4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "%cd ..\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from content.api import Benchmark\n",
        "import content.utils as utils\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2sQv_05N8qJ",
        "colab_type": "code",
        "outputId": "5759b300-86d0-4047-e850-b2a50c7b4353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "bench_dir = dir_path+\"fashion_mnist.json\"\n",
        "bench = Benchmark(bench_dir, cache=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Loading data...\n",
            "==> No cached data found or cache set to False.\n",
            "==> Reading json data...\n",
            "==> Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eq4DxuuN8qe",
        "colab_type": "code",
        "outputId": "85117980-f7f6-4b41-8bb0-76b75c077083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Read data\n",
        "def cut_data(data, cut_position):\n",
        "    targets = []\n",
        "    for dp in data:\n",
        "        targets.append(dp[\"Train/val_accuracy\"][50])\n",
        "        for tag in dp:\n",
        "            if tag.startswith(\"Train/\"):\n",
        "                dp[tag] = dp[tag][0:cut_position]\n",
        "    return data, targets\n",
        "\n",
        "def read_data():\n",
        "    dataset_name = 'Fashion-MNIST'\n",
        "    n_configs = bench.get_number_of_configs(dataset_name)\n",
        "    \n",
        "    # Query API\n",
        "    data = []\n",
        "    for config_id in range(n_configs):\n",
        "        data_point = dict()\n",
        "        data_point[\"config\"] = bench.query(dataset_name=dataset_name, tag=\"config\", config_id=config_id)\n",
        "        for tag in bench.get_queriable_tags(dataset_name=dataset_name, config_id=config_id):\n",
        "            if tag.startswith(\"Train/\"):\n",
        "                data_point[tag] = bench.query(dataset_name=dataset_name, tag=tag, config_id=config_id)    \n",
        "        data.append(data_point)\n",
        "        \n",
        "    # Split: 50% train, 25% validation, 25% test (the data is already shuffled)\n",
        "    indices = np.arange(n_configs)\n",
        "    ind_train = indices[0:int(np.floor(0.5*n_configs))]\n",
        "    ind_val = indices[int(np.floor(0.5*n_configs)):int(np.floor(0.75*n_configs))]\n",
        "    ind_test = indices[int(np.floor(0.75*n_configs)):]\n",
        "\n",
        "    array_data = np.array(data)\n",
        "    train_data = array_data[ind_train]\n",
        "    val_data = array_data[ind_val]\n",
        "    test_data = array_data[ind_test]\n",
        "    \n",
        "    # Cut curves for validation and test\n",
        "    cut_position = 11\n",
        "    val_data, val_targets = cut_data(val_data, cut_position)\n",
        "    test_data, test_targets = cut_data(test_data, cut_position)\n",
        "    train_data, train_targets = cut_data(train_data, 51)   # Cut last value as it is repeated\n",
        "    \n",
        "    return train_data, val_data, test_data, train_targets, val_targets, test_targets\n",
        "    \n",
        "train_data, val_data, test_data, train_targets, val_targets, test_targets = read_data()\n",
        "\n",
        "print(\"Train:\", len(train_data))\n",
        "print(\"Validation:\", len(val_data))\n",
        "print(\"Test:\", len(test_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 1000\n",
            "Validation: 500\n",
            "Test: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PphdZ2aN8qv",
        "colab_type": "text"
      },
      "source": [
        "The data contains the configuration of the trained model and learning curves as well as global and layer-wise gradient statistics.\n",
        "\n",
        "__Note__: Not all parameters vary across different configurations. The varying parameters are batch_size, max_dropout, max_units, num_layers, learning_rate, momentum, weight_decay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybnns3VKN8q0",
        "colab_type": "code",
        "outputId": "86f51f93-031b-4cfb-cbd8-442990437179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Config\n",
        "print(\"Config example:\", train_data[0][\"config\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Config example: {'batch_size': 71, 'imputation_strategy': 'mean', 'learning_rate_scheduler': 'cosine_annealing', 'loss': 'cross_entropy_weighted', 'network': 'shapedmlpnet', 'max_dropout': 0.025926231827891333, 'normalization_strategy': 'standardize', 'optimizer': 'sgd', 'cosine_annealing_T_max': 50, 'cosine_annealing_eta_min': 1e-08, 'activation': 'relu', 'max_units': 293, 'mlp_shape': 'funnel', 'num_layers': 3, 'learning_rate': 0.0018243300267253295, 'momentum': 0.21325193168301043, 'weight_decay': 0.020472816917443872}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua6h3ORrlA5N",
        "colab_type": "code",
        "outputId": "075233fe-b32f-4ea1-ba22-3c9a5dcc284b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "train_data[1][\"config\"]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu',\n",
              " 'batch_size': 457,\n",
              " 'cosine_annealing_T_max': 50,\n",
              " 'cosine_annealing_eta_min': 1e-08,\n",
              " 'imputation_strategy': 'mean',\n",
              " 'learning_rate': 0.01239328605026128,\n",
              " 'learning_rate_scheduler': 'cosine_annealing',\n",
              " 'loss': 'cross_entropy_weighted',\n",
              " 'max_dropout': 0.5472322491757223,\n",
              " 'max_units': 950,\n",
              " 'mlp_shape': 'funnel',\n",
              " 'momentum': 0.16411425552061212,\n",
              " 'network': 'shapedmlpnet',\n",
              " 'normalization_strategy': 'standardize',\n",
              " 'num_layers': 4,\n",
              " 'optimizer': 'sgd',\n",
              " 'weight_decay': 0.09762768273307641}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTl67TOTN8rD",
        "colab_type": "code",
        "outputId": "185ff5e6-6e57-468f-a7f9-ab1b7bd55702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "# Learning curve\n",
        "plt.plot(train_data[10][\"Train/val_accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc795fe4a58>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXGuPeBKN8rM",
        "colab_type": "code",
        "outputId": "7cdf601d-43b6-4cc8-eca2-371858747561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "# Gradient statistics\n",
        "plt.plot(train_data[10][\"Train/layer_wise_gradient_mean_layer_0\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc795b30e80>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAD4CAYAAAA6j0u4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7v8DbRIJDShuWn8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaYC_PH4N8rQ",
        "colab_type": "text"
      },
      "source": [
        "## A simple baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJrvEgLzN8rS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleLearningCurvePredictor():\n",
        "    \"\"\"A learning curve predictor that predicts the last observed epoch of the validation accuracy as final performance\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        pass\n",
        "    \n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for datapoint in X:\n",
        "            predictions.append(datapoint[\"Train/val_accuracy\"][-1])\n",
        "        return predictions\n",
        "    \n",
        "def score(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d23UZhgHN8rW",
        "colab_type": "code",
        "outputId": "6312fb03-c601-4582-9c3e-3d533d64919f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Training & tuning\n",
        "predictor = SimpleLearningCurvePredictor()\n",
        "predictor.fit(train_data, train_targets)\n",
        "preds = predictor.predict(val_data)\n",
        "mse = score(val_targets, preds)\n",
        "print(\"Score on validation set:\", mse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score on validation set: 31.921338670622784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMWiY0XbN8rc",
        "colab_type": "code",
        "outputId": "801c9724-ef88-4baa-9e51-85160227668d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Final evaluation (after tuning)\n",
        "final_preds = predictor.predict(test_data)\n",
        "final_score = score(test_targets, final_preds)\n",
        "print(\"Final test score:\", final_score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final test score: 24.199496266785523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8rs7p0Jyqog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = utils.check_cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnEQJbbLyGG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_loader = utils.prep_data(train_data, train_targets, batch_size=32,normalization_factor_temporal_data=[100])\n",
        "val_data_loader = utils.prep_data(val_data, val_targets, batch_size=32,normalization_factor_temporal_data=[100])\n",
        "test_data_loader = utils.prep_data(test_data, test_targets, batch_size=32,normalization_factor_temporal_data=[100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOrgWvjIaU8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InterCondUniLSTM(torch.nn.Module):\n",
        "    \"\"\"An inter-conditional univariate LSTM that predicts from a single input sequence the final validation accuracy\"\"\"\n",
        "    \n",
        "    def __init__(self,input_size, hidden_size, output_size, \n",
        "                 num_layers=1, lstm_dropout=0, bidirectional=False,fc_dropout=0):\n",
        "        super(InterCondUniLSTM,self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm_dropout = lstm_dropout\n",
        "        self.bidirectional = bidirectional\n",
        "        self.fc_dropout = fc_dropout\n",
        "\n",
        "        self.relu = torch.nn.functional.relu\n",
        "\n",
        "        self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
        "                            dropout=lstm_dropout, bidirectional=bidirectional)\n",
        "        \n",
        "        self.fc = torch.nn.Linear(self.hidden_size*10*2 if self.bidirectional else hidden_size*10,output_size)\n",
        "\n",
        "    def forward(self,x):\n",
        "        seq, config = x\n",
        "        batch_size = seq.size(0)\n",
        "\n",
        "        h0 = torch.zeros(self.num_layers*2 if self.bidirectional else self.num_layers, config.size()[0], self.hidden_size)\n",
        "        c0 = torch.zeros(self.num_layers*2 if self.bidirectional else self.num_layers, config.size()[0], self.hidden_size)\n",
        "        seq = torch.transpose(seq,1,0)\n",
        "        seq = seq.unsqueeze(-1)\n",
        "\n",
        "        inter_seq = []\n",
        "        for i in range(10):\n",
        "          inter_seq.append(torch.cat((seq[i],config),-1))\n",
        "        inter_seq = torch.stack(inter_seq)\n",
        "\n",
        "        lstm_out, _ = self.lstm(inter_seq,(h0,c0))\n",
        "        lstm_out = lstm_out.permute(1,0,2)\n",
        "        lstm_out = lstm_out.contiguous().view(batch_size,-1)\n",
        "      \n",
        "        forecast = self.fc(lstm_out)\n",
        "        return forecast"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqyIN3cLHlwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, criterion, clip=5):\n",
        "    model.train()\n",
        "    epoch_loss = []\n",
        "    for val_acc, configs, targets in train_data_loader:\n",
        "      optimizer.zero_grad()\n",
        "      output = model([val_acc,configs])\n",
        "      loss = criterion(output, targets)\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
        "      optimizer.step()\n",
        "      epoch_loss.append(loss.item())\n",
        "    return np.array(epoch_loss).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lom0qSLfHh-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = []\n",
        "  with torch.no_grad():\n",
        "    for val_acc, configs, targets in val_data_loader:\n",
        "      output = model([val_acc, configs])\n",
        "      loss = criterion(output, targets)\n",
        "      epoch_loss.append(loss.item())\n",
        "  return np.array(epoch_loss).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OFaN6Rx1Njn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, criterion):\n",
        "    #model.load_state_dict(torch.load('content/models/model.pt'))\n",
        "    model.eval()\n",
        "    epoch_loss=[]\n",
        "    with torch.no_grad():\n",
        "      for val_acc, configs, targets in test_data_loader:\n",
        "        output = model([val_acc, configs])\n",
        "        loss = criterion(output, targets)\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "    return np.array(epoch_loss).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBuizjkSCrXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_error(model, criterion):\n",
        "    #model.load_state_dict(torch.load('content/models/model.pt'))\n",
        "    model.eval()\n",
        "    epoch_loss=[]\n",
        "    with torch.no_grad():\n",
        "      for val_acc, configs, targets in test_data_loader:\n",
        "        output = model([val_acc, configs])\n",
        "        loss = np.abs(output.detach().numpy()-targets.detach().numpy().reshape(-1,1))\n",
        "        epoch_loss += loss.tolist()\n",
        "        \n",
        "    return np.array(epoch_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqtn_wsjIJxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "      torch.nn.init.uniform_(param.data, -0.08, 0.08)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUioFDjfIIer",
        "colab_type": "code",
        "outputId": "e7f79f98-887f-40d4-d280-fd94248db6c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "input_size = 8\n",
        "outcome_dim = 1\n",
        "hidden_dim=35\n",
        "num_layers=2\n",
        "config_size = 7\n",
        "bidirectional = True\n",
        "lstm_dropout=0.5\n",
        "fc_dropout=0.0\n",
        "\n",
        "model = InterCondUniLSTM(input_size, hidden_dim, outcome_dim, num_layers,\n",
        "                      lstm_dropout=lstm_dropout,bidirectional=bidirectional,fc_dropout=fc_dropout)\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "print(pytorch_total_params)\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43261\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "InterCondUniLSTM(\n",
              "  (lstm): LSTM(8, 35, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=700, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR-Zp7gmnXU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs=200\n",
        "lr=0.01\n",
        "weight_decay = 10e-3\n",
        "T_0 = int(epochs/4)\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnjIfsKuwSs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple\n",
        "train_stats = namedtuple(\"Stats\",[\"train_loss\", \"val_loss\"])\n",
        "stats = train_stats(train_loss=np.zeros(epochs),\n",
        "                     val_loss=np.zeros(epochs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e565yxk3H7mX",
        "colab_type": "code",
        "outputId": "d218bae9-1fa4-453f-c872-87378c4233d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss = train(model, optimizer, criterion)\n",
        "  val_loss = evaluate(model, criterion)\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    torch.save(model.state_dict(),\"content/models/model_5.pt\")    \n",
        "    print('Val loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(best_val_loss,val_loss))\n",
        "    best_val_loss = val_loss\n",
        "\n",
        "  print(f'Epoch: {epoch}\\t Train Loss: {train_loss:.3f}\\t Val. Loss: {val_loss:.3f}')\n",
        "  stats.train_loss[epoch] = train_loss\n",
        "  stats.val_loss[epoch] = val_loss\n",
        "\n",
        "  scheduler.step()"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss decreased (inf --> 65.781340).  Saving model ...\n",
            "Epoch: 0\t Train Loss: 1377.565\t Val. Loss: 65.781\n",
            "Val loss decreased (65.781340 --> 62.199132).  Saving model ...\n",
            "Epoch: 1\t Train Loss: 76.590\t Val. Loss: 62.199\n",
            "Val loss decreased (62.199132 --> 62.188114).  Saving model ...\n",
            "Epoch: 2\t Train Loss: 79.774\t Val. Loss: 62.188\n",
            "Epoch: 3\t Train Loss: 79.011\t Val. Loss: 88.090\n",
            "Epoch: 4\t Train Loss: 83.941\t Val. Loss: 62.997\n",
            "Val loss decreased (62.188114 --> 62.152932).  Saving model ...\n",
            "Epoch: 5\t Train Loss: 80.615\t Val. Loss: 62.153\n",
            "Epoch: 6\t Train Loss: 80.000\t Val. Loss: 62.777\n",
            "Epoch: 7\t Train Loss: 79.289\t Val. Loss: 96.774\n",
            "Epoch: 8\t Train Loss: 79.823\t Val. Loss: 63.923\n",
            "Epoch: 9\t Train Loss: 80.042\t Val. Loss: 76.855\n",
            "Epoch: 10\t Train Loss: 79.865\t Val. Loss: 92.278\n",
            "Epoch: 11\t Train Loss: 78.971\t Val. Loss: 65.389\n",
            "Epoch: 12\t Train Loss: 79.068\t Val. Loss: 101.373\n",
            "Epoch: 13\t Train Loss: 80.002\t Val. Loss: 63.059\n",
            "Epoch: 14\t Train Loss: 78.638\t Val. Loss: 72.071\n",
            "Epoch: 15\t Train Loss: 77.601\t Val. Loss: 70.756\n",
            "Epoch: 16\t Train Loss: 78.520\t Val. Loss: 66.065\n",
            "Epoch: 17\t Train Loss: 78.479\t Val. Loss: 101.521\n",
            "Epoch: 18\t Train Loss: 84.073\t Val. Loss: 63.741\n",
            "Epoch: 19\t Train Loss: 78.602\t Val. Loss: 98.715\n",
            "Epoch: 20\t Train Loss: 79.704\t Val. Loss: 63.463\n",
            "Epoch: 21\t Train Loss: 78.569\t Val. Loss: 68.103\n",
            "Epoch: 22\t Train Loss: 77.164\t Val. Loss: 65.430\n",
            "Epoch: 23\t Train Loss: 92.944\t Val. Loss: 62.523\n",
            "Epoch: 24\t Train Loss: 79.411\t Val. Loss: 66.994\n",
            "Epoch: 25\t Train Loss: 76.939\t Val. Loss: 63.017\n",
            "Epoch: 26\t Train Loss: 82.045\t Val. Loss: 63.340\n",
            "Val loss decreased (62.152932 --> 62.144857).  Saving model ...\n",
            "Epoch: 27\t Train Loss: 80.027\t Val. Loss: 62.145\n",
            "Epoch: 28\t Train Loss: 78.616\t Val. Loss: 87.386\n",
            "Epoch: 29\t Train Loss: 79.482\t Val. Loss: 62.145\n",
            "Epoch: 30\t Train Loss: 77.658\t Val. Loss: 77.555\n",
            "Epoch: 31\t Train Loss: 77.470\t Val. Loss: 68.464\n",
            "Epoch: 32\t Train Loss: 77.144\t Val. Loss: 64.081\n",
            "Epoch: 33\t Train Loss: 80.336\t Val. Loss: 67.996\n",
            "Epoch: 34\t Train Loss: 77.797\t Val. Loss: 89.066\n",
            "Epoch: 35\t Train Loss: 79.035\t Val. Loss: 76.806\n",
            "Epoch: 36\t Train Loss: 78.104\t Val. Loss: 73.891\n",
            "Epoch: 37\t Train Loss: 77.517\t Val. Loss: 69.636\n",
            "Epoch: 38\t Train Loss: 77.906\t Val. Loss: 64.807\n",
            "Epoch: 39\t Train Loss: 76.391\t Val. Loss: 69.439\n",
            "Epoch: 40\t Train Loss: 77.238\t Val. Loss: 75.982\n",
            "Epoch: 41\t Train Loss: 77.685\t Val. Loss: 64.483\n",
            "Epoch: 42\t Train Loss: 76.311\t Val. Loss: 68.802\n",
            "Epoch: 43\t Train Loss: 76.766\t Val. Loss: 68.236\n",
            "Epoch: 44\t Train Loss: 77.451\t Val. Loss: 81.228\n",
            "Epoch: 45\t Train Loss: 79.837\t Val. Loss: 68.711\n",
            "Epoch: 46\t Train Loss: 81.219\t Val. Loss: 72.451\n",
            "Epoch: 47\t Train Loss: 77.272\t Val. Loss: 73.710\n",
            "Epoch: 48\t Train Loss: 85.076\t Val. Loss: 65.689\n",
            "Epoch: 49\t Train Loss: 78.080\t Val. Loss: 62.336\n",
            "Epoch: 50\t Train Loss: 76.595\t Val. Loss: 63.156\n",
            "Epoch: 51\t Train Loss: 77.316\t Val. Loss: 75.803\n",
            "Epoch: 52\t Train Loss: 79.119\t Val. Loss: 62.208\n",
            "Epoch: 53\t Train Loss: 76.356\t Val. Loss: 70.313\n",
            "Epoch: 54\t Train Loss: 77.192\t Val. Loss: 67.870\n",
            "Epoch: 55\t Train Loss: 76.334\t Val. Loss: 65.781\n",
            "Epoch: 56\t Train Loss: 78.916\t Val. Loss: 80.385\n",
            "Epoch: 57\t Train Loss: 81.858\t Val. Loss: 64.667\n",
            "Epoch: 58\t Train Loss: 75.713\t Val. Loss: 62.196\n",
            "Epoch: 59\t Train Loss: 76.201\t Val. Loss: 62.820\n",
            "Epoch: 60\t Train Loss: 76.482\t Val. Loss: 62.490\n",
            "Epoch: 61\t Train Loss: 76.150\t Val. Loss: 67.988\n",
            "Epoch: 62\t Train Loss: 77.947\t Val. Loss: 62.313\n",
            "Epoch: 63\t Train Loss: 76.051\t Val. Loss: 65.305\n",
            "Epoch: 64\t Train Loss: 76.613\t Val. Loss: 64.490\n",
            "Epoch: 65\t Train Loss: 78.463\t Val. Loss: 77.361\n",
            "Epoch: 66\t Train Loss: 80.735\t Val. Loss: 69.373\n",
            "Epoch: 67\t Train Loss: 77.055\t Val. Loss: 63.082\n",
            "Epoch: 68\t Train Loss: 76.922\t Val. Loss: 70.009\n",
            "Epoch: 69\t Train Loss: 76.894\t Val. Loss: 62.629\n",
            "Epoch: 70\t Train Loss: 76.257\t Val. Loss: 62.862\n",
            "Epoch: 71\t Train Loss: 75.773\t Val. Loss: 62.191\n",
            "Epoch: 72\t Train Loss: 76.547\t Val. Loss: 62.507\n",
            "Epoch: 73\t Train Loss: 76.614\t Val. Loss: 81.690\n",
            "Epoch: 74\t Train Loss: 79.432\t Val. Loss: 72.082\n",
            "Epoch: 75\t Train Loss: 76.590\t Val. Loss: 62.212\n",
            "Epoch: 76\t Train Loss: 77.486\t Val. Loss: 86.734\n",
            "Epoch: 77\t Train Loss: 81.372\t Val. Loss: 63.610\n",
            "Epoch: 78\t Train Loss: 76.003\t Val. Loss: 64.795\n",
            "Epoch: 79\t Train Loss: 75.524\t Val. Loss: 63.115\n",
            "Epoch: 80\t Train Loss: 77.733\t Val. Loss: 63.620\n",
            "Epoch: 81\t Train Loss: 76.394\t Val. Loss: 66.153\n",
            "Epoch: 82\t Train Loss: 76.748\t Val. Loss: 67.172\n",
            "Epoch: 83\t Train Loss: 76.071\t Val. Loss: 71.669\n",
            "Epoch: 84\t Train Loss: 75.946\t Val. Loss: 62.591\n",
            "Epoch: 85\t Train Loss: 76.058\t Val. Loss: 65.133\n",
            "Epoch: 86\t Train Loss: 76.245\t Val. Loss: 66.508\n",
            "Epoch: 87\t Train Loss: 75.174\t Val. Loss: 62.394\n",
            "Epoch: 88\t Train Loss: 76.533\t Val. Loss: 75.485\n",
            "Epoch: 89\t Train Loss: 76.557\t Val. Loss: 62.362\n",
            "Epoch: 90\t Train Loss: 75.816\t Val. Loss: 65.177\n",
            "Epoch: 91\t Train Loss: 75.861\t Val. Loss: 69.908\n",
            "Epoch: 92\t Train Loss: 78.917\t Val. Loss: 64.750\n",
            "Epoch: 93\t Train Loss: 75.441\t Val. Loss: 68.665\n",
            "Epoch: 94\t Train Loss: 75.391\t Val. Loss: 63.115\n",
            "Epoch: 95\t Train Loss: 76.539\t Val. Loss: 65.745\n",
            "Epoch: 96\t Train Loss: 74.914\t Val. Loss: 62.945\n",
            "Epoch: 97\t Train Loss: 75.455\t Val. Loss: 66.502\n",
            "Epoch: 98\t Train Loss: 76.819\t Val. Loss: 65.216\n",
            "Epoch: 99\t Train Loss: 76.591\t Val. Loss: 72.068\n",
            "Epoch: 100\t Train Loss: 76.550\t Val. Loss: 67.292\n",
            "Epoch: 101\t Train Loss: 75.862\t Val. Loss: 73.956\n",
            "Epoch: 102\t Train Loss: 78.428\t Val. Loss: 62.733\n",
            "Epoch: 103\t Train Loss: 76.391\t Val. Loss: 64.457\n",
            "Epoch: 104\t Train Loss: 77.834\t Val. Loss: 63.503\n",
            "Epoch: 105\t Train Loss: 75.574\t Val. Loss: 70.164\n",
            "Epoch: 106\t Train Loss: 76.374\t Val. Loss: 63.545\n",
            "Epoch: 107\t Train Loss: 75.315\t Val. Loss: 63.439\n",
            "Epoch: 108\t Train Loss: 76.057\t Val. Loss: 69.004\n",
            "Epoch: 109\t Train Loss: 77.749\t Val. Loss: 62.162\n",
            "Epoch: 110\t Train Loss: 75.701\t Val. Loss: 64.238\n",
            "Epoch: 111\t Train Loss: 75.250\t Val. Loss: 64.116\n",
            "Epoch: 112\t Train Loss: 75.174\t Val. Loss: 65.961\n",
            "Epoch: 113\t Train Loss: 75.791\t Val. Loss: 69.372\n",
            "Epoch: 114\t Train Loss: 75.773\t Val. Loss: 66.443\n",
            "Epoch: 115\t Train Loss: 78.381\t Val. Loss: 65.225\n",
            "Epoch: 116\t Train Loss: 77.452\t Val. Loss: 63.776\n",
            "Epoch: 117\t Train Loss: 75.676\t Val. Loss: 63.789\n",
            "Epoch: 118\t Train Loss: 75.198\t Val. Loss: 64.796\n",
            "Epoch: 119\t Train Loss: 75.151\t Val. Loss: 62.648\n",
            "Epoch: 120\t Train Loss: 76.190\t Val. Loss: 62.221\n",
            "Epoch: 121\t Train Loss: 75.667\t Val. Loss: 63.333\n",
            "Epoch: 122\t Train Loss: 75.822\t Val. Loss: 62.159\n",
            "Epoch: 123\t Train Loss: 75.369\t Val. Loss: 63.090\n",
            "Epoch: 124\t Train Loss: 75.340\t Val. Loss: 62.409\n",
            "Epoch: 125\t Train Loss: 76.363\t Val. Loss: 62.470\n",
            "Epoch: 126\t Train Loss: 76.011\t Val. Loss: 62.611\n",
            "Epoch: 127\t Train Loss: 75.642\t Val. Loss: 63.239\n",
            "Epoch: 128\t Train Loss: 75.095\t Val. Loss: 62.949\n",
            "Epoch: 129\t Train Loss: 75.151\t Val. Loss: 62.619\n",
            "Epoch: 130\t Train Loss: 74.767\t Val. Loss: 62.469\n",
            "Epoch: 131\t Train Loss: 74.890\t Val. Loss: 63.508\n",
            "Epoch: 132\t Train Loss: 75.580\t Val. Loss: 62.195\n",
            "Epoch: 133\t Train Loss: 75.334\t Val. Loss: 62.586\n",
            "Epoch: 134\t Train Loss: 75.025\t Val. Loss: 63.406\n",
            "Epoch: 135\t Train Loss: 75.387\t Val. Loss: 62.154\n",
            "Epoch: 136\t Train Loss: 75.331\t Val. Loss: 62.902\n",
            "Val loss decreased (62.144857 --> 62.128070).  Saving model ...\n",
            "Epoch: 137\t Train Loss: 75.286\t Val. Loss: 62.128\n",
            "Epoch: 138\t Train Loss: 75.591\t Val. Loss: 62.197\n",
            "Epoch: 139\t Train Loss: 75.277\t Val. Loss: 62.823\n",
            "Epoch: 140\t Train Loss: 75.265\t Val. Loss: 62.133\n",
            "Epoch: 141\t Train Loss: 75.534\t Val. Loss: 62.330\n",
            "Epoch: 142\t Train Loss: 74.949\t Val. Loss: 63.125\n",
            "Epoch: 143\t Train Loss: 74.570\t Val. Loss: 62.170\n",
            "Epoch: 144\t Train Loss: 75.310\t Val. Loss: 62.424\n",
            "Epoch: 145\t Train Loss: 75.254\t Val. Loss: 62.502\n",
            "Epoch: 146\t Train Loss: 75.009\t Val. Loss: 62.200\n",
            "Epoch: 147\t Train Loss: 75.520\t Val. Loss: 62.378\n",
            "Epoch: 148\t Train Loss: 75.012\t Val. Loss: 62.563\n",
            "Epoch: 149\t Train Loss: 74.812\t Val. Loss: 62.166\n",
            "Epoch: 150\t Train Loss: 75.332\t Val. Loss: 62.475\n",
            "Epoch: 151\t Train Loss: 75.008\t Val. Loss: 62.285\n",
            "Epoch: 152\t Train Loss: 75.452\t Val. Loss: 62.323\n",
            "Epoch: 153\t Train Loss: 75.314\t Val. Loss: 62.129\n",
            "Epoch: 154\t Train Loss: 76.060\t Val. Loss: 62.164\n",
            "Epoch: 155\t Train Loss: 75.523\t Val. Loss: 62.474\n",
            "Epoch: 156\t Train Loss: 75.288\t Val. Loss: 62.455\n",
            "Epoch: 157\t Train Loss: 75.881\t Val. Loss: 62.878\n",
            "Epoch: 158\t Train Loss: 76.268\t Val. Loss: 63.557\n",
            "Epoch: 159\t Train Loss: 76.220\t Val. Loss: 62.590\n",
            "Epoch: 160\t Train Loss: 75.870\t Val. Loss: 62.797\n",
            "Epoch: 161\t Train Loss: 75.680\t Val. Loss: 62.168\n",
            "Epoch: 162\t Train Loss: 75.630\t Val. Loss: 62.515\n",
            "Epoch: 163\t Train Loss: 75.661\t Val. Loss: 62.300\n",
            "Epoch: 164\t Train Loss: 75.579\t Val. Loss: 62.148\n",
            "Epoch: 165\t Train Loss: 75.418\t Val. Loss: 62.231\n",
            "Epoch: 166\t Train Loss: 75.537\t Val. Loss: 62.255\n",
            "Epoch: 167\t Train Loss: 75.622\t Val. Loss: 62.312\n",
            "Epoch: 168\t Train Loss: 75.672\t Val. Loss: 62.328\n",
            "Epoch: 169\t Train Loss: 75.337\t Val. Loss: 62.245\n",
            "Epoch: 170\t Train Loss: 75.303\t Val. Loss: 62.130\n",
            "Epoch: 171\t Train Loss: 75.360\t Val. Loss: 62.247\n",
            "Val loss decreased (62.128070 --> 62.127015).  Saving model ...\n",
            "Epoch: 172\t Train Loss: 75.085\t Val. Loss: 62.127\n",
            "Epoch: 173\t Train Loss: 75.363\t Val. Loss: 62.140\n",
            "Epoch: 174\t Train Loss: 75.066\t Val. Loss: 62.134\n",
            "Epoch: 175\t Train Loss: 75.083\t Val. Loss: 62.137\n",
            "Epoch: 176\t Train Loss: 74.924\t Val. Loss: 62.147\n",
            "Epoch: 177\t Train Loss: 75.207\t Val. Loss: 62.142\n",
            "Epoch: 178\t Train Loss: 74.778\t Val. Loss: 62.127\n",
            "Epoch: 179\t Train Loss: 74.998\t Val. Loss: 62.169\n",
            "Epoch: 180\t Train Loss: 74.785\t Val. Loss: 62.129\n",
            "Epoch: 181\t Train Loss: 74.791\t Val. Loss: 62.160\n",
            "Epoch: 182\t Train Loss: 74.746\t Val. Loss: 62.156\n",
            "Epoch: 183\t Train Loss: 74.746\t Val. Loss: 62.169\n",
            "Epoch: 184\t Train Loss: 74.738\t Val. Loss: 62.169\n",
            "Epoch: 185\t Train Loss: 74.657\t Val. Loss: 62.160\n",
            "Epoch: 186\t Train Loss: 74.673\t Val. Loss: 62.151\n",
            "Epoch: 187\t Train Loss: 74.585\t Val. Loss: 62.169\n",
            "Epoch: 188\t Train Loss: 74.619\t Val. Loss: 62.177\n",
            "Epoch: 189\t Train Loss: 74.588\t Val. Loss: 62.169\n",
            "Epoch: 190\t Train Loss: 74.570\t Val. Loss: 62.176\n",
            "Epoch: 191\t Train Loss: 74.571\t Val. Loss: 62.191\n",
            "Epoch: 192\t Train Loss: 74.533\t Val. Loss: 62.195\n",
            "Epoch: 193\t Train Loss: 74.554\t Val. Loss: 62.198\n",
            "Epoch: 194\t Train Loss: 74.522\t Val. Loss: 62.214\n",
            "Epoch: 195\t Train Loss: 74.559\t Val. Loss: 62.213\n",
            "Epoch: 196\t Train Loss: 74.599\t Val. Loss: 62.213\n",
            "Epoch: 197\t Train Loss: 74.598\t Val. Loss: 62.214\n",
            "Epoch: 198\t Train Loss: 74.533\t Val. Loss: 62.216\n",
            "Epoch: 199\t Train Loss: 74.488\t Val. Loss: 62.216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYsehDzCx2Km",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(\"content/train_stats_1.npy\",stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5loOywuhHem-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RMSELoss(yhat,y):\n",
        "    return torch.sqrt(torch.mean((yhat-y)**2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vKyR0Rq1JxW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "871b80af-443e-44f9-ffba-01d5d562d442"
      },
      "source": [
        "test_loss = test(model, criterion)\n",
        "print(test_loss)"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "63.4225355386734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3veSufNOAfJN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "afc43149-f3c0-4885-839c-743588cc890e"
      },
      "source": [
        "model_list = []\n",
        "for i in range(5):\n",
        "  model = UnivariatMultiStepLSTM(input_size, hidden_dim, outcome_dim, num_layers,\n",
        "                      lstm_dropout=lstm_dropout,bidirectional=bidirectional,fc_dropout=fc_dropout)\n",
        "  model.load_state_dict(torch.load(\"content/model_\"+str(i+1)+\".pt\"))\n",
        "  model_list.append(model)"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-190-72829df33af1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   model = UnivariatMultiStepLSTM(input_size, hidden_dim, outcome_dim, num_layers,\n\u001b[1;32m      4\u001b[0m                       lstm_dropout=lstm_dropout,bidirectional=bidirectional,fc_dropout=fc_dropout)\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content/model_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mmodel_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 830\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for UnivariatMultiStepLSTM:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([140, 1]) from checkpoint, the shape in current model is torch.Size([140, 8]).\n\tsize mismatch for lstm.weight_ih_l0_reverse: copying a param with shape torch.Size([140, 1]) from checkpoint, the shape in current model is torch.Size([140, 8])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ENml8i-BpID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_losses = []\n",
        "for i in range(5):\n",
        "  test_losses.append(test(model_list[i],criterion))\n",
        "print(np.array(test_losses).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz44UzJkCMhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_losses = []\n",
        "for i in range(5):\n",
        "  test_losses.append(test(model_list[i],RMSELoss))\n",
        "print(np.array(test_losses).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5q7TUrvDCrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp = max_error(model_list[0],RMSELoss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wym5y3h6C4pr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_errors = []\n",
        "for i in range(5):\n",
        "  max_errors.append(max_error(model_list[i],RMSELoss).max())\n",
        "print(np.array(max_errors).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK2YtfIJHWf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_errors = []\n",
        "for i in range(5):\n",
        "  max_errors.append(max_error(model_list[i],RMSELoss))\n",
        "print(np.array(max_errors).mean(axis=0).shape)\n",
        "np.save(\"max_errors\",np.array(max_errors).mean(axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ5fKFDuvW75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_ensemble(model_list, criterion):\n",
        "    #model.load_state_dict(torch.load('content/models/model.pt'))\n",
        "    epoch_loss=[]\n",
        "    with torch.no_grad():\n",
        "      for val_acc, configs, targets in test_data_loader:\n",
        "        output = []\n",
        "        for model in model_list:\n",
        "          model.eval()\n",
        "          output.append(model([val_acc, configs]))\n",
        "        output = torch.stack(output)\n",
        "        output = torch.mean(output,dim=0)\n",
        "        loss = criterion(output, targets.unsqueeze(-1))\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "    return np.array(epoch_loss).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY1OeI5jvrE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_losses=test_ensemble(model_list,criterion)\n",
        "print(test_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIEwvzjUwzee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_losses=test_ensemble(model_list,RMSELoss)\n",
        "print(test_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZMc9hPvw5CE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_error_ensemble(model_list, criterion):\n",
        "    epoch_loss=[]\n",
        "    with torch.no_grad():\n",
        "      for val_acc, configs, targets in test_data_loader:\n",
        "        output = []\n",
        "        for model in model_list:\n",
        "          model.eval()\n",
        "          output.append(model([val_acc, configs]))\n",
        "        output = torch.stack(output)\n",
        "        output = torch.mean(output,dim=0)\n",
        "        loss = np.abs(output.detach().numpy()-targets.detach().numpy().reshape(-1,1))\n",
        "        epoch_loss += loss.tolist()\n",
        "        \n",
        "    return np.array(epoch_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVPlCR9jxLko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_losses=max_error_ensemble(model_list,RMSELoss)\n",
        "print(test_losses.max())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}