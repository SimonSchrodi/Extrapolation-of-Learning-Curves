{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "Conditional Multi-Step Univariate LSTM.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJd_VxDKN8pn",
        "colab_type": "text"
      },
      "source": [
        "# Task A: Creating a Performance Predictor\n",
        "\n",
        "In this task, you will use training data from 2000 configurations on a single OpenML dataset to train a performance predictor. The data will be splitted into train, test and validation set and we will only use the first 10 epochs of the learning curves for predicitons. You are provided with the full benchmark logs for Fashion-MNIST, that is learning curves, config parameters and gradient statistics, and you can use them freely.\n",
        "\n",
        "For questions, you can contact zimmerl@informatik.uni-freiburg.\n",
        "\n",
        "__Note: Please use the dataloading and splits you are provided with in this notebook.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqPBWpd5N8pt",
        "colab_type": "text"
      },
      "source": [
        "## Specifications:\n",
        "\n",
        "* Data: fashion_mnist.json\n",
        "* Number of datasets: 1\n",
        "* Number of configurations: 2000\n",
        "* Number of epochs seed during prediction: 10\n",
        "* Available data: Learning curves, architecture parameters and hyperparameters, gradient statistics \n",
        "* Target: Final validation accuracy\n",
        "* Evaluation metric: MSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eYlSG9BN8py",
        "colab_type": "text"
      },
      "source": [
        "## Importing and splitting data\n",
        "\n",
        "__Note__: There are 51 steps logged, 50 epochs plus the 0th epoch, prior to any weight updates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT8tEKUJOqNy",
        "colab_type": "code",
        "outputId": "fe6ede0f-a58b-4690-c9ab-fce1e2a92651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "!pip install wget\n",
        "!pip install zipfile36"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=a0b3eea70003b5a1a24c828e96b4d19712064da8464d4506b9342c7d248238c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting zipfile36\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/8a/3b7da0b0bd87d1ef05b74207827c72d348b56a0d6d83242582be18a81e02/zipfile36-0.1.3-py3-none-any.whl\n",
            "Installing collected packages: zipfile36\n",
            "Successfully installed zipfile36-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GSY73FpOPdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wget\n",
        "import zipfile\n",
        "dir_path = 'content/'\n",
        "filename=wget.download('https://ndownloader.figshare.com/files/21001311')\n",
        "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"\")\n",
        "!rm fashion_mnist.zip\n",
        "wget.download('https://raw.githubusercontent.com/automl/LCBench/master/api.py')\n",
        "wget.download('https://raw.githubusercontent.com/infomon/Extrapolation-of-Learning-Curves/master/utils.py')\n",
        "!mkdir content/models\n",
        "!mkdir models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql1OqCfgN8p4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "%cd ..\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from api import Benchmark\n",
        "import utils as utils\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2sQv_05N8qJ",
        "colab_type": "code",
        "outputId": "2a8bd3a4-6858-4629-d131-855a4474c674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "bench_dir = dir_path+\"fashion_mnist.json\"\n",
        "bench = Benchmark(bench_dir, cache=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Loading data...\n",
            "==> No cached data found or cache set to False.\n",
            "==> Reading json data...\n",
            "==> Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eq4DxuuN8qe",
        "colab_type": "code",
        "outputId": "6b8cf167-4419-45cc-d72c-74734a515d86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Read data\n",
        "def cut_data(data, cut_position):\n",
        "    targets = []\n",
        "    for dp in data:\n",
        "        targets.append(dp[\"Train/val_accuracy\"][50])\n",
        "        for tag in dp:\n",
        "            if tag.startswith(\"Train/\"):\n",
        "                dp[tag] = dp[tag][0:cut_position]\n",
        "    return data, targets\n",
        "\n",
        "def read_data():\n",
        "    dataset_name = 'Fashion-MNIST'\n",
        "    n_configs = bench.get_number_of_configs(dataset_name)\n",
        "    \n",
        "    # Query API\n",
        "    data = []\n",
        "    for config_id in range(n_configs):\n",
        "        data_point = dict()\n",
        "        data_point[\"config\"] = bench.query(dataset_name=dataset_name, tag=\"config\", config_id=config_id)\n",
        "        for tag in bench.get_queriable_tags(dataset_name=dataset_name, config_id=config_id):\n",
        "            if tag.startswith(\"Train/\"):\n",
        "                data_point[tag] = bench.query(dataset_name=dataset_name, tag=tag, config_id=config_id)    \n",
        "        data.append(data_point)\n",
        "        \n",
        "    # Split: 50% train, 25% validation, 25% test (the data is already shuffled)\n",
        "    indices = np.arange(n_configs)\n",
        "    ind_train = indices[0:int(np.floor(0.5*n_configs))]\n",
        "    ind_val = indices[int(np.floor(0.5*n_configs)):int(np.floor(0.75*n_configs))]\n",
        "    ind_test = indices[int(np.floor(0.75*n_configs)):]\n",
        "\n",
        "    array_data = np.array(data)\n",
        "    train_data = array_data[ind_train]\n",
        "    val_data = array_data[ind_val]\n",
        "    test_data = array_data[ind_test]\n",
        "    \n",
        "    # Cut curves for validation and test\n",
        "    cut_position = 11\n",
        "    #val_data, val_targets = cut_data(val_data, cut_position)\n",
        "    #test_data, test_targets = cut_data(test_data, cut_position)\n",
        "    val_data, val_targets = cut_data(val_data, 51)\n",
        "    test_data, test_targets = cut_data(test_data, 51)\n",
        "    train_data, train_targets = cut_data(train_data, 51)   # Cut last value as it is repeated\n",
        "    \n",
        "    return train_data, val_data, test_data, train_targets, val_targets, test_targets\n",
        "    \n",
        "train_data, val_data, test_data, train_targets, val_targets, test_targets = read_data()\n",
        "\n",
        "print(\"Train:\", len(train_data))\n",
        "print(\"Validation:\", len(val_data))\n",
        "print(\"Test:\", len(test_data))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 1000\n",
            "Validation: 500\n",
            "Test: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PphdZ2aN8qv",
        "colab_type": "text"
      },
      "source": [
        "The data contains the configuration of the trained model and learning curves as well as global and layer-wise gradient statistics.\n",
        "\n",
        "__Note__: Not all parameters vary across different configurations. The varying parameters are batch_size, max_dropout, max_units, num_layers, learning_rate, momentum, weight_decay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybnns3VKN8q0",
        "colab_type": "code",
        "outputId": "8863e664-69c6-4ecf-8604-ee9c95f56fa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Config\n",
        "print(\"Config example:\", train_data[0][\"config\"])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Config example: {'batch_size': 71, 'imputation_strategy': 'mean', 'learning_rate_scheduler': 'cosine_annealing', 'loss': 'cross_entropy_weighted', 'network': 'shapedmlpnet', 'max_dropout': 0.025926231827891333, 'normalization_strategy': 'standardize', 'optimizer': 'sgd', 'cosine_annealing_T_max': 50, 'cosine_annealing_eta_min': 1e-08, 'activation': 'relu', 'max_units': 293, 'mlp_shape': 'funnel', 'num_layers': 3, 'learning_rate': 0.0018243300267253295, 'momentum': 0.21325193168301043, 'weight_decay': 0.020472816917443872}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua6h3ORrlA5N",
        "colab_type": "code",
        "outputId": "857bfd65-6bdb-4512-e362-92d790494363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "train_data[1][\"config\"]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu',\n",
              " 'batch_size': 457,\n",
              " 'cosine_annealing_T_max': 50,\n",
              " 'cosine_annealing_eta_min': 1e-08,\n",
              " 'imputation_strategy': 'mean',\n",
              " 'learning_rate': 0.01239328605026128,\n",
              " 'learning_rate_scheduler': 'cosine_annealing',\n",
              " 'loss': 'cross_entropy_weighted',\n",
              " 'max_dropout': 0.5472322491757223,\n",
              " 'max_units': 950,\n",
              " 'mlp_shape': 'funnel',\n",
              " 'momentum': 0.16411425552061212,\n",
              " 'network': 'shapedmlpnet',\n",
              " 'normalization_strategy': 'standardize',\n",
              " 'num_layers': 4,\n",
              " 'optimizer': 'sgd',\n",
              " 'weight_decay': 0.09762768273307641}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTl67TOTN8rD",
        "colab_type": "code",
        "outputId": "37add259-21a7-4d65-cfbf-4c5625eda43a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "# Learning curve\n",
        "plt.plot(train_data[10][\"Train/val_accuracy\"])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7faae2244438>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXGuPeBKN8rM",
        "colab_type": "code",
        "outputId": "1ec375da-1ad4-44ea-f66a-8460523c3678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "# Gradient statistics\n",
        "plt.plot(train_data[10][\"Train/layer_wise_gradient_mean_layer_0\"])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7faae1d0c898>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAD4CAYAAAA6j0u4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7v8DbRIJDShuWn8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaYC_PH4N8rQ",
        "colab_type": "text"
      },
      "source": [
        "## A simple baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJrvEgLzN8rS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleLearningCurvePredictor():\n",
        "    \"\"\"A learning curve predictor that predicts the last observed epoch of the validation accuracy as final performance\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        pass\n",
        "    \n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for datapoint in X:\n",
        "            predictions.append(datapoint[\"Train/val_accuracy\"][-1])\n",
        "        return predictions\n",
        "    \n",
        "def score(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d23UZhgHN8rW",
        "colab_type": "code",
        "outputId": "1fa6d5d4-9174-45cb-d1b2-d2d15b727b0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Training & tuning\n",
        "predictor = SimpleLearningCurvePredictor()\n",
        "predictor.fit(train_data, train_targets)\n",
        "preds = predictor.predict(val_data)\n",
        "mse = score(val_targets, preds)\n",
        "print(\"Score on validation set:\", mse)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score on validation set: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMWiY0XbN8rc",
        "colab_type": "code",
        "outputId": "a705f4fa-e6e7-468f-b253-5e48343d0290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Final evaluation (after tuning)\n",
        "final_preds = predictor.predict(test_data)\n",
        "final_score = score(test_targets, final_preds)\n",
        "print(\"Final test score:\", final_score)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final test score: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnEQJbbLyGG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_loader = utils.prep_data(train_data, train_targets, batch_size=32,normalization_factor_temporal_data=[100], one_shot=True)\n",
        "val_data_loader = utils.prep_data(val_data, val_targets, batch_size=32,normalization_factor_temporal_data=[100], one_shot=True)\n",
        "test_data_loader = utils.prep_data(test_data, test_targets, batch_size=32,normalization_factor_temporal_data=[100],one_shot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBGim3i6CoCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, nof_configs, num_layers, dropout = 0.5, bidirectional=False):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        self.nof_configs = nof_configs\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = torch.nn.LSTM(input_size=input_size, \n",
        "                                  hidden_size=hidden_size,\n",
        "                                  num_layers=num_layers,\n",
        "                                  dropout=dropout,\n",
        "                                  bidirectional=bidirectional)\n",
        "\n",
        "        self.relu = torch.nn.functional.relu\n",
        "\n",
        "        self.encode_fc1 = torch.nn.Linear(self.nof_configs,int(self.hidden_size/2))\n",
        "        self.encode_bn1 = torch.nn.BatchNorm1d(int(self.hidden_size/2))\n",
        "        self.encode_fc2 = torch.nn.Linear(int(self.hidden_size/2),self.hidden_size)\n",
        "        self.encode_bn2 = torch.nn.BatchNorm1d(self.hidden_size)\n",
        "\n",
        "    def forward(self, seq, config):\n",
        "        h0 = self.initHidden(config)\n",
        "        c0 = self.initCell(seq.size()[0])\n",
        "        seq = torch.t(seq)\n",
        "        seq = seq.unsqueeze(-1)\n",
        "        output, (hidden,cell) = self.lstm(seq, (h0,c0))\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def initHidden(self, config):\n",
        "        x = self.relu(self.encode_bn1(self.encode_fc1(config)))\n",
        "        x = self.relu(self.encode_bn2(self.encode_fc2(x)))\n",
        "        return torch.stack([x for _ in range(self.num_layers*self.num_directions)])\n",
        "\n",
        "    def initCell(self, batch_size):\n",
        "        return torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dErX6on9FYyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout = 0.5, bidirectional=False):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = torch.nn.LSTM(input_size=input_size, \n",
        "                                  hidden_size=hidden_size,\n",
        "                                  num_layers=num_layers,\n",
        "                                  dropout=dropout,\n",
        "                                  bidirectional=bidirectional)\n",
        "        \n",
        "        self.fc_out = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.relu = torch.nn.functional.relu\n",
        "\n",
        "    def forward(self, seq, h0, c0):\n",
        "        seq = seq.unsqueeze(0)\n",
        "        seq = seq.unsqueeze(-1)\n",
        "        output, (hidden,cell) = self.lstm(seq, (h0,c0))\n",
        "        output = self.fc_out(output)\n",
        "        return output.squeeze(), hidden, cell\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwbesz1Slf4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(torch.nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "    assert encoder.hidden_size == decoder.hidden_size\n",
        "    #assert encoder.num_layers == decoder.num_layers\n",
        "\n",
        "  def forward(self, source, target, teacher_forcing_ratio = 0.5):\n",
        "    batch_size = target.size()[0]\n",
        "    target_len = target.size()[1]\n",
        "\n",
        "    outputs = torch.zeros(target_len, batch_size, 1)\n",
        "\n",
        "    seq , config = source\n",
        "    output, hidden, cell = self.encoder(seq, config)\n",
        "\n",
        "    decoder_input = target[:,0]\n",
        "    for t in range(1, target_len):\n",
        "      output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
        "      outputs[t] = output.unsqueeze(-1)\n",
        "      use_teacher_forcing = np.random.random() < teacher_forcing_ratio\n",
        "      decoder_input = target[:,t] if use_teacher_forcing else output\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVWyDKngncVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "  for name, param in m.named_parameters():\n",
        "        torch.nn.init.uniform_(param.data, -0.08, 0.08)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR-Zp7gmnXU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 1\n",
        "outcome_dim = 1\n",
        "hidden_dim = 32\n",
        "num_layers = 2\n",
        "config_size = 7\n",
        "bidirectional=True\n",
        "lr=0.01\n",
        "weight_decay=10e-3\n",
        "epochs=500\n",
        "T_0 = int(epochs/8)\n",
        "\n",
        "encoder = EncoderRNN(input_size, hidden_dim, config_size, num_layers,bidirectional=True)\n",
        "decoder = DecoderRNN(input_size, hidden_dim, outcome_dim, 2*num_layers if bidirectional else num_layers)\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "model.apply(init_weights)\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=T_0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk5Jat93mO_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, criterion, clip=5):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for val_acc, configs, targets in train_data_loader:\n",
        "      optimizer.zero_grad()\n",
        "      output = model([val_acc,configs],targets)\n",
        "      output = output.squeeze()\n",
        "      output = torch.t(output)\n",
        "      loss = criterion(output, targets)\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss.item()\n",
        "    return epoch_loss/len(train_data_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GulE-3SosoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for val_acc, configs, targets in val_data_loader:\n",
        "      output = model([val_acc, configs], targets, 0)\n",
        "      output = output.squeeze()\n",
        "      output = torch.t(output)\n",
        "      loss = criterion(output, targets)\n",
        "      epoch_loss += loss.item()\n",
        "  return epoch_loss / len(val_data_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqBLkuRznl9y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6f8b59b7-e621-42c5-cf50-cc29e9d850ad"
      },
      "source": [
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss = train(model, optimizer, criterion)\n",
        "  val_loss = evaluate(model, criterion)\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    torch.save(model.state_dict(),\"content/models/model.pt\")    \n",
        "    print('Val loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(best_val_loss,val_loss))\n",
        "    best_val_loss = val_loss\n",
        "\n",
        "  print(f'Epoch: {epoch}\\t Train Loss: {train_loss:.3f}\\t Val. Loss: {val_loss:.3f}')\n",
        "  scheduler.step()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Val loss decreased (inf --> 4301.206055).  Saving model ...\n",
            "Epoch: 0\t Train Loss: 5100.444\t Val. Loss: 4301.206\n",
            "Val loss decreased (4301.206055 --> 3032.438019).  Saving model ...\n",
            "Epoch: 1\t Train Loss: 3679.214\t Val. Loss: 3032.438\n",
            "Val loss decreased (3032.438019 --> 2002.973480).  Saving model ...\n",
            "Epoch: 2\t Train Loss: 2527.829\t Val. Loss: 2002.973\n",
            "Val loss decreased (2002.973480 --> 1202.829376).  Saving model ...\n",
            "Epoch: 3\t Train Loss: 1611.532\t Val. Loss: 1202.829\n",
            "Val loss decreased (1202.829376 --> 624.132610).  Saving model ...\n",
            "Epoch: 4\t Train Loss: 920.210\t Val. Loss: 624.133\n",
            "Val loss decreased (624.132610 --> 261.125906).  Saving model ...\n",
            "Epoch: 5\t Train Loss: 447.533\t Val. Loss: 261.126\n",
            "Val loss decreased (261.125906 --> 106.993584).  Saving model ...\n",
            "Epoch: 6\t Train Loss: 188.378\t Val. Loss: 106.994\n",
            "Val loss decreased (106.993584 --> 67.704506).  Saving model ...\n",
            "Epoch: 7\t Train Loss: 109.364\t Val. Loss: 67.705\n",
            "Val loss decreased (67.704506 --> 19.414078).  Saving model ...\n",
            "Epoch: 8\t Train Loss: 65.140\t Val. Loss: 19.414\n",
            "Epoch: 9\t Train Loss: 26.402\t Val. Loss: 29.070\n",
            "Val loss decreased (19.414078 --> 16.990633).  Saving model ...\n",
            "Epoch: 10\t Train Loss: 27.133\t Val. Loss: 16.991\n",
            "Epoch: 11\t Train Loss: 19.725\t Val. Loss: 26.875\n",
            "Val loss decreased (16.990633 --> 14.817405).  Saving model ...\n",
            "Epoch: 12\t Train Loss: 21.061\t Val. Loss: 14.817\n",
            "Epoch: 13\t Train Loss: 21.938\t Val. Loss: 21.802\n",
            "Val loss decreased (14.817405 --> 13.516224).  Saving model ...\n",
            "Epoch: 14\t Train Loss: 15.951\t Val. Loss: 13.516\n",
            "Epoch: 15\t Train Loss: 21.423\t Val. Loss: 36.401\n",
            "Epoch: 16\t Train Loss: 18.222\t Val. Loss: 17.535\n",
            "Epoch: 17\t Train Loss: 19.746\t Val. Loss: 18.748\n",
            "Epoch: 18\t Train Loss: 17.418\t Val. Loss: 26.890\n",
            "Epoch: 19\t Train Loss: 22.087\t Val. Loss: 17.543\n",
            "Epoch: 20\t Train Loss: 21.115\t Val. Loss: 14.346\n",
            "Epoch: 21\t Train Loss: 16.457\t Val. Loss: 19.993\n",
            "Epoch: 22\t Train Loss: 17.315\t Val. Loss: 15.166\n",
            "Epoch: 23\t Train Loss: 14.193\t Val. Loss: 14.863\n",
            "Epoch: 24\t Train Loss: 16.394\t Val. Loss: 14.554\n",
            "Val loss decreased (13.516224 --> 12.160236).  Saving model ...\n",
            "Epoch: 25\t Train Loss: 13.838\t Val. Loss: 12.160\n",
            "Epoch: 26\t Train Loss: 14.324\t Val. Loss: 13.825\n",
            "Val loss decreased (12.160236 --> 10.606780).  Saving model ...\n",
            "Epoch: 27\t Train Loss: 14.120\t Val. Loss: 10.607\n",
            "Val loss decreased (10.606780 --> 10.420105).  Saving model ...\n",
            "Epoch: 28\t Train Loss: 12.830\t Val. Loss: 10.420\n",
            "Epoch: 29\t Train Loss: 12.998\t Val. Loss: 11.307\n",
            "Epoch: 30\t Train Loss: 14.234\t Val. Loss: 14.838\n",
            "Epoch: 31\t Train Loss: 14.294\t Val. Loss: 10.715\n",
            "Epoch: 32\t Train Loss: 12.217\t Val. Loss: 10.759\n",
            "Epoch: 33\t Train Loss: 14.243\t Val. Loss: 13.945\n",
            "Epoch: 34\t Train Loss: 14.209\t Val. Loss: 11.549\n",
            "Epoch: 35\t Train Loss: 13.501\t Val. Loss: 12.409\n",
            "Epoch: 36\t Train Loss: 14.162\t Val. Loss: 11.240\n",
            "Epoch: 37\t Train Loss: 12.646\t Val. Loss: 11.798\n",
            "Epoch: 38\t Train Loss: 12.596\t Val. Loss: 11.717\n",
            "Epoch: 39\t Train Loss: 13.486\t Val. Loss: 10.569\n",
            "Epoch: 40\t Train Loss: 12.032\t Val. Loss: 10.449\n",
            "Epoch: 41\t Train Loss: 11.551\t Val. Loss: 10.755\n",
            "Epoch: 42\t Train Loss: 12.244\t Val. Loss: 10.584\n",
            "Val loss decreased (10.420105 --> 10.195422).  Saving model ...\n",
            "Epoch: 43\t Train Loss: 11.692\t Val. Loss: 10.195\n",
            "Epoch: 44\t Train Loss: 11.315\t Val. Loss: 10.999\n",
            "Epoch: 45\t Train Loss: 11.316\t Val. Loss: 10.614\n",
            "Epoch: 46\t Train Loss: 11.564\t Val. Loss: 10.485\n",
            "Epoch: 47\t Train Loss: 11.692\t Val. Loss: 10.952\n",
            "Epoch: 48\t Train Loss: 11.541\t Val. Loss: 10.619\n",
            "Epoch: 49\t Train Loss: 11.299\t Val. Loss: 10.198\n",
            "Epoch: 50\t Train Loss: 10.900\t Val. Loss: 10.293\n",
            "Val loss decreased (10.195422 --> 10.025176).  Saving model ...\n",
            "Epoch: 51\t Train Loss: 11.564\t Val. Loss: 10.025\n",
            "Epoch: 52\t Train Loss: 11.048\t Val. Loss: 10.098\n",
            "Val loss decreased (10.025176 --> 9.663430).  Saving model ...\n",
            "Epoch: 53\t Train Loss: 10.788\t Val. Loss: 9.663\n",
            "Val loss decreased (9.663430 --> 9.592563).  Saving model ...\n",
            "Epoch: 54\t Train Loss: 10.709\t Val. Loss: 9.593\n",
            "Val loss decreased (9.592563 --> 9.428063).  Saving model ...\n",
            "Epoch: 55\t Train Loss: 9.994\t Val. Loss: 9.428\n",
            "Epoch: 56\t Train Loss: 10.123\t Val. Loss: 9.483\n",
            "Val loss decreased (9.428063 --> 9.363831).  Saving model ...\n",
            "Epoch: 57\t Train Loss: 10.612\t Val. Loss: 9.364\n",
            "Val loss decreased (9.363831 --> 9.295733).  Saving model ...\n",
            "Epoch: 58\t Train Loss: 10.678\t Val. Loss: 9.296\n",
            "Epoch: 59\t Train Loss: 10.209\t Val. Loss: 9.317\n",
            "Epoch: 60\t Train Loss: 10.197\t Val. Loss: 9.300\n",
            "Val loss decreased (9.295733 --> 9.295406).  Saving model ...\n",
            "Epoch: 61\t Train Loss: 10.262\t Val. Loss: 9.295\n",
            "Epoch: 62\t Train Loss: 15.768\t Val. Loss: 14.087\n",
            "Epoch: 63\t Train Loss: 16.415\t Val. Loss: 15.048\n",
            "Epoch: 64\t Train Loss: 16.080\t Val. Loss: 21.504\n",
            "Epoch: 65\t Train Loss: 18.064\t Val. Loss: 13.427\n",
            "Epoch: 66\t Train Loss: 18.693\t Val. Loss: 14.015\n",
            "Epoch: 67\t Train Loss: 15.649\t Val. Loss: 17.912\n",
            "Epoch: 68\t Train Loss: 15.501\t Val. Loss: 20.791\n",
            "Epoch: 69\t Train Loss: 15.565\t Val. Loss: 14.940\n",
            "Epoch: 70\t Train Loss: 19.299\t Val. Loss: 22.894\n",
            "Epoch: 71\t Train Loss: 19.077\t Val. Loss: 13.838\n",
            "Epoch: 72\t Train Loss: 14.737\t Val. Loss: 12.862\n",
            "Epoch: 73\t Train Loss: 17.015\t Val. Loss: 12.239\n",
            "Epoch: 74\t Train Loss: 16.342\t Val. Loss: 18.044\n",
            "Epoch: 75\t Train Loss: 17.249\t Val. Loss: 11.322\n",
            "Epoch: 76\t Train Loss: 13.382\t Val. Loss: 13.470\n",
            "Epoch: 77\t Train Loss: 16.664\t Val. Loss: 14.176\n",
            "Epoch: 78\t Train Loss: 14.347\t Val. Loss: 13.890\n",
            "Epoch: 79\t Train Loss: 14.758\t Val. Loss: 16.273\n",
            "Epoch: 80\t Train Loss: 15.776\t Val. Loss: 11.930\n",
            "Epoch: 81\t Train Loss: 13.290\t Val. Loss: 11.061\n",
            "Epoch: 82\t Train Loss: 12.859\t Val. Loss: 11.851\n",
            "Epoch: 83\t Train Loss: 12.656\t Val. Loss: 15.109\n",
            "Epoch: 84\t Train Loss: 13.464\t Val. Loss: 11.861\n",
            "Epoch: 85\t Train Loss: 14.031\t Val. Loss: 11.380\n",
            "Epoch: 86\t Train Loss: 14.038\t Val. Loss: 14.012\n",
            "Epoch: 87\t Train Loss: 13.537\t Val. Loss: 12.037\n",
            "Epoch: 88\t Train Loss: 12.723\t Val. Loss: 10.263\n",
            "Epoch: 89\t Train Loss: 12.705\t Val. Loss: 10.807\n",
            "Epoch: 90\t Train Loss: 13.024\t Val. Loss: 13.396\n",
            "Epoch: 91\t Train Loss: 15.322\t Val. Loss: 11.283\n",
            "Epoch: 92\t Train Loss: 12.047\t Val. Loss: 10.906\n",
            "Epoch: 93\t Train Loss: 12.632\t Val. Loss: 10.391\n",
            "Epoch: 94\t Train Loss: 13.222\t Val. Loss: 10.836\n",
            "Epoch: 95\t Train Loss: 10.838\t Val. Loss: 11.285\n",
            "Epoch: 96\t Train Loss: 12.316\t Val. Loss: 9.694\n",
            "Epoch: 97\t Train Loss: 12.616\t Val. Loss: 10.163\n",
            "Epoch: 98\t Train Loss: 11.977\t Val. Loss: 10.485\n",
            "Val loss decreased (9.295406 --> 9.243576).  Saving model ...\n",
            "Epoch: 99\t Train Loss: 10.985\t Val. Loss: 9.244\n",
            "Epoch: 100\t Train Loss: 11.007\t Val. Loss: 9.875\n",
            "Epoch: 101\t Train Loss: 11.117\t Val. Loss: 9.676\n",
            "Epoch: 102\t Train Loss: 11.487\t Val. Loss: 11.865\n",
            "Epoch: 103\t Train Loss: 10.452\t Val. Loss: 10.312\n",
            "Epoch: 104\t Train Loss: 11.048\t Val. Loss: 11.590\n",
            "Val loss decreased (9.243576 --> 9.180577).  Saving model ...\n",
            "Epoch: 105\t Train Loss: 11.114\t Val. Loss: 9.181\n",
            "Epoch: 106\t Train Loss: 12.012\t Val. Loss: 9.710\n",
            "Epoch: 107\t Train Loss: 11.097\t Val. Loss: 9.824\n",
            "Epoch: 108\t Train Loss: 10.426\t Val. Loss: 9.967\n",
            "Epoch: 109\t Train Loss: 10.012\t Val. Loss: 9.419\n",
            "Epoch: 110\t Train Loss: 10.173\t Val. Loss: 9.278\n",
            "Epoch: 111\t Train Loss: 10.435\t Val. Loss: 9.373\n",
            "Epoch: 112\t Train Loss: 10.247\t Val. Loss: 9.323\n",
            "Val loss decreased (9.180577 --> 9.122853).  Saving model ...\n",
            "Epoch: 113\t Train Loss: 9.715\t Val. Loss: 9.123\n",
            "Epoch: 114\t Train Loss: 9.991\t Val. Loss: 9.239\n",
            "Val loss decreased (9.122853 --> 8.693835).  Saving model ...\n",
            "Epoch: 115\t Train Loss: 9.386\t Val. Loss: 8.694\n",
            "Epoch: 116\t Train Loss: 9.865\t Val. Loss: 8.798\n",
            "Epoch: 117\t Train Loss: 9.176\t Val. Loss: 8.805\n",
            "Epoch: 118\t Train Loss: 9.270\t Val. Loss: 8.783\n",
            "Epoch: 119\t Train Loss: 9.377\t Val. Loss: 8.738\n",
            "Val loss decreased (8.693835 --> 8.555847).  Saving model ...\n",
            "Epoch: 120\t Train Loss: 9.104\t Val. Loss: 8.556\n",
            "Val loss decreased (8.555847 --> 8.553075).  Saving model ...\n",
            "Epoch: 121\t Train Loss: 8.941\t Val. Loss: 8.553\n",
            "Val loss decreased (8.553075 --> 8.541219).  Saving model ...\n",
            "Epoch: 122\t Train Loss: 9.142\t Val. Loss: 8.541\n",
            "Val loss decreased (8.541219 --> 8.539618).  Saving model ...\n",
            "Epoch: 123\t Train Loss: 9.034\t Val. Loss: 8.540\n",
            "Epoch: 124\t Train Loss: 16.080\t Val. Loss: 13.826\n",
            "Epoch: 125\t Train Loss: 15.720\t Val. Loss: 12.310\n",
            "Epoch: 126\t Train Loss: 13.868\t Val. Loss: 16.384\n",
            "Epoch: 127\t Train Loss: 13.076\t Val. Loss: 10.972\n",
            "Epoch: 128\t Train Loss: 13.380\t Val. Loss: 12.816\n",
            "Epoch: 129\t Train Loss: 15.800\t Val. Loss: 11.107\n",
            "Epoch: 130\t Train Loss: 14.292\t Val. Loss: 12.344\n",
            "Epoch: 131\t Train Loss: 14.138\t Val. Loss: 11.862\n",
            "Epoch: 132\t Train Loss: 15.118\t Val. Loss: 15.656\n",
            "Epoch: 133\t Train Loss: 17.040\t Val. Loss: 14.764\n",
            "Epoch: 134\t Train Loss: 16.394\t Val. Loss: 13.557\n",
            "Epoch: 135\t Train Loss: 14.860\t Val. Loss: 11.982\n",
            "Epoch: 136\t Train Loss: 12.955\t Val. Loss: 10.484\n",
            "Epoch: 137\t Train Loss: 13.245\t Val. Loss: 10.931\n",
            "Epoch: 138\t Train Loss: 14.620\t Val. Loss: 16.977\n",
            "Epoch: 139\t Train Loss: 14.250\t Val. Loss: 13.771\n",
            "Epoch: 140\t Train Loss: 15.539\t Val. Loss: 10.163\n",
            "Epoch: 141\t Train Loss: 13.179\t Val. Loss: 10.557\n",
            "Epoch: 142\t Train Loss: 12.571\t Val. Loss: 13.356\n",
            "Epoch: 143\t Train Loss: 16.560\t Val. Loss: 12.534\n",
            "Epoch: 144\t Train Loss: 15.110\t Val. Loss: 16.243\n",
            "Epoch: 145\t Train Loss: 14.465\t Val. Loss: 16.697\n",
            "Epoch: 146\t Train Loss: 17.190\t Val. Loss: 15.673\n",
            "Epoch: 147\t Train Loss: 16.685\t Val. Loss: 11.663\n",
            "Epoch: 148\t Train Loss: 12.569\t Val. Loss: 9.780\n",
            "Epoch: 149\t Train Loss: 15.722\t Val. Loss: 18.283\n",
            "Epoch: 150\t Train Loss: 13.729\t Val. Loss: 10.701\n",
            "Epoch: 151\t Train Loss: 13.656\t Val. Loss: 10.276\n",
            "Epoch: 152\t Train Loss: 13.165\t Val. Loss: 11.633\n",
            "Epoch: 153\t Train Loss: 12.763\t Val. Loss: 9.988\n",
            "Epoch: 154\t Train Loss: 12.405\t Val. Loss: 11.445\n",
            "Epoch: 155\t Train Loss: 14.372\t Val. Loss: 11.980\n",
            "Epoch: 156\t Train Loss: 15.716\t Val. Loss: 11.579\n",
            "Epoch: 157\t Train Loss: 11.839\t Val. Loss: 10.809\n",
            "Epoch: 158\t Train Loss: 12.167\t Val. Loss: 9.469\n",
            "Epoch: 159\t Train Loss: 12.934\t Val. Loss: 12.495\n",
            "Epoch: 160\t Train Loss: 12.229\t Val. Loss: 9.537\n",
            "Epoch: 161\t Train Loss: 11.453\t Val. Loss: 9.909\n",
            "Epoch: 162\t Train Loss: 11.079\t Val. Loss: 9.499\n",
            "Epoch: 163\t Train Loss: 10.806\t Val. Loss: 11.186\n",
            "Epoch: 164\t Train Loss: 11.661\t Val. Loss: 9.546\n",
            "Epoch: 165\t Train Loss: 10.812\t Val. Loss: 9.329\n",
            "Epoch: 166\t Train Loss: 11.306\t Val. Loss: 10.872\n",
            "Epoch: 167\t Train Loss: 11.075\t Val. Loss: 9.624\n",
            "Epoch: 168\t Train Loss: 11.111\t Val. Loss: 9.999\n",
            "Epoch: 169\t Train Loss: 10.586\t Val. Loss: 9.203\n",
            "Epoch: 170\t Train Loss: 10.197\t Val. Loss: 9.361\n",
            "Epoch: 171\t Train Loss: 11.137\t Val. Loss: 9.360\n",
            "Epoch: 172\t Train Loss: 10.079\t Val. Loss: 8.722\n",
            "Epoch: 173\t Train Loss: 9.989\t Val. Loss: 8.846\n",
            "Epoch: 174\t Train Loss: 10.107\t Val. Loss: 9.200\n",
            "Epoch: 175\t Train Loss: 9.959\t Val. Loss: 9.331\n",
            "Epoch: 176\t Train Loss: 9.786\t Val. Loss: 9.275\n",
            "Epoch: 177\t Train Loss: 9.485\t Val. Loss: 8.652\n",
            "Val loss decreased (8.539618 --> 8.529131).  Saving model ...\n",
            "Epoch: 178\t Train Loss: 8.983\t Val. Loss: 8.529\n",
            "Val loss decreased (8.529131 --> 8.453192).  Saving model ...\n",
            "Epoch: 179\t Train Loss: 9.292\t Val. Loss: 8.453\n",
            "Epoch: 180\t Train Loss: 8.865\t Val. Loss: 8.618\n",
            "Val loss decreased (8.453192 --> 8.451517).  Saving model ...\n",
            "Epoch: 181\t Train Loss: 9.451\t Val. Loss: 8.452\n",
            "Val loss decreased (8.451517 --> 8.408041).  Saving model ...\n",
            "Epoch: 182\t Train Loss: 8.875\t Val. Loss: 8.408\n",
            "Val loss decreased (8.408041 --> 8.397700).  Saving model ...\n",
            "Epoch: 183\t Train Loss: 8.825\t Val. Loss: 8.398\n",
            "Val loss decreased (8.397700 --> 8.383538).  Saving model ...\n",
            "Epoch: 184\t Train Loss: 8.701\t Val. Loss: 8.384\n",
            "Epoch: 185\t Train Loss: 9.051\t Val. Loss: 8.384\n",
            "Epoch: 186\t Train Loss: 15.357\t Val. Loss: 14.225\n",
            "Epoch: 187\t Train Loss: 15.419\t Val. Loss: 13.735\n",
            "Epoch: 188\t Train Loss: 15.950\t Val. Loss: 13.425\n",
            "Epoch: 189\t Train Loss: 13.938\t Val. Loss: 11.962\n",
            "Epoch: 190\t Train Loss: 13.375\t Val. Loss: 11.074\n",
            "Epoch: 191\t Train Loss: 15.782\t Val. Loss: 12.699\n",
            "Epoch: 192\t Train Loss: 15.782\t Val. Loss: 19.703\n",
            "Epoch: 193\t Train Loss: 17.138\t Val. Loss: 14.179\n",
            "Epoch: 194\t Train Loss: 15.080\t Val. Loss: 14.046\n",
            "Epoch: 195\t Train Loss: 16.035\t Val. Loss: 14.277\n",
            "Epoch: 196\t Train Loss: 14.804\t Val. Loss: 11.764\n",
            "Epoch: 197\t Train Loss: 13.678\t Val. Loss: 17.810\n",
            "Epoch: 198\t Train Loss: 26.421\t Val. Loss: 15.119\n",
            "Epoch: 199\t Train Loss: 17.247\t Val. Loss: 13.286\n",
            "Epoch: 200\t Train Loss: 17.143\t Val. Loss: 13.129\n",
            "Epoch: 201\t Train Loss: 16.512\t Val. Loss: 11.418\n",
            "Epoch: 202\t Train Loss: 14.804\t Val. Loss: 12.499\n",
            "Epoch: 203\t Train Loss: 15.054\t Val. Loss: 14.229\n",
            "Epoch: 204\t Train Loss: 16.534\t Val. Loss: 13.955\n",
            "Epoch: 205\t Train Loss: 14.352\t Val. Loss: 11.522\n",
            "Epoch: 206\t Train Loss: 17.221\t Val. Loss: 16.566\n",
            "Epoch: 207\t Train Loss: 18.819\t Val. Loss: 14.712\n",
            "Epoch: 208\t Train Loss: 15.318\t Val. Loss: 11.074\n",
            "Epoch: 209\t Train Loss: 13.730\t Val. Loss: 11.109\n",
            "Epoch: 210\t Train Loss: 13.127\t Val. Loss: 11.372\n",
            "Epoch: 211\t Train Loss: 14.705\t Val. Loss: 11.672\n",
            "Epoch: 212\t Train Loss: 14.084\t Val. Loss: 11.940\n",
            "Epoch: 213\t Train Loss: 14.087\t Val. Loss: 12.589\n",
            "Epoch: 214\t Train Loss: 14.053\t Val. Loss: 10.192\n",
            "Epoch: 215\t Train Loss: 11.926\t Val. Loss: 11.575\n",
            "Epoch: 216\t Train Loss: 13.178\t Val. Loss: 11.408\n",
            "Epoch: 217\t Train Loss: 13.730\t Val. Loss: 12.440\n",
            "Epoch: 218\t Train Loss: 13.633\t Val. Loss: 12.250\n",
            "Epoch: 219\t Train Loss: 14.971\t Val. Loss: 16.650\n",
            "Epoch: 220\t Train Loss: 13.495\t Val. Loss: 11.534\n",
            "Epoch: 221\t Train Loss: 11.629\t Val. Loss: 9.950\n",
            "Epoch: 222\t Train Loss: 11.618\t Val. Loss: 10.436\n",
            "Epoch: 223\t Train Loss: 11.553\t Val. Loss: 11.030\n",
            "Epoch: 224\t Train Loss: 12.401\t Val. Loss: 11.430\n",
            "Epoch: 225\t Train Loss: 11.288\t Val. Loss: 9.652\n",
            "Epoch: 226\t Train Loss: 11.660\t Val. Loss: 11.333\n",
            "Epoch: 227\t Train Loss: 11.209\t Val. Loss: 10.043\n",
            "Epoch: 228\t Train Loss: 11.546\t Val. Loss: 11.895\n",
            "Epoch: 229\t Train Loss: 11.798\t Val. Loss: 10.262\n",
            "Epoch: 230\t Train Loss: 10.776\t Val. Loss: 10.158\n",
            "Epoch: 231\t Train Loss: 10.243\t Val. Loss: 9.403\n",
            "Epoch: 232\t Train Loss: 10.310\t Val. Loss: 9.903\n",
            "Epoch: 233\t Train Loss: 9.869\t Val. Loss: 8.989\n",
            "Epoch: 234\t Train Loss: 10.340\t Val. Loss: 9.128\n",
            "Epoch: 235\t Train Loss: 9.766\t Val. Loss: 9.029\n",
            "Epoch: 236\t Train Loss: 9.524\t Val. Loss: 8.877\n",
            "Epoch: 237\t Train Loss: 9.570\t Val. Loss: 9.039\n",
            "Epoch: 238\t Train Loss: 9.681\t Val. Loss: 9.005\n",
            "Epoch: 239\t Train Loss: 9.560\t Val. Loss: 9.170\n",
            "Epoch: 240\t Train Loss: 10.279\t Val. Loss: 8.906\n",
            "Epoch: 241\t Train Loss: 9.176\t Val. Loss: 9.010\n",
            "Epoch: 242\t Train Loss: 9.350\t Val. Loss: 8.845\n",
            "Epoch: 243\t Train Loss: 9.105\t Val. Loss: 8.810\n",
            "Epoch: 244\t Train Loss: 8.950\t Val. Loss: 8.808\n",
            "Epoch: 245\t Train Loss: 9.274\t Val. Loss: 8.696\n",
            "Epoch: 246\t Train Loss: 9.186\t Val. Loss: 8.687\n",
            "Epoch: 247\t Train Loss: 8.989\t Val. Loss: 8.683\n",
            "Epoch: 248\t Train Loss: 12.905\t Val. Loss: 15.234\n",
            "Epoch: 249\t Train Loss: 16.108\t Val. Loss: 13.406\n",
            "Epoch: 250\t Train Loss: 17.583\t Val. Loss: 15.198\n",
            "Epoch: 251\t Train Loss: 16.989\t Val. Loss: 11.683\n",
            "Epoch: 252\t Train Loss: 15.259\t Val. Loss: 15.289\n",
            "Epoch: 253\t Train Loss: 16.561\t Val. Loss: 14.280\n",
            "Epoch: 254\t Train Loss: 16.959\t Val. Loss: 14.691\n",
            "Epoch: 255\t Train Loss: 19.223\t Val. Loss: 18.220\n",
            "Epoch: 256\t Train Loss: 16.718\t Val. Loss: 14.944\n",
            "Epoch: 257\t Train Loss: 15.523\t Val. Loss: 11.555\n",
            "Epoch: 258\t Train Loss: 15.250\t Val. Loss: 12.248\n",
            "Epoch: 259\t Train Loss: 16.392\t Val. Loss: 13.260\n",
            "Epoch: 260\t Train Loss: 15.321\t Val. Loss: 12.477\n",
            "Epoch: 261\t Train Loss: 14.534\t Val. Loss: 15.047\n",
            "Epoch: 262\t Train Loss: 16.607\t Val. Loss: 15.005\n",
            "Epoch: 263\t Train Loss: 15.022\t Val. Loss: 13.104\n",
            "Epoch: 264\t Train Loss: 14.322\t Val. Loss: 11.348\n",
            "Epoch: 265\t Train Loss: 13.430\t Val. Loss: 10.740\n",
            "Epoch: 266\t Train Loss: 16.113\t Val. Loss: 11.008\n",
            "Epoch: 267\t Train Loss: 17.747\t Val. Loss: 11.303\n",
            "Epoch: 268\t Train Loss: 13.053\t Val. Loss: 11.913\n",
            "Epoch: 269\t Train Loss: 13.686\t Val. Loss: 11.305\n",
            "Epoch: 270\t Train Loss: 14.756\t Val. Loss: 13.880\n",
            "Epoch: 271\t Train Loss: 13.133\t Val. Loss: 12.622\n",
            "Epoch: 272\t Train Loss: 14.880\t Val. Loss: 13.815\n",
            "Epoch: 273\t Train Loss: 15.031\t Val. Loss: 12.221\n",
            "Epoch: 274\t Train Loss: 15.446\t Val. Loss: 12.304\n",
            "Epoch: 275\t Train Loss: 14.623\t Val. Loss: 12.232\n",
            "Epoch: 276\t Train Loss: 13.911\t Val. Loss: 11.781\n",
            "Epoch: 277\t Train Loss: 13.285\t Val. Loss: 10.822\n",
            "Epoch: 278\t Train Loss: 12.420\t Val. Loss: 10.838\n",
            "Epoch: 279\t Train Loss: 12.668\t Val. Loss: 10.423\n",
            "Epoch: 280\t Train Loss: 11.107\t Val. Loss: 10.815\n",
            "Epoch: 281\t Train Loss: 15.457\t Val. Loss: 11.144\n",
            "Epoch: 282\t Train Loss: 13.433\t Val. Loss: 13.346\n",
            "Epoch: 283\t Train Loss: 12.485\t Val. Loss: 9.791\n",
            "Epoch: 284\t Train Loss: 13.985\t Val. Loss: 12.460\n",
            "Epoch: 285\t Train Loss: 12.538\t Val. Loss: 10.136\n",
            "Epoch: 286\t Train Loss: 12.419\t Val. Loss: 12.002\n",
            "Epoch: 287\t Train Loss: 12.250\t Val. Loss: 10.768\n",
            "Epoch: 288\t Train Loss: 13.265\t Val. Loss: 10.752\n",
            "Epoch: 289\t Train Loss: 12.046\t Val. Loss: 10.473\n",
            "Epoch: 290\t Train Loss: 12.061\t Val. Loss: 10.555\n",
            "Epoch: 291\t Train Loss: 11.217\t Val. Loss: 9.757\n",
            "Epoch: 292\t Train Loss: 10.763\t Val. Loss: 10.204\n",
            "Epoch: 293\t Train Loss: 10.679\t Val. Loss: 9.360\n",
            "Epoch: 294\t Train Loss: 10.846\t Val. Loss: 9.957\n",
            "Epoch: 295\t Train Loss: 10.247\t Val. Loss: 9.383\n",
            "Epoch: 296\t Train Loss: 9.996\t Val. Loss: 9.185\n",
            "Epoch: 297\t Train Loss: 9.656\t Val. Loss: 9.157\n",
            "Epoch: 298\t Train Loss: 10.116\t Val. Loss: 9.451\n",
            "Epoch: 299\t Train Loss: 9.506\t Val. Loss: 9.002\n",
            "Epoch: 300\t Train Loss: 9.551\t Val. Loss: 9.415\n",
            "Epoch: 301\t Train Loss: 9.968\t Val. Loss: 9.033\n",
            "Epoch: 302\t Train Loss: 9.653\t Val. Loss: 8.848\n",
            "Epoch: 303\t Train Loss: 9.261\t Val. Loss: 8.913\n",
            "Epoch: 304\t Train Loss: 9.349\t Val. Loss: 8.824\n",
            "Epoch: 305\t Train Loss: 9.296\t Val. Loss: 8.877\n",
            "Epoch: 306\t Train Loss: 9.020\t Val. Loss: 8.772\n",
            "Epoch: 307\t Train Loss: 9.025\t Val. Loss: 8.767\n",
            "Epoch: 308\t Train Loss: 8.951\t Val. Loss: 8.752\n",
            "Epoch: 309\t Train Loss: 9.139\t Val. Loss: 8.752\n",
            "Epoch: 310\t Train Loss: 18.473\t Val. Loss: 12.974\n",
            "Epoch: 311\t Train Loss: 15.955\t Val. Loss: 14.740\n",
            "Epoch: 312\t Train Loss: 14.544\t Val. Loss: 23.766\n",
            "Epoch: 313\t Train Loss: 19.582\t Val. Loss: 15.640\n",
            "Epoch: 314\t Train Loss: 16.086\t Val. Loss: 13.917\n",
            "Epoch: 315\t Train Loss: 15.153\t Val. Loss: 14.517\n",
            "Epoch: 316\t Train Loss: 15.002\t Val. Loss: 11.869\n",
            "Epoch: 317\t Train Loss: 19.206\t Val. Loss: 20.404\n",
            "Epoch: 318\t Train Loss: 21.755\t Val. Loss: 15.105\n",
            "Epoch: 319\t Train Loss: 15.456\t Val. Loss: 13.445\n",
            "Epoch: 320\t Train Loss: 16.847\t Val. Loss: 14.652\n",
            "Epoch: 321\t Train Loss: 16.982\t Val. Loss: 11.676\n",
            "Epoch: 322\t Train Loss: 15.214\t Val. Loss: 14.818\n",
            "Epoch: 323\t Train Loss: 15.361\t Val. Loss: 16.107\n",
            "Epoch: 324\t Train Loss: 18.566\t Val. Loss: 15.736\n",
            "Epoch: 325\t Train Loss: 14.648\t Val. Loss: 12.847\n",
            "Epoch: 326\t Train Loss: 14.600\t Val. Loss: 12.586\n",
            "Epoch: 327\t Train Loss: 19.568\t Val. Loss: 17.068\n",
            "Epoch: 328\t Train Loss: 15.426\t Val. Loss: 18.161\n",
            "Epoch: 329\t Train Loss: 14.289\t Val. Loss: 10.964\n",
            "Epoch: 330\t Train Loss: 15.398\t Val. Loss: 12.952\n",
            "Epoch: 331\t Train Loss: 17.822\t Val. Loss: 17.928\n",
            "Epoch: 332\t Train Loss: 18.264\t Val. Loss: 16.077\n",
            "Epoch: 333\t Train Loss: 16.296\t Val. Loss: 14.119\n",
            "Epoch: 334\t Train Loss: 15.761\t Val. Loss: 16.162\n",
            "Epoch: 335\t Train Loss: 12.702\t Val. Loss: 12.750\n",
            "Epoch: 336\t Train Loss: 15.981\t Val. Loss: 23.922\n",
            "Epoch: 337\t Train Loss: 17.368\t Val. Loss: 19.122\n",
            "Epoch: 338\t Train Loss: 15.080\t Val. Loss: 13.367\n",
            "Epoch: 339\t Train Loss: 14.038\t Val. Loss: 13.277\n",
            "Epoch: 340\t Train Loss: 13.518\t Val. Loss: 12.090\n",
            "Epoch: 341\t Train Loss: 13.964\t Val. Loss: 10.167\n",
            "Epoch: 342\t Train Loss: 14.321\t Val. Loss: 14.184\n",
            "Epoch: 343\t Train Loss: 12.437\t Val. Loss: 12.160\n",
            "Epoch: 344\t Train Loss: 13.088\t Val. Loss: 10.252\n",
            "Epoch: 345\t Train Loss: 11.684\t Val. Loss: 10.432\n",
            "Epoch: 346\t Train Loss: 11.775\t Val. Loss: 12.965\n",
            "Epoch: 347\t Train Loss: 13.527\t Val. Loss: 10.597\n",
            "Epoch: 348\t Train Loss: 11.772\t Val. Loss: 10.141\n",
            "Epoch: 349\t Train Loss: 10.987\t Val. Loss: 11.367\n",
            "Epoch: 350\t Train Loss: 11.227\t Val. Loss: 9.719\n",
            "Epoch: 351\t Train Loss: 10.887\t Val. Loss: 10.133\n",
            "Epoch: 352\t Train Loss: 11.907\t Val. Loss: 9.721\n",
            "Epoch: 353\t Train Loss: 12.091\t Val. Loss: 9.880\n",
            "Epoch: 354\t Train Loss: 12.158\t Val. Loss: 10.461\n",
            "Epoch: 355\t Train Loss: 10.874\t Val. Loss: 9.585\n",
            "Epoch: 356\t Train Loss: 10.461\t Val. Loss: 9.849\n",
            "Epoch: 357\t Train Loss: 10.682\t Val. Loss: 9.367\n",
            "Epoch: 358\t Train Loss: 10.132\t Val. Loss: 9.321\n",
            "Epoch: 359\t Train Loss: 10.116\t Val. Loss: 9.520\n",
            "Epoch: 360\t Train Loss: 9.975\t Val. Loss: 9.876\n",
            "Epoch: 361\t Train Loss: 10.024\t Val. Loss: 9.814\n",
            "Epoch: 362\t Train Loss: 9.815\t Val. Loss: 9.128\n",
            "Epoch: 363\t Train Loss: 9.615\t Val. Loss: 9.118\n",
            "Epoch: 364\t Train Loss: 9.427\t Val. Loss: 9.119\n",
            "Epoch: 365\t Train Loss: 9.272\t Val. Loss: 9.144\n",
            "Epoch: 366\t Train Loss: 9.364\t Val. Loss: 9.081\n",
            "Epoch: 367\t Train Loss: 9.049\t Val. Loss: 8.944\n",
            "Epoch: 368\t Train Loss: 9.172\t Val. Loss: 8.875\n",
            "Epoch: 369\t Train Loss: 9.089\t Val. Loss: 8.882\n",
            "Epoch: 370\t Train Loss: 9.252\t Val. Loss: 8.860\n",
            "Epoch: 371\t Train Loss: 9.274\t Val. Loss: 8.861\n",
            "Epoch: 372\t Train Loss: 16.026\t Val. Loss: 14.295\n",
            "Epoch: 373\t Train Loss: 16.629\t Val. Loss: 18.549\n",
            "Epoch: 374\t Train Loss: 15.646\t Val. Loss: 15.006\n",
            "Epoch: 375\t Train Loss: 14.688\t Val. Loss: 17.845\n",
            "Epoch: 376\t Train Loss: 17.740\t Val. Loss: 23.050\n",
            "Epoch: 377\t Train Loss: 18.748\t Val. Loss: 16.626\n",
            "Epoch: 378\t Train Loss: 16.252\t Val. Loss: 14.551\n",
            "Epoch: 379\t Train Loss: 16.590\t Val. Loss: 13.403\n",
            "Epoch: 380\t Train Loss: 16.252\t Val. Loss: 13.696\n",
            "Epoch: 381\t Train Loss: 17.273\t Val. Loss: 20.728\n",
            "Epoch: 382\t Train Loss: 21.880\t Val. Loss: 16.926\n",
            "Epoch: 383\t Train Loss: 17.771\t Val. Loss: 14.787\n",
            "Epoch: 384\t Train Loss: 14.463\t Val. Loss: 13.225\n",
            "Epoch: 385\t Train Loss: 17.886\t Val. Loss: 15.304\n",
            "Epoch: 386\t Train Loss: 22.028\t Val. Loss: 13.738\n",
            "Epoch: 387\t Train Loss: 18.440\t Val. Loss: 16.770\n",
            "Epoch: 388\t Train Loss: 15.792\t Val. Loss: 11.823\n",
            "Epoch: 389\t Train Loss: 17.339\t Val. Loss: 14.609\n",
            "Epoch: 390\t Train Loss: 14.609\t Val. Loss: 13.017\n",
            "Epoch: 391\t Train Loss: 14.764\t Val. Loss: 13.113\n",
            "Epoch: 392\t Train Loss: 16.260\t Val. Loss: 16.080\n",
            "Epoch: 393\t Train Loss: 14.710\t Val. Loss: 9.959\n",
            "Epoch: 394\t Train Loss: 15.571\t Val. Loss: 11.124\n",
            "Epoch: 395\t Train Loss: 12.572\t Val. Loss: 10.686\n",
            "Epoch: 396\t Train Loss: 14.295\t Val. Loss: 13.920\n",
            "Epoch: 397\t Train Loss: 12.825\t Val. Loss: 11.355\n",
            "Epoch: 398\t Train Loss: 12.633\t Val. Loss: 10.765\n",
            "Epoch: 399\t Train Loss: 12.666\t Val. Loss: 10.225\n",
            "Epoch: 400\t Train Loss: 13.309\t Val. Loss: 11.524\n",
            "Epoch: 401\t Train Loss: 11.601\t Val. Loss: 9.606\n",
            "Epoch: 402\t Train Loss: 13.254\t Val. Loss: 11.350\n",
            "Epoch: 403\t Train Loss: 14.150\t Val. Loss: 14.744\n",
            "Epoch: 404\t Train Loss: 13.798\t Val. Loss: 10.108\n",
            "Epoch: 405\t Train Loss: 11.162\t Val. Loss: 10.319\n",
            "Epoch: 406\t Train Loss: 13.902\t Val. Loss: 12.985\n",
            "Epoch: 407\t Train Loss: 12.327\t Val. Loss: 10.454\n",
            "Epoch: 408\t Train Loss: 12.033\t Val. Loss: 12.500\n",
            "Epoch: 409\t Train Loss: 11.876\t Val. Loss: 10.305\n",
            "Epoch: 410\t Train Loss: 12.561\t Val. Loss: 10.851\n",
            "Epoch: 411\t Train Loss: 11.718\t Val. Loss: 9.452\n",
            "Epoch: 412\t Train Loss: 11.041\t Val. Loss: 10.007\n",
            "Epoch: 413\t Train Loss: 11.319\t Val. Loss: 11.127\n",
            "Epoch: 414\t Train Loss: 11.493\t Val. Loss: 9.535\n",
            "Epoch: 415\t Train Loss: 10.322\t Val. Loss: 9.463\n",
            "Epoch: 416\t Train Loss: 10.530\t Val. Loss: 10.055\n",
            "Epoch: 417\t Train Loss: 10.737\t Val. Loss: 9.487\n",
            "Epoch: 418\t Train Loss: 10.461\t Val. Loss: 9.143\n",
            "Epoch: 419\t Train Loss: 10.121\t Val. Loss: 9.294\n",
            "Epoch: 420\t Train Loss: 10.242\t Val. Loss: 9.108\n",
            "Epoch: 421\t Train Loss: 10.033\t Val. Loss: 9.201\n",
            "Epoch: 422\t Train Loss: 9.997\t Val. Loss: 9.331\n",
            "Epoch: 423\t Train Loss: 10.179\t Val. Loss: 9.493\n",
            "Epoch: 424\t Train Loss: 9.796\t Val. Loss: 9.397\n",
            "Epoch: 425\t Train Loss: 9.487\t Val. Loss: 9.346\n",
            "Epoch: 426\t Train Loss: 9.552\t Val. Loss: 8.919\n",
            "Epoch: 427\t Train Loss: 9.258\t Val. Loss: 8.989\n",
            "Epoch: 428\t Train Loss: 9.121\t Val. Loss: 8.843\n",
            "Epoch: 429\t Train Loss: 9.277\t Val. Loss: 8.865\n",
            "Epoch: 430\t Train Loss: 9.210\t Val. Loss: 8.820\n",
            "Epoch: 431\t Train Loss: 9.016\t Val. Loss: 8.775\n",
            "Epoch: 432\t Train Loss: 8.962\t Val. Loss: 8.765\n",
            "Epoch: 433\t Train Loss: 9.020\t Val. Loss: 8.762\n",
            "Epoch: 434\t Train Loss: 17.251\t Val. Loss: 11.484\n",
            "Epoch: 435\t Train Loss: 21.040\t Val. Loss: 21.844\n",
            "Epoch: 436\t Train Loss: 16.624\t Val. Loss: 14.721\n",
            "Epoch: 437\t Train Loss: 14.759\t Val. Loss: 14.825\n",
            "Epoch: 438\t Train Loss: 15.174\t Val. Loss: 16.939\n",
            "Epoch: 439\t Train Loss: 19.392\t Val. Loss: 12.409\n",
            "Epoch: 440\t Train Loss: 16.910\t Val. Loss: 16.096\n",
            "Epoch: 441\t Train Loss: 19.865\t Val. Loss: 12.847\n",
            "Epoch: 442\t Train Loss: 13.701\t Val. Loss: 11.074\n",
            "Epoch: 443\t Train Loss: 13.606\t Val. Loss: 10.892\n",
            "Epoch: 444\t Train Loss: 13.836\t Val. Loss: 15.176\n",
            "Epoch: 445\t Train Loss: 15.589\t Val. Loss: 14.878\n",
            "Epoch: 446\t Train Loss: 13.347\t Val. Loss: 10.998\n",
            "Epoch: 447\t Train Loss: 16.732\t Val. Loss: 13.319\n",
            "Epoch: 448\t Train Loss: 14.800\t Val. Loss: 13.348\n",
            "Epoch: 449\t Train Loss: 14.350\t Val. Loss: 12.206\n",
            "Epoch: 450\t Train Loss: 16.931\t Val. Loss: 16.574\n",
            "Epoch: 451\t Train Loss: 14.575\t Val. Loss: 10.627\n",
            "Epoch: 452\t Train Loss: 13.202\t Val. Loss: 10.330\n",
            "Epoch: 453\t Train Loss: 15.341\t Val. Loss: 11.340\n",
            "Epoch: 454\t Train Loss: 16.874\t Val. Loss: 13.419\n",
            "Epoch: 455\t Train Loss: 14.583\t Val. Loss: 15.387\n",
            "Epoch: 456\t Train Loss: 15.586\t Val. Loss: 12.969\n",
            "Epoch: 457\t Train Loss: 14.456\t Val. Loss: 12.225\n",
            "Epoch: 458\t Train Loss: 13.854\t Val. Loss: 11.456\n",
            "Epoch: 459\t Train Loss: 13.177\t Val. Loss: 12.343\n",
            "Epoch: 460\t Train Loss: 12.830\t Val. Loss: 9.962\n",
            "Epoch: 461\t Train Loss: 14.956\t Val. Loss: 14.457\n",
            "Epoch: 462\t Train Loss: 14.673\t Val. Loss: 15.099\n",
            "Epoch: 463\t Train Loss: 14.311\t Val. Loss: 13.469\n",
            "Epoch: 464\t Train Loss: 13.338\t Val. Loss: 11.118\n",
            "Epoch: 465\t Train Loss: 13.492\t Val. Loss: 11.241\n",
            "Epoch: 466\t Train Loss: 14.794\t Val. Loss: 13.488\n",
            "Epoch: 467\t Train Loss: 12.310\t Val. Loss: 11.134\n",
            "Epoch: 468\t Train Loss: 15.995\t Val. Loss: 15.771\n",
            "Epoch: 469\t Train Loss: 14.506\t Val. Loss: 10.323\n",
            "Epoch: 470\t Train Loss: 12.520\t Val. Loss: 11.754\n",
            "Epoch: 471\t Train Loss: 12.733\t Val. Loss: 11.713\n",
            "Epoch: 472\t Train Loss: 12.624\t Val. Loss: 13.158\n",
            "Epoch: 473\t Train Loss: 12.217\t Val. Loss: 12.107\n",
            "Epoch: 474\t Train Loss: 12.156\t Val. Loss: 11.310\n",
            "Epoch: 475\t Train Loss: 12.492\t Val. Loss: 10.066\n",
            "Epoch: 476\t Train Loss: 11.375\t Val. Loss: 10.910\n",
            "Epoch: 477\t Train Loss: 10.823\t Val. Loss: 9.835\n",
            "Epoch: 478\t Train Loss: 11.675\t Val. Loss: 10.711\n",
            "Epoch: 479\t Train Loss: 10.504\t Val. Loss: 9.449\n",
            "Epoch: 480\t Train Loss: 11.046\t Val. Loss: 10.307\n",
            "Epoch: 481\t Train Loss: 10.472\t Val. Loss: 9.779\n",
            "Epoch: 482\t Train Loss: 10.142\t Val. Loss: 9.227\n",
            "Epoch: 483\t Train Loss: 9.691\t Val. Loss: 10.036\n",
            "Epoch: 484\t Train Loss: 9.830\t Val. Loss: 9.405\n",
            "Epoch: 485\t Train Loss: 9.309\t Val. Loss: 9.023\n",
            "Epoch: 486\t Train Loss: 9.940\t Val. Loss: 9.166\n",
            "Epoch: 487\t Train Loss: 9.573\t Val. Loss: 9.372\n",
            "Epoch: 488\t Train Loss: 9.335\t Val. Loss: 9.071\n",
            "Epoch: 489\t Train Loss: 9.239\t Val. Loss: 9.059\n",
            "Epoch: 490\t Train Loss: 9.147\t Val. Loss: 8.956\n",
            "Epoch: 491\t Train Loss: 9.288\t Val. Loss: 8.983\n",
            "Epoch: 492\t Train Loss: 9.050\t Val. Loss: 8.940\n",
            "Epoch: 493\t Train Loss: 9.071\t Val. Loss: 8.940\n",
            "Epoch: 494\t Train Loss: 9.055\t Val. Loss: 8.910\n",
            "Epoch: 495\t Train Loss: 9.210\t Val. Loss: 8.905\n",
            "Epoch: 496\t Train Loss: 15.958\t Val. Loss: 14.921\n",
            "Epoch: 497\t Train Loss: 13.730\t Val. Loss: 14.099\n",
            "Epoch: 498\t Train Loss: 15.314\t Val. Loss: 13.538\n",
            "Epoch: 499\t Train Loss: 17.598\t Val. Loss: 13.357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8R89hbqJp6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RMSELoss(yhat,y):\n",
        "    return torch.sqrt(torch.mean((yhat-y)**2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OFaN6Rx1Njn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, criterion):\n",
        "    model.load_state_dict(torch.load('content/models/model.pt'))\n",
        "    model.eval()\n",
        "    test_losses = []\n",
        "    with torch.no_grad():\n",
        "      for val_acc, configs, targets in test_data_loader:\n",
        "        output = model([val_acc, configs], targets, 0)\n",
        "        output = output.squeeze()\n",
        "        output = torch.t(output)\n",
        "        loss = criterion(output[:,-1], targets[:,-1])\n",
        "        test_losses.append(loss.item())\n",
        "        \n",
        "    return np.mean(test_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vKyR0Rq1JxW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "209fc76b-4389-4c8e-8bb5-509d7eb1250d"
      },
      "source": [
        "mse_test = test(model, criterion)\n",
        "print(\"MSE Test loss: {:.3f}\".format(np.mean(mse_test)))\n",
        "rmse_test = test(model, RMSELoss)\n",
        "print(\"RMSE Test loss: {:.3f}\".format(np.mean(rmse_test)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE Test loss: 4.995\n",
            "RMSE Test loss: 2.117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GTtrISY_Xru",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "2a942b37-88c8-49df-b397-147bc6e96cce"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_curve(model):\n",
        "  model.load_state_dict(torch.load('content/models/model.pt'))\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for val_acc, configs, targets in test_data_loader:\n",
        "        output = model([val_acc, configs], targets, 0)\n",
        "        output = output.squeeze()\n",
        "        output = torch.t(output)\n",
        "        plt.plot(np.arange(51),targets[2],c='g')\n",
        "        plt.plot(np.arange(51),output[2],c='r')\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "plot_curve(model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1P8HH54eRivLxuEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}