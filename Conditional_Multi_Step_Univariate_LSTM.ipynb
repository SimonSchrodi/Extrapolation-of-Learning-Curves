{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "Conditional Multi-Step Univariate LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x5tMN7p1iqoZ",
        "IiSCqyhijWzd"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJd_VxDKN8pn",
        "colab_type": "text"
      },
      "source": [
        "# Task A: Creating a Performance Predictor\n",
        "\n",
        "In this task, you will use training data from 2000 configurations on a single OpenML dataset to train a performance predictor. The data will be splitted into train, test and validation set and we will only use the first 10 epochs of the learning curves for predicitons. You are provided with the full benchmark logs for Fashion-MNIST, that is learning curves, config parameters and gradient statistics, and you can use them freely.\n",
        "\n",
        "For questions, you can contact zimmerl@informatik.uni-freiburg.\n",
        "\n",
        "__Note: Please use the dataloading and splits you are provided with in this notebook.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqPBWpd5N8pt",
        "colab_type": "text"
      },
      "source": [
        "## Specifications:\n",
        "\n",
        "* Data: fashion_mnist.json\n",
        "* Number of datasets: 1\n",
        "* Number of configurations: 2000\n",
        "* Number of epochs seed during prediction: 10\n",
        "* Available data: Learning curves, architecture parameters and hyperparameters, gradient statistics \n",
        "* Target: Final validation accuracy\n",
        "* Evaluation metric: MSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eYlSG9BN8py",
        "colab_type": "text"
      },
      "source": [
        "## Importing and splitting data\n",
        "\n",
        "__Note__: There are 51 steps logged, 50 epochs plus the 0th epoch, prior to any weight updates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX6ZD931iPpX",
        "colab_type": "code",
        "outputId": "8eeaff4d-32f0-4002-c090-a9a44010872b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        }
      },
      "source": [
        "!pip install torchvision\n",
        "!pip install hpbandster"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.2.2)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Processing /root/.cache/pip/wheels/9d/57/62/6b00c8011bac96e0c404adc5be4e16964ba4544614240b4e23/hpbandster-0.7.4-cp36-none-any.whl\n",
            "Collecting Pyro4\n",
            "  Using cached https://files.pythonhosted.org/packages/e5/24/123d570d039bbe167f266025c2551683e115ffa475ed5d66208b0c8c97c1/Pyro4-4.78-py2.py3-none-any.whl\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from hpbandster) (0.10.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hpbandster) (1.4.1)\n",
            "Collecting netifaces\n",
            "  Using cached https://files.pythonhosted.org/packages/0c/9b/c4c7eb09189548d45939a3d3a6b3d53979c67d124459b27a094c365c347f/netifaces-0.10.9-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting serpent\n",
            "  Using cached https://files.pythonhosted.org/packages/b4/a1/24871492bfc34ea18aee3bf38e0cee22d8c11d8d5e765ccc921103140747/serpent-1.30.2-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hpbandster) (1.17.5)\n",
            "Processing /root/.cache/pip/wheels/ae/b2/85/feafec2387f97065d914a72c42ff3a0f6f60d8e1fd03c6bd4b/ConfigSpace-0.4.12-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->hpbandster) (0.5.1)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from statsmodels->hpbandster) (0.25.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from ConfigSpace->hpbandster) (0.29.14)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace->hpbandster) (2.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.0->statsmodels->hpbandster) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels->hpbandster) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels->hpbandster) (2.6.1)\n",
            "Installing collected packages: serpent, Pyro4, netifaces, ConfigSpace, hpbandster\n",
            "Successfully installed ConfigSpace-0.4.12 Pyro4-4.78 hpbandster-0.7.4 netifaces-0.10.9 serpent-1.30.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT8tEKUJOqNy",
        "colab_type": "code",
        "outputId": "567ac7ac-4d8b-444d-cad7-71ee980c9089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "!pip install wget\n",
        "!pip install zipfile36"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=4eb4570024b04e68dc7c2bb375e4a5c16a497cb85d20707d1d853c088ba61629\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting zipfile36\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/8a/3b7da0b0bd87d1ef05b74207827c72d348b56a0d6d83242582be18a81e02/zipfile36-0.1.3-py3-none-any.whl\n",
            "Installing collected packages: zipfile36\n",
            "Successfully installed zipfile36-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GSY73FpOPdD",
        "colab_type": "code",
        "outputId": "4b293af4-a3e8-4094-df1b-ad0f76c9bf8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import wget\n",
        "import zipfile\n",
        "dir_path = 'content/'\n",
        "filename=wget.download('https://ndownloader.figshare.com/files/21001311')\n",
        "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"\")\n",
        "!rm fashion_mnist.zip\n",
        "wget.download('https://raw.githubusercontent.com/automl/LCBench/master/api.py')\n",
        "wget.download('https://raw.githubusercontent.com/infomon/Extrapolation-of-Learning-Curves/master/utils.py')\n",
        "!mkdir content/models\n",
        "!mkdir models"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘content/models’: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql1OqCfgN8p4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "%cd ..\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from content.api import Benchmark\n",
        "import content.utils as utils\n",
        "import torch\n",
        "\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKGiEJ_IidBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ConfigSpace as CS\n",
        "import ConfigSpace.hyperparameters as CSH\n",
        "\n",
        "import pickle\n",
        "import logging\n",
        "\n",
        "from hpbandster.core.worker import Worker\n",
        "import hpbandster.core.nameserver as hpns\n",
        "import hpbandster.core.result as hpres\n",
        "from hpbandster.optimizers import BOHB\n",
        "\n",
        "logging.getLogger('hpbandster').setLevel(logging.DEBUG)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2sQv_05N8qJ",
        "colab_type": "code",
        "outputId": "04555dce-f48b-41b1-c2d6-c1614739b6b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "bench_dir = dir_path+\"fashion_mnist.json\"\n",
        "bench = Benchmark(bench_dir, cache=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Loading data...\n",
            "==> No cached data found or cache set to False.\n",
            "==> Reading json data...\n",
            "==> Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eq4DxuuN8qe",
        "colab_type": "code",
        "outputId": "b14f8b02-a4f4-4e3a-86f0-931bec3bad46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Read data\n",
        "def cut_data(data, cut_position):\n",
        "    targets = []\n",
        "    for dp in data:\n",
        "        targets.append(dp[\"Train/val_accuracy\"][50])\n",
        "        for tag in dp:\n",
        "            if tag.startswith(\"Train/\"):\n",
        "                dp[tag] = dp[tag][0:cut_position]\n",
        "    return data, targets\n",
        "\n",
        "def read_data():\n",
        "    dataset_name = 'Fashion-MNIST'\n",
        "    n_configs = bench.get_number_of_configs(dataset_name)\n",
        "    \n",
        "    # Query API\n",
        "    data = []\n",
        "    for config_id in range(n_configs):\n",
        "        data_point = dict()\n",
        "        data_point[\"config\"] = bench.query(dataset_name=dataset_name, tag=\"config\", config_id=config_id)\n",
        "        for tag in bench.get_queriable_tags(dataset_name=dataset_name, config_id=config_id):\n",
        "            if tag.startswith(\"Train/\"):\n",
        "                data_point[tag] = bench.query(dataset_name=dataset_name, tag=tag, config_id=config_id)    \n",
        "        data.append(data_point)\n",
        "        \n",
        "    # Split: 50% train, 25% validation, 25% test (the data is already shuffled)\n",
        "    indices = np.arange(n_configs)\n",
        "    ind_train = indices[0:int(np.floor(0.5*n_configs))]\n",
        "    ind_val = indices[int(np.floor(0.5*n_configs)):int(np.floor(0.75*n_configs))]\n",
        "    ind_test = indices[int(np.floor(0.75*n_configs)):]\n",
        "\n",
        "    array_data = np.array(data)\n",
        "    train_data = array_data[ind_train]\n",
        "    val_data = array_data[ind_val]\n",
        "    test_data = array_data[ind_test]\n",
        "    \n",
        "    # Cut curves for validation and test\n",
        "    cut_position = 11\n",
        "    #val_data, val_targets = cut_data(val_data, cut_position)\n",
        "    #test_data, test_targets = cut_data(test_data, cut_position)\n",
        "    val_data, val_targets = cut_data(val_data, 51)\n",
        "    test_data, test_targets = cut_data(test_data, 51)\n",
        "    train_data, train_targets = cut_data(train_data, 51)   # Cut last value as it is repeated\n",
        "    \n",
        "    return train_data, val_data, test_data, train_targets, val_targets, test_targets\n",
        "    \n",
        "train_data, val_data, test_data, train_targets, val_targets, test_targets = read_data()\n",
        "\n",
        "print(\"Train:\", len(train_data))\n",
        "print(\"Validation:\", len(val_data))\n",
        "print(\"Test:\", len(test_data))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 1000\n",
            "Validation: 500\n",
            "Test: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PphdZ2aN8qv",
        "colab_type": "text"
      },
      "source": [
        "The data contains the configuration of the trained model and learning curves as well as global and layer-wise gradient statistics.\n",
        "\n",
        "__Note__: Not all parameters vary across different configurations. The varying parameters are batch_size, max_dropout, max_units, num_layers, learning_rate, momentum, weight_decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5tMN7p1iqoZ",
        "colab_type": "text"
      },
      "source": [
        "## Sample use of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybnns3VKN8q0",
        "colab_type": "code",
        "outputId": "52e77e22-fd95-46dc-f3f7-a8f1b9e9e65e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Config\n",
        "print(\"Config example:\", train_data[0][\"config\"])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Config example: {'batch_size': 71, 'imputation_strategy': 'mean', 'learning_rate_scheduler': 'cosine_annealing', 'loss': 'cross_entropy_weighted', 'network': 'shapedmlpnet', 'max_dropout': 0.025926231827891333, 'normalization_strategy': 'standardize', 'optimizer': 'sgd', 'cosine_annealing_T_max': 50, 'cosine_annealing_eta_min': 1e-08, 'activation': 'relu', 'max_units': 293, 'mlp_shape': 'funnel', 'num_layers': 3, 'learning_rate': 0.0018243300267253295, 'momentum': 0.21325193168301043, 'weight_decay': 0.020472816917443872}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua6h3ORrlA5N",
        "colab_type": "code",
        "outputId": "7f556b6a-2226-4277-9ae5-0e750d4b7600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "train_data[1][\"config\"]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu',\n",
              " 'batch_size': 457,\n",
              " 'cosine_annealing_T_max': 50,\n",
              " 'cosine_annealing_eta_min': 1e-08,\n",
              " 'imputation_strategy': 'mean',\n",
              " 'learning_rate': 0.01239328605026128,\n",
              " 'learning_rate_scheduler': 'cosine_annealing',\n",
              " 'loss': 'cross_entropy_weighted',\n",
              " 'max_dropout': 0.5472322491757223,\n",
              " 'max_units': 950,\n",
              " 'mlp_shape': 'funnel',\n",
              " 'momentum': 0.16411425552061212,\n",
              " 'network': 'shapedmlpnet',\n",
              " 'normalization_strategy': 'standardize',\n",
              " 'num_layers': 4,\n",
              " 'optimizer': 'sgd',\n",
              " 'weight_decay': 0.09762768273307641}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTl67TOTN8rD",
        "colab_type": "code",
        "outputId": "3cfd0e25-512b-4544-cdb6-8a121428ae1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Learning curve\n",
        "plt.plot(train_data[10][\"Train/val_accuracy\"])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff14ebb5630>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXGuPeBKN8rM",
        "colab_type": "code",
        "outputId": "2e084e4e-1027-4a0a-e2a5-92c4c7a79b8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Gradient statistics\n",
        "plt.plot(train_data[10][\"Train/layer_wise_gradient_mean_layer_0\"])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff14e67ea58>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAD4CAYAAAA6j0u4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3de3xV5Z3v8c9v751swjWJhBgCiCjg\n4B2jYGuttYpoLzjT1tNOp1BHRMeec6adnrZ22jnO0XbGmTMzbZ3OOEM9KGgvYrWFtlpFKs6MFiXg\nBUQF5B4CBBIuIZDr7/yxV2Ab906EfVm5fN+v137ttZ79rOeiIb88l72WuTsiIiK5FAm7ASIi0v8p\n2IiISM4p2IiISM4p2IiISM4p2IiISM7Fwm5AbzVy5EgfP3582M0QEelTVq9evc/dy7qmK9ikMX78\neKqrq8NuhohIn2Jm21KlaxpNRERyTsFGRERyTsFGRERyTsFGRERyTsFGRERyLivBxsxmmtnbZrbJ\nzO5M8XnczB4NPn/JzMYnffbNIP1tM7uupzLN7MygjE1BmYWnWoeIiORHxsHGzKLAvwDXA1OAz5nZ\nlC7ZbgEa3P1s4HvA3wXXTgE+C5wLzAT+1cyiPZT5d8D3grIagrJPuo5M+y0iIu9fNr5ncxmwyd03\nA5jZz4BZwPqkPLOAvw6Ofw780MwsSP+ZuzcDW8xsU1Aeqco0szeBq4E/DvIsDMq9/xTq+H0W+v4e\nD72whfojLSk/u3JSGVXjS3NRrYhIr5aNYFMJ7Eg63wlMS5fH3dvM7CBwWpC+ssu1lcFxqjJPAw64\ne1uK/KdSx7uY2TxgHsC4cePSdrg7P3l5Oxv3Nr4n3R1Wbqln8W2Xn1K5IiJ9me4gkMTd5wPzAaqq\nqk7pqXLPfOXDKdO/+ODLaUc8IiL9XTY2CNQAY5POxwRpKfOYWQwYAezv5tp06fuB4qCMrnWdbB15\nFY9FaG7tyHe1IiK9QjaCzSpgYrBLrJDEYvzSLnmWAnOC408Dv/PE86iXAp8NdpKdCUwEXk5XZnDN\nc0EZBGUuOcU68ioei9LSrmAjIgNTxtNowfrIfweeBqLAAnd/w8zuBqrdfSnw/4CHg8X5ehLBgyDf\nYhKbCdqAL7l7O0CqMoMqvwH8zMy+A7wSlM2p1JFPiZFN3qsVEekVLPHHv3RVVVXl2bzr87d/uZan\n1u5m9V9dm7UyRUR6GzNb7e5VXdN1B4E8iceiNLdpGk1EBiYFmzyJxyI0t2kaTUQGJgWbPInHorS2\nO+0dmrYUkYFHwSZP4gWJ/9QtmkoTkQFIwSZPCqOJ/9SaShORgUjBJk86RzbaJCAiA5GCTZ7EY4kb\nTWsaTUQGIgWbPInHNI0mIgOXgk2edAabY7o/mogMQAo2eRIvSEyjac1GRAYiBZs80TSaiAxkCjZ5\nciLYaGQjIgOPgk2eFHYGG63ZiMgApGCTJ51bnzWNJiIDkR4LnSed02j6no1I3+DuNLd1cKy1nWOt\nHRxtbedoSztHW9s51tpOS1sHzW0dtLR30NKWeLW2d76cts7jjsRxS5C3uS3xeUtbooy2jsQ9E9s6\nnI7Od0+kdXiiHe7Q4R68oLW9g7b2RN62jg7a2xOfRczAIGJGxMDMAGgP6mjvcNr9xD0aoxEjFrGk\n9wixiLHia1cxKNjUlC0KNnmiOwiI5E97h3PoaCsHj7Zy4GgrB5paOHi0lUNHWznc3EbjsTYag/fD\nzW0c6Xy1tL/rOBs3zi2IGrFIhMJY8Iq++z0WTfyij5gRL4hQZCd++UMiaETMiETAOPFZLGrEoong\nEItEMON4UIITwQkgFokQMSMagWgkQnD3LNo7oL3j3QGvvd2DurNLwSZPTkyjKdiIvF8dHU5DUwv1\nR1poaGqloamFA00njg82JQJK11djcxvdPRcyFjGGDYoxdFCMIYUxhsZjFA8uZExJjMGFUYbEYwyJ\nRxlcGKOoIEpRYZSigiiDCiIMKogyqCBKPAge8ViEwmj0eDApiBoFQRCIRuz46GKgU7DJE219FjnB\n3Tl4tJXag8eoPXiUXQcS73sPNVPX2Ezd4Wb2NTazr7El7eiiMBphxOACiosKGF5UQPnwQUwqH8aI\n4Ly4qIDiwYnXiKJCigcXMHxQAcMGxYjHIgoCeZZRsDGzUuBRYDywFbjJ3RtS5JsDfDs4/Y67LwzS\nLwEeAoqAJ4E/d3dPV64lfjp+ANwANAFfdPc1PdTxXWA2UOLuQzPpbybi2o0mA8ix1nZ2NjRRc+AY\ntQeOsutg4r324DF2HTzK7oPHaGp59x9e0YhRNjRO2bA45cMHce7o4ZQNi1M2NE7p0DglgwsoGZwI\nGiWDCxlcGFXA6EMyHdncCSx393vN7M7g/BvJGYLAcRdQBTiw2syWBkHpfuBW4CUSwWYm8FQ35V4P\nTAxe04Lrp/VQx6+AHwIbM+xrRsyMwlhE02jSbxw61srmuiNs2dfItv1NbK9vYkd94n3PoeZ35TWD\nUcPiVIwo4pzTh/GRyaOoGDGIihFFVBQPYvSIIsqGxYnmYK1AeodMg80s4KrgeCGwgi7BBrgOWObu\n9QBmtgyYaWYrgOHuvjJIXwTcSCLYpCt3FrDI3R1YaWbFZlYR5H1PHcBPk8rPsKuZ06Ohpa9xd3Yf\nOsaGPY1s3HOYd+qO8E5dI5vrjrCv8URAMYOK4YMYWzqYKyeWMa50MGNLB1NZUkTFiEGUDx9EQVTf\ntBjIMg025e5eGxzvBspT5KkEdiSd7wzSKoPjrundldtdWanST4qZzQPmAYwbN+5kL+9RXCMb6cUO\nH2tl/a5DvFl7iLf3NLJhz2E27DnM4WNtx/OUDC5gQtlQrj6njAllQ5kwcggTyoYytrTo+CYYkVR6\nDDZm9ixweoqPvpV8Eqy1ZL5PsItclZumrvnAfICqqqqs1xmPRbVmI71Cw5EWXtt5gDd2HWL9rkOs\n23WQbfubjn9ePLiASeXDmHXRaCaXD2Ni+TAmlQ+jdEhhiK2WvqzHYOPu16T7zMz2mFmFu9cG01l7\nU2Sr4cSUGMAYEtNiNcFxcnpNcJyu3BpgbIpr0tXRq8RjEVraFWwkv9raO3h7z2HWbD/AK9sbeHX7\nATbvO3L883Glgzl39HA+c8kYzh09gimjhzNqWLxXTD1L/5HpNNpSYA5wb/C+JEWep4G/MbOS4HwG\n8E13rzezQ2Y2ncQGgdnAP/dQ7lLgv5vZz0hsEDgYBKSUdWTYt6wrjEVobtWajeTWoWOtrNnWwOpt\nDVRvbeC1nQeO7/waObSQi8eV8OmqMVw0tphzR49gRFFByC2WgSDTYHMvsNjMbgG2ATcBmFkVcLu7\nzw2Cyj3AquCauzsX8oE7OLH1+anglbZcEjvWbgA2kdj6fDNAd3WY2d8DfwwMNrOdwAPu/tcZ9vuU\nxAuiWrORrNvf2MyL7+znpS37qd7awNt7DuOe2Er8BxXDuKlqLBePK2bquBLGlBRpxCKhMO/ua7YD\nWFVVlVdXV2e1zJv+/fdEDH427/KslisDS2NzGy9v2c8Lm/bzwqZ9vLX7MABDCqNMPaOEqjNKqRpf\nwkVjixkS1/e2Jb/MbLW7V3VN109iHsVjERqb23rOKNLFzoYmnnljD8+s30311gbaOpzCWISqM0r4\n2nWT+cBZp3F+5Qhi2l4svZSCTR7FY1H2N7aE3QzpA9ydt/cc5ul1iQDzxq5DAEwqH8qtV07girNH\ncskZJVm/M69IrijY5FG8QF/qlPTa2jtYva2BZ9bvYdn6PWyvb8IMpo4r4ZvXn8OMc0/nzJFDwm6m\nyClRsMmjeFRf6pR3a2pp4z827GPZ+j387q09NDS1UhiN8IGzT+O2D0/g2inljBo2KOxmimRMwSaP\nEiMbBZuBrqPDWbl5P4+vqeGpdbU0tbQzfFCMq88ZxYxzT+fKSWUM1cK+9DP6ic6jeCyqJ3UOYO/U\nNfLEmp38Yk0Nuw4eY2g8xicuGM0nLxrNZWeW6t5h0q8p2OSRbsQ58Bxrbec3r9fy45e2sWb7ASIG\nH5pYxp03/AHX/kE5RYVa4JeBQcEmjzpvxOnu+mJdP7e5rpEfv7Sdn6/eycGjrUwoG8Jf3nAON15U\nyajhWoORgUfBJo/iBVHcobXdKYwp2PQ3be0dLFu/h4dXbuPFd/YTixjXnXc6fzLtDKZPKNUfGDKg\nKdjkUfKjoQtjmp/vLxqb23h01Q4efGELOxuOUllcxNeum8xnqsZoJ5lIQMEmj04Emw6GhdwWyVzt\nwaM89MJWfvLydg4fa+PS8SV8+2NTuHZKuZ44KdKFgk0edT5cStuf+7ZNew/zL8+9w69e20WHO9ef\nX8GtH5rARWOLw26aSK+lYJNHnVNnesxA37S5rpH7lm9kyWu7KCqIMvvy8dz8wfGMLR0cdtNEej0F\nmzxKnkaTvmPLviP88/KN/PLVGuKxKPOunMC8D03gtKHxsJsm0mco2ORRvCARbPTFzr6h5sBRvrds\nA794pYaCqHHLFWdy24fPYqSCjMhJU7DJI63Z9A2NzW3cv2ITD/znFhz44gfGc9uHJ2hnmUgGFGzy\nKHnrs/Q+be0dLK7eyT8te5t9jS3ceNFovjbzHCqLi8Jumkifp2CTR8dHNq0a2fQ2z2+o47u/Wc+G\nPY1cOr6EB+Zcqt1lIlmU0TcLzazUzJaZ2cbgvSRNvjlBno1mNicp/RIzW2tmm8zsPgu+Yp2uXEu4\nL8j/uplN7a4OMxtsZr8xs7fM7A0zuzeT/maqc81G02i9x+6Dx7jt4WrmLHiZ5rYO7v/8VBbfdrkC\njUiWZfo19juB5e4+EVgenL+LmZUCdwHTgMuAu5KC0v3ArcDE4DWzh3KvT8o7L7i+pzr+wd3PAS4G\nPmhm12fY51OmabTeo73DeeiFLVzzT8+z4u06vnbdZJ75ypVcf36FbisjkgOZBptZwMLgeCFwY4o8\n1wHL3L3e3RuAZcBMM6sAhrv7Snd3YFHS9enKnQUs8oSVQHFQTso63L3J3Z8DcPcWYA0wJsM+n7JC\nbX3uFdbvOsQf3f8if/2r9Vw8rphnvnIlX/rI2cenOUUk+zJdsyl399rgeDdQniJPJbAj6XxnkFYZ\nHHdN767c7spKlX6cmRUDnwB+kK4zZjaPxIiJcePGpct2yk6s2WhkE4ajLe18/9kNPPBfWygZXMAP\nPnsRn7xwtEYyInnQY7Axs2eB01N89K3kE3d3M/NsNSyb5ZpZDPgpcJ+7b+6mrvnAfICqqqqs96Vz\nGq2lXSObfHth0z7ufOJ1dtQf5bOXjuXO68+heHBh2M0SGTB6DDbufk26z8xsj5lVuHttMJ21N0W2\nGuCqpPMxwIogfUyX9JrgOF25NcDYFNekq6PTfGCju38/XV/y4fiajXaj5c3Bpla+++R6Flfv5MyR\nQ/jZvOlMn3Ba2M0SGXAyXbNZCnTuLpsDLEmR52lghpmVBIv2M4Cng2myQ2Y2PdiFNjvp+nTlLgVm\nB7vSpgMHg3JS1gFgZt8BRgBfzrCvGYtFI0QjpjWbPPntulqu+d7zPL6mhts/fBZP/fmHFGhEQpLp\nms29wGIzuwXYBtwEYGZVwO3uPtfd683sHmBVcM3d7l4fHN8BPAQUAU8Fr7TlAk8CNwCbgCbgZoB0\ndZjZGBLTfW8Ba4K5+R+6+wMZ9vuU6dHQube/sZlv/3IdT63bzZSK4Tz4xUs5r3JE2M0SGdAyCjbu\nvh/4aIr0amBu0vkCYEGafOedRLkOfClNW95Th7vvBHrV6m/no6ElN17bcYDbH1nN/iMtfH3mZG79\n0AQKonpQnUjYdAeBPIvHolqzyZHFq3bw7SXrKBsa54k/+4BGMyK9iIJNnsULNI2WbS1tHdz96zd4\nZOV2rjh7JPd97mJKh2inmUhvomCTZ4VRTaNl095Dx/izH69h9bYGbvvwBL42YzIxTZuJ9DoKNnmW\nGNko2GTDmu0N3P7wag4fa+OHf3wxH79gdNhNEpE0FGzyLB6L6uFpWfCr13bx1cde4/Thg1h0y2Wc\nc/rwsJskIt1QsMkzbX3OjLvzw99t4h+XbeDS8SX8+xeqtD4j0gco2ORZPBahsbkt7Gb0Sc1t7Xzz\nibU8saaGP7y4kns/db5uninSRyjY5Jm2Pp+ahiMt3PbIal7eUs9fXDuJ/3H12bqBpkgfomCTZ9r6\nfPK27DvCzQ++zK4Dx/jBZy9i1kWVPV8kIr2Kgk2e6Q4CJ2ddzUHmLHgZB35y6zSqxpeG3SQROQUK\nNnkWj0UVbN6nVVvr+dMHVzFsUIxH5k5jQtnQsJskIqdIwSbPCmMRPTztfVjx9l5uf2Q1o0cU8fDc\naVQWF4XdJBHJgIJNnmkarWe/eb2WLz/6ChNHDWPRLZcxcmg87CaJSIZ0X488i8eitHU47R1ZfxBo\nv/Doqu38j5+u4cIxxfx03nQFGpF+QsEmz+IFwaOhNbp5jwX/tYVvPL6WKyaW8fAt0xhRVBB2k0Qk\nSxRs8uz4o6G1/fldfvzSNu7+9Xpmnns6D8yuoqhQX9YU6U8UbPKs8xvvWrc54Rev7OTbv1zHRyaX\ncd/nLqYwph9Lkf5G/6rz7PjIRncRAOC363bzvx57nelnnsb9f3KJAo1IP5XRv2wzKzWzZWa2MXgv\nSZNvTpBno5nNSUq/xMzWmtkmM7vPgvuPpCvXEu4L8r9uZlPfRx2/NbPXzOwNM/s3Mwt1fqZzzUbT\naPD8hjr+509f4YIxI/jRnCoGFWjqTKS/yvTPyDuB5e4+EVgenL+LmZUCdwHTgMuAu5KC0v3ArcDE\n4DWzh3KvT8o7L7i+pzpucvcLgfOAMuAzGfY5I5pGS3hp835ue7ias0cN5aEvXsbQuHbhi/RnmQab\nWcDC4HghcGOKPNcBy9y93t0bgGXATDOrAIa7+0p3d2BR0vXpyp0FLPKElUBxUE7KOgDc/VBwbQwo\nBELdc1yoDQK8tuMAtyysprK4iEW3XMaIwdp1JtLfZRpsyt29NjjeDZSnyFMJ7Eg63xmkVQbHXdO7\nK7e7slKlA2BmTwN7gcPAz9N1xszmmVm1mVXX1dWly5aRE7vRBubIZvv+Jm5+aBUlQwr48Vx9j0Zk\noOgx2JjZs2a2LsVrVnK+YHSS9VFDNsp19+uACiAOXN1NvvnuXuXuVWVlZZlUmdZADjYHm1r54kMv\n0+HOoj+dxukjBoXdJBHJkx4nyt39mnSfmdkeM6tw99pgOmtvimw1wFVJ52OAFUH6mC7pNcFxunJr\ngLEprklXR3I/jpnZEhJTccvS9SnXjq/ZDLDdaC1tHdz2SDU764/yyNxpnDlySNhNEpE8ynQabSnQ\nufNrDrAkRZ6ngRlmVhIs2s8Ang6myQ6Z2fRgF9rspOvTlbsUmB3sSpsOHAzKSVmHmQ0NghVmFgM+\nBryVYZ8zMhB3o7k733xiLSs31/P3n76Ay87UYwJEBppMtwDdCyw2s1uAbcBNAGZWBdzu7nPdvd7M\n7gFWBdfc7e71wfEdwENAEfBU8EpbLvAkcAOwCWgCbgZIV4eZlQNLzSxOIrA+B/xbhn3OyECcRvvh\n7zbx+JqdfOWaSdx4sR58JjIQZRRs3H0/8NEU6dXA3KTzBcCCNPnOO4lyHfhSmra8pw533wNc2lM/\n8mmgbX1e8moN/7hsA390cSX/86Nnh90cEQmJvq6dZ8en0QbAM21Wba3na4+9zrQzS/nbT51P8J1d\nERmAFGzyrDA6MKbRag8e5baHVzOmpIh//8Ilx0d0IjIwKdjk2UBYs2lp6+COH6+hubWdH82ponhw\nYdhNEpGQ6R4heWZmFMYi/fp5Nn/z5Ju8sv0A//r5qZxVNjTs5ohIL6CRTQgSj4bun2s2S16t4aEX\ntzL3ijO54fyKsJsjIr2Egk0I4rFov5xG27DnMHc+vpZLx5fwjevPCbs5ItKLKNiEIB6L9Ls7CDQ2\nt3H7I6sZEo/xwz+eSkFUP1oicoLWbEIQL+hf02juzjd+/jrb9jfx47nTKB+ue56JyLvpz88Q9Ldp\ntAUvbOU3a2v5+nWTmT7htLCbIyK9kIJNCBIbBPpHsFm78yB/++SbzJhSzrwrJ4TdHBHppRRsQlAY\ni/SLOwgcbWnny4++wsihcf7+0xfoDgEikpaCTQj6y8jm3qfe5J26I/zDZy7UFzdFpFsKNiGIx6J9\n/kudz2+oY+Hvt3HzB8dzxcSRYTdHRHo5BZsQ9PXdaA1HWvjaY68xcdRQvjFT36cRkZ5p63MI+vI0\nmrvzl79YS0NTCw/efCmDCnSDTRHpmUY2IejLW58fX1PDU+t28xfXTubc0SPCbo6I9BEKNiGI99Hd\naDvqm/jrpW9w2fhSbXMWkZOiYBOCxJpN3xrZdHQ4X138GgD/eNOFRCPa5iwi719GwcbMSs1smZlt\nDN5L0uSbE+TZaGZzktIvMbO1ZrbJzO6z4Isa6cq1hPuC/K+b2dSe6kj6fKmZrcukv9nSOY2WeMp1\n3/DIS9t4eWs9d31iCmNLB4fdHBHpYzId2dwJLHf3icDy4PxdzKwUuAuYBlwG3JUUlO4HbgUmBq+Z\nPZR7fVLeecH1PdWBmf0R0JhhX7Om8wFqLe19Y3RTe/Aof//bt/nQxJF8+pIxYTdHRPqgTIPNLGBh\ncLwQuDFFnuuAZe5e7+4NwDJgpplVAMPdfaUn/sRflHR9unJnAYs8YSVQHJSTsg4AMxsK/AXwnQz7\nmjV97Wmddy15g7aODr574/m6S4CInJJMg025u9cGx7uB8hR5KoEdSec7g7TK4LhrenfldldWqnSA\ne4B/BJp66oyZzTOzajOrrqur6yn7KTs+sukDwea363bzzPo9fPmaSYw7TdNnInJqevyejZk9C5ye\n4qNvJZ+4u5tZ1hchMinXzC4CznL3r5jZ+PdR13xgPkBVVVXOFlTiscR3U3r7yObQsVbuWrqOP6gY\nzi1XnBl2c0SkD+sx2Lj7Nek+M7M9Zlbh7rXBdNbeFNlqgKuSzscAK4L0MV3Sa4LjdOXWAGNTXJOu\njsuBKjPbSqKvo8xshbsn5827eEEwjdbLtz//39++Td3hZuZ/oUoPQxORjGT6G2Qp0Lnzaw6wJEWe\np4EZZlYSLNrPAJ4OpskOmdn0YBfa7KTr05W7FJgd7EqbDhwMyklXx/3uPtrdxwNXABvCDjTQN9Zs\nVm+r55GXtvHFD5zJhWOLw26OiPRxmd6u5l5gsZndAmwDbgIwsyrgdnef6+71ZnYPsCq45m53rw+O\n7wAeAoqAp4JX2nKBJ4EbgE0k1mBuBuihjl6nt0+jtbR18M0n1jJ6RBFfnTEp7OaISD+QUbBx9/3A\nR1OkVwNzk84XAAvS5DvvJMp14Etp2pKyjqTPt6aqKwzHRza9dBrt359/hw17GlnwxSqGxHX7PBHJ\nnCbiQ3B8zaYXjmy27T/CPz+3iY9dUMHV56TaXCgicvIUbEJQGO2902h/++RbxCLG//74lLCbIiL9\niIJNCDpHNr3teza/f2c/v31jN3dcdRblwweF3RwR6UcUbEJwYjda71mzae9wvvOb9VQWFzH3Q7qj\ns4hkl4JNCHrjbrTHV+/kjV2H+Mb15+iBaCKSdQo2Iehtu9Eam9v4v8+8zdRxxXzigoqwmyMi/ZCC\nTQh62260+1dsou5wM3/18Sm60aaI5ISCTQgKo70n2OxsaOJH/7mFP7y4kovHpXwckYhIxhRsQhCL\nRohFrFdsELj3qbeIGHx95uSwmyIi/ZiCTUgKYxGaW8Md2azeVs+vX6/ltivPomJEUahtEZH+TcEm\nJPFYJNRptI4O5+5frad8eJzbPqytziKSWwo2IYnHoqF+qfM3a2t5bedBvn7dOQwu1P3PRCS3FGxC\nEi+IhLZm09bewfee3cDk8mH84cWVPV8gIpIhBZuQhDmNtuTVXWyuO8JXrp1EJKKtziKSewo2IYnH\noqEEm9b2Dn6wfCPnVQ7nunN1V2cRyQ8Fm5AkRjb5n0b7+eqdbK9v4qvXTtYXOEUkbxRsQhIvyP/W\n52Ot7dy3fCNTxxVz1eSyvNYtIgObgk1IwphG+9nL26k9eIyvztCoRkTyK6NgY2alZrbMzDYG7ynv\nd2Jmc4I8G81sTlL6JWa21sw2mdl9FvwGTFeuJdwX5H/dzKa+jzpWmNnbZvZq8BqVSZ+zpTCa32m0\noy3t/MuKd5g+oZQPnHVa3uoVEYHMRzZ3AsvdfSKwPDh/FzMrBe4CpgGXAXclBaX7gVuBicFrZg/l\nXp+Ud15wfU91AHze3S8KXnsz7HNWJLY+529k8/DKrdQdbtaoRkRCkWmwmQUsDI4XAjemyHMdsMzd\n6929AVgGzDSzCmC4u690dwcWJV2frtxZwCJPWAkUB+WkrCPDvuVUPBbJ25c6G5vbuH/FO1w5qYxL\nx5fmpU4RkWSZBptyd68NjncDqfbSVgI7ks53BmmVwXHX9O7K7a6sVOmdHgym0P7Kuvmz3szmmVm1\nmVXX1dWly5YV+VyzeeiFLTQ0tfLVayflpT4Rka56vE+JmT0LnJ7io28ln7i7m5lnq2FZLPfz7l5j\nZsOAx4EvkBhFpaprPjAfoKqqKut9SRaPRfLy8LSDR1uZ/x+buXZKOReOLc55fSIiqfQYbNz9mnSf\nmdkeM6tw99pgOivVekgNcFXS+RhgRZA+pkt6TXCcrtwaYGyKa9LVgbvXBO+HzewnJNZ0UgabfMrX\nms2iF7dy6FgbX7lGoxoRCU+m02hLgc6dX3OAJSnyPA3MMLOSYNF+BvB0ME12yMymB1Nbs5OuT1fu\nUmB2sCttOnAwKCdlHWYWM7ORAGZWAHwcWJdhn7MiHovS1uG0tecu4BxtaefBF7fy0XNGMWX08JzV\nIyLSk0xv93svsNjMbgG2ATcBmFkVcLu7z3X3ejO7B1gVXHO3u9cHx3cADwFFwFPBK225wJPADcAm\noAm4GSBdHWY2hETQKQCiwLPAjzLsc1bEY4k439LeQSyam687La7eQf2RFv7sqrNyUr6IyPuVUbBx\n9/3AR1OkVwNzk84XAAvS5DvvJMp14Etp2vKeOtz9CHBJT/0IQ2ewaW7tYHBh9stvbe9g/n9s5tLx\nJVRpB5qIhEx3EAhJYSwKkLN1m1+9touaA0c1qhGRXkHBJiTHRzY5uItAR4fzb8+/w+TyYXxkcq+4\nYYKIDHAKNiGJFwRrNjkY2W5gOMwAAA1HSURBVPzurb1s2NPIn111lu4WICK9goJNSOI5mkZzd/51\nxSbGlBTx8Qsqslq2iMipUrAJSa6m0VZtbWDN9gPMu3JCzna5iYicLP02CknybrRsun/FJk4bUshn\nLhnbc2YRkTxRsAlJvCD702hv1h7iubfruPmD4ykqjGatXBGRTCnYhCQX02j/9vw7DCmM8oXp47NW\npohINijYhKTweLDJzshmR30Tv3ptF5+ffgYjBhdkpUwRkWxRsAlJttdsHnxhKxEz/vSDZ2alPBGR\nbFKwCcnxrc9ZuBHnkeY2Hlu9gxvOr+D0EYMyLk9EJNsUbELS+aXObDzT5pev1nD4WBuzLz8j47JE\nRHJBwSYk8Syt2bg7i17cxpSK4VxyRkk2miYiknUKNiEpjGYn2Ly0pZ639xxmzgfO0K1pRKTXUrAJ\niZklHg2d4dbnRb/fyoiiAj55YWV2GiYikgMKNiGKxyIZ7UarPXiUp9/Yw3+7dKy+xCkivZqCTYji\nBdGMptF++tJ2Otz5k2naGCAivZuCTYgKo6c+jdbc1s5PXt7O1ZNHMe60wVlumYhIdmUUbMys1MyW\nmdnG4D3ldigzmxPk2Whmc5LSLzGztWa2yczus2CFO125lnBfkP91M5v6PuooNLP5ZrbBzN4ys09l\n0udsihdETnlk89t1u9nX2MLsD4zPbqNERHIg05HNncByd58ILA/O38XMSoG7gGnAZcBdSUHpfuBW\nYGLwmtlDudcn5Z0XXN9THd8C9rr7JGAK8HyGfc6aeCx6yg9PW/jiVs4cOYQPnT0yy60SEcm+TIPN\nLGBhcLwQuDFFnuuAZe5e7+4NwDJgpplVAMPdfaW7O7Ao6fp05c4CFnnCSqA4KCdlHcE1fwr8LYC7\nd7j7vgz7nDWJ3WgnH2zW7jzImu0H+ML0M4hEtN1ZRHq/TINNubvXBse7gfIUeSqBHUnnO4O0yuC4\na3p35XZX1nvSzaw4OL/HzNaY2WNmlqqNAJjZPDOrNrPqurq6dNmyJrEb7eTXbBb9fiuDC6N86pIx\n2W+UiEgO9BhszOxZM1uX4jUrOV8wOvFsNzDDcmPAGOBFd58K/B74h27qmu/uVe5eVVZWdopVvn+n\nshut4UgLS17bxR9eXMmIIt3dWUT6hlhPGdz9mnSfmdkeM6tw99pgOmtvimw1wFVJ52OAFUH6mC7p\nNcFxunJrgLEprklXx36gCXgiSH8MuCVdf/LtVKbRFlfvoKWtg9mXj89No0REciDTabSlQOfOrznA\nkhR5ngZmmFlJsGg/A3g6mCY7ZGbTg11os5OuT1fuUmB2sCttOnAwKCddHQ78ihOB6KPA+gz7nDUn\newcBd+fR6h1UnVHC5NOH5bBlIiLZ1ePIpgf3AovN7BZgG3ATgJlVAbe7+1x3rzeze4BVwTV3u3t9\ncHwH8BBQBDwVvNKWCzwJ3ABsIjFiuRmghzq+ATxsZt8H6jqv6Q3isehJ3UFgzfYGNtcd4fZPnZXD\nVomIZF9Gwcbd95MYLXRNrwbmJp0vABakyXfeSZTrwJfStCVdHduAK7vrR1gKT3IabfGqnQwujHLD\nBRU5bJWISPbpDgIhOplptCPNbfz69V18/IIKhsYzHZCKiOSXgk2I4gWR9/2lzt+sreVISzs3VY3t\nObOISC+jYBOieCyx9TkxO9i9x6p3MGHkED0gTUT6JAWbEHU+rbOlvfvRzea6RlZtbeAzVWP1gDQR\n6ZMUbEL0fh8N/djqnUQjxqem6gFpItI3KdiEKF6QeOBZd9uf29o7eHz1Tj4yuYxRwwflq2kiIlml\nYBOiEyOb9DvSnt9Qx97DzXxGGwNEpA9TsAnR+5lGW1y9g5FDC7n6nFH5apaISNYp2IToeLBJM422\nr7GZ5W/u5Y+mjqEgqv9VItJ36TdYiOKxxJpNut1ov1hTQ1uHc1OVHiUgIn2bgk2IToxs3rtm4+4s\nrt7B1HHFnD1KN90Ukb5NwSZE8YL0azav7jjAxr2NumOAiPQLCjYh6pxGSxVsHlu9k6KCKB/TTTdF\npB9QsAlRuq3PLW0dPLm2lhnnljNskJ7GKSJ9n4JNiI6PbLrsRvuvTXUcaGpl1kWjw2iWiEjWKdiE\nKN2azZJXd1E8uIArzi4Lo1kiIlmnYBOiwuh7p9GaWtp45o093HB+BYUx/e8Rkf5Bv81ClGpks2z9\nHo62tjPrQk2hiUj/kVGwMbNSM1tmZhuD95QPWzGzOUGejWY2Jyn9EjNba2abzOw+C+6fn65cS7gv\nyP+6mU3trg4zG2Zmrya99pnZ9zPpczZ1jmySH6C29NVdVIwYxKXjS8NqlohI1mU6srkTWO7uE4Hl\nwfm7mFkpcBcwDbgMuCspKN0P3ApMDF4zeyj3+qS884Lr09bh7ofd/aLOF7ANeCLDPmdNLBohFrHj\n02gNR1p4fkMdn7xwNJGInlsjIv1HpsFmFrAwOF4I3Jgiz3XAMnevd/cGYBkw08wqgOHuvtITj6pc\nlHR9unJnAYs8YSVQHJSTso7kRpjZJGAU8J8Z9jmr4rHI8d1oT63bTVuH8wlNoYlIP5NpsCl399rg\neDdQniJPJbAj6XxnkFYZHHdN767c7spKlZ7ss8Cj3s0zmM1snplVm1l1XV1dumxZFS+IHl+zWfJq\nDWeVDeHc0cPzUreISL7EespgZs8Cp6f46FvJJ+7uZpb2F/mpymK5nwW+0ENd84H5AFVVVVnvSyrx\nWITmtnZ2HTjKy1vr+co1k/ToZxHpd3oMNu5+TbrPzGyPmVW4e20wnbU3RbYa4Kqk8zHAiiB9TJf0\nmuA4Xbk1wNgU16Sro7OdFwIxd1+dri9hSQSbDn79+i7c4ZOaQhORfijTabSlQOfusjnAkhR5ngZm\nmFlJsDFgBvB0ME12yMymB7vQZiddn67cpcDsYFfadOBgUE7KOpLa8Dngpxn2NSfisSjNrR0seXUX\nF44tZvzIIWE3SUQk6zINNvcC15rZRuCa4BwzqzKzBwDcvR64B1gVvO4O0gDuAB4ANgHvAE91Vy7w\nJLA5yP+j4Pqe6gC4iV4abApjEd7cfYg3dh3Sd2tEpN+ybtbLB7Sqqiqvrq7OeT2fvv9Fqrc1EDFY\n+c2PMmr4oJzXKSKSK2a22t2ruqbrDgIh67yLwOVnnaZAIyL9loJNyDrv/Dzrwq47tUVE+g8Fm5DF\nYxEKoxGuOy/V7nIRkf6hx63Pklufn3YGH55UxogiPSRNRPovBZuQXTFxZNhNEBHJOU2jiYhIzinY\niIhIzinYiIhIzinYiIhIzinYiIhIzinYiIhIzinYiIhIzinYiIhIzumuz2mYWR2w7RQvHwnsy2Jz\n+gL1eWAYaH0eaP2FzPt8hruXdU1UsMkBM6tOdYvt/kx9HhgGWp8HWn8hd33WNJqIiOScgo2IiOSc\ngk1uzA+7ASFQnweGgdbngdZfyFGftWYjIiI5p5GNiIjknIKNiIjknIJNFpnZTDN728w2mdmdYbcn\nV8xsgZntNbN1SWmlZrbMzDYG7yVhtjGbzGysmT1nZuvN7A0z+/MgvT/3eZCZvWxmrwV9/j9B+plm\n9lLwM/6omRWG3dZsM7Oomb1iZr8Ozvt1n81sq5mtNbNXzaw6SMv6z7aCTZaYWRT4F+B6YArwOTOb\nEm6rcuYhYGaXtDuB5e4+EVgenPcXbcBX3X0KMB34UvD/tj/3uRm42t0vBC4CZprZdODvgO+5+9lA\nA3BLiG3MlT8H3kw6Hwh9/oi7X5T0/Zqs/2wr2GTPZcAmd9/s7i3Az4BZIbcpJ9z9P4D6LsmzgIXB\n8ULgxrw2Kofcvdbd1wTHh0n8Iqqkf/fZ3b0xOC0IXg5cDfw8SO9XfQYwszHAx4AHgnOjn/c5jaz/\nbCvYZE8lsCPpfGeQNlCUu3ttcLwbKA+zMbliZuOBi4GX6Od9DqaTXgX2AsuAd4AD7t4WZOmPP+Pf\nB74OdATnp9H/++zAM2a22szmBWlZ/9mOZVqASFfu7mbW7/bUm9lQ4HHgy+5+KPFHb0J/7LO7twMX\nmVkx8AvgnJCblFNm9nFgr7uvNrOrwm5PHl3h7jVmNgpYZmZvJX+YrZ9tjWyypwYYm3Q+JkgbKPaY\nWQVA8L435PZklZkVkAg0P3b3J4Lkft3nTu5+AHgOuBwoNrPOP1L728/4B4FPmtlWEtPgVwM/oH/3\nGXevCd73kvij4jJy8LOtYJM9q4CJwc6VQuCzwNKQ25RPS4E5wfEcYEmIbcmqYN7+/wFvuvs/JX3U\nn/tcFoxoMLMi4FoSa1XPAZ8OsvWrPrv7N919jLuPJ/Hv93fu/nn6cZ/NbIiZDes8BmYA68jBz7bu\nIJBFZnYDiTnfKLDA3b8bcpNywsx+ClxF4lbke4C7gF8Ci4FxJB7NcJO7d91E0CeZ2RXAfwJrOTGX\n/5ck1m36a58vILEwHCXxR+lid7/bzCaQ+Ku/FHgF+BN3bw6vpbkRTKP9L3f/eH/uc9C3XwSnMeAn\n7v5dMzuNLP9sK9iIiEjOaRpNRERyTsFGRERyTsFGRERyTsFGRERyTsFGRERyTsFGRERyTsFGRERy\n7v8DbRIJDShuWn8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaYC_PH4N8rQ",
        "colab_type": "text"
      },
      "source": [
        "## A simple baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJrvEgLzN8rS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleLearningCurvePredictor():\n",
        "    \"\"\"A learning curve predictor that predicts the last observed epoch of the validation accuracy as final performance\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        pass\n",
        "    \n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for datapoint in X:\n",
        "            predictions.append(datapoint[\"Train/val_accuracy\"][-1])\n",
        "        return predictions\n",
        "    \n",
        "def score(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d23UZhgHN8rW",
        "colab_type": "code",
        "outputId": "2d6fa30a-16ea-4b38-8fc9-321cb956ad28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Training & tuning\n",
        "predictor = SimpleLearningCurvePredictor()\n",
        "predictor.fit(train_data, train_targets)\n",
        "preds = predictor.predict(val_data)\n",
        "mse = score(val_targets, preds)\n",
        "print(\"Score on validation set:\", mse)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score on validation set: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMWiY0XbN8rc",
        "colab_type": "code",
        "outputId": "e4d59dfe-ad0b-4ed1-b103-912e5cc0885a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Final evaluation (after tuning)\n",
        "final_preds = predictor.predict(test_data)\n",
        "final_score = score(test_targets, final_preds)\n",
        "print(\"Final test score:\", final_score)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final test score: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNOcxEQji6du",
        "colab_type": "text"
      },
      "source": [
        "## Conditional Multi-Step Univariate LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqojlKAujgIt",
        "colab_type": "text"
      },
      "source": [
        "### Data Preperation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnEQJbbLyGG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_loader = utils.prep_data(train_data, train_targets, batch_size=32,normalization_factor_temporal_data=[100], one_shot=True)\n",
        "val_data_loader = utils.prep_data(val_data, val_targets, batch_size=32,normalization_factor_temporal_data=[100], one_shot=True)\n",
        "test_data_loader = utils.prep_data(test_data, test_targets, batch_size=32,normalization_factor_temporal_data=[100],one_shot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt-kazBujFr0",
        "colab_type": "text"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBGim3i6CoCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, nof_configs, num_layers, dropout = 0.5, bidirectional=False):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        self.nof_configs = nof_configs\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = torch.nn.LSTM(input_size=input_size, \n",
        "                                  hidden_size=hidden_size,\n",
        "                                  num_layers=num_layers,\n",
        "                                  dropout=dropout,\n",
        "                                  bidirectional=bidirectional)\n",
        "\n",
        "        self.relu = torch.nn.functional.relu\n",
        "\n",
        "        self.encode_fc1 = torch.nn.Linear(self.nof_configs,int(self.hidden_size/2))\n",
        "        self.encode_bn1 = torch.nn.BatchNorm1d(int(self.hidden_size/2))\n",
        "        self.encode_fc2 = torch.nn.Linear(int(self.hidden_size/2),self.hidden_size)\n",
        "        self.encode_bn2 = torch.nn.BatchNorm1d(self.hidden_size)\n",
        "\n",
        "    def forward(self, seq, config):\n",
        "        h0 = self.initHidden(config)\n",
        "        c0 = self.initCell(seq.size()[0])\n",
        "        seq = torch.t(seq)\n",
        "        seq = seq.unsqueeze(-1)\n",
        "        output, (hidden,cell) = self.lstm(seq, (h0,c0))\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def initHidden(self, config):\n",
        "        x = self.relu(self.encode_bn1(self.encode_fc1(config)))\n",
        "        x = self.relu(self.encode_bn2(self.encode_fc2(x)))\n",
        "        return torch.stack([x for _ in range(self.num_layers*self.num_directions)])\n",
        "\n",
        "    def initCell(self, batch_size):\n",
        "        return torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dErX6on9FYyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout = 0.5):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = torch.nn.LSTM(input_size=input_size, \n",
        "                                  hidden_size=hidden_size,\n",
        "                                  num_layers=num_layers,\n",
        "                                  dropout=dropout,\n",
        "                                  bidirectional=False)\n",
        "        \n",
        "        self.fc_out = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.relu = torch.nn.functional.relu\n",
        "\n",
        "    def forward(self, seq, h0, c0):\n",
        "        seq = seq.unsqueeze(0)\n",
        "        seq = seq.unsqueeze(-1)\n",
        "        output, (hidden,cell) = self.lstm(seq, (h0,c0))\n",
        "        output = self.fc_out(output)\n",
        "        return output.squeeze(), hidden, cell\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwbesz1Slf4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(torch.nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "    assert encoder.hidden_size == decoder.hidden_size\n",
        "    #assert encoder.num_layers == decoder.num_layers\n",
        "\n",
        "  def forward(self, source, target, teacher_forcing_ratio = 0.5):\n",
        "    batch_size = target.size()[0]\n",
        "    target_len = target.size()[1]\n",
        "\n",
        "    outputs = torch.zeros(target_len, batch_size, 1)\n",
        "\n",
        "    seq , config = source\n",
        "    output, hidden, cell = self.encoder(seq, config)\n",
        "\n",
        "    decoder_input = target[:,0]\n",
        "    for t in range(1, target_len):\n",
        "      output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
        "      outputs[t] = output.unsqueeze(-1)\n",
        "      use_teacher_forcing = np.random.random() < teacher_forcing_ratio\n",
        "      decoder_input = target[:,t] if use_teacher_forcing else output\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flpCWz4sjL2P",
        "colab_type": "text"
      },
      "source": [
        "### Train, eval & test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk5Jat93mO_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, criterion, clip=5):\n",
        "    model.train()\n",
        "    epoch_loss = []\n",
        "    for val_acc, configs, targets in train_data_loader:\n",
        "      optimizer.zero_grad()\n",
        "      output = model([val_acc,configs],targets)\n",
        "      output = output.squeeze()\n",
        "      output = torch.t(output)\n",
        "      loss = criterion(output, targets)\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
        "      optimizer.step()\n",
        "      epoch_loss.append(loss.item())\n",
        "    return np.array(epoch_loss).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GulE-3SosoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = []\n",
        "  with torch.no_grad():\n",
        "    for val_acc, configs, targets in val_data_loader:\n",
        "      output = model([val_acc, configs], targets, 0)\n",
        "      output = output.squeeze()\n",
        "      output = torch.t(output)\n",
        "      loss = criterion(output, targets)\n",
        "      epoch_loss.append(loss.item())\n",
        "  return np.array(epoch_loss).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8R89hbqJp6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RMSELoss(yhat,y):\n",
        "    return torch.sqrt(torch.mean((yhat-y)**2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OFaN6Rx1Njn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, criterion):\n",
        "    model.load_state_dict(torch.load('content/models/model.pt'))\n",
        "    model.eval()\n",
        "    epoch_loss=[]\n",
        "    with torch.no_grad():\n",
        "      for val_acc, configs, targets in test_data_loader:\n",
        "        output = model([val_acc, configs], targets, 0)\n",
        "        output = output.squeeze()\n",
        "        output = torch.t(output)\n",
        "        loss = criterion(output[:,-1], targets[:,-1])\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "    return np.array(epoch_loss).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5u3PuTVjrKx",
        "colab_type": "text"
      },
      "source": [
        "### BOHB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYUBJMijLLQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_bohb=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR-Zp7gmnXU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(config):\n",
        "  def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "      torch.nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "  \n",
        "  input_size = 1\n",
        "  outcome_dim = 1\n",
        "  config_size = 7\n",
        "  bidirectional = bool(config['bidirectional'])\n",
        "  encoder = EncoderRNN(input_size, config['hidden_dim'], config_size, config['num_layers'],\n",
        "                       dropout=config['encoder_dropout'],bidirectional=bidirectional)\n",
        "  decoder = DecoderRNN(input_size, config['hidden_dim'], outcome_dim, \n",
        "                       num_layers=2*config['num_layers'] if bidirectional else config['num_layers'],dropout=config['decoder_dropout'])\n",
        "  model = Seq2Seq(encoder, decoder)\n",
        "  model.apply(init_weights)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdHviALlrSaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "working_dir = os.curdir\n",
        "# minimum budget that BOHB uses\n",
        "min_budget = 30\n",
        "# largest budget BOHB will use\n",
        "max_budget = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXq6mTaBjvWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PyTorchWorker(Worker):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.train_loader, self.validation_loader, self.test_loader = train_data_loader, val_data_loader, test_data_loader\n",
        "\n",
        "    @staticmethod\n",
        "    def get_model(config: CS.Configuration) -> torch.nn.Module:\n",
        "        \"\"\" Define a configurable convolution model.\n",
        "            \n",
        "        See description of get_conv_model above for more details on the model.\n",
        "        \"\"\"\n",
        "        return get_model(config)\n",
        "        \n",
        "    @staticmethod\n",
        "    def get_configspace() -> CS.Configuration:\n",
        "        \"\"\" \n",
        "        Define a conditional hyperparameter search-space.\n",
        "        \"\"\"\n",
        "        cs = CS.ConfigurationSpace()\n",
        "        hidden_dim = CSH.UniformIntegerHyperparameter(\"hidden_dim\",lower=5,upper=500,log=True)\n",
        "        num_layers = CSH.UniformIntegerHyperparameter(\"num_layers\",lower=1,upper=5)\n",
        "        encoder_dropout = CSH.UniformFloatHyperparameter(\"encoder_dropout\",lower=0.1,upper=0.8)\n",
        "        decoder_dropout = CSH.UniformFloatHyperparameter(\"decoder_dropout\",lower=0.1,upper=0.8)\n",
        "        bidirectional = CSH.UniformIntegerHyperparameter(\"bidirectional\",lower=0,upper=1)\n",
        "\n",
        "        lr = CSH.UniformFloatHyperparameter(\"lr\",lower=1e-6,upper=1e-1,log=True)\n",
        "        weight_decay = CSH.UniformFloatHyperparameter(\"weight_decay\",lower=1e-6,upper=1e-1,log=True)\n",
        "        sgd_momentum = CSH.UniformFloatHyperparameter(\"sgd_momentum\",lower=0.00,upper=0.99)\n",
        "        optimizer = CSH.CategoricalHyperparameter('optimizer', choices=['Adam', 'SGD'])\n",
        "        \n",
        "        scheduler = CSH.CategoricalHyperparameter('scheduler', choices=['CosAnn', 'CosAnnWarm'])\n",
        "        T_max = CSH.UniformIntegerHyperparameter(\"T_max\",lower=10,upper=max_budget)\n",
        "        T_0 = CSH.UniformIntegerHyperparameter(\"T_0\",lower=10,upper=int(max_budget/3))\n",
        "\n",
        "        cs.add_hyperparameters([hidden_dim, num_layers, encoder_dropout, decoder_dropout, bidirectional,\n",
        "                                lr, weight_decay, sgd_momentum,optimizer,scheduler,T_max,T_0])\n",
        "    \n",
        "        condition1 = CS.EqualsCondition(sgd_momentum,optimizer,'SGD')\n",
        "        cs.add_condition(condition1)\n",
        "        condition2 = CS.EqualsCondition(T_max,scheduler,'CosAnn')\n",
        "        cs.add_condition(condition2)\n",
        "        condition3 = CS.EqualsCondition(T_0,scheduler,'CosAnnWarm')\n",
        "        cs.add_condition(condition3)\n",
        "        return cs\n",
        "\n",
        "    def compute(self, config: CS.Configuration, budget: float, working_directory: str,\n",
        "                *args, **kwargs) -> dict:\n",
        "        \"\"\"Evaluate a function with the given config and budget and return a loss.\n",
        "        \n",
        "        Bohb tries to minimize the returned loss.\n",
        "        \n",
        "        In our case the function is the training and validation of a model,\n",
        "        the budget is the number of epochs and the loss is the validation error.\n",
        "        \"\"\"\n",
        "        model = self.get_model(config)\n",
        "\n",
        "        if config['optimizer'] == 'Adam':\n",
        "          optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "        else:\n",
        "          optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=config['sgd_momentum'])\n",
        "        \n",
        "        if config['scheduler'] == 'CosAnnWarm':\n",
        "          scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=config['T_0'])\n",
        "        else:\n",
        "          scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['T_max'])\n",
        "\n",
        "        criterion = torch.nn.MSELoss()\n",
        "        clip = 5\n",
        "\n",
        "        for epoch in range(int(budget)):\n",
        "          loss = 0\n",
        "          model.train()\n",
        "          for val_acc, configs, targets in self.train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model([val_acc,configs],targets)\n",
        "            output = output.squeeze()\n",
        "            output = torch.t(output)\n",
        "            loss = criterion(output, targets)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
        "            optimizer.step()\n",
        "          scheduler.step()\n",
        "        \n",
        "        train_loss = evaluate(model, self.train_loader)\n",
        "        validation_loss = evaluate(model, self.validation_loader)\n",
        "        test_loss = evaluate(model, self.test_loader)\n",
        "        \n",
        "        return ({\n",
        "                'loss': validation_loss,  # remember: HpBandSter minimizes the loss!\n",
        "                'info': {'test_loss': test_loss,\n",
        "                         'train_loss': train_loss,\n",
        "                         'validation_loss': validation_loss,\n",
        "                         'model': str(model)}\n",
        "                })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWKQurHYsbkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if run_bohb:\n",
        "  worker = PyTorchWorker(run_id='0')\n",
        "  cs = worker.get_configspace()\n",
        "\n",
        "  config = cs.sample_configuration().get_dictionary()\n",
        "  print(config)\n",
        "\n",
        "  res = worker.compute(config=config, budget=min_budget, working_directory=working_dir)\n",
        "  print(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O50di5m_sfvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if run_bohb:\n",
        "  result_file = os.path.join(working_dir, 'bohb_result.pkl')\n",
        "  nic_name = 'lo0'\n",
        "  port = 0\n",
        "  run_id = 'bohb_run_1'\n",
        "  n_bohb_iterations = 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR8rgsM7wTdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if run_bohb:\n",
        "  try:\n",
        "      # Start a nameserver\n",
        "      worker = PyTorchWorker(run_id='0')\n",
        "      ns = hpns.NameServer(run_id=run_id, host='127.0.0.1', port=port,\n",
        "                          working_directory=working_dir)\n",
        "      ns_host, ns_port = ns.start()\n",
        "      # Start local worker\n",
        "      w = PyTorchWorker(run_id=run_id, host='127.0.0.1', nameserver=ns_host,\n",
        "                        nameserver_port=ns_port, timeout=120)\n",
        "      w.run(background=True)\n",
        "\n",
        "      # Run an optimizer\n",
        "      bohb = BOHB(configspace=worker.get_configspace(),\n",
        "                  run_id=run_id,\n",
        "                  host='127.0.0.1',\n",
        "                  nameserver=ns_host,\n",
        "                  nameserver_port=ns_port,\n",
        "                  min_budget=min_budget, max_budget=max_budget)\n",
        "\n",
        "      result = bohb.run(n_iterations=n_bohb_iterations)\n",
        "      print(\"Write result to file {}\".format(result_file))\n",
        "      with open(result_file, 'wb') as f:\n",
        "          pickle.dump(result, f)\n",
        "  finally:\n",
        "      bohb.shutdown(shutdown_workers=True)\n",
        "      ns.shutdown()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACwkRPySjTCu",
        "colab_type": "text"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsgAX8SUKpLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "      torch.nn.init.uniform_(param.data, -0.08, 0.08)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8TtwTZiKH2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 500\n",
        "\n",
        "  \n",
        "input_size = 1\n",
        "outcome_size = 1\n",
        "config_size = 7\n",
        "hidden_size=50\n",
        "encoder_dropout=0.5\n",
        "decoder_dropout=0.5\n",
        "num_layers=2\n",
        "bidirectional = True\n",
        "encoder = EncoderRNN(input_size, hidden_size=hidden_size, nof_configs=config_size, num_layers=num_layers,\n",
        "                      dropout=encoder_dropout,bidirectional=bidirectional)\n",
        "decoder = DecoderRNN(input_size, hidden_size=hidden_size, output_size=outcome_size, \n",
        "                      num_layers=2*num_layers if bidirectional else num_layers,dropout=decoder_dropout)\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "model.apply(init_weights)\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=10e-3)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqBLkuRznl9y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9247b94a-1b56-4457-8b7d-28fdb1c0cbc0"
      },
      "source": [
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss = train(model, optimizer, criterion)\n",
        "  val_loss = evaluate(model, criterion)\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    torch.save(model.state_dict(),\"content/models/model.pt\")    \n",
        "    print('Val loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(best_val_loss,val_loss))\n",
        "    best_val_loss = val_loss\n",
        "\n",
        "  print(f'Epoch: {epoch}\\t Train Loss: {train_loss:.3f}\\t Val. Loss: {val_loss:.3f}')\n",
        "  scheduler.step()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Val loss decreased (inf --> 3582.461945).  Saving model ...\n",
            "Epoch: 0\t Train Loss: 4720.527\t Val. Loss: 3582.462\n",
            "Val loss decreased (3582.461945 --> 1910.862183).  Saving model ...\n",
            "Epoch: 1\t Train Loss: 2739.413\t Val. Loss: 1910.862\n",
            "Val loss decreased (1910.862183 --> 794.515984).  Saving model ...\n",
            "Epoch: 2\t Train Loss: 1341.020\t Val. Loss: 794.516\n",
            "Val loss decreased (794.515984 --> 207.986318).  Saving model ...\n",
            "Epoch: 3\t Train Loss: 484.728\t Val. Loss: 207.986\n",
            "Val loss decreased (207.986318 --> 103.715823).  Saving model ...\n",
            "Epoch: 4\t Train Loss: 150.725\t Val. Loss: 103.716\n",
            "Epoch: 5\t Train Loss: 119.966\t Val. Loss: 170.303\n",
            "Epoch: 6\t Train Loss: 130.518\t Val. Loss: 115.142\n",
            "Epoch: 7\t Train Loss: 61.341\t Val. Loss: 213.994\n",
            "Val loss decreased (103.715823 --> 28.630703).  Saving model ...\n",
            "Epoch: 8\t Train Loss: 70.895\t Val. Loss: 28.631\n",
            "Epoch: 9\t Train Loss: 32.823\t Val. Loss: 29.661\n",
            "Val loss decreased (28.630703 --> 17.456374).  Saving model ...\n",
            "Epoch: 10\t Train Loss: 27.093\t Val. Loss: 17.456\n",
            "Epoch: 11\t Train Loss: 22.747\t Val. Loss: 23.888\n",
            "Epoch: 12\t Train Loss: 20.773\t Val. Loss: 21.208\n",
            "Epoch: 13\t Train Loss: 20.571\t Val. Loss: 18.424\n",
            "Val loss decreased (17.456374 --> 15.796952).  Saving model ...\n",
            "Epoch: 14\t Train Loss: 28.741\t Val. Loss: 15.797\n",
            "Val loss decreased (15.796952 --> 14.948881).  Saving model ...\n",
            "Epoch: 15\t Train Loss: 19.840\t Val. Loss: 14.949\n",
            "Epoch: 16\t Train Loss: 19.309\t Val. Loss: 17.429\n",
            "Epoch: 17\t Train Loss: 18.700\t Val. Loss: 16.454\n",
            "Val loss decreased (14.948881 --> 12.801527).  Saving model ...\n",
            "Epoch: 18\t Train Loss: 16.433\t Val. Loss: 12.802\n",
            "Epoch: 19\t Train Loss: 17.495\t Val. Loss: 27.544\n",
            "Epoch: 20\t Train Loss: 20.654\t Val. Loss: 15.640\n",
            "Epoch: 21\t Train Loss: 20.183\t Val. Loss: 28.786\n",
            "Val loss decreased (12.801527 --> 12.044409).  Saving model ...\n",
            "Epoch: 22\t Train Loss: 16.198\t Val. Loss: 12.044\n",
            "Epoch: 23\t Train Loss: 16.667\t Val. Loss: 17.385\n",
            "Epoch: 24\t Train Loss: 17.479\t Val. Loss: 14.318\n",
            "Epoch: 25\t Train Loss: 23.623\t Val. Loss: 13.600\n",
            "Epoch: 26\t Train Loss: 15.671\t Val. Loss: 14.971\n",
            "Epoch: 27\t Train Loss: 16.860\t Val. Loss: 18.368\n",
            "Epoch: 28\t Train Loss: 16.834\t Val. Loss: 15.466\n",
            "Epoch: 29\t Train Loss: 14.887\t Val. Loss: 15.243\n",
            "Epoch: 30\t Train Loss: 16.085\t Val. Loss: 16.616\n",
            "Epoch: 31\t Train Loss: 17.649\t Val. Loss: 15.379\n",
            "Epoch: 32\t Train Loss: 16.364\t Val. Loss: 14.960\n",
            "Epoch: 33\t Train Loss: 13.532\t Val. Loss: 14.638\n",
            "Epoch: 34\t Train Loss: 14.537\t Val. Loss: 13.222\n",
            "Epoch: 35\t Train Loss: 14.587\t Val. Loss: 14.667\n",
            "Epoch: 36\t Train Loss: 13.422\t Val. Loss: 12.892\n",
            "Epoch: 37\t Train Loss: 12.708\t Val. Loss: 12.659\n",
            "Val loss decreased (12.044409 --> 11.867882).  Saving model ...\n",
            "Epoch: 38\t Train Loss: 12.048\t Val. Loss: 11.868\n",
            "Val loss decreased (11.867882 --> 11.667893).  Saving model ...\n",
            "Epoch: 39\t Train Loss: 11.806\t Val. Loss: 11.668\n",
            "Val loss decreased (11.667893 --> 10.812500).  Saving model ...\n",
            "Epoch: 40\t Train Loss: 11.421\t Val. Loss: 10.813\n",
            "Val loss decreased (10.812500 --> 10.540978).  Saving model ...\n",
            "Epoch: 41\t Train Loss: 10.872\t Val. Loss: 10.541\n",
            "Val loss decreased (10.540978 --> 10.175011).  Saving model ...\n",
            "Epoch: 42\t Train Loss: 11.102\t Val. Loss: 10.175\n",
            "Epoch: 43\t Train Loss: 10.825\t Val. Loss: 11.148\n",
            "Epoch: 44\t Train Loss: 10.835\t Val. Loss: 10.220\n",
            "Epoch: 45\t Train Loss: 10.727\t Val. Loss: 10.220\n",
            "Val loss decreased (10.175011 --> 9.747638).  Saving model ...\n",
            "Epoch: 46\t Train Loss: 10.313\t Val. Loss: 9.748\n",
            "Epoch: 47\t Train Loss: 10.304\t Val. Loss: 9.751\n",
            "Val loss decreased (9.747638 --> 9.682880).  Saving model ...\n",
            "Epoch: 48\t Train Loss: 9.979\t Val. Loss: 9.683\n",
            "Val loss decreased (9.682880 --> 9.680793).  Saving model ...\n",
            "Epoch: 49\t Train Loss: 10.124\t Val. Loss: 9.681\n",
            "Epoch: 50\t Train Loss: 24.808\t Val. Loss: 24.004\n",
            "Epoch: 51\t Train Loss: 33.144\t Val. Loss: 20.246\n",
            "Epoch: 52\t Train Loss: 23.573\t Val. Loss: 16.478\n",
            "Epoch: 53\t Train Loss: 22.264\t Val. Loss: 47.303\n",
            "Epoch: 54\t Train Loss: 25.803\t Val. Loss: 19.034\n",
            "Epoch: 55\t Train Loss: 19.510\t Val. Loss: 15.865\n",
            "Epoch: 56\t Train Loss: 19.137\t Val. Loss: 15.572\n",
            "Epoch: 57\t Train Loss: 17.633\t Val. Loss: 13.427\n",
            "Epoch: 58\t Train Loss: 26.167\t Val. Loss: 17.774\n",
            "Epoch: 59\t Train Loss: 16.926\t Val. Loss: 16.196\n",
            "Epoch: 60\t Train Loss: 16.136\t Val. Loss: 12.338\n",
            "Epoch: 61\t Train Loss: 16.971\t Val. Loss: 13.026\n",
            "Epoch: 62\t Train Loss: 18.465\t Val. Loss: 14.263\n",
            "Epoch: 63\t Train Loss: 17.792\t Val. Loss: 14.577\n",
            "Epoch: 64\t Train Loss: 14.752\t Val. Loss: 17.027\n",
            "Epoch: 65\t Train Loss: 18.810\t Val. Loss: 13.158\n",
            "Epoch: 66\t Train Loss: 18.618\t Val. Loss: 20.200\n",
            "Epoch: 67\t Train Loss: 23.293\t Val. Loss: 16.758\n",
            "Epoch: 68\t Train Loss: 15.746\t Val. Loss: 12.276\n",
            "Epoch: 69\t Train Loss: 14.393\t Val. Loss: 14.353\n",
            "Epoch: 70\t Train Loss: 14.983\t Val. Loss: 13.161\n",
            "Epoch: 71\t Train Loss: 16.642\t Val. Loss: 15.912\n",
            "Epoch: 72\t Train Loss: 21.980\t Val. Loss: 16.124\n",
            "Epoch: 73\t Train Loss: 15.902\t Val. Loss: 13.046\n",
            "Epoch: 74\t Train Loss: 17.097\t Val. Loss: 24.646\n",
            "Epoch: 75\t Train Loss: 17.030\t Val. Loss: 12.571\n",
            "Epoch: 76\t Train Loss: 15.218\t Val. Loss: 16.149\n",
            "Epoch: 77\t Train Loss: 15.819\t Val. Loss: 11.458\n",
            "Epoch: 78\t Train Loss: 14.843\t Val. Loss: 11.656\n",
            "Epoch: 79\t Train Loss: 13.167\t Val. Loss: 13.004\n",
            "Epoch: 80\t Train Loss: 13.371\t Val. Loss: 14.787\n",
            "Epoch: 81\t Train Loss: 14.352\t Val. Loss: 13.014\n",
            "Epoch: 82\t Train Loss: 12.772\t Val. Loss: 12.740\n",
            "Epoch: 83\t Train Loss: 11.732\t Val. Loss: 12.168\n",
            "Epoch: 84\t Train Loss: 12.344\t Val. Loss: 10.852\n",
            "Epoch: 85\t Train Loss: 10.834\t Val. Loss: 12.105\n",
            "Epoch: 86\t Train Loss: 12.200\t Val. Loss: 11.264\n",
            "Epoch: 87\t Train Loss: 11.167\t Val. Loss: 10.037\n",
            "Epoch: 88\t Train Loss: 10.446\t Val. Loss: 10.300\n",
            "Epoch: 89\t Train Loss: 10.276\t Val. Loss: 11.277\n",
            "Epoch: 90\t Train Loss: 10.983\t Val. Loss: 10.933\n",
            "Epoch: 91\t Train Loss: 10.666\t Val. Loss: 10.238\n",
            "Val loss decreased (9.680793 --> 9.568059).  Saving model ...\n",
            "Epoch: 92\t Train Loss: 10.113\t Val. Loss: 9.568\n",
            "Val loss decreased (9.568059 --> 9.347473).  Saving model ...\n",
            "Epoch: 93\t Train Loss: 9.973\t Val. Loss: 9.347\n",
            "Epoch: 94\t Train Loss: 10.075\t Val. Loss: 9.595\n",
            "Val loss decreased (9.347473 --> 9.240424).  Saving model ...\n",
            "Epoch: 95\t Train Loss: 9.628\t Val. Loss: 9.240\n",
            "Val loss decreased (9.240424 --> 9.079055).  Saving model ...\n",
            "Epoch: 96\t Train Loss: 9.277\t Val. Loss: 9.079\n",
            "Val loss decreased (9.079055 --> 9.000679).  Saving model ...\n",
            "Epoch: 97\t Train Loss: 9.259\t Val. Loss: 9.001\n",
            "Val loss decreased (9.000679 --> 8.930805).  Saving model ...\n",
            "Epoch: 98\t Train Loss: 9.152\t Val. Loss: 8.931\n",
            "Val loss decreased (8.930805 --> 8.922148).  Saving model ...\n",
            "Epoch: 99\t Train Loss: 9.073\t Val. Loss: 8.922\n",
            "Epoch: 100\t Train Loss: 23.163\t Val. Loss: 19.545\n",
            "Epoch: 101\t Train Loss: 24.354\t Val. Loss: 20.399\n",
            "Epoch: 102\t Train Loss: 21.575\t Val. Loss: 22.153\n",
            "Epoch: 103\t Train Loss: 19.685\t Val. Loss: 16.229\n",
            "Epoch: 104\t Train Loss: 17.288\t Val. Loss: 12.041\n",
            "Epoch: 105\t Train Loss: 18.101\t Val. Loss: 18.791\n",
            "Epoch: 106\t Train Loss: 18.350\t Val. Loss: 15.123\n",
            "Epoch: 107\t Train Loss: 17.537\t Val. Loss: 14.208\n",
            "Epoch: 108\t Train Loss: 16.172\t Val. Loss: 14.196\n",
            "Epoch: 109\t Train Loss: 19.613\t Val. Loss: 15.467\n",
            "Epoch: 110\t Train Loss: 16.626\t Val. Loss: 14.615\n",
            "Epoch: 111\t Train Loss: 21.291\t Val. Loss: 15.153\n",
            "Epoch: 112\t Train Loss: 18.285\t Val. Loss: 16.700\n",
            "Epoch: 113\t Train Loss: 17.183\t Val. Loss: 11.299\n",
            "Epoch: 114\t Train Loss: 18.478\t Val. Loss: 16.869\n",
            "Epoch: 115\t Train Loss: 19.115\t Val. Loss: 15.416\n",
            "Epoch: 116\t Train Loss: 20.230\t Val. Loss: 16.112\n",
            "Epoch: 117\t Train Loss: 15.616\t Val. Loss: 15.882\n",
            "Epoch: 118\t Train Loss: 22.569\t Val. Loss: 14.336\n",
            "Epoch: 119\t Train Loss: 14.203\t Val. Loss: 10.967\n",
            "Epoch: 120\t Train Loss: 15.373\t Val. Loss: 14.379\n",
            "Epoch: 121\t Train Loss: 14.192\t Val. Loss: 13.276\n",
            "Epoch: 122\t Train Loss: 12.442\t Val. Loss: 10.582\n",
            "Epoch: 123\t Train Loss: 13.931\t Val. Loss: 14.517\n",
            "Epoch: 124\t Train Loss: 14.825\t Val. Loss: 10.943\n",
            "Epoch: 125\t Train Loss: 11.790\t Val. Loss: 12.733\n",
            "Epoch: 126\t Train Loss: 15.004\t Val. Loss: 12.222\n",
            "Epoch: 127\t Train Loss: 13.008\t Val. Loss: 10.972\n",
            "Epoch: 128\t Train Loss: 11.982\t Val. Loss: 9.459\n",
            "Epoch: 129\t Train Loss: 12.706\t Val. Loss: 12.811\n",
            "Epoch: 130\t Train Loss: 13.420\t Val. Loss: 15.454\n",
            "Epoch: 131\t Train Loss: 12.942\t Val. Loss: 10.195\n",
            "Epoch: 132\t Train Loss: 10.830\t Val. Loss: 10.005\n",
            "Epoch: 133\t Train Loss: 10.477\t Val. Loss: 9.404\n",
            "Epoch: 134\t Train Loss: 10.618\t Val. Loss: 9.363\n",
            "Epoch: 135\t Train Loss: 9.901\t Val. Loss: 9.173\n",
            "Epoch: 136\t Train Loss: 9.503\t Val. Loss: 9.026\n",
            "Epoch: 137\t Train Loss: 9.612\t Val. Loss: 9.391\n",
            "Epoch: 138\t Train Loss: 9.509\t Val. Loss: 9.575\n",
            "Epoch: 139\t Train Loss: 9.573\t Val. Loss: 9.424\n",
            "Epoch: 140\t Train Loss: 9.251\t Val. Loss: 9.495\n",
            "Epoch: 141\t Train Loss: 9.063\t Val. Loss: 9.244\n",
            "Val loss decreased (8.922148 --> 8.613086).  Saving model ...\n",
            "Epoch: 142\t Train Loss: 8.860\t Val. Loss: 8.613\n",
            "Val loss decreased (8.613086 --> 8.343722).  Saving model ...\n",
            "Epoch: 143\t Train Loss: 8.329\t Val. Loss: 8.344\n",
            "Epoch: 144\t Train Loss: 8.768\t Val. Loss: 9.045\n",
            "Epoch: 145\t Train Loss: 8.463\t Val. Loss: 8.480\n",
            "Val loss decreased (8.343722 --> 8.311597).  Saving model ...\n",
            "Epoch: 146\t Train Loss: 8.220\t Val. Loss: 8.312\n",
            "Val loss decreased (8.311597 --> 8.261396).  Saving model ...\n",
            "Epoch: 147\t Train Loss: 8.282\t Val. Loss: 8.261\n",
            "Val loss decreased (8.261396 --> 8.163061).  Saving model ...\n",
            "Epoch: 148\t Train Loss: 8.195\t Val. Loss: 8.163\n",
            "Epoch: 149\t Train Loss: 8.109\t Val. Loss: 8.177\n",
            "Epoch: 150\t Train Loss: 17.167\t Val. Loss: 12.298\n",
            "Epoch: 151\t Train Loss: 17.242\t Val. Loss: 17.576\n",
            "Epoch: 152\t Train Loss: 21.587\t Val. Loss: 11.994\n",
            "Epoch: 153\t Train Loss: 18.216\t Val. Loss: 15.923\n",
            "Epoch: 154\t Train Loss: 19.761\t Val. Loss: 18.655\n",
            "Epoch: 155\t Train Loss: 15.746\t Val. Loss: 12.151\n",
            "Epoch: 156\t Train Loss: 18.040\t Val. Loss: 34.491\n",
            "Epoch: 157\t Train Loss: 22.911\t Val. Loss: 10.281\n",
            "Epoch: 158\t Train Loss: 13.962\t Val. Loss: 13.319\n",
            "Epoch: 159\t Train Loss: 16.089\t Val. Loss: 17.449\n",
            "Epoch: 160\t Train Loss: 15.635\t Val. Loss: 11.324\n",
            "Epoch: 161\t Train Loss: 17.682\t Val. Loss: 13.261\n",
            "Epoch: 162\t Train Loss: 13.806\t Val. Loss: 15.474\n",
            "Epoch: 163\t Train Loss: 21.029\t Val. Loss: 14.218\n",
            "Epoch: 164\t Train Loss: 17.874\t Val. Loss: 14.212\n",
            "Epoch: 165\t Train Loss: 14.034\t Val. Loss: 12.310\n",
            "Epoch: 166\t Train Loss: 15.815\t Val. Loss: 11.552\n",
            "Epoch: 167\t Train Loss: 14.160\t Val. Loss: 13.264\n",
            "Epoch: 168\t Train Loss: 18.886\t Val. Loss: 22.321\n",
            "Epoch: 169\t Train Loss: 18.420\t Val. Loss: 14.557\n",
            "Epoch: 170\t Train Loss: 13.892\t Val. Loss: 12.616\n",
            "Epoch: 171\t Train Loss: 13.549\t Val. Loss: 14.329\n",
            "Epoch: 172\t Train Loss: 12.281\t Val. Loss: 11.135\n",
            "Epoch: 173\t Train Loss: 11.876\t Val. Loss: 11.303\n",
            "Epoch: 174\t Train Loss: 11.018\t Val. Loss: 11.627\n",
            "Epoch: 175\t Train Loss: 14.072\t Val. Loss: 10.678\n",
            "Epoch: 176\t Train Loss: 12.025\t Val. Loss: 11.901\n",
            "Epoch: 177\t Train Loss: 12.510\t Val. Loss: 12.298\n",
            "Epoch: 178\t Train Loss: 12.614\t Val. Loss: 10.797\n",
            "Epoch: 179\t Train Loss: 13.163\t Val. Loss: 14.035\n",
            "Epoch: 180\t Train Loss: 11.074\t Val. Loss: 10.078\n",
            "Epoch: 181\t Train Loss: 11.298\t Val. Loss: 10.004\n",
            "Epoch: 182\t Train Loss: 10.726\t Val. Loss: 9.012\n",
            "Epoch: 183\t Train Loss: 10.372\t Val. Loss: 9.240\n",
            "Epoch: 184\t Train Loss: 10.591\t Val. Loss: 11.429\n",
            "Epoch: 185\t Train Loss: 9.399\t Val. Loss: 9.383\n",
            "Epoch: 186\t Train Loss: 9.842\t Val. Loss: 8.972\n",
            "Epoch: 187\t Train Loss: 10.349\t Val. Loss: 10.635\n",
            "Epoch: 188\t Train Loss: 9.701\t Val. Loss: 10.263\n",
            "Epoch: 189\t Train Loss: 9.273\t Val. Loss: 9.244\n",
            "Epoch: 190\t Train Loss: 9.394\t Val. Loss: 9.297\n",
            "Epoch: 191\t Train Loss: 8.601\t Val. Loss: 8.581\n",
            "Val loss decreased (8.163061 --> 8.084445).  Saving model ...\n",
            "Epoch: 192\t Train Loss: 8.401\t Val. Loss: 8.084\n",
            "Epoch: 193\t Train Loss: 8.499\t Val. Loss: 8.578\n",
            "Epoch: 194\t Train Loss: 8.435\t Val. Loss: 8.300\n",
            "Epoch: 195\t Train Loss: 8.394\t Val. Loss: 8.192\n",
            "Val loss decreased (8.084445 --> 7.925019).  Saving model ...\n",
            "Epoch: 196\t Train Loss: 7.736\t Val. Loss: 7.925\n",
            "Epoch: 197\t Train Loss: 7.823\t Val. Loss: 8.000\n",
            "Epoch: 198\t Train Loss: 7.759\t Val. Loss: 7.992\n",
            "Epoch: 199\t Train Loss: 7.715\t Val. Loss: 7.981\n",
            "Epoch: 200\t Train Loss: 18.414\t Val. Loss: 18.837\n",
            "Epoch: 201\t Train Loss: 15.885\t Val. Loss: 13.184\n",
            "Epoch: 202\t Train Loss: 20.578\t Val. Loss: 14.704\n",
            "Epoch: 203\t Train Loss: 22.097\t Val. Loss: 13.338\n",
            "Epoch: 204\t Train Loss: 19.021\t Val. Loss: 23.064\n",
            "Epoch: 205\t Train Loss: 16.787\t Val. Loss: 14.506\n",
            "Epoch: 206\t Train Loss: 15.862\t Val. Loss: 14.219\n",
            "Epoch: 207\t Train Loss: 14.285\t Val. Loss: 12.297\n",
            "Epoch: 208\t Train Loss: 17.362\t Val. Loss: 14.006\n",
            "Epoch: 209\t Train Loss: 17.596\t Val. Loss: 18.615\n",
            "Epoch: 210\t Train Loss: 16.874\t Val. Loss: 14.685\n",
            "Epoch: 211\t Train Loss: 15.175\t Val. Loss: 10.364\n",
            "Epoch: 212\t Train Loss: 13.669\t Val. Loss: 10.280\n",
            "Epoch: 213\t Train Loss: 13.237\t Val. Loss: 11.801\n",
            "Epoch: 214\t Train Loss: 13.050\t Val. Loss: 18.673\n",
            "Epoch: 215\t Train Loss: 12.989\t Val. Loss: 13.119\n",
            "Epoch: 216\t Train Loss: 14.581\t Val. Loss: 12.094\n",
            "Epoch: 217\t Train Loss: 12.628\t Val. Loss: 11.775\n",
            "Epoch: 218\t Train Loss: 13.508\t Val. Loss: 11.922\n",
            "Epoch: 219\t Train Loss: 12.362\t Val. Loss: 13.356\n",
            "Epoch: 220\t Train Loss: 12.847\t Val. Loss: 13.740\n",
            "Epoch: 221\t Train Loss: 11.407\t Val. Loss: 9.982\n",
            "Epoch: 222\t Train Loss: 14.803\t Val. Loss: 14.304\n",
            "Epoch: 223\t Train Loss: 13.333\t Val. Loss: 10.308\n",
            "Epoch: 224\t Train Loss: 12.301\t Val. Loss: 11.392\n",
            "Epoch: 225\t Train Loss: 12.954\t Val. Loss: 10.351\n",
            "Epoch: 226\t Train Loss: 12.925\t Val. Loss: 11.971\n",
            "Epoch: 227\t Train Loss: 11.734\t Val. Loss: 10.503\n",
            "Epoch: 228\t Train Loss: 11.642\t Val. Loss: 19.668\n",
            "Epoch: 229\t Train Loss: 13.565\t Val. Loss: 11.048\n",
            "Epoch: 230\t Train Loss: 9.709\t Val. Loss: 9.249\n",
            "Epoch: 231\t Train Loss: 9.730\t Val. Loss: 8.835\n",
            "Epoch: 232\t Train Loss: 11.046\t Val. Loss: 10.549\n",
            "Epoch: 233\t Train Loss: 9.817\t Val. Loss: 9.029\n",
            "Epoch: 234\t Train Loss: 9.191\t Val. Loss: 9.541\n",
            "Epoch: 235\t Train Loss: 8.786\t Val. Loss: 8.814\n",
            "Epoch: 236\t Train Loss: 9.055\t Val. Loss: 8.796\n",
            "Epoch: 237\t Train Loss: 8.873\t Val. Loss: 10.793\n",
            "Epoch: 238\t Train Loss: 9.207\t Val. Loss: 9.382\n",
            "Epoch: 239\t Train Loss: 8.730\t Val. Loss: 8.721\n",
            "Epoch: 240\t Train Loss: 8.313\t Val. Loss: 8.939\n",
            "Epoch: 241\t Train Loss: 8.105\t Val. Loss: 8.339\n",
            "Epoch: 242\t Train Loss: 7.969\t Val. Loss: 8.056\n",
            "Epoch: 243\t Train Loss: 8.040\t Val. Loss: 8.093\n",
            "Val loss decreased (7.925019 --> 7.901141).  Saving model ...\n",
            "Epoch: 244\t Train Loss: 7.832\t Val. Loss: 7.901\n",
            "Val loss decreased (7.901141 --> 7.813985).  Saving model ...\n",
            "Epoch: 245\t Train Loss: 7.734\t Val. Loss: 7.814\n",
            "Val loss decreased (7.813985 --> 7.730596).  Saving model ...\n",
            "Epoch: 246\t Train Loss: 7.658\t Val. Loss: 7.731\n",
            "Val loss decreased (7.730596 --> 7.719596).  Saving model ...\n",
            "Epoch: 247\t Train Loss: 7.657\t Val. Loss: 7.720\n",
            "Epoch: 248\t Train Loss: 7.453\t Val. Loss: 7.726\n",
            "Val loss decreased (7.719596 --> 7.713463).  Saving model ...\n",
            "Epoch: 249\t Train Loss: 7.416\t Val. Loss: 7.713\n",
            "Epoch: 250\t Train Loss: 21.453\t Val. Loss: 16.488\n",
            "Epoch: 251\t Train Loss: 18.843\t Val. Loss: 20.343\n",
            "Epoch: 252\t Train Loss: 26.577\t Val. Loss: 17.074\n",
            "Epoch: 253\t Train Loss: 19.410\t Val. Loss: 28.633\n",
            "Epoch: 254\t Train Loss: 17.778\t Val. Loss: 17.332\n",
            "Epoch: 255\t Train Loss: 23.335\t Val. Loss: 15.543\n",
            "Epoch: 256\t Train Loss: 18.857\t Val. Loss: 13.180\n",
            "Epoch: 257\t Train Loss: 16.274\t Val. Loss: 14.960\n",
            "Epoch: 258\t Train Loss: 15.301\t Val. Loss: 13.162\n",
            "Epoch: 259\t Train Loss: 13.732\t Val. Loss: 11.441\n",
            "Epoch: 260\t Train Loss: 16.410\t Val. Loss: 18.017\n",
            "Epoch: 261\t Train Loss: 14.172\t Val. Loss: 12.416\n",
            "Epoch: 262\t Train Loss: 15.203\t Val. Loss: 13.419\n",
            "Epoch: 263\t Train Loss: 12.931\t Val. Loss: 13.130\n",
            "Epoch: 264\t Train Loss: 15.785\t Val. Loss: 12.088\n",
            "Epoch: 265\t Train Loss: 14.592\t Val. Loss: 12.330\n",
            "Epoch: 266\t Train Loss: 14.372\t Val. Loss: 10.154\n",
            "Epoch: 267\t Train Loss: 13.473\t Val. Loss: 13.981\n",
            "Epoch: 268\t Train Loss: 18.680\t Val. Loss: 17.308\n",
            "Epoch: 269\t Train Loss: 15.508\t Val. Loss: 12.757\n",
            "Epoch: 270\t Train Loss: 12.749\t Val. Loss: 12.151\n",
            "Epoch: 271\t Train Loss: 12.189\t Val. Loss: 11.817\n",
            "Epoch: 272\t Train Loss: 13.619\t Val. Loss: 15.764\n",
            "Epoch: 273\t Train Loss: 14.246\t Val. Loss: 10.604\n",
            "Epoch: 274\t Train Loss: 11.655\t Val. Loss: 10.538\n",
            "Epoch: 275\t Train Loss: 10.895\t Val. Loss: 9.818\n",
            "Epoch: 276\t Train Loss: 13.028\t Val. Loss: 12.655\n",
            "Epoch: 277\t Train Loss: 12.702\t Val. Loss: 10.254\n",
            "Epoch: 278\t Train Loss: 11.730\t Val. Loss: 10.366\n",
            "Epoch: 279\t Train Loss: 9.695\t Val. Loss: 10.260\n",
            "Epoch: 280\t Train Loss: 9.981\t Val. Loss: 8.765\n",
            "Epoch: 281\t Train Loss: 10.495\t Val. Loss: 10.538\n",
            "Epoch: 282\t Train Loss: 10.318\t Val. Loss: 11.444\n",
            "Epoch: 283\t Train Loss: 9.777\t Val. Loss: 9.540\n",
            "Epoch: 284\t Train Loss: 9.154\t Val. Loss: 8.592\n",
            "Epoch: 285\t Train Loss: 8.968\t Val. Loss: 8.539\n",
            "Epoch: 286\t Train Loss: 9.186\t Val. Loss: 8.518\n",
            "Epoch: 287\t Train Loss: 8.947\t Val. Loss: 9.558\n",
            "Epoch: 288\t Train Loss: 8.722\t Val. Loss: 8.588\n",
            "Epoch: 289\t Train Loss: 8.383\t Val. Loss: 8.970\n",
            "Epoch: 290\t Train Loss: 8.390\t Val. Loss: 8.568\n",
            "Epoch: 291\t Train Loss: 8.246\t Val. Loss: 8.287\n",
            "Epoch: 292\t Train Loss: 8.066\t Val. Loss: 8.185\n",
            "Epoch: 293\t Train Loss: 7.973\t Val. Loss: 8.098\n",
            "Epoch: 294\t Train Loss: 8.022\t Val. Loss: 8.038\n",
            "Epoch: 295\t Train Loss: 7.877\t Val. Loss: 7.736\n",
            "Epoch: 296\t Train Loss: 7.685\t Val. Loss: 7.934\n",
            "Val loss decreased (7.713463 --> 7.668298).  Saving model ...\n",
            "Epoch: 297\t Train Loss: 7.796\t Val. Loss: 7.668\n",
            "Epoch: 298\t Train Loss: 7.356\t Val. Loss: 7.694\n",
            "Epoch: 299\t Train Loss: 7.506\t Val. Loss: 7.683\n",
            "Epoch: 300\t Train Loss: 18.610\t Val. Loss: 12.791\n",
            "Epoch: 301\t Train Loss: 18.790\t Val. Loss: 16.892\n",
            "Epoch: 302\t Train Loss: 16.251\t Val. Loss: 17.486\n",
            "Epoch: 303\t Train Loss: 21.618\t Val. Loss: 12.190\n",
            "Epoch: 304\t Train Loss: 16.888\t Val. Loss: 12.936\n",
            "Epoch: 305\t Train Loss: 16.169\t Val. Loss: 13.299\n",
            "Epoch: 306\t Train Loss: 14.379\t Val. Loss: 10.756\n",
            "Epoch: 307\t Train Loss: 15.002\t Val. Loss: 10.427\n",
            "Epoch: 308\t Train Loss: 14.028\t Val. Loss: 12.418\n",
            "Epoch: 309\t Train Loss: 15.535\t Val. Loss: 10.096\n",
            "Epoch: 310\t Train Loss: 13.780\t Val. Loss: 12.178\n",
            "Epoch: 311\t Train Loss: 18.432\t Val. Loss: 13.358\n",
            "Epoch: 312\t Train Loss: 16.949\t Val. Loss: 12.766\n",
            "Epoch: 313\t Train Loss: 15.431\t Val. Loss: 10.645\n",
            "Epoch: 314\t Train Loss: 16.913\t Val. Loss: 10.395\n",
            "Epoch: 315\t Train Loss: 12.879\t Val. Loss: 16.336\n",
            "Epoch: 316\t Train Loss: 14.581\t Val. Loss: 11.180\n",
            "Epoch: 317\t Train Loss: 13.477\t Val. Loss: 12.793\n",
            "Epoch: 318\t Train Loss: 15.245\t Val. Loss: 16.309\n",
            "Epoch: 319\t Train Loss: 17.487\t Val. Loss: 14.535\n",
            "Epoch: 320\t Train Loss: 11.903\t Val. Loss: 9.342\n",
            "Epoch: 321\t Train Loss: 11.179\t Val. Loss: 10.043\n",
            "Epoch: 322\t Train Loss: 12.485\t Val. Loss: 10.583\n",
            "Epoch: 323\t Train Loss: 12.652\t Val. Loss: 10.690\n",
            "Epoch: 324\t Train Loss: 11.386\t Val. Loss: 9.530\n",
            "Epoch: 325\t Train Loss: 10.895\t Val. Loss: 11.806\n",
            "Epoch: 326\t Train Loss: 11.303\t Val. Loss: 9.104\n",
            "Epoch: 327\t Train Loss: 10.857\t Val. Loss: 9.936\n",
            "Epoch: 328\t Train Loss: 11.115\t Val. Loss: 8.646\n",
            "Epoch: 329\t Train Loss: 11.820\t Val. Loss: 10.585\n",
            "Epoch: 330\t Train Loss: 9.810\t Val. Loss: 8.378\n",
            "Epoch: 331\t Train Loss: 9.940\t Val. Loss: 9.510\n",
            "Epoch: 332\t Train Loss: 10.121\t Val. Loss: 8.893\n",
            "Epoch: 333\t Train Loss: 9.173\t Val. Loss: 8.857\n",
            "Epoch: 334\t Train Loss: 9.435\t Val. Loss: 10.931\n",
            "Epoch: 335\t Train Loss: 9.063\t Val. Loss: 8.776\n",
            "Epoch: 336\t Train Loss: 8.506\t Val. Loss: 8.334\n",
            "Epoch: 337\t Train Loss: 8.477\t Val. Loss: 8.094\n",
            "Epoch: 338\t Train Loss: 8.421\t Val. Loss: 9.460\n",
            "Epoch: 339\t Train Loss: 8.447\t Val. Loss: 9.268\n",
            "Epoch: 340\t Train Loss: 8.499\t Val. Loss: 8.733\n",
            "Epoch: 341\t Train Loss: 8.204\t Val. Loss: 8.832\n",
            "Epoch: 342\t Train Loss: 7.848\t Val. Loss: 8.048\n",
            "Epoch: 343\t Train Loss: 7.901\t Val. Loss: 7.699\n",
            "Val loss decreased (7.668298 --> 7.624730).  Saving model ...\n",
            "Epoch: 344\t Train Loss: 7.664\t Val. Loss: 7.625\n",
            "Epoch: 345\t Train Loss: 7.608\t Val. Loss: 7.807\n",
            "Epoch: 346\t Train Loss: 7.475\t Val. Loss: 7.663\n",
            "Val loss decreased (7.624730 --> 7.589856).  Saving model ...\n",
            "Epoch: 347\t Train Loss: 7.492\t Val. Loss: 7.590\n",
            "Val loss decreased (7.589856 --> 7.558365).  Saving model ...\n",
            "Epoch: 348\t Train Loss: 7.318\t Val. Loss: 7.558\n",
            "Val loss decreased (7.558365 --> 7.551872).  Saving model ...\n",
            "Epoch: 349\t Train Loss: 7.354\t Val. Loss: 7.552\n",
            "Epoch: 350\t Train Loss: 22.320\t Val. Loss: 13.845\n",
            "Epoch: 351\t Train Loss: 17.988\t Val. Loss: 16.380\n",
            "Epoch: 352\t Train Loss: 18.364\t Val. Loss: 22.831\n",
            "Epoch: 353\t Train Loss: 15.881\t Val. Loss: 12.226\n",
            "Epoch: 354\t Train Loss: 16.774\t Val. Loss: 22.638\n",
            "Epoch: 355\t Train Loss: 16.581\t Val. Loss: 14.568\n",
            "Epoch: 356\t Train Loss: 13.584\t Val. Loss: 11.992\n",
            "Epoch: 357\t Train Loss: 17.302\t Val. Loss: 11.160\n",
            "Epoch: 358\t Train Loss: 13.797\t Val. Loss: 12.750\n",
            "Epoch: 359\t Train Loss: 16.398\t Val. Loss: 13.054\n",
            "Epoch: 360\t Train Loss: 14.850\t Val. Loss: 11.271\n",
            "Epoch: 361\t Train Loss: 12.856\t Val. Loss: 13.700\n",
            "Epoch: 362\t Train Loss: 15.370\t Val. Loss: 17.047\n",
            "Epoch: 363\t Train Loss: 14.356\t Val. Loss: 11.835\n",
            "Epoch: 364\t Train Loss: 13.250\t Val. Loss: 12.111\n",
            "Epoch: 365\t Train Loss: 14.890\t Val. Loss: 15.398\n",
            "Epoch: 366\t Train Loss: 13.046\t Val. Loss: 10.029\n",
            "Epoch: 367\t Train Loss: 12.924\t Val. Loss: 10.049\n",
            "Epoch: 368\t Train Loss: 13.784\t Val. Loss: 12.518\n",
            "Epoch: 369\t Train Loss: 11.002\t Val. Loss: 14.424\n",
            "Epoch: 370\t Train Loss: 13.661\t Val. Loss: 9.926\n",
            "Epoch: 371\t Train Loss: 11.895\t Val. Loss: 11.057\n",
            "Epoch: 372\t Train Loss: 12.088\t Val. Loss: 9.738\n",
            "Epoch: 373\t Train Loss: 12.040\t Val. Loss: 11.156\n",
            "Epoch: 374\t Train Loss: 11.408\t Val. Loss: 9.595\n",
            "Epoch: 375\t Train Loss: 10.807\t Val. Loss: 8.649\n",
            "Epoch: 376\t Train Loss: 11.444\t Val. Loss: 10.993\n",
            "Epoch: 377\t Train Loss: 9.935\t Val. Loss: 9.256\n",
            "Epoch: 378\t Train Loss: 10.133\t Val. Loss: 8.328\n",
            "Epoch: 379\t Train Loss: 9.595\t Val. Loss: 9.041\n",
            "Epoch: 380\t Train Loss: 9.568\t Val. Loss: 9.713\n",
            "Epoch: 381\t Train Loss: 10.116\t Val. Loss: 9.739\n",
            "Epoch: 382\t Train Loss: 10.933\t Val. Loss: 10.975\n",
            "Epoch: 383\t Train Loss: 9.067\t Val. Loss: 8.513\n",
            "Epoch: 384\t Train Loss: 8.891\t Val. Loss: 9.660\n",
            "Epoch: 385\t Train Loss: 9.490\t Val. Loss: 8.999\n",
            "Epoch: 386\t Train Loss: 8.974\t Val. Loss: 8.459\n",
            "Epoch: 387\t Train Loss: 8.336\t Val. Loss: 8.394\n",
            "Epoch: 388\t Train Loss: 8.551\t Val. Loss: 8.630\n",
            "Epoch: 389\t Train Loss: 8.629\t Val. Loss: 9.251\n",
            "Epoch: 390\t Train Loss: 8.339\t Val. Loss: 8.378\n",
            "Epoch: 391\t Train Loss: 8.252\t Val. Loss: 8.480\n",
            "Epoch: 392\t Train Loss: 8.144\t Val. Loss: 8.479\n",
            "Epoch: 393\t Train Loss: 7.956\t Val. Loss: 7.906\n",
            "Epoch: 394\t Train Loss: 8.103\t Val. Loss: 7.960\n",
            "Epoch: 395\t Train Loss: 7.664\t Val. Loss: 7.773\n",
            "Epoch: 396\t Train Loss: 7.689\t Val. Loss: 7.734\n",
            "Epoch: 397\t Train Loss: 7.483\t Val. Loss: 7.679\n",
            "Epoch: 398\t Train Loss: 7.542\t Val. Loss: 7.656\n",
            "Epoch: 399\t Train Loss: 7.431\t Val. Loss: 7.651\n",
            "Epoch: 400\t Train Loss: 19.303\t Val. Loss: 16.708\n",
            "Epoch: 401\t Train Loss: 19.978\t Val. Loss: 20.630\n",
            "Epoch: 402\t Train Loss: 15.380\t Val. Loss: 18.932\n",
            "Epoch: 403\t Train Loss: 23.360\t Val. Loss: 16.217\n",
            "Epoch: 404\t Train Loss: 18.098\t Val. Loss: 19.030\n",
            "Epoch: 405\t Train Loss: 18.988\t Val. Loss: 15.642\n",
            "Epoch: 406\t Train Loss: 15.611\t Val. Loss: 16.522\n",
            "Epoch: 407\t Train Loss: 14.390\t Val. Loss: 11.869\n",
            "Epoch: 408\t Train Loss: 15.148\t Val. Loss: 12.387\n",
            "Epoch: 409\t Train Loss: 14.729\t Val. Loss: 21.271\n",
            "Epoch: 410\t Train Loss: 17.106\t Val. Loss: 12.094\n",
            "Epoch: 411\t Train Loss: 18.490\t Val. Loss: 11.429\n",
            "Epoch: 412\t Train Loss: 16.217\t Val. Loss: 12.629\n",
            "Epoch: 413\t Train Loss: 13.741\t Val. Loss: 11.239\n",
            "Epoch: 414\t Train Loss: 14.656\t Val. Loss: 11.530\n",
            "Epoch: 415\t Train Loss: 13.492\t Val. Loss: 10.798\n",
            "Epoch: 416\t Train Loss: 13.791\t Val. Loss: 9.416\n",
            "Epoch: 417\t Train Loss: 12.605\t Val. Loss: 10.583\n",
            "Epoch: 418\t Train Loss: 13.388\t Val. Loss: 13.776\n",
            "Epoch: 419\t Train Loss: 12.290\t Val. Loss: 12.600\n",
            "Epoch: 420\t Train Loss: 11.623\t Val. Loss: 10.506\n",
            "Epoch: 421\t Train Loss: 11.983\t Val. Loss: 10.332\n",
            "Epoch: 422\t Train Loss: 14.287\t Val. Loss: 21.328\n",
            "Epoch: 423\t Train Loss: 12.135\t Val. Loss: 9.396\n",
            "Epoch: 424\t Train Loss: 10.278\t Val. Loss: 11.574\n",
            "Epoch: 425\t Train Loss: 12.232\t Val. Loss: 9.881\n",
            "Epoch: 426\t Train Loss: 12.309\t Val. Loss: 10.114\n",
            "Epoch: 427\t Train Loss: 10.408\t Val. Loss: 9.218\n",
            "Epoch: 428\t Train Loss: 10.155\t Val. Loss: 8.975\n",
            "Epoch: 429\t Train Loss: 9.806\t Val. Loss: 8.862\n",
            "Epoch: 430\t Train Loss: 9.317\t Val. Loss: 8.464\n",
            "Epoch: 431\t Train Loss: 9.827\t Val. Loss: 9.198\n",
            "Epoch: 432\t Train Loss: 10.608\t Val. Loss: 15.536\n",
            "Epoch: 433\t Train Loss: 10.722\t Val. Loss: 9.289\n",
            "Epoch: 434\t Train Loss: 9.429\t Val. Loss: 8.596\n",
            "Epoch: 435\t Train Loss: 9.117\t Val. Loss: 9.457\n",
            "Epoch: 436\t Train Loss: 8.974\t Val. Loss: 8.763\n",
            "Epoch: 437\t Train Loss: 8.971\t Val. Loss: 8.639\n",
            "Epoch: 438\t Train Loss: 8.831\t Val. Loss: 9.288\n",
            "Epoch: 439\t Train Loss: 8.860\t Val. Loss: 7.888\n",
            "Epoch: 440\t Train Loss: 8.335\t Val. Loss: 8.256\n",
            "Epoch: 441\t Train Loss: 8.065\t Val. Loss: 7.820\n",
            "Epoch: 442\t Train Loss: 8.188\t Val. Loss: 7.647\n",
            "Epoch: 443\t Train Loss: 7.773\t Val. Loss: 7.646\n",
            "Epoch: 444\t Train Loss: 7.890\t Val. Loss: 7.627\n",
            "Epoch: 445\t Train Loss: 7.723\t Val. Loss: 7.774\n",
            "Epoch: 446\t Train Loss: 7.760\t Val. Loss: 7.752\n",
            "Epoch: 447\t Train Loss: 7.589\t Val. Loss: 7.555\n",
            "Epoch: 448\t Train Loss: 7.309\t Val. Loss: 7.555\n",
            "Val loss decreased (7.551872 --> 7.535606).  Saving model ...\n",
            "Epoch: 449\t Train Loss: 7.427\t Val. Loss: 7.536\n",
            "Epoch: 450\t Train Loss: 15.307\t Val. Loss: 11.214\n",
            "Epoch: 451\t Train Loss: 16.885\t Val. Loss: 16.183\n",
            "Epoch: 452\t Train Loss: 13.618\t Val. Loss: 23.142\n",
            "Epoch: 453\t Train Loss: 18.267\t Val. Loss: 12.748\n",
            "Epoch: 454\t Train Loss: 17.321\t Val. Loss: 12.100\n",
            "Epoch: 455\t Train Loss: 14.336\t Val. Loss: 10.154\n",
            "Epoch: 456\t Train Loss: 14.175\t Val. Loss: 11.635\n",
            "Epoch: 457\t Train Loss: 17.703\t Val. Loss: 19.534\n",
            "Epoch: 458\t Train Loss: 20.759\t Val. Loss: 12.326\n",
            "Epoch: 459\t Train Loss: 15.623\t Val. Loss: 25.075\n",
            "Epoch: 460\t Train Loss: 15.068\t Val. Loss: 15.052\n",
            "Epoch: 461\t Train Loss: 12.476\t Val. Loss: 10.800\n",
            "Epoch: 462\t Train Loss: 12.816\t Val. Loss: 10.805\n",
            "Epoch: 463\t Train Loss: 11.720\t Val. Loss: 9.534\n",
            "Epoch: 464\t Train Loss: 12.486\t Val. Loss: 9.318\n",
            "Epoch: 465\t Train Loss: 11.918\t Val. Loss: 11.407\n",
            "Epoch: 466\t Train Loss: 11.739\t Val. Loss: 12.434\n",
            "Epoch: 467\t Train Loss: 15.132\t Val. Loss: 12.262\n",
            "Epoch: 468\t Train Loss: 14.008\t Val. Loss: 9.946\n",
            "Epoch: 469\t Train Loss: 11.764\t Val. Loss: 12.605\n",
            "Epoch: 470\t Train Loss: 12.966\t Val. Loss: 10.099\n",
            "Epoch: 471\t Train Loss: 11.916\t Val. Loss: 9.657\n",
            "Epoch: 472\t Train Loss: 10.474\t Val. Loss: 10.581\n",
            "Epoch: 473\t Train Loss: 11.720\t Val. Loss: 10.835\n",
            "Epoch: 474\t Train Loss: 11.666\t Val. Loss: 9.774\n",
            "Epoch: 475\t Train Loss: 11.135\t Val. Loss: 9.697\n",
            "Epoch: 476\t Train Loss: 10.560\t Val. Loss: 9.502\n",
            "Epoch: 477\t Train Loss: 11.000\t Val. Loss: 10.463\n",
            "Epoch: 478\t Train Loss: 12.575\t Val. Loss: 9.777\n",
            "Epoch: 479\t Train Loss: 10.648\t Val. Loss: 9.816\n",
            "Epoch: 480\t Train Loss: 9.864\t Val. Loss: 8.299\n",
            "Epoch: 481\t Train Loss: 9.309\t Val. Loss: 8.686\n",
            "Epoch: 482\t Train Loss: 9.483\t Val. Loss: 8.971\n",
            "Epoch: 483\t Train Loss: 9.157\t Val. Loss: 8.210\n",
            "Epoch: 484\t Train Loss: 8.355\t Val. Loss: 8.010\n",
            "Epoch: 485\t Train Loss: 9.141\t Val. Loss: 11.373\n",
            "Epoch: 486\t Train Loss: 9.467\t Val. Loss: 9.693\n",
            "Epoch: 487\t Train Loss: 8.937\t Val. Loss: 8.437\n",
            "Epoch: 488\t Train Loss: 8.161\t Val. Loss: 8.890\n",
            "Epoch: 489\t Train Loss: 8.405\t Val. Loss: 8.948\n",
            "Epoch: 490\t Train Loss: 8.361\t Val. Loss: 9.015\n",
            "Epoch: 491\t Train Loss: 8.223\t Val. Loss: 8.064\n",
            "Epoch: 492\t Train Loss: 8.049\t Val. Loss: 7.543\n",
            "Epoch: 493\t Train Loss: 7.675\t Val. Loss: 7.760\n",
            "Epoch: 494\t Train Loss: 7.676\t Val. Loss: 7.763\n",
            "Epoch: 495\t Train Loss: 7.618\t Val. Loss: 7.708\n",
            "Epoch: 496\t Train Loss: 7.542\t Val. Loss: 7.726\n",
            "Epoch: 497\t Train Loss: 7.658\t Val. Loss: 7.540\n",
            "Epoch: 498\t Train Loss: 7.417\t Val. Loss: 7.545\n",
            "Epoch: 499\t Train Loss: 7.360\t Val. Loss: 7.536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiSCqyhijWzd",
        "colab_type": "text"
      },
      "source": [
        "### Test results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vKyR0Rq1JxW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bb68c9f7-09e5-4065-a35d-64ce073a96a2"
      },
      "source": [
        "mse_test = test(model, criterion)\n",
        "print(\"MSE Test loss: {:.3f}\".format(np.mean(mse_test)))\n",
        "rmse_test = test(model, RMSELoss)\n",
        "print(\"RMSE Test loss: {:.3f}\".format(np.mean(rmse_test)))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE Test loss: 3.747\n",
            "RMSE Test loss: 1.870\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GTtrISY_Xru",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "6378bd74-0727-4e48-fc5f-91a23523d3dc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_curve(model):\n",
        "  model.load_state_dict(torch.load('content/models/model.pt'))\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for val_acc, configs, targets in test_data_loader:\n",
        "        output = model([val_acc, configs], targets, 0)\n",
        "        output = output.squeeze()\n",
        "        output = torch.t(output)\n",
        "        plt.plot(np.arange(51),targets[2],c='g')\n",
        "        plt.plot(np.arange(51),output[2],c='r')\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "plot_curve(model)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZGIUoCLiESUAlxEJKL+P6w6DTZ+b/QjAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}