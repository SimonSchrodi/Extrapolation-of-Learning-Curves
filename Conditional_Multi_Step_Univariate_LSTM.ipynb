{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "Conditional_Multi_Step_Univariate_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x5tMN7p1iqoZ",
        "IiSCqyhijWzd"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJd_VxDKN8pn",
        "colab_type": "text"
      },
      "source": [
        "# Task A: Creating a Performance Predictor\n",
        "\n",
        "In this task, you will use training data from 2000 configurations on a single OpenML dataset to train a performance predictor. The data will be splitted into train, test and validation set and we will only use the first 10 epochs of the learning curves for predicitons. You are provided with the full benchmark logs for Fashion-MNIST, that is learning curves, config parameters and gradient statistics, and you can use them freely.\n",
        "\n",
        "For questions, you can contact zimmerl@informatik.uni-freiburg.\n",
        "\n",
        "__Note: Please use the dataloading and splits you are provided with in this notebook.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqPBWpd5N8pt",
        "colab_type": "text"
      },
      "source": [
        "## Specifications:\n",
        "\n",
        "* Data: fashion_mnist.json\n",
        "* Number of datasets: 1\n",
        "* Number of configurations: 2000\n",
        "* Number of epochs seed during prediction: 10\n",
        "* Available data: Learning curves, architecture parameters and hyperparameters, gradient statistics \n",
        "* Target: Final validation accuracy\n",
        "* Evaluation metric: MSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eYlSG9BN8py",
        "colab_type": "text"
      },
      "source": [
        "## Importing and splitting data\n",
        "\n",
        "__Note__: There are 51 steps logged, 50 epochs plus the 0th epoch, prior to any weight updates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX6ZD931iPpX",
        "colab_type": "code",
        "outputId": "418e94e3-6982-4e30-9d23-dc8d3a6f63af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        }
      },
      "source": [
        "!pip install torchvision\n",
        "!pip install hpbandster"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.2.2)\n",
            "Collecting hpbandster\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/3e/62192b0bb527d9353d222b4b6df14400b3c4f36a92a2b138f11a5eafe000/hpbandster-0.7.4.tar.gz (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.3MB/s \n",
            "\u001b[?25hCollecting Pyro4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/24/123d570d039bbe167f266025c2551683e115ffa475ed5d66208b0c8c97c1/Pyro4-4.78-py2.py3-none-any.whl (90kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 5.1MB/s \n",
            "\u001b[?25hCollecting serpent\n",
            "  Downloading https://files.pythonhosted.org/packages/b4/a1/24871492bfc34ea18aee3bf38e0cee22d8c11d8d5e765ccc921103140747/serpent-1.30.2-py3-none-any.whl\n",
            "Collecting ConfigSpace\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/26/eddb59c773bfcc6376ae8e05ddb9b6469c9d7e6abed28d48211a6e5d6541/ConfigSpace-0.4.12.tar.gz (966kB)\n",
            "\u001b[K     |████████████████████████████████| 972kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hpbandster) (1.17.5)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from hpbandster) (0.10.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hpbandster) (1.4.1)\n",
            "Collecting netifaces\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/9b/c4c7eb09189548d45939a3d3a6b3d53979c67d124459b27a094c365c347f/netifaces-0.10.9-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from ConfigSpace->hpbandster) (0.29.14)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace->hpbandster) (2.4.6)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->hpbandster) (0.5.1)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from statsmodels->hpbandster) (0.25.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.0->statsmodels->hpbandster) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels->hpbandster) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels->hpbandster) (2.6.1)\n",
            "Building wheels for collected packages: hpbandster, ConfigSpace\n",
            "  Building wheel for hpbandster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hpbandster: filename=hpbandster-0.7.4-cp36-none-any.whl size=80007 sha256=8eec35de3a83f6f18544d6b4c862ec3860bda6f87a09c2ce044c0d8399bc71bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/57/62/6b00c8011bac96e0c404adc5be4e16964ba4544614240b4e23\n",
            "  Building wheel for ConfigSpace (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ConfigSpace: filename=ConfigSpace-0.4.12-cp36-cp36m-linux_x86_64.whl size=2925951 sha256=c09b8ae92a97c9d07f817a53ecb76d118a5178f323f8075a3bf61a7fee5a5dde\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/b2/85/feafec2387f97065d914a72c42ff3a0f6f60d8e1fd03c6bd4b\n",
            "Successfully built hpbandster ConfigSpace\n",
            "Installing collected packages: serpent, Pyro4, ConfigSpace, netifaces, hpbandster\n",
            "Successfully installed ConfigSpace-0.4.12 Pyro4-4.78 hpbandster-0.7.4 netifaces-0.10.9 serpent-1.30.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT8tEKUJOqNy",
        "colab_type": "code",
        "outputId": "b8e6f567-0ede-4587-ec0c-9c329757f011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "!pip install wget\n",
        "!pip install zipfile36"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=b0a4aff22cf2018c139e38cced45ba4ba07288dde7e74a90c1b07f2fae82f0d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting zipfile36\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/8a/3b7da0b0bd87d1ef05b74207827c72d348b56a0d6d83242582be18a81e02/zipfile36-0.1.3-py3-none-any.whl\n",
            "Installing collected packages: zipfile36\n",
            "Successfully installed zipfile36-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GSY73FpOPdD",
        "colab_type": "code",
        "outputId": "e11e29cc-1d58-43a4-82c9-d5c7cffc3960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import wget\n",
        "import zipfile\n",
        "dir_path = 'content/'\n",
        "filename=wget.download('https://ndownloader.figshare.com/files/21001311')\n",
        "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"\")\n",
        "!rm fashion_mnist.zip\n",
        "wget.download('https://raw.githubusercontent.com/automl/LCBench/master/api.py')\n",
        "wget.download('https://raw.githubusercontent.com/infomon/Extrapolation-of-Learning-Curves/master/utils.py')\n",
        "!mkdir content/models\n",
        "!mkdir models"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘content/models’: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql1OqCfgN8p4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "%cd ..\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from content.api import Benchmark\n",
        "import content.utils as utils\n",
        "import torch\n",
        "\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKGiEJ_IidBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ConfigSpace as CS\n",
        "import ConfigSpace.hyperparameters as CSH\n",
        "\n",
        "import pickle\n",
        "import logging\n",
        "\n",
        "from hpbandster.core.worker import Worker\n",
        "import hpbandster.core.nameserver as hpns\n",
        "import hpbandster.core.result as hpres\n",
        "from hpbandster.optimizers import BOHB\n",
        "\n",
        "logging.getLogger('hpbandster').setLevel(logging.DEBUG)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2sQv_05N8qJ",
        "colab_type": "code",
        "outputId": "4624e4c3-8b27-41d9-8e74-ed37d177fb7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "bench_dir = dir_path+\"fashion_mnist.json\"\n",
        "bench = Benchmark(bench_dir, cache=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Loading data...\n",
            "==> No cached data found or cache set to False.\n",
            "==> Reading json data...\n",
            "==> Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eq4DxuuN8qe",
        "colab_type": "code",
        "outputId": "d8a681c1-c9e3-45e2-9f6e-b68c7bfa6885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Read data\n",
        "def cut_data(data, cut_position):\n",
        "    targets = []\n",
        "    for dp in data:\n",
        "        targets.append(dp[\"Train/val_accuracy\"][50])\n",
        "        for tag in dp:\n",
        "            if tag.startswith(\"Train/\"):\n",
        "                dp[tag] = dp[tag][0:cut_position]\n",
        "    return data, targets\n",
        "\n",
        "def read_data():\n",
        "    dataset_name = 'Fashion-MNIST'\n",
        "    n_configs = bench.get_number_of_configs(dataset_name)\n",
        "    \n",
        "    # Query API\n",
        "    data = []\n",
        "    for config_id in range(n_configs):\n",
        "        data_point = dict()\n",
        "        data_point[\"config\"] = bench.query(dataset_name=dataset_name, tag=\"config\", config_id=config_id)\n",
        "        for tag in bench.get_queriable_tags(dataset_name=dataset_name, config_id=config_id):\n",
        "            if tag.startswith(\"Train/\"):\n",
        "                data_point[tag] = bench.query(dataset_name=dataset_name, tag=tag, config_id=config_id)    \n",
        "        data.append(data_point)\n",
        "        \n",
        "    # Split: 50% train, 25% validation, 25% test (the data is already shuffled)\n",
        "    indices = np.arange(n_configs)\n",
        "    ind_train = indices[0:int(np.floor(0.5*n_configs))]\n",
        "    ind_val = indices[int(np.floor(0.5*n_configs)):int(np.floor(0.75*n_configs))]\n",
        "    ind_test = indices[int(np.floor(0.75*n_configs)):]\n",
        "\n",
        "    array_data = np.array(data)\n",
        "    train_data = array_data[ind_train]\n",
        "    val_data = array_data[ind_val]\n",
        "    test_data = array_data[ind_test]\n",
        "    \n",
        "    # Cut curves for validation and test\n",
        "    cut_position = 11\n",
        "    #val_data, val_targets = cut_data(val_data, cut_position)\n",
        "    #test_data, test_targets = cut_data(test_data, cut_position)\n",
        "    val_data, val_targets = cut_data(val_data, 51)\n",
        "    test_data, test_targets = cut_data(test_data, 51)\n",
        "    train_data, train_targets = cut_data(train_data, 51)   # Cut last value as it is repeated\n",
        "    \n",
        "    return train_data, val_data, test_data, train_targets, val_targets, test_targets\n",
        "    \n",
        "train_data, val_data, test_data, train_targets, val_targets, test_targets = read_data()\n",
        "\n",
        "print(\"Train:\", len(train_data))\n",
        "print(\"Validation:\", len(val_data))\n",
        "print(\"Test:\", len(test_data))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 1000\n",
            "Validation: 500\n",
            "Test: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PphdZ2aN8qv",
        "colab_type": "text"
      },
      "source": [
        "The data contains the configuration of the trained model and learning curves as well as global and layer-wise gradient statistics.\n",
        "\n",
        "__Note__: Not all parameters vary across different configurations. The varying parameters are batch_size, max_dropout, max_units, num_layers, learning_rate, momentum, weight_decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5tMN7p1iqoZ",
        "colab_type": "text"
      },
      "source": [
        "## Sample use of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybnns3VKN8q0",
        "colab_type": "code",
        "outputId": "ba89ad17-ee81-468e-948c-15f97154755a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Config\n",
        "print(\"Config example:\", train_data[0][\"config\"])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Config example: {'batch_size': 71, 'imputation_strategy': 'mean', 'learning_rate_scheduler': 'cosine_annealing', 'loss': 'cross_entropy_weighted', 'network': 'shapedmlpnet', 'max_dropout': 0.025926231827891333, 'normalization_strategy': 'standardize', 'optimizer': 'sgd', 'cosine_annealing_T_max': 50, 'cosine_annealing_eta_min': 1e-08, 'activation': 'relu', 'max_units': 293, 'mlp_shape': 'funnel', 'num_layers': 3, 'learning_rate': 0.0018243300267253295, 'momentum': 0.21325193168301043, 'weight_decay': 0.020472816917443872}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua6h3ORrlA5N",
        "colab_type": "code",
        "outputId": "62f52a96-6256-40db-adb4-152ac34d32ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "train_data[1][\"config\"]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu',\n",
              " 'batch_size': 457,\n",
              " 'cosine_annealing_T_max': 50,\n",
              " 'cosine_annealing_eta_min': 1e-08,\n",
              " 'imputation_strategy': 'mean',\n",
              " 'learning_rate': 0.01239328605026128,\n",
              " 'learning_rate_scheduler': 'cosine_annealing',\n",
              " 'loss': 'cross_entropy_weighted',\n",
              " 'max_dropout': 0.5472322491757223,\n",
              " 'max_units': 950,\n",
              " 'mlp_shape': 'funnel',\n",
              " 'momentum': 0.16411425552061212,\n",
              " 'network': 'shapedmlpnet',\n",
              " 'normalization_strategy': 'standardize',\n",
              " 'num_layers': 4,\n",
              " 'optimizer': 'sgd',\n",
              " 'weight_decay': 0.09762768273307641}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTl67TOTN8rD",
        "colab_type": "code",
        "outputId": "893382c4-9929-4429-f428-6a32e044b575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Learning curve\n",
        "plt.plot(train_data[10][\"Train/val_accuracy\"])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4984eb5c50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXGuPeBKN8rM",
        "colab_type": "code",
        "outputId": "4224d09a-0cf5-486f-ed3e-4396f2d8dc6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Gradient statistics\n",
        "plt.plot(train_data[10][\"Train/layer_wise_gradient_mean_layer_0\"])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f49849890b8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAD4CAYAAAA6j0u4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de3xV5Z3v8c9v751swjWJhBgCiCjg\n4B2jYGuttYpoLzjT1tNOp1BHRMeec6adnrZ22jnO0XbGmTMzbZ3OOEM9KGgvYrWFtlpFKs6MFiXg\nBUQF5B4CBBIuIZDr7/yxV2Ab906EfVm5fN+v137ttZ79rOeiIb88l72WuTsiIiK5FAm7ASIi0v8p\n2IiISM4p2IiISM4p2IiISM4p2IiISM7Fwm5AbzVy5EgfP3582M0QEelTVq9evc/dy7qmK9ikMX78\neKqrq8NuhohIn2Jm21KlaxpNRERyTsFGRERyTsFGRERyTsFGRERyTsFGRERyLivBxsxmmtnbZrbJ\nzO5M8XnczB4NPn/JzMYnffbNIP1tM7uupzLN7MygjE1BmYWnWoeIiORHxsHGzKLAvwDXA1OAz5nZ\nlC7ZbgEa3P1s4HvA3wXXTgE+C5wLzAT+1cyiPZT5d8D3grIagrJPuo5M+y0iIu9fNr5ncxmwyd03\nA5jZz4BZwPqkPLOAvw6Ofw780MwsSP+ZuzcDW8xsU1Aeqco0szeBq4E/DvIsDMq9/xTq+H0W+v4e\nD72whfojLSk/u3JSGVXjS3NRrYhIr5aNYFMJ7Eg63wlMS5fH3dvM7CBwWpC+ssu1lcFxqjJPAw64\ne1uK/KdSx7uY2TxgHsC4cePSdrg7P3l5Oxv3Nr4n3R1Wbqln8W2Xn1K5IiJ9me4gkMTd5wPzAaqq\nqk7pqXLPfOXDKdO/+ODLaUc8IiL9XTY2CNQAY5POxwRpKfOYWQwYAezv5tp06fuB4qCMrnWdbB15\nFY9FaG7tyHe1IiK9QjaCzSpgYrBLrJDEYvzSLnmWAnOC408Dv/PE86iXAp8NdpKdCUwEXk5XZnDN\nc0EZBGUuOcU68ioei9LSrmAjIgNTxtNowfrIfweeBqLAAnd/w8zuBqrdfSnw/4CHg8X5ehLBgyDf\nYhKbCdqAL7l7O0CqMoMqvwH8zMy+A7wSlM2p1JFPiZFN3qsVEekVLPHHv3RVVVXl2bzr87d/uZan\n1u5m9V9dm7UyRUR6GzNb7e5VXdN1B4E8iceiNLdpGk1EBiYFmzyJxyI0t2kaTUQGJgWbPInHorS2\nO+0dmrYUkYFHwSZP4gWJ/9QtmkoTkQFIwSZPCqOJ/9SaShORgUjBJk86RzbaJCAiA5GCTZ7EY4kb\nTWsaTUQGIgWbPInHNI0mIgOXgk2edAabY7o/mogMQAo2eRIvSEyjac1GRAYiBZs80TSaiAxkCjZ5\nciLYaGQjIgOPgk2eFHYGG63ZiMgApGCTJ51bnzWNJiIDkR4LnSed02j6no1I3+DuNLd1cKy1nWOt\nHRxtbedoSztHW9s51tpOS1sHzW0dtLR30NKWeLW2d76cts7jjsRxS5C3uS3xeUtbooy2jsQ9E9s6\nnI7Od0+kdXiiHe7Q4R68oLW9g7b2RN62jg7a2xOfRczAIGJGxMDMAGgP6mjvcNr9xD0aoxEjFrGk\n9wixiLHia1cxKNjUlC0KNnmiOwiI5E97h3PoaCsHj7Zy4GgrB5paOHi0lUNHWznc3EbjsTYag/fD\nzW0c6Xy1tL/rOBs3zi2IGrFIhMJY8Iq++z0WTfyij5gRL4hQZCd++UMiaETMiETAOPFZLGrEoong\nEItEMON4UIITwQkgFokQMSMagWgkQnD3LNo7oL3j3QGvvd2DurNLwSZPTkyjKdiIvF8dHU5DUwv1\nR1poaGqloamFA00njg82JQJK11djcxvdPRcyFjGGDYoxdFCMIYUxhsZjFA8uZExJjMGFUYbEYwyJ\nRxlcGKOoIEpRYZSigiiDCiIMKogyqCBKPAge8ViEwmj0eDApiBoFQRCIRuz46GKgU7DJE219FjnB\n3Tl4tJXag8eoPXiUXQcS73sPNVPX2Ezd4Wb2NTazr7El7eiiMBphxOACiosKGF5UQPnwQUwqH8aI\n4Ly4qIDiwYnXiKJCigcXMHxQAcMGxYjHIgoCeZZRsDGzUuBRYDywFbjJ3RtS5JsDfDs4/Y67LwzS\nLwEeAoqAJ4E/d3dPV64lfjp+ANwANAFfdPc1PdTxXWA2UOLuQzPpbybi2o0mA8ix1nZ2NjRRc+AY\ntQeOsutg4r324DF2HTzK7oPHaGp59x9e0YhRNjRO2bA45cMHce7o4ZQNi1M2NE7p0DglgwsoGZwI\nGiWDCxlcGFXA6EMyHdncCSx393vN7M7g/BvJGYLAcRdQBTiw2syWBkHpfuBW4CUSwWYm8FQ35V4P\nTAxe04Lrp/VQx6+AHwIbM+xrRsyMwlhE02jSbxw61srmuiNs2dfItv1NbK9vYkd94n3PoeZ35TWD\nUcPiVIwo4pzTh/GRyaOoGDGIihFFVBQPYvSIIsqGxYnmYK1AeodMg80s4KrgeCGwgi7BBrgOWObu\n9QBmtgyYaWYrgOHuvjJIXwTcSCLYpCt3FrDI3R1YaWbFZlYR5H1PHcBPk8rPsKuZ06Ohpa9xd3Yf\nOsaGPY1s3HOYd+qO8E5dI5vrjrCv8URAMYOK4YMYWzqYKyeWMa50MGNLB1NZUkTFiEGUDx9EQVTf\ntBjIMg025e5eGxzvBspT5KkEdiSd7wzSKoPjrundldtdWanST4qZzQPmAYwbN+5kL+9RXCMb6cUO\nH2tl/a5DvFl7iLf3NLJhz2E27DnM4WNtx/OUDC5gQtlQrj6njAllQ5kwcggTyoYytrTo+CYYkVR6\nDDZm9ixweoqPvpV8Eqy1ZL5PsItclZumrvnAfICqqqqs1xmPRbVmI71Cw5EWXtt5gDd2HWL9rkOs\n23WQbfubjn9ePLiASeXDmHXRaCaXD2Ni+TAmlQ+jdEhhiK2WvqzHYOPu16T7zMz2mFmFu9cG01l7\nU2Sr4cSUGMAYEtNiNcFxcnpNcJyu3BpgbIpr0tXRq8RjEVraFWwkv9raO3h7z2HWbD/AK9sbeHX7\nATbvO3L883Glgzl39HA+c8kYzh09gimjhzNqWLxXTD1L/5HpNNpSYA5wb/C+JEWep4G/MbOS4HwG\n8E13rzezQ2Y2ncQGgdnAP/dQ7lLgv5vZz0hsEDgYBKSUdWTYt6wrjEVobtWajeTWoWOtrNnWwOpt\nDVRvbeC1nQeO7/waObSQi8eV8OmqMVw0tphzR49gRFFByC2WgSDTYHMvsNjMbgG2ATcBmFkVcLu7\nzw2Cyj3AquCauzsX8oE7OLH1+anglbZcEjvWbgA2kdj6fDNAd3WY2d8DfwwMNrOdwAPu/tcZ9vuU\nxAuiWrORrNvf2MyL7+znpS37qd7awNt7DuOe2Er8BxXDuKlqLBePK2bquBLGlBRpxCKhMO/ua7YD\nWFVVlVdXV2e1zJv+/fdEDH427/KslisDS2NzGy9v2c8Lm/bzwqZ9vLX7MABDCqNMPaOEqjNKqRpf\nwkVjixkS1/e2Jb/MbLW7V3VN109iHsVjERqb23rOKNLFzoYmnnljD8+s30311gbaOpzCWISqM0r4\n2nWT+cBZp3F+5Qhi2l4svZSCTR7FY1H2N7aE3QzpA9ydt/cc5ul1iQDzxq5DAEwqH8qtV07girNH\ncskZJVm/M69IrijY5FG8QF/qlPTa2jtYva2BZ9bvYdn6PWyvb8IMpo4r4ZvXn8OMc0/nzJFDwm6m\nyClRsMmjeFRf6pR3a2pp4z827GPZ+j387q09NDS1UhiN8IGzT+O2D0/g2inljBo2KOxmimRMwSaP\nEiMbBZuBrqPDWbl5P4+vqeGpdbU0tbQzfFCMq88ZxYxzT+fKSWUM1cK+9DP6ic6jeCyqJ3UOYO/U\nNfLEmp38Yk0Nuw4eY2g8xicuGM0nLxrNZWeW6t5h0q8p2OSRbsQ58Bxrbec3r9fy45e2sWb7ASIG\nH5pYxp03/AHX/kE5RYVa4JeBQcEmjzpvxOnu+mJdP7e5rpEfv7Sdn6/eycGjrUwoG8Jf3nAON15U\nyajhWoORgUfBJo/iBVHcobXdKYwp2PQ3be0dLFu/h4dXbuPFd/YTixjXnXc6fzLtDKZPKNUfGDKg\nKdjkUfKjoQtjmp/vLxqb23h01Q4efGELOxuOUllcxNeum8xnqsZoJ5lIQMEmj04Emw6GhdwWyVzt\nwaM89MJWfvLydg4fa+PS8SV8+2NTuHZKuZ44KdKFgk0edT5cStuf+7ZNew/zL8+9w69e20WHO9ef\nX8GtH5rARWOLw26aSK+lYJNHnVNnesxA37S5rpH7lm9kyWu7KCqIMvvy8dz8wfGMLR0cdtNEej0F\nmzxKnkaTvmPLviP88/KN/PLVGuKxKPOunMC8D03gtKHxsJsm0mco2ORRvCARbPTFzr6h5sBRvrds\nA794pYaCqHHLFWdy24fPYqSCjMhJU7DJI63Z9A2NzW3cv2ITD/znFhz44gfGc9uHJ2hnmUgGFGzy\nKHnrs/Q+be0dLK7eyT8te5t9jS3ceNFovjbzHCqLi8Jumkifp2CTR8dHNq0a2fQ2z2+o47u/Wc+G\nPY1cOr6EB+Zcqt1lIlmU0TcLzazUzJaZ2cbgvSRNvjlBno1mNicp/RIzW2tmm8zsPgu+Yp2uXEu4\nL8j/uplN7a4OMxtsZr8xs7fM7A0zuzeT/maqc81G02i9x+6Dx7jt4WrmLHiZ5rYO7v/8VBbfdrkC\njUiWZfo19juB5e4+EVgenL+LmZUCdwHTgMuAu5KC0v3ArcDE4DWzh3KvT8o7L7i+pzr+wd3PAS4G\nPmhm12fY51OmabTeo73DeeiFLVzzT8+z4u06vnbdZJ75ypVcf36FbisjkgOZBptZwMLgeCFwY4o8\n1wHL3L3e3RuAZcBMM6sAhrv7Snd3YFHS9enKnQUs8oSVQHFQTso63L3J3Z8DcPcWYA0wJsM+n7JC\nbX3uFdbvOsQf3f8if/2r9Vw8rphnvnIlX/rI2cenOUUk+zJdsyl399rgeDdQniJPJbAj6XxnkFYZ\nHHdN767c7spKlX6cmRUDnwB+kK4zZjaPxIiJcePGpct2yk6s2WhkE4ajLe18/9kNPPBfWygZXMAP\nPnsRn7xwtEYyInnQY7Axs2eB01N89K3kE3d3M/NsNSyb5ZpZDPgpcJ+7b+6mrvnAfICqqqqs96Vz\nGq2lXSObfHth0z7ufOJ1dtQf5bOXjuXO68+heHBh2M0SGTB6DDbufk26z8xsj5lVuHttMJ21N0W2\nGuCqpPMxwIogfUyX9JrgOF25NcDYFNekq6PTfGCju38/XV/y4fiajXaj5c3Bpla+++R6Flfv5MyR\nQ/jZvOlMn3Ba2M0SGXAyXbNZCnTuLpsDLEmR52lghpmVBIv2M4Cng2myQ2Y2PdiFNjvp+nTlLgVm\nB7vSpgMHg3JS1gFgZt8BRgBfzrCvGYtFI0QjpjWbPPntulqu+d7zPL6mhts/fBZP/fmHFGhEQpLp\nms29wGIzuwXYBtwEYGZVwO3uPtfd683sHmBVcM3d7l4fHN8BPAQUAU8Fr7TlAk8CNwCbgCbgZoB0\ndZjZGBLTfW8Ba4K5+R+6+wMZ9vuU6dHQube/sZlv/3IdT63bzZSK4Tz4xUs5r3JE2M0SGdAyCjbu\nvh/4aIr0amBu0vkCYEGafOedRLkOfClNW95Th7vvBHrV6m/no6ElN17bcYDbH1nN/iMtfH3mZG79\n0AQKonpQnUjYdAeBPIvHolqzyZHFq3bw7SXrKBsa54k/+4BGMyK9iIJNnsULNI2WbS1tHdz96zd4\nZOV2rjh7JPd97mJKh2inmUhvomCTZ4VRTaNl095Dx/izH69h9bYGbvvwBL42YzIxTZuJ9DoKNnmW\nGNko2GTDmu0N3P7wag4fa+OHf3wxH79gdNhNEpE0FGzyLB6L6uFpWfCr13bx1cde4/Thg1h0y2Wc\nc/rwsJskIt1QsMkzbX3OjLvzw99t4h+XbeDS8SX8+xeqtD4j0gco2ORZPBahsbkt7Gb0Sc1t7Xzz\nibU8saaGP7y4kns/db5uninSRyjY5Jm2Pp+ahiMt3PbIal7eUs9fXDuJ/3H12bqBpkgfomCTZ9r6\nfPK27DvCzQ++zK4Dx/jBZy9i1kWVPV8kIr2Kgk2e6Q4CJ2ddzUHmLHgZB35y6zSqxpeG3SQROQUK\nNnkWj0UVbN6nVVvr+dMHVzFsUIxH5k5jQtnQsJskIqdIwSbPCmMRPTztfVjx9l5uf2Q1o0cU8fDc\naVQWF4XdJBHJgIJNnmkarWe/eb2WLz/6ChNHDWPRLZcxcmg87CaJSIZ0X488i8eitHU47R1ZfxBo\nv/Doqu38j5+u4cIxxfx03nQFGpF+QsEmz+IFwaOhNbp5jwX/tYVvPL6WKyaW8fAt0xhRVBB2k0Qk\nSxRs8uz4o6G1/fldfvzSNu7+9Xpmnns6D8yuoqhQX9YU6U8UbPKs8xvvWrc54Rev7OTbv1zHRyaX\ncd/nLqYwph9Lkf5G/6rz7PjIRncRAOC363bzvx57nelnnsb9f3KJAo1IP5XRv2wzKzWzZWa2MXgv\nSZNvTpBno5nNSUq/xMzWmtkmM7vPgvuPpCvXEu4L8r9uZlPfRx2/NbPXzOwNM/s3Mwt1fqZzzUbT\naPD8hjr+509f4YIxI/jRnCoGFWjqTKS/yvTPyDuB5e4+EVgenL+LmZUCdwHTgMuAu5KC0v3ArcDE\n4DWzh3KvT8o7L7i+pzpucvcLgfOAMuAzGfY5I5pGS3hp835ue7ias0cN5aEvXsbQuHbhi/RnmQab\nWcDC4HghcGOKPNcBy9y93t0bgGXATDOrAIa7+0p3d2BR0vXpyp0FLPKElUBxUE7KOgDc/VBwbQwo\nBELdc1yoDQK8tuMAtyysprK4iEW3XMaIwdp1JtLfZRpsyt29NjjeDZSnyFMJ7Eg63xmkVQbHXdO7\nK7e7slKlA2BmTwN7gcPAz9N1xszmmVm1mVXX1dWly5aRE7vRBubIZvv+Jm5+aBUlQwr48Vx9j0Zk\noOgx2JjZs2a2LsVrVnK+YHSS9VFDNsp19+uACiAOXN1NvvnuXuXuVWVlZZlUmdZADjYHm1r54kMv\n0+HOoj+dxukjBoXdJBHJkx4nyt39mnSfmdkeM6tw99pgOmtvimw1wFVJ52OAFUH6mC7pNcFxunJr\ngLEprklXR3I/jpnZEhJTccvS9SnXjq/ZDLDdaC1tHdz2SDU764/yyNxpnDlySNhNEpE8ynQabSnQ\nufNrDrAkRZ6ngRlmVhIs2s8Ang6myQ6Z2fRgF9rspOvTlbsUmB3sSpsOHAzKSVmHmQ0NghVmFgM+\nBryVYZ8zMhB3o7k733xiLSs31/P3n76Ay87UYwJEBppMtwDdCyw2s1uAbcBNAGZWBdzu7nPdvd7M\n7gFWBdfc7e71wfEdwENAEfBU8EpbLvAkcAOwCWgCbgZIV4eZlQNLzSxOIrA+B/xbhn3OyECcRvvh\n7zbx+JqdfOWaSdx4sR58JjIQZRRs3H0/8NEU6dXA3KTzBcCCNPnOO4lyHfhSmra8pw533wNc2lM/\n8mmgbX1e8moN/7hsA390cSX/86Nnh90cEQmJvq6dZ8en0QbAM21Wba3na4+9zrQzS/nbT51P8J1d\nERmAFGzyrDA6MKbRag8e5baHVzOmpIh//8Ilx0d0IjIwKdjk2UBYs2lp6+COH6+hubWdH82ponhw\nYdhNEpGQ6R4heWZmFMYi/fp5Nn/z5Ju8sv0A//r5qZxVNjTs5ohIL6CRTQgSj4bun2s2S16t4aEX\ntzL3ijO54fyKsJsjIr2Egk0I4rFov5xG27DnMHc+vpZLx5fwjevPCbs5ItKLKNiEIB6L9Ls7CDQ2\nt3H7I6sZEo/xwz+eSkFUP1oicoLWbEIQL+hf02juzjd+/jrb9jfx47nTKB+ue56JyLvpz88Q9Ldp\ntAUvbOU3a2v5+nWTmT7htLCbIyK9kIJNCBIbBPpHsFm78yB/++SbzJhSzrwrJ4TdHBHppRRsQlAY\ni/SLOwgcbWnny4++wsihcf7+0xfoDgEikpaCTQj6y8jm3qfe5J26I/zDZy7UFzdFpFsKNiGIx6J9\n/kudz2+oY+Hvt3HzB8dzxcSRYTdHRHo5BZsQ9PXdaA1HWvjaY68xcdRQvjFT36cRkZ5p63MI+vI0\nmrvzl79YS0NTCw/efCmDCnSDTRHpmUY2IejLW58fX1PDU+t28xfXTubc0SPCbo6I9BEKNiGI99Hd\naDvqm/jrpW9w2fhSbXMWkZOiYBOCxJpN3xrZdHQ4X138GgD/eNOFRCPa5iwi719GwcbMSs1smZlt\nDN5L0uSbE+TZaGZzktIvMbO1ZrbJzO6z4Isa6cq1hPuC/K+b2dSe6kj6fKmZrcukv9nSOY2WeMp1\n3/DIS9t4eWs9d31iCmNLB4fdHBHpYzId2dwJLHf3icDy4PxdzKwUuAuYBlwG3JUUlO4HbgUmBq+Z\nPZR7fVLeecH1PdWBmf0R0JhhX7Om8wFqLe19Y3RTe/Aof//bt/nQxJF8+pIxYTdHRPqgTIPNLGBh\ncLwQuDFFnuuAZe5e7+4NwDJgpplVAMPdfaUn/sRflHR9unJnAYs8YSVQHJSTsg4AMxsK/AXwnQz7\nmjV97Wmddy15g7aODr574/m6S4CInJJMg025u9cGx7uB8hR5KoEdSec7g7TK4LhrenfldldWqnSA\ne4B/BJp66oyZzTOzajOrrqur6yn7KTs+sukDwea363bzzPo9fPmaSYw7TdNnInJqevyejZk9C5ye\n4qNvJZ+4u5tZ1hchMinXzC4CznL3r5jZ+PdR13xgPkBVVVXOFlTiscR3U3r7yObQsVbuWrqOP6gY\nzi1XnBl2c0SkD+sx2Lj7Nek+M7M9Zlbh7rXBdNbeFNlqgKuSzscAK4L0MV3Sa4LjdOXWAGNTXJOu\njsuBKjPbSqKvo8xshbsn5827eEEwjdbLtz//39++Td3hZuZ/oUoPQxORjGT6G2Qp0Lnzaw6wJEWe\np4EZZlYSLNrPAJ4OpskOmdn0YBfa7KTr05W7FJgd7EqbDhwMyklXx/3uPtrdxwNXABvCDjTQN9Zs\nVm+r55GXtvHFD5zJhWOLw26OiPRxmd6u5l5gsZndAmwDbgIwsyrgdnef6+71ZnYPsCq45m53rw+O\n7wAeAoqAp4JX2nKBJ4EbgE0k1mBuBuihjl6nt0+jtbR18M0n1jJ6RBFfnTEp7OaISD+QUbBx9/3A\nR1OkVwNzk84XAAvS5DvvJMp14Etp2pKyjqTPt6aqKwzHRza9dBrt359/hw17GlnwxSqGxHX7PBHJ\nnCbiQ3B8zaYXjmy27T/CPz+3iY9dUMHV56TaXCgicvIUbEJQGO2902h/++RbxCLG//74lLCbIiL9\niIJNCDpHNr3teza/f2c/v31jN3dcdRblwweF3RwR6UcUbEJwYjda71mzae9wvvOb9VQWFzH3Q7qj\ns4hkl4JNCHrjbrTHV+/kjV2H+Mb15+iBaCKSdQo2Iehtu9Eam9v4v8+8zdRxxXzigoqwmyMi/ZCC\nTQh62260+1dsou5wM3/18Sm60aaI5ISCTQgKo70n2OxsaOJH/7mFP7y4kovHpXwckYhIxhRsQhCL\nRohFrFdsELj3qbeIGHx95uSwmyIi/ZiCTUgKYxGaW8Md2azeVs+vX6/ltivPomJEUahtEZH+TcEm\nJPFYJNRptI4O5+5frad8eJzbPqytziKSWwo2IYnHoqF+qfM3a2t5bedBvn7dOQwu1P3PRCS3FGxC\nEi+IhLZm09bewfee3cDk8mH84cWVPV8gIpIhBZuQhDmNtuTVXWyuO8JXrp1EJKKtziKSewo2IYnH\noqEEm9b2Dn6wfCPnVQ7nunN1V2cRyQ8Fm5AkRjb5n0b7+eqdbK9v4qvXTtYXOEUkbxRsQhIvyP/W\n52Ot7dy3fCNTxxVz1eSyvNYtIgObgk1IwphG+9nL26k9eIyvztCoRkTyK6NgY2alZrbMzDYG7ynv\nd2Jmc4I8G81sTlL6JWa21sw2mdl9FvwGTFeuJdwX5H/dzKa+jzpWmNnbZvZq8BqVSZ+zpTCa32m0\noy3t/MuKd5g+oZQPnHVa3uoVEYHMRzZ3AsvdfSKwPDh/FzMrBe4CpgGXAXclBaX7gVuBicFrZg/l\nXp+Ud15wfU91AHze3S8KXnsz7HNWJLY+529k8/DKrdQdbtaoRkRCkWmwmQUsDI4XAjemyHMdsMzd\n6929AVgGzDSzCmC4u690dwcWJV2frtxZwCJPWAkUB+WkrCPDvuVUPBbJ25c6G5vbuH/FO1w5qYxL\nx5fmpU4RkWSZBptyd68NjncDqfbSVgI7ks53BmmVwXHX9O7K7a6sVOmdHgym0P7Kuvmz3szmmVm1\nmVXX1dWly5YV+VyzeeiFLTQ0tfLVayflpT4Rka56vE+JmT0LnJ7io28ln7i7m5lnq2FZLPfz7l5j\nZsOAx4EvkBhFpaprPjAfoKqqKut9SRaPRfLy8LSDR1uZ/x+buXZKOReOLc55fSIiqfQYbNz9mnSf\nmdkeM6tw99pgOivVekgNcFXS+RhgRZA+pkt6TXCcrtwaYGyKa9LVgbvXBO+HzewnJNZ0UgabfMrX\nms2iF7dy6FgbX7lGoxoRCU+m02hLgc6dX3OAJSnyPA3MMLOSYNF+BvB0ME12yMymB1Nbs5OuT1fu\nUmB2sCttOnAwKCdlHWYWM7ORAGZWAHwcWJdhn7MiHovS1uG0tecu4BxtaefBF7fy0XNGMWX08JzV\nIyLSk0xv93svsNjMbgG2ATcBmFkVcLu7z3X3ejO7B1gVXHO3u9cHx3cADwFFwFPBK225wJPADcAm\noAm4GSBdHWY2hETQKQCiwLPAjzLsc1bEY4k439LeQSyam687La7eQf2RFv7sqrNyUr6IyPuVUbBx\n9/3AR1OkVwNzk84XAAvS5DvvJMp14Etp2vKeOtz9CHBJT/0IQ2ewaW7tYHBh9stvbe9g/n9s5tLx\nJVRpB5qIhEx3EAhJYSwKkLN1m1+9touaA0c1qhGRXkHBJiTHRzY5uItAR4fzb8+/w+TyYXxkcq+4\nYYKIDHAKNiGJFwRrNjkY2W5gOMwAAA1HSURBVPzurb1s2NPIn111lu4WICK9goJNSOI5mkZzd/51\nxSbGlBTx8Qsqslq2iMipUrAJSa6m0VZtbWDN9gPMu3JCzna5iYicLP02CknybrRsun/FJk4bUshn\nLhnbc2YRkTxRsAlJvCD702hv1h7iubfruPmD4ykqjGatXBGRTCnYhCQX02j/9vw7DCmM8oXp47NW\npohINijYhKTweLDJzshmR30Tv3ptF5+ffgYjBhdkpUwRkWxRsAlJttdsHnxhKxEz/vSDZ2alPBGR\nbFKwCcnxrc9ZuBHnkeY2Hlu9gxvOr+D0EYMyLk9EJNsUbELS+aXObDzT5pev1nD4WBuzLz8j47JE\nRHJBwSYk8Syt2bg7i17cxpSK4VxyRkk2miYiknUKNiEpjGYn2Ly0pZ639xxmzgfO0K1pRKTXUrAJ\niZklHg2d4dbnRb/fyoiiAj55YWV2GiYikgMKNiGKxyIZ7UarPXiUp9/Yw3+7dKy+xCkivZqCTYji\nBdGMptF++tJ2Otz5k2naGCAivZuCTYgKo6c+jdbc1s5PXt7O1ZNHMe60wVlumYhIdmUUbMys1MyW\nmdnG4D3ldigzmxPk2Whmc5LSLzGztWa2yczus2CFO125lnBfkP91M5v6PuooNLP5ZrbBzN4ys09l\n0udsihdETnlk89t1u9nX2MLsD4zPbqNERHIg05HNncByd58ILA/O38XMSoG7gGnAZcBdSUHpfuBW\nYGLwmtlDudcn5Z0XXN9THd8C9rr7JGAK8HyGfc6aeCx6yg9PW/jiVs4cOYQPnT0yy60SEcm+TIPN\nLGBhcLwQuDFFnuuAZe5e7+4NwDJgpplVAMPdfaW7O7Ao6fp05c4CFnnCSqA4KCdlHcE1fwr8LYC7\nd7j7vgz7nDWJ3WgnH2zW7jzImu0H+ML0M4hEtN1ZRHq/TINNubvXBse7gfIUeSqBHUnnO4O0yuC4\na3p35XZX1nvSzaw4OL/HzNaY2WNmlqqNAJjZPDOrNrPqurq6dNmyJrEb7eTXbBb9fiuDC6N86pIx\n2W+UiEgO9BhszOxZM1uX4jUrOV8wOvFsNzDDcmPAGOBFd58K/B74h27qmu/uVe5eVVZWdopVvn+n\nshut4UgLS17bxR9eXMmIIt3dWUT6hlhPGdz9mnSfmdkeM6tw99pgOmtvimw1wFVJ52OAFUH6mC7p\nNcFxunJrgLEprklXx36gCXgiSH8MuCVdf/LtVKbRFlfvoKWtg9mXj89No0REciDTabSlQOfOrznA\nkhR5ngZmmFlJsGg/A3g6mCY7ZGbTg11os5OuT1fuUmB2sCttOnAwKCddHQ78ihOB6KPA+gz7nDUn\newcBd+fR6h1UnVHC5NOH5bBlIiLZ1ePIpgf3AovN7BZgG3ATgJlVAbe7+1x3rzeze4BVwTV3u3t9\ncHwH8BBQBDwVvNKWCzwJ3ABsIjFiuRmghzq+ATxsZt8H6jqv6Q3isehJ3UFgzfYGNtcd4fZPnZXD\nVomIZF9Gwcbd95MYLXRNrwbmJp0vABakyXfeSZTrwJfStCVdHduAK7vrR1gKT3IabfGqnQwujHLD\nBRU5bJWISPbpDgIhOplptCPNbfz69V18/IIKhsYzHZCKiOSXgk2I4gWR9/2lzt+sreVISzs3VY3t\nObOISC+jYBOieCyx9TkxO9i9x6p3MGHkED0gTUT6JAWbEHU+rbOlvfvRzea6RlZtbeAzVWP1gDQR\n6ZMUbEL0fh8N/djqnUQjxqem6gFpItI3KdiEKF6QeOBZd9uf29o7eHz1Tj4yuYxRwwflq2kiIlml\nYBOiEyOb9DvSnt9Qx97DzXxGGwNEpA9TsAnR+5lGW1y9g5FDC7n6nFH5apaISNYp2IToeLBJM422\nr7GZ5W/u5Y+mjqEgqv9VItJ36TdYiOKxxJpNut1ov1hTQ1uHc1OVHiUgIn2bgk2IToxs3rtm4+4s\nrt7B1HHFnD1KN90Ukb5NwSZE8YL0azav7jjAxr2NumOAiPQLCjYh6pxGSxVsHlu9k6KCKB/TTTdF\npB9QsAlRuq3PLW0dPLm2lhnnljNskJ7GKSJ9n4JNiI6PbLrsRvuvTXUcaGpl1kWjw2iWiEjWKdiE\nKN2azZJXd1E8uIArzi4Lo1kiIlmnYBOiwuh7p9GaWtp45o093HB+BYUx/e8Rkf5Bv81ClGpks2z9\nHo62tjPrQk2hiUj/kVGwMbNSM1tmZhuD95QPWzGzOUGejWY2Jyn9EjNba2abzOw+C+6fn65cS7gv\nyP+6mU3trg4zG2Zmrya99pnZ9zPpczZ1jmySH6C29NVdVIwYxKXjS8NqlohI1mU6srkTWO7uE4Hl\nwfm7mFkpcBcwDbgMuCspKN0P3ApMDF4zeyj3+qS884Lr09bh7ofd/aLOF7ANeCLDPmdNLBohFrHj\n02gNR1p4fkMdn7xwNJGInlsjIv1HpsFmFrAwOF4I3Jgiz3XAMnevd/cGYBkw08wqgOHuvtITj6pc\nlHR9unJnAYs8YSVQHJSTso7kRpjZJGAU8J8Z9jmr4rHI8d1oT63bTVuH8wlNoYlIP5NpsCl399rg\neDdQniJPJbAj6XxnkFYZHHdN767c7spKlZ7ss8Cj3s0zmM1snplVm1l1XV1dumxZFS+IHl+zWfJq\nDWeVDeHc0cPzUreISL7EespgZs8Cp6f46FvJJ+7uZpb2F/mpymK5nwW+0ENd84H5AFVVVVnvSyrx\nWITmtnZ2HTjKy1vr+co1k/ToZxHpd3oMNu5+TbrPzGyPmVW4e20wnbU3RbYa4Kqk8zHAiiB9TJf0\nmuA4Xbk1wNgU16Sro7OdFwIxd1+dri9hSQSbDn79+i7c4ZOaQhORfijTabSlQOfusjnAkhR5ngZm\nmFlJsDFgBvB0ME12yMymB7vQZiddn67cpcDsYFfadOBgUE7KOpLa8Dngpxn2NSfisSjNrR0seXUX\nF44tZvzIIWE3SUQk6zINNvcC15rZRuCa4BwzqzKzBwDcvR64B1gVvO4O0gDuAB4ANgHvAE91Vy7w\nJLA5yP+j4Pqe6gC4iV4abApjEd7cfYg3dh3Sd2tEpN+ybtbLB7Sqqiqvrq7OeT2fvv9Fqrc1EDFY\n+c2PMmr4oJzXKSKSK2a22t2ruqbrDgIh67yLwOVnnaZAIyL9loJNyDrv/Dzrwq47tUVE+g8Fm5DF\nYxEKoxGuOy/V7nIRkf6hx63Pklufn3YGH55UxogiPSRNRPovBZuQXTFxZNhNEBHJOU2jiYhIzinY\niIhIzinYiIhIzinYiIhIzinYiIhIzinYiIhIzinYiIhIzinYiIhIzumuz2mYWR2w7RQvHwnsy2Jz\n+gL1eWAYaH0eaP2FzPt8hruXdU1UsMkBM6tOdYvt/kx9HhgGWp8HWn8hd33WNJqIiOScgo2IiOSc\ngk1uzA+7ASFQnweGgdbngdZfyFGftWYjIiI5p5GNiIjknIKNiIjknIJNFpnZTDN728w2mdmdYbcn\nV8xsgZntNbN1SWmlZrbMzDYG7yVhtjGbzGysmT1nZuvN7A0z+/MgvT/3eZCZvWxmrwV9/j9B+plm\n9lLwM/6omRWG3dZsM7Oomb1iZr8Ozvt1n81sq5mtNbNXzaw6SMv6z7aCTZaYWRT4F+B6YArwOTOb\nEm6rcuYhYGaXtDuB5e4+EVgenPcXbcBX3X0KMB34UvD/tj/3uRm42t0vBC4CZprZdODvgO+5+9lA\nA3BLiG3MlT8H3kw6Hwh9/oi7X5T0/Zqs/2wr2GTPZcAmd9/s7i3Az4BZIbcpJ9z9P4D6LsmzgIXB\n8ULgxrw2Kofcvdbd1wTHh0n8Iqqkf/fZ3b0xOC0IXg5cDfw8SO9XfQYwszHAx4AHgnOjn/c5jaz/\nbCvYZE8lsCPpfGeQNlCUu3ttcLwbKA+zMbliZuOBi4GX6Od9DqaTXgX2AsuAd4AD7t4WZOmPP+Pf\nB74OdATnp9H/++zAM2a22szmBWlZ/9mOZVqASFfu7mbW7/bUm9lQ4HHgy+5+KPFHb0J/7LO7twMX\nmVkx8AvgnJCblFNm9nFgr7uvNrOrwm5PHl3h7jVmNgpYZmZvJX+YrZ9tjWyypwYYm3Q+JkgbKPaY\nWQVA8L435PZklZkVkAg0P3b3J4Lkft3nTu5+AHgOuBwoNrPOP1L728/4B4FPmtlWEtPgVwM/oH/3\nGXevCd73kvij4jJy8LOtYJM9q4CJwc6VQuCzwNKQ25RPS4E5wfEcYEmIbcmqYN7+/wFvuvs/JX3U\nn/tcFoxoMLMi4FoSa1XPAZ8OsvWrPrv7N919jLuPJ/Hv93fu/nn6cZ/NbIiZDes8BmYA68jBz7bu\nIJBFZnYDiTnfKLDA3b8bcpNywsx+ClxF4lbke4C7gF8Ci4FxJB7NcJO7d91E0CeZ2RXAfwJrOTGX\n/5ck1m36a58vILEwHCXxR+lid7/bzCaQ+Ku/FHgF+BN3bw6vpbkRTKP9L3f/eH/uc9C3XwSnMeAn\n7v5dMzuNLP9sK9iIiEjOaRpNRERyTsFGRERyTsFGRERyTsFGRERyTsFGRERyTsFGRERyTsFGRERy\n7v8DbRIJDShuWn8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaYC_PH4N8rQ",
        "colab_type": "text"
      },
      "source": [
        "## A simple baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJrvEgLzN8rS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleLearningCurvePredictor():\n",
        "    \"\"\"A learning curve predictor that predicts the last observed epoch of the validation accuracy as final performance\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        pass\n",
        "    \n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for datapoint in X:\n",
        "            predictions.append(datapoint[\"Train/val_accuracy\"][-1])\n",
        "        return predictions\n",
        "    \n",
        "def score(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d23UZhgHN8rW",
        "colab_type": "code",
        "outputId": "a92369a5-2878-464b-98a5-1667134899ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Training & tuning\n",
        "predictor = SimpleLearningCurvePredictor()\n",
        "predictor.fit(train_data, train_targets)\n",
        "preds = predictor.predict(val_data)\n",
        "mse = score(val_targets, preds)\n",
        "print(\"Score on validation set:\", mse)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score on validation set: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMWiY0XbN8rc",
        "colab_type": "code",
        "outputId": "63aba1d5-591c-4c6f-80bf-d0fd7eacc804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Final evaluation (after tuning)\n",
        "final_preds = predictor.predict(test_data)\n",
        "final_score = score(test_targets, final_preds)\n",
        "print(\"Final test score:\", final_score)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final test score: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNOcxEQji6du",
        "colab_type": "text"
      },
      "source": [
        "## Conditional Multi-Step Univariate LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyPMeBHetWGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multivariate = False\n",
        "nof_multi = 7\n",
        "input_size = nof_multi if multivariate else 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqojlKAujgIt",
        "colab_type": "text"
      },
      "source": [
        "### Data Preperation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnEQJbbLyGG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if multivariate:\n",
        "  train_data_loader = utils.prep_data(train_data, train_targets, batch_size=32,\n",
        "                                      temporal_keys=['Train/val_accuracy','Train/loss','Train/train_accuracy','Train/train_cross_entropy','Train/val_cross_entropy','Train/gradient_mean','Train/lr'],\n",
        "                                      normalization_factor_temporal_data=[100,1,100,1,1,1,1], one_shot=True)\n",
        "  val_data_loader = utils.prep_data(val_data, val_targets, batch_size=32,\n",
        "                                    temporal_keys=['Train/val_accuracy','Train/loss','Train/train_accuracy','Train/train_cross_entropy','Train/val_cross_entropy','Train/gradient_mean','Train/lr'],\n",
        "                                    normalization_factor_temporal_data=[100,1,100,1,1,1,1], one_shot=True)\n",
        "  test_data_loader = utils.prep_data(test_data, test_targets, batch_size=32,\n",
        "                                    temporal_keys=['Train/val_accuracy','Train/loss','Train/train_accuracy','Train/train_cross_entropy','Train/val_cross_entropy','Train/gradient_mean','Train/lr'],\n",
        "                                    normalization_factor_temporal_data=[100,1,100,1,1,1,1],one_shot=True)\n",
        "else:\n",
        "  train_data_loader = utils.prep_data(train_data, train_targets, batch_size=32, normalization_factor_temporal_data=[100],one_shot=True)\n",
        "  val_data_loader = utils.prep_data(val_data, val_targets, batch_size=32, normalization_factor_temporal_data=[100], one_shot=True)\n",
        "  test_data_loader = utils.prep_data(test_data, test_targets, batch_size=32, normalization_factor_temporal_data=[100],one_shot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt-kazBujFr0",
        "colab_type": "text"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBGim3i6CoCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, nof_configs, num_layers, dropout = 0.5, bidirectional=False):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        self.nof_configs = nof_configs\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = torch.nn.LSTM(input_size=input_size, \n",
        "                                  hidden_size=hidden_size,\n",
        "                                  num_layers=num_layers,\n",
        "                                  dropout=dropout,\n",
        "                                  bidirectional=bidirectional)\n",
        "\n",
        "        self.relu = torch.nn.functional.relu\n",
        "\n",
        "        self.encode_fc1 = torch.nn.Linear(self.nof_configs,int(self.hidden_size/2))\n",
        "        self.encode_bn1 = torch.nn.BatchNorm1d(int(self.hidden_size/2))\n",
        "        self.encode_fc2 = torch.nn.Linear(int(self.hidden_size/2),self.hidden_size)\n",
        "        self.encode_bn2 = torch.nn.BatchNorm1d(self.hidden_size)\n",
        "\n",
        "    def forward(self, seq, config):\n",
        "        h0 = self.initHidden(config)\n",
        "        c0 = self.initCell(seq.size()[1])\n",
        "        seq = seq.permute(2,1,0)\n",
        "        output, (hidden,cell) = self.lstm(seq, (h0,c0))\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def initHidden(self, config):\n",
        "        x = self.relu(self.encode_bn1(self.encode_fc1(config)))\n",
        "        x = self.relu(self.encode_bn2(self.encode_fc2(x)))\n",
        "        return torch.stack([x for _ in range(self.num_layers*self.num_directions)])\n",
        "\n",
        "    def initCell(self, batch_size):\n",
        "        return torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dErX6on9FYyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout = 0.5):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = torch.nn.LSTM(input_size=input_size, \n",
        "                                  hidden_size=hidden_size,\n",
        "                                  num_layers=num_layers,\n",
        "                                  dropout=dropout,\n",
        "                                  bidirectional=False)\n",
        "        \n",
        "        self.fc_out = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.relu = torch.nn.functional.relu\n",
        "\n",
        "    def forward(self, seq, h0, c0):\n",
        "        seq = seq.unsqueeze(0)\n",
        "        seq = seq.unsqueeze(-1)\n",
        "        output, (hidden,cell) = self.lstm(seq, (h0,c0))\n",
        "        output = self.fc_out(output)\n",
        "        return output.squeeze(), hidden, cell\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwbesz1Slf4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(torch.nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "    assert encoder.hidden_size == decoder.hidden_size\n",
        "    #assert encoder.num_layers == decoder.num_layers\n",
        "\n",
        "  def forward(self, source, target, teacher_forcing_ratio = 0.5):\n",
        "    batch_size = target.size()[0]\n",
        "    target_len = target.size()[1]\n",
        "\n",
        "    outputs = torch.zeros(target_len, batch_size, 1)\n",
        "\n",
        "    seq , config = source\n",
        "    output, hidden, cell = self.encoder(seq, config)\n",
        "\n",
        "    decoder_input = target[:,0]\n",
        "    for t in range(1, target_len):\n",
        "      output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
        "      outputs[t] = output.unsqueeze(-1)\n",
        "      use_teacher_forcing = np.random.random() < teacher_forcing_ratio\n",
        "      decoder_input = target[:,t] if use_teacher_forcing else output\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flpCWz4sjL2P",
        "colab_type": "text"
      },
      "source": [
        "### Train, eval & test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lam0RyyPwD_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_batch(batch):\n",
        "  temporal = batch[:nof_multi if multivariate else 1]\n",
        "  temporal = torch.stack([t for t in temporal],dim=0)\n",
        "  configs = batch[-2]\n",
        "  targets = batch[-1]\n",
        "  return temporal, configs, targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk5Jat93mO_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, criterion, clip=5):\n",
        "    model.train()\n",
        "    epoch_loss = []\n",
        "    for batch in train_data_loader:\n",
        "      temporal, configs, targets = preprocess_batch(batch)\n",
        "      optimizer.zero_grad()\n",
        "      output = model([temporal,configs],targets)\n",
        "      output = output.squeeze()\n",
        "      output = torch.t(output)\n",
        "      loss = criterion(output, targets)\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
        "      optimizer.step()\n",
        "      epoch_loss.append(loss.item())\n",
        "    return np.array(epoch_loss).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GulE-3SosoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = []\n",
        "  with torch.no_grad():\n",
        "    for batch in val_data_loader:\n",
        "      temporal, configs, targets = preprocess_batch(batch)\n",
        "      output = model([temporal, configs], targets, 0)\n",
        "      output = output.squeeze()\n",
        "      output = torch.t(output)\n",
        "      loss = criterion(output, targets)\n",
        "      epoch_loss.append(loss.item())\n",
        "  return np.array(epoch_loss).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8R89hbqJp6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RMSELoss(yhat,y):\n",
        "    return torch.sqrt(torch.mean((yhat-y)**2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OFaN6Rx1Njn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, criterion):\n",
        "    model.load_state_dict(torch.load('content/models/model.pt'))\n",
        "    model.eval()\n",
        "    epoch_loss=[]\n",
        "    with torch.no_grad():\n",
        "      for batch in test_data_loader:\n",
        "        temporal, configs, targets = preprocess_batch(batch)\n",
        "        output = model([temporal, configs], targets, 0)\n",
        "        output = output.squeeze()\n",
        "        output = torch.t(output)\n",
        "        loss = criterion(output[:,-1], targets[:,-1])\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "    return np.array(epoch_loss).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5u3PuTVjrKx",
        "colab_type": "text"
      },
      "source": [
        "### BOHB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYUBJMijLLQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_bohb=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR-Zp7gmnXU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(config):\n",
        "  def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "      torch.nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "  \n",
        "  input_size = 1\n",
        "  outcome_dim = 1\n",
        "  config_size = 7\n",
        "  bidirectional = bool(config['bidirectional'])\n",
        "  encoder = EncoderRNN(input_size, config['hidden_dim'], config_size, config['num_layers'],\n",
        "                       dropout=config['encoder_dropout'],bidirectional=bidirectional)\n",
        "  decoder = DecoderRNN(input_size, config['hidden_dim'], outcome_dim, \n",
        "                       num_layers=2*config['num_layers'] if bidirectional else config['num_layers'],dropout=config['decoder_dropout'])\n",
        "  model = Seq2Seq(encoder, decoder)\n",
        "  model.apply(init_weights)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdHviALlrSaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "working_dir = os.curdir\n",
        "# minimum budget that BOHB uses\n",
        "min_budget = 30\n",
        "# largest budget BOHB will use\n",
        "max_budget = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXq6mTaBjvWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PyTorchWorker(Worker):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.train_loader, self.validation_loader, self.test_loader = train_data_loader, val_data_loader, test_data_loader\n",
        "\n",
        "    @staticmethod\n",
        "    def get_model(config: CS.Configuration) -> torch.nn.Module:\n",
        "        \"\"\" Define a configurable convolution model.\n",
        "            \n",
        "        See description of get_conv_model above for more details on the model.\n",
        "        \"\"\"\n",
        "        return get_model(config)\n",
        "        \n",
        "    @staticmethod\n",
        "    def get_configspace() -> CS.Configuration:\n",
        "        \"\"\" \n",
        "        Define a conditional hyperparameter search-space.\n",
        "        \"\"\"\n",
        "        cs = CS.ConfigurationSpace()\n",
        "        hidden_dim = CSH.UniformIntegerHyperparameter(\"hidden_dim\",lower=5,upper=500,log=True)\n",
        "        num_layers = CSH.UniformIntegerHyperparameter(\"num_layers\",lower=1,upper=5)\n",
        "        encoder_dropout = CSH.UniformFloatHyperparameter(\"encoder_dropout\",lower=0.1,upper=0.8)\n",
        "        decoder_dropout = CSH.UniformFloatHyperparameter(\"decoder_dropout\",lower=0.1,upper=0.8)\n",
        "        bidirectional = CSH.UniformIntegerHyperparameter(\"bidirectional\",lower=0,upper=1)\n",
        "\n",
        "        lr = CSH.UniformFloatHyperparameter(\"lr\",lower=1e-6,upper=1e-1,log=True)\n",
        "        weight_decay = CSH.UniformFloatHyperparameter(\"weight_decay\",lower=1e-6,upper=1e-1,log=True)\n",
        "        sgd_momentum = CSH.UniformFloatHyperparameter(\"sgd_momentum\",lower=0.00,upper=0.99)\n",
        "        optimizer = CSH.CategoricalHyperparameter('optimizer', choices=['Adam', 'SGD'])\n",
        "        \n",
        "        scheduler = CSH.CategoricalHyperparameter('scheduler', choices=['CosAnn', 'CosAnnWarm'])\n",
        "        T_max = CSH.UniformIntegerHyperparameter(\"T_max\",lower=10,upper=max_budget)\n",
        "        T_0 = CSH.UniformIntegerHyperparameter(\"T_0\",lower=10,upper=int(max_budget/3))\n",
        "\n",
        "        cs.add_hyperparameters([hidden_dim, num_layers, encoder_dropout, decoder_dropout, bidirectional,\n",
        "                                lr, weight_decay, sgd_momentum,optimizer,scheduler,T_max,T_0])\n",
        "    \n",
        "        condition1 = CS.EqualsCondition(sgd_momentum,optimizer,'SGD')\n",
        "        cs.add_condition(condition1)\n",
        "        condition2 = CS.EqualsCondition(T_max,scheduler,'CosAnn')\n",
        "        cs.add_condition(condition2)\n",
        "        condition3 = CS.EqualsCondition(T_0,scheduler,'CosAnnWarm')\n",
        "        cs.add_condition(condition3)\n",
        "        return cs\n",
        "\n",
        "    def compute(self, config: CS.Configuration, budget: float, working_directory: str,\n",
        "                *args, **kwargs) -> dict:\n",
        "        \"\"\"Evaluate a function with the given config and budget and return a loss.\n",
        "        \n",
        "        Bohb tries to minimize the returned loss.\n",
        "        \n",
        "        In our case the function is the training and validation of a model,\n",
        "        the budget is the number of epochs and the loss is the validation error.\n",
        "        \"\"\"\n",
        "        model = self.get_model(config)\n",
        "\n",
        "        if config['optimizer'] == 'Adam':\n",
        "          optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "        else:\n",
        "          optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=config['sgd_momentum'])\n",
        "        \n",
        "        if config['scheduler'] == 'CosAnnWarm':\n",
        "          scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=config['T_0'])\n",
        "        else:\n",
        "          scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['T_max'])\n",
        "\n",
        "        criterion = torch.nn.MSELoss()\n",
        "        clip = 5\n",
        "\n",
        "        for epoch in range(int(budget)):\n",
        "          loss = 0\n",
        "          model.train()\n",
        "          for val_acc, configs, targets in self.train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model([val_acc,configs],targets)\n",
        "            output = output.squeeze()\n",
        "            output = torch.t(output)\n",
        "            loss = criterion(output, targets)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
        "            optimizer.step()\n",
        "          scheduler.step()\n",
        "        \n",
        "        train_loss = evaluate(model, self.train_loader)\n",
        "        validation_loss = evaluate(model, self.validation_loader)\n",
        "        test_loss = evaluate(model, self.test_loader)\n",
        "        \n",
        "        return ({\n",
        "                'loss': validation_loss,  # remember: HpBandSter minimizes the loss!\n",
        "                'info': {'test_loss': test_loss,\n",
        "                         'train_loss': train_loss,\n",
        "                         'validation_loss': validation_loss,\n",
        "                         'model': str(model)}\n",
        "                })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWKQurHYsbkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if run_bohb:\n",
        "  worker = PyTorchWorker(run_id='0')\n",
        "  cs = worker.get_configspace()\n",
        "\n",
        "  config = cs.sample_configuration().get_dictionary()\n",
        "  print(config)\n",
        "\n",
        "  res = worker.compute(config=config, budget=min_budget, working_directory=working_dir)\n",
        "  print(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O50di5m_sfvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if run_bohb:\n",
        "  result_file = os.path.join(working_dir, 'bohb_result.pkl')\n",
        "  nic_name = 'lo0'\n",
        "  port = 0\n",
        "  run_id = 'bohb_run_1'\n",
        "  n_bohb_iterations = 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR8rgsM7wTdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if run_bohb:\n",
        "  try:\n",
        "      # Start a nameserver\n",
        "      worker = PyTorchWorker(run_id='0')\n",
        "      ns = hpns.NameServer(run_id=run_id, host='127.0.0.1', port=port,\n",
        "                          working_directory=working_dir)\n",
        "      ns_host, ns_port = ns.start()\n",
        "      # Start local worker\n",
        "      w = PyTorchWorker(run_id=run_id, host='127.0.0.1', nameserver=ns_host,\n",
        "                        nameserver_port=ns_port, timeout=120)\n",
        "      w.run(background=True)\n",
        "\n",
        "      # Run an optimizer\n",
        "      bohb = BOHB(configspace=worker.get_configspace(),\n",
        "                  run_id=run_id,\n",
        "                  host='127.0.0.1',\n",
        "                  nameserver=ns_host,\n",
        "                  nameserver_port=ns_port,\n",
        "                  min_budget=min_budget, max_budget=max_budget)\n",
        "\n",
        "      result = bohb.run(n_iterations=n_bohb_iterations)\n",
        "      print(\"Write result to file {}\".format(result_file))\n",
        "      with open(result_file, 'wb') as f:\n",
        "          pickle.dump(result, f)\n",
        "  finally:\n",
        "      bohb.shutdown(shutdown_workers=True)\n",
        "      ns.shutdown()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACwkRPySjTCu",
        "colab_type": "text"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsgAX8SUKpLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "      torch.nn.init.uniform_(param.data, -0.08, 0.08)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8TtwTZiKH2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 2500\n",
        "\n",
        "decoder_input_size = 1\n",
        "output_size = 1\n",
        "config_size = 7\n",
        "hidden_size=50\n",
        "encoder_dropout=0.5\n",
        "decoder_dropout=0.5\n",
        "num_layers=2\n",
        "bidirectional = True\n",
        "encoder = EncoderRNN(input_size, hidden_size=hidden_size, nof_configs=config_size, num_layers=num_layers,\n",
        "                      dropout=encoder_dropout,bidirectional=bidirectional)\n",
        "decoder = DecoderRNN(decoder_input_size, hidden_size=hidden_size, output_size=output_size, \n",
        "                      num_layers=2*num_layers if bidirectional else num_layers,dropout=decoder_dropout)\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "model.apply(init_weights)\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=10e-3)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqBLkuRznl9y",
        "colab_type": "code",
        "outputId": "bbd596c6-88e6-400f-9f5c-6a973d86c6b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss = train(model, optimizer, criterion)\n",
        "  val_loss = evaluate(model, criterion)\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    torch.save(model.state_dict(),\"content/models/model.pt\")    \n",
        "    print('Val loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(best_val_loss,val_loss))\n",
        "    best_val_loss = val_loss\n",
        "\n",
        "  print(f'Epoch: {epoch}\\t Train Loss: {train_loss:.3f}\\t Val. Loss: {val_loss:.3f}')\n",
        "  scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Val loss decreased (inf --> 3622.436050).  Saving model ...\n",
            "Epoch: 0\t Train Loss: 4760.393\t Val. Loss: 3622.436\n",
            "Val loss decreased (3622.436050 --> 1938.671989).  Saving model ...\n",
            "Epoch: 1\t Train Loss: 2773.007\t Val. Loss: 1938.672\n",
            "Val loss decreased (1938.671989 --> 811.669666).  Saving model ...\n",
            "Epoch: 2\t Train Loss: 1363.551\t Val. Loss: 811.670\n",
            "Val loss decreased (811.669666 --> 215.193925).  Saving model ...\n",
            "Epoch: 3\t Train Loss: 496.718\t Val. Loss: 215.194\n",
            "Val loss decreased (215.193925 --> 97.085549).  Saving model ...\n",
            "Epoch: 4\t Train Loss: 151.644\t Val. Loss: 97.086\n",
            "Epoch: 5\t Train Loss: 128.940\t Val. Loss: 122.513\n",
            "Val loss decreased (97.085549 --> 34.376609).  Saving model ...\n",
            "Epoch: 6\t Train Loss: 96.702\t Val. Loss: 34.377\n",
            "Val loss decreased (34.376609 --> 24.790811).  Saving model ...\n",
            "Epoch: 7\t Train Loss: 38.353\t Val. Loss: 24.791\n",
            "Epoch: 8\t Train Loss: 31.923\t Val. Loss: 57.102\n",
            "Epoch: 9\t Train Loss: 30.429\t Val. Loss: 26.767\n",
            "Val loss decreased (24.790811 --> 18.205824).  Saving model ...\n",
            "Epoch: 10\t Train Loss: 22.470\t Val. Loss: 18.206\n",
            "Epoch: 11\t Train Loss: 25.115\t Val. Loss: 29.835\n",
            "Val loss decreased (18.205824 --> 17.439513).  Saving model ...\n",
            "Epoch: 12\t Train Loss: 23.297\t Val. Loss: 17.440\n",
            "Epoch: 13\t Train Loss: 25.200\t Val. Loss: 17.517\n",
            "Val loss decreased (17.439513 --> 13.704079).  Saving model ...\n",
            "Epoch: 14\t Train Loss: 19.935\t Val. Loss: 13.704\n",
            "Epoch: 15\t Train Loss: 18.674\t Val. Loss: 16.881\n",
            "Epoch: 16\t Train Loss: 19.700\t Val. Loss: 15.574\n",
            "Epoch: 17\t Train Loss: 16.956\t Val. Loss: 15.719\n",
            "Epoch: 18\t Train Loss: 17.829\t Val. Loss: 14.940\n",
            "Val loss decreased (13.704079 --> 13.607945).  Saving model ...\n",
            "Epoch: 19\t Train Loss: 16.743\t Val. Loss: 13.608\n",
            "Epoch: 20\t Train Loss: 19.053\t Val. Loss: 15.872\n",
            "Epoch: 21\t Train Loss: 15.566\t Val. Loss: 13.674\n",
            "Val loss decreased (13.607945 --> 13.557538).  Saving model ...\n",
            "Epoch: 22\t Train Loss: 15.285\t Val. Loss: 13.558\n",
            "Epoch: 23\t Train Loss: 16.681\t Val. Loss: 14.282\n",
            "Val loss decreased (13.557538 --> 12.204663).  Saving model ...\n",
            "Epoch: 24\t Train Loss: 20.092\t Val. Loss: 12.205\n",
            "Epoch: 25\t Train Loss: 17.325\t Val. Loss: 14.719\n",
            "Val loss decreased (12.204663 --> 11.090251).  Saving model ...\n",
            "Epoch: 26\t Train Loss: 14.796\t Val. Loss: 11.090\n",
            "Epoch: 27\t Train Loss: 13.052\t Val. Loss: 14.350\n",
            "Epoch: 28\t Train Loss: 16.023\t Val. Loss: 14.960\n",
            "Epoch: 29\t Train Loss: 14.970\t Val. Loss: 13.092\n",
            "Epoch: 30\t Train Loss: 13.740\t Val. Loss: 11.708\n",
            "Epoch: 31\t Train Loss: 12.769\t Val. Loss: 12.375\n",
            "Epoch: 32\t Train Loss: 14.664\t Val. Loss: 11.100\n",
            "Val loss decreased (11.090251 --> 9.944596).  Saving model ...\n",
            "Epoch: 33\t Train Loss: 11.538\t Val. Loss: 9.945\n",
            "Epoch: 34\t Train Loss: 13.852\t Val. Loss: 13.728\n",
            "Epoch: 35\t Train Loss: 12.397\t Val. Loss: 10.783\n",
            "Epoch: 36\t Train Loss: 12.042\t Val. Loss: 11.190\n",
            "Epoch: 37\t Train Loss: 11.327\t Val. Loss: 10.330\n",
            "Epoch: 38\t Train Loss: 12.997\t Val. Loss: 10.153\n",
            "Val loss decreased (9.944596 --> 9.874830).  Saving model ...\n",
            "Epoch: 39\t Train Loss: 11.091\t Val. Loss: 9.875\n",
            "Epoch: 40\t Train Loss: 11.263\t Val. Loss: 10.175\n",
            "Val loss decreased (9.874830 --> 9.758214).  Saving model ...\n",
            "Epoch: 41\t Train Loss: 11.345\t Val. Loss: 9.758\n",
            "Epoch: 42\t Train Loss: 11.009\t Val. Loss: 9.983\n",
            "Val loss decreased (9.758214 --> 9.729768).  Saving model ...\n",
            "Epoch: 43\t Train Loss: 11.228\t Val. Loss: 9.730\n",
            "Epoch: 44\t Train Loss: 10.253\t Val. Loss: 9.922\n",
            "Val loss decreased (9.729768 --> 9.640692).  Saving model ...\n",
            "Epoch: 45\t Train Loss: 10.435\t Val. Loss: 9.641\n",
            "Val loss decreased (9.640692 --> 9.562268).  Saving model ...\n",
            "Epoch: 46\t Train Loss: 10.440\t Val. Loss: 9.562\n",
            "Val loss decreased (9.562268 --> 9.335893).  Saving model ...\n",
            "Epoch: 47\t Train Loss: 10.293\t Val. Loss: 9.336\n",
            "Val loss decreased (9.335893 --> 9.316041).  Saving model ...\n",
            "Epoch: 48\t Train Loss: 10.535\t Val. Loss: 9.316\n",
            "Epoch: 49\t Train Loss: 10.161\t Val. Loss: 9.322\n",
            "Epoch: 50\t Train Loss: 17.039\t Val. Loss: 19.200\n",
            "Epoch: 51\t Train Loss: 21.262\t Val. Loss: 14.621\n",
            "Epoch: 52\t Train Loss: 17.548\t Val. Loss: 11.633\n",
            "Epoch: 53\t Train Loss: 16.897\t Val. Loss: 15.443\n",
            "Epoch: 54\t Train Loss: 15.654\t Val. Loss: 12.944\n",
            "Epoch: 55\t Train Loss: 16.634\t Val. Loss: 15.349\n",
            "Epoch: 56\t Train Loss: 19.748\t Val. Loss: 16.820\n",
            "Epoch: 57\t Train Loss: 17.899\t Val. Loss: 17.395\n",
            "Epoch: 58\t Train Loss: 21.362\t Val. Loss: 16.651\n",
            "Epoch: 59\t Train Loss: 17.408\t Val. Loss: 13.121\n",
            "Epoch: 60\t Train Loss: 17.848\t Val. Loss: 15.114\n",
            "Epoch: 61\t Train Loss: 20.469\t Val. Loss: 14.094\n",
            "Epoch: 62\t Train Loss: 19.590\t Val. Loss: 14.573\n",
            "Epoch: 63\t Train Loss: 16.187\t Val. Loss: 16.923\n",
            "Epoch: 64\t Train Loss: 16.509\t Val. Loss: 14.413\n",
            "Epoch: 65\t Train Loss: 16.291\t Val. Loss: 13.422\n",
            "Epoch: 66\t Train Loss: 17.765\t Val. Loss: 15.912\n",
            "Epoch: 67\t Train Loss: 18.550\t Val. Loss: 13.169\n",
            "Epoch: 68\t Train Loss: 19.304\t Val. Loss: 15.026\n",
            "Epoch: 69\t Train Loss: 18.016\t Val. Loss: 19.150\n",
            "Epoch: 70\t Train Loss: 17.398\t Val. Loss: 14.603\n",
            "Epoch: 71\t Train Loss: 17.166\t Val. Loss: 16.947\n",
            "Epoch: 72\t Train Loss: 14.553\t Val. Loss: 18.004\n",
            "Epoch: 73\t Train Loss: 13.720\t Val. Loss: 11.403\n",
            "Epoch: 74\t Train Loss: 14.126\t Val. Loss: 12.358\n",
            "Epoch: 75\t Train Loss: 19.765\t Val. Loss: 16.260\n",
            "Epoch: 76\t Train Loss: 21.415\t Val. Loss: 14.957\n",
            "Epoch: 77\t Train Loss: 15.213\t Val. Loss: 15.912\n",
            "Epoch: 78\t Train Loss: 13.419\t Val. Loss: 12.905\n",
            "Epoch: 79\t Train Loss: 15.193\t Val. Loss: 14.922\n",
            "Epoch: 80\t Train Loss: 13.597\t Val. Loss: 12.175\n",
            "Epoch: 81\t Train Loss: 14.652\t Val. Loss: 11.001\n",
            "Epoch: 82\t Train Loss: 12.379\t Val. Loss: 10.179\n",
            "Epoch: 83\t Train Loss: 12.872\t Val. Loss: 10.932\n",
            "Epoch: 84\t Train Loss: 12.361\t Val. Loss: 11.185\n",
            "Epoch: 85\t Train Loss: 12.263\t Val. Loss: 10.126\n",
            "Epoch: 86\t Train Loss: 11.922\t Val. Loss: 10.817\n",
            "Epoch: 87\t Train Loss: 12.154\t Val. Loss: 10.776\n",
            "Epoch: 88\t Train Loss: 11.192\t Val. Loss: 9.748\n",
            "Epoch: 89\t Train Loss: 10.707\t Val. Loss: 9.838\n",
            "Epoch: 90\t Train Loss: 12.047\t Val. Loss: 9.831\n",
            "Epoch: 91\t Train Loss: 11.148\t Val. Loss: 9.805\n",
            "Epoch: 92\t Train Loss: 11.232\t Val. Loss: 9.565\n",
            "Epoch: 93\t Train Loss: 10.434\t Val. Loss: 9.598\n",
            "Epoch: 94\t Train Loss: 10.564\t Val. Loss: 9.604\n",
            "Epoch: 95\t Train Loss: 10.452\t Val. Loss: 9.349\n",
            "Val loss decreased (9.316041 --> 9.311764).  Saving model ...\n",
            "Epoch: 96\t Train Loss: 10.088\t Val. Loss: 9.312\n",
            "Val loss decreased (9.311764 --> 9.236604).  Saving model ...\n",
            "Epoch: 97\t Train Loss: 9.997\t Val. Loss: 9.237\n",
            "Val loss decreased (9.236604 --> 9.155556).  Saving model ...\n",
            "Epoch: 98\t Train Loss: 9.891\t Val. Loss: 9.156\n",
            "Val loss decreased (9.155556 --> 9.148669).  Saving model ...\n",
            "Epoch: 99\t Train Loss: 10.023\t Val. Loss: 9.149\n",
            "Epoch: 100\t Train Loss: 19.287\t Val. Loss: 11.757\n",
            "Epoch: 101\t Train Loss: 18.769\t Val. Loss: 33.020\n",
            "Epoch: 102\t Train Loss: 17.037\t Val. Loss: 12.989\n",
            "Epoch: 103\t Train Loss: 16.109\t Val. Loss: 13.797\n",
            "Epoch: 104\t Train Loss: 18.591\t Val. Loss: 16.223\n",
            "Epoch: 105\t Train Loss: 16.940\t Val. Loss: 16.346\n",
            "Epoch: 106\t Train Loss: 19.222\t Val. Loss: 11.787\n",
            "Epoch: 107\t Train Loss: 18.462\t Val. Loss: 13.873\n",
            "Epoch: 108\t Train Loss: 14.848\t Val. Loss: 16.639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiSCqyhijWzd",
        "colab_type": "text"
      },
      "source": [
        "### Test results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vKyR0Rq1JxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mse_test = test(model, criterion)\n",
        "print(\"MSE Test loss: {:.3f}\".format(np.mean(mse_test)))\n",
        "rmse_test = test(model, RMSELoss)\n",
        "print(\"RMSE Test loss: {:.3f}\".format(np.mean(rmse_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GTtrISY_Xru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_curve(model):\n",
        "  model.load_state_dict(torch.load('content/models/model.pt'))\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for batch in test_data_loader:\n",
        "        temporal, configs, targets = preprocess_batch(batch)\n",
        "        output = model([temporal, configs], targets, 0)\n",
        "        output = output.squeeze()\n",
        "        output = torch.t(output)\n",
        "        plt.plot(np.arange(51),targets[2],c='g')\n",
        "        plt.plot(np.arange(51),output[2],c='r')\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "plot_curve(model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}