{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task A: Creating a Performance Predictor\n",
    "\n",
    "In this task, you will use training data from 2000 configurations on a single OpenML dataset to train a performance predictor. The data will be splitted into train, test and validation set and we will only use the first 10 epochs of the learning curves for predicitons. You are provided with the full benchmark logs for Fashion-MNIST, that is learning curves, config parameters and gradient statistics, and you can use them freely.\n",
    "\n",
    "For questions, you can contact zimmerl@informatik.uni-freiburg.\n",
    "\n",
    "__Note: Please use the dataloading and splits you are provided with in this notebook.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifications:\n",
    "\n",
    "* Data: fashion_mnist.json\n",
    "* Number of datasets: 1\n",
    "* Number of configurations: 2000\n",
    "* Number of epochs seed during prediction: 10\n",
    "* Available data: Learning curves, architecture parameters and hyperparameters, gradient statistics \n",
    "* Target: Final validation accuracy\n",
    "* Evaluation metric: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and splitting data\n",
    "\n",
    "__Note__: There are 51 steps logged, 50 epochs plus the 0th epoch, prior to any weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd ..\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from api import Benchmark\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading data...\n",
      "==> No cached data found or cache set to False.\n",
      "==> Reading json data...\n",
      "==> Done.\n"
     ]
    }
   ],
   "source": [
    "bench_dir = \"/home/sven/LCBench/data/11604705/fashion_mnist.json\"\n",
    "bench = Benchmark(bench_dir, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1000\n",
      "Validation: 500\n",
      "Test: 500\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "def cut_data(data, cut_position):\n",
    "    targets = []\n",
    "    for dp in data:\n",
    "        targets.append(dp[\"Train/val_accuracy\"][50])\n",
    "        for tag in dp:\n",
    "            if tag.startswith(\"Train/\"):\n",
    "                dp[tag] = dp[tag][0:cut_position]\n",
    "    return data, targets\n",
    "\n",
    "def read_data():\n",
    "    dataset_name = 'Fashion-MNIST'\n",
    "    n_configs = bench.get_number_of_configs(dataset_name)\n",
    "    \n",
    "    # Query API\n",
    "    data = []\n",
    "    for config_id in range(n_configs):\n",
    "        data_point = dict()\n",
    "        data_point[\"config\"] = bench.query(dataset_name=dataset_name, tag=\"config\", config_id=config_id)\n",
    "        for tag in bench.get_queriable_tags(dataset_name=dataset_name, config_id=config_id):\n",
    "            if tag.startswith(\"Train/\"):\n",
    "                data_point[tag] = bench.query(dataset_name=dataset_name, tag=tag, config_id=config_id)    \n",
    "        data.append(data_point)\n",
    "        \n",
    "    # Split: 50% train, 25% validation, 25% test (the data is already shuffled)\n",
    "    indices = np.arange(n_configs)\n",
    "    ind_train = indices[0:int(np.floor(0.5*n_configs))]\n",
    "    ind_val = indices[int(np.floor(0.5*n_configs)):int(np.floor(0.75*n_configs))]\n",
    "    ind_test = indices[int(np.floor(0.75*n_configs)):]\n",
    "\n",
    "    array_data = np.array(data)\n",
    "    train_data = array_data[ind_train]\n",
    "    val_data = array_data[ind_val]\n",
    "    test_data = array_data[ind_test]\n",
    "    \n",
    "    # Cut curves for validation and test\n",
    "    cut_position = 11\n",
    "    val_data, val_targets = cut_data(val_data, cut_position)\n",
    "    test_data, test_targets = cut_data(test_data, cut_position)\n",
    "    train_data, train_targets = cut_data(train_data, 51)   # Cut last value as it is repeated\n",
    "    \n",
    "    return train_data, val_data, test_data, train_targets, val_targets, test_targets\n",
    "    \n",
    "train_data, val_data, test_data, train_targets, val_targets, test_targets = read_data()\n",
    "\n",
    "print(\"Train:\", len(train_data))\n",
    "print(\"Validation:\", len(val_data))\n",
    "print(\"Test:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains the configuration of the trained model and learning curves as well as global and layer-wise gradient statistics.\n",
    "\n",
    "__Note__: Not all parameters vary across different configurations. The varying parameters are batch_size, max_dropout, max_units, num_layers, learning_rate, momentum, weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config example: {'batch_size': 71, 'imputation_strategy': 'mean', 'learning_rate_scheduler': 'cosine_annealing', 'loss': 'cross_entropy_weighted', 'network': 'shapedmlpnet', 'max_dropout': 0.025926231827891333, 'normalization_strategy': 'standardize', 'optimizer': 'sgd', 'cosine_annealing_T_max': 50, 'cosine_annealing_eta_min': 1e-08, 'activation': 'relu', 'max_units': 293, 'mlp_shape': 'funnel', 'num_layers': 3, 'learning_rate': 0.0018243300267253295, 'momentum': 0.21325193168301043, 'weight_decay': 0.020472816917443872}\n",
      "\n",
      "\n",
      "DATA Keys: dict_keys(['config', 'Train/loss', 'Train/train_accuracy', 'Train/val_accuracy', 'Train/train_cross_entropy', 'Train/val_cross_entropy', 'Train/train_balanced_accuracy', 'Train/val_balanced_accuracy', 'Train/test_result', 'Train/test_cross_entropy', 'Train/test_balanced_accuracy', 'Train/gradient_max', 'Train/gradient_mean', 'Train/gradient_median', 'Train/gradient_std', 'Train/gradient_q10', 'Train/gradient_q25', 'Train/gradient_q75', 'Train/gradient_q90', 'Train/layer_wise_gradient_max_layer_0', 'Train/layer_wise_gradient_max_layer_1', 'Train/layer_wise_gradient_max_layer_2', 'Train/layer_wise_gradient_mean_layer_0', 'Train/layer_wise_gradient_mean_layer_1', 'Train/layer_wise_gradient_mean_layer_2', 'Train/layer_wise_gradient_median_layer_0', 'Train/layer_wise_gradient_median_layer_1', 'Train/layer_wise_gradient_median_layer_2', 'Train/layer_wise_gradient_std_layer_0', 'Train/layer_wise_gradient_std_layer_1', 'Train/layer_wise_gradient_std_layer_2', 'Train/layer_wise_gradient_q10_layer_0', 'Train/layer_wise_gradient_q10_layer_1', 'Train/layer_wise_gradient_q10_layer_2', 'Train/layer_wise_gradient_q25_layer_0', 'Train/layer_wise_gradient_q25_layer_1', 'Train/layer_wise_gradient_q25_layer_2', 'Train/layer_wise_gradient_q75_layer_0', 'Train/layer_wise_gradient_q75_layer_1', 'Train/layer_wise_gradient_q75_layer_2', 'Train/layer_wise_gradient_q90_layer_0', 'Train/layer_wise_gradient_q90_layer_1', 'Train/layer_wise_gradient_q90_layer_2', 'Train/gradient_norm', 'Train/lr'])\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "print(\"Config example:\", train_data[0][\"config\"])\n",
    "print(\"\\n\")\n",
    "print(\"DATA Keys:\", train_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0ecaa5a400>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Learning curve\n",
    "plt.plot(train_data[10][\"Train/val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0ec8edc3c8>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gradient statistics\n",
    "plt.plot(train_data[10][\"Train/layer_wise_gradient_mean_layer_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.202998161315918, 48.087608337402344, 53.46297836303711, 43.72011947631836, 44.48249053955078, 45.03165817260742, 46.65331268310547, 49.52190017700195, 51.55704879760742, 53.92169570922852]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn\n",
    "\n",
    "\n",
    "\n",
    "# select only the first 10 epochs\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# choose a couple of dictionary keys\n",
    "complete_data = []\n",
    "\n",
    "\n",
    "for data_sets in [train_data, test_data, val_data]:\n",
    "    data_list = []\n",
    "    for data in data_sets:\n",
    "        custom_data =  []\n",
    "        custom_data.append(data['Train/val_accuracy'][0:10])\n",
    "        #custom_data.append([data[\"config\"]])\n",
    "        \n",
    "\n",
    "\n",
    "        data_list.append(custom_data)\n",
    "        \n",
    "    complete_data.append(data_list)\n",
    "    \n",
    "print(complete_data[0][1])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "#training_set = Dataset(complete_data[0], train_targets)\n",
    "#test_set = Dataset(complete_data[1], test_targets)\n",
    "#val_set = Dataset(complete_data[2], val_targets)\n",
    "\n",
    "train_targets = torch.tensor(train_targets, dtype=torch.float, device=device)\n",
    "test_targets = torch.tensor(test_targets, dtype=torch.float, device=device)\n",
    "val_targets = torch.tensor(val_targets, dtype=torch.float, device=device)\n",
    "\n",
    "\n",
    "training_set = TensorDataset(torch.FloatTensor(complete_data[0]), train_targets)\n",
    "test_set = TensorDataset(torch.FloatTensor(complete_data[1]), test_targets)\n",
    "val_set = TensorDataset(torch.FloatTensor(complete_data[2]), val_targets)\n",
    "\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(dataset = training_set, batch_size = 32)\n",
    "test_data_loader = DataLoader(dataset = test_set, batch_size = 32)\n",
    "val_data_loader = DataLoader(dataset = val_set, batch_size = 32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_classes, embedding_dim, hidden_size, seq_length, \n",
    "                 num_layers, bidirectional = False, drop_prob=0.5):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        \n",
    "        # self.forecaster = torch.nn.Embedding(100, embedding_dim)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(drop_prob)\n",
    "        self.lstm = torch.nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_dim, \n",
    "                            num_layers = self.num_layers, dropout =drop_prob, bidirectional = bidirectional)\n",
    "        \n",
    " \n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(self.hidden_dim, num_classes)\n",
    "        self.linear2 = torch.nn.Linear(self.hidden_dim, num_classes)\n",
    "        self.linear3 = torch.nn.Linear(self.hidden_dim, num_classes)\n",
    "\n",
    "        \n",
    "        self.linear = torch.nn.Linear(self.hidden_dim, num_classes)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = x.float()\n",
    "            \n",
    "        #print((x.size()))\n",
    "        \n",
    "        #forecast = self.forecaster(x)\n",
    "\n",
    "        x = np.swapaxes(x,0,1)\n",
    "\n",
    "        lstm_x, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        lstm_x = lstm_x.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        x = self.dropout(lstm_x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        x = x[:,-1]\n",
    "        \n",
    "\n",
    "        return x, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size = 32):\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "outcome_dim = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 50\n",
    "seq_length = 1\n",
    "num_layers = 3\n",
    "\n",
    "\n",
    "model = LSTM_Net(input_size, outcome_dim, embedding_dim, hidden_dim, seq_length, num_layers)\n",
    "model.to(device)\n",
    "\n",
    "lr=0.001\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6184.987305\n",
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6325.366699\n",
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6350.398763\n",
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6336.203857\n",
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6307.235840\n",
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6272.949300\n",
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6304.253627\n",
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6301.783447\n",
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6326.162164\n",
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6308.883105\n",
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6294.634411\n",
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6301.865397\n",
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6312.667218\n",
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6303.983573\n",
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6282.347461\n",
      "Epoch: 1/1000... Step: 32... Loss: 6077.204590... Val Loss: 6262.675201\n",
      "Validation loss decreased (inf --> 6262.675201).  Saving model ...\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 5889.525879\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 6026.195312\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 6050.799154\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 6036.869629\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 6008.720996\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 5975.692952\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 6006.156250\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 6003.685364\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 6027.413466\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 6010.605664\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 5996.782759\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 6003.875651\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 6014.373798\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 6005.900809\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 5984.812044\n",
      "Epoch: 2/1000... Step: 64... Loss: 5801.642090... Val Loss: 5965.619507\n",
      "Validation loss decreased (6262.675201 --> 5965.619507).  Saving model ...\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5358.436523\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5488.138428\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5512.005697\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5498.529663\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5471.931641\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5441.205322\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5470.072196\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5467.603455\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5490.101183\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5474.212793\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5461.189098\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5468.020630\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5477.938965\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5469.860700\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5449.811328\n",
      "Epoch: 3/1000... Step: 96... Loss: 5269.593750... Val Loss: 5431.551697\n",
      "Validation loss decreased (5965.619507 --> 5431.551697).  Saving model ...\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5057.330078\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5182.960205\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5206.383626\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5193.180054\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5167.491309\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5138.123372\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5166.056989\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5163.585510\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5185.364421\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5170.007764\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5157.449796\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5164.129842\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5173.709548\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5165.862688\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5146.419987\n",
      "Epoch: 4/1000... Step: 128... Loss: 4967.874023... Val Loss: 5128.700073\n",
      "Validation loss decreased (5431.551697 --> 5128.700073).  Saving model ...\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4825.047852\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4947.429443\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4970.491211\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4957.504883\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4932.539844\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4904.247640\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4931.438198\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4928.971252\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4950.178765\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4935.244922\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4923.058904\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4929.618774\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4938.933331\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4931.269810\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4912.309831\n",
      "Epoch: 5/1000... Step: 160... Loss: 4736.897949... Val Loss: 4895.023560\n",
      "Validation loss decreased (5128.700073 --> 4895.023560).  Saving model ...\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4618.010742\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4737.435791\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4760.174642\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4747.386108\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4723.079883\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4695.771403\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4722.285784\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4719.820068\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4740.506402\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4725.958252\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4714.110929\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4720.560669\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4729.631085\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4722.134975\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4703.615104\n",
      "Epoch: 6/1000... Step: 192... Loss: 4531.649902... Val Loss: 4686.722473\n",
      "Validation loss decreased (4895.023560 --> 4686.722473).  Saving model ...\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4424.508789\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4541.109375\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4563.539714\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4550.940430\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4527.263086\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4500.894694\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4526.763393\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4524.298584\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4544.487305\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4530.307471\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4518.784002\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4525.128459\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4533.965370\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4526.629150\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4508.529492\n",
      "Epoch: 7/1000... Step: 224... Loss: 4340.029297... Val Loss: 4492.012421\n",
      "Validation loss decreased (4686.722473 --> 4492.012421).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4240.375000\n",
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4354.227539\n",
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4376.358236\n",
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4363.942505\n",
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4340.877344\n",
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4315.423014\n",
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4340.663504\n",
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4338.199951\n",
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4357.904297\n",
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4344.083008\n",
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4332.874379\n",
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4339.116455\n",
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4347.726600\n",
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4340.545898\n",
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4322.855273\n",
      "Epoch: 8/1000... Step: 256... Loss: 4157.788574... Val Loss: 4306.704315\n",
      "Validation loss decreased (4492.012421 --> 4306.704315).  Saving model ...\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4063.867432\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4175.029663\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4196.866455\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4184.630798\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4162.164844\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4137.605876\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4162.231236\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4159.768738\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4178.999023\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4165.528711\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4154.628507\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4160.770304\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4169.158166\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4162.129761\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4144.839535\n",
      "Epoch: 9/1000... Step: 288... Loss: 3983.179932... Val Loss: 4129.046661\n",
      "Validation loss decreased (4306.704315 --> 4129.046661).  Saving model ...\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 3893.687500\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 4002.198730\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 4023.746338\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 4011.688049\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 3989.812598\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 3966.135742\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 3990.155273\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 3987.693787\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 4006.456977\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 3993.332471\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 3982.736128\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 3988.779114\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 3996.948036\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 3990.069685\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 3973.173893\n",
      "Epoch: 10/1000... Step: 320... Loss: 3814.897949... Val Loss: 3957.733826\n",
      "Validation loss decreased (4129.046661 --> 3957.733826).  Saving model ...\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3729.055176\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3834.945801\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3856.207275\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3844.324402\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3823.032568\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3800.227946\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3823.648333\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3821.187866\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3839.489176\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3826.706543\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3816.410667\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3822.355937\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3830.308331\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3823.578317\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3807.072526\n",
      "Epoch: 11/1000... Step: 352... Loss: 3652.158936... Val Loss: 3791.981247\n",
      "Validation loss decreased (3957.733826 --> 3791.981247).  Saving model ...\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3569.446777\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3672.739624\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3693.717855\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3682.008667\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3661.295410\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3639.355265\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3662.181850\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3659.722382\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3677.565972\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3665.122217\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3655.124112\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3660.972595\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3668.710430\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3662.127441\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3646.008138\n",
      "Epoch: 12/1000... Step: 384... Loss: 3494.439697... Val Loss: 3631.262558\n",
      "Validation loss decreased (3791.981247 --> 3631.262558).  Saving model ...\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3414.490723\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3515.205322\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3535.901774\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3524.365173\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3504.226270\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3483.144287\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3505.381417\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3502.922974\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3520.312147\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3508.204810\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3498.502375\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3504.254740\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3511.779579\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3505.342547\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3489.606999\n",
      "Epoch: 13/1000... Step: 416... Loss: 3341.367676... Val Loss: 3475.204605\n",
      "Validation loss decreased (3631.262558 --> 3475.204605).  Saving model ...\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3263.912598\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3362.063721\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3382.480550\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3371.115540\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3351.547510\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3331.318644\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3352.969761\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3350.512268\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3367.449707\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3355.676758\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3346.268177\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3351.924967\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3359.238037\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3352.946080\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3337.591943\n",
      "Epoch: 14/1000... Step: 448... Loss: 3192.668945... Val Loss: 3323.530731\n",
      "Validation loss decreased (3475.204605 --> 3323.530731).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3117.505127\n",
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3213.105469\n",
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3233.243734\n",
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3222.049438\n",
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3203.049561\n",
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3183.669596\n",
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3204.737688\n",
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3202.281158\n",
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3218.769151\n",
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3207.328931\n",
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3198.212802\n",
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3203.774516\n",
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3210.876897\n",
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3204.729318\n",
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3189.754753\n",
      "Epoch: 15/1000... Step: 480... Loss: 3048.136475... Val Loss: 3176.033020\n",
      "Validation loss decreased (3323.530731 --> 3176.033020).  Saving model ...\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 2975.105469\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 3068.165894\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 3088.027018\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 3077.002686\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 3058.568457\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 3040.033773\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 3060.521205\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 3058.065674\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 3074.106038\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 3062.997144\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 3054.172208\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 3059.639221\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 3066.531776\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 3060.527954\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 3045.931331\n",
      "Epoch: 16/1000... Step: 512... Loss: 2907.608154... Val Loss: 3032.547653\n",
      "Validation loss decreased (3176.033020 --> 3032.547653).  Saving model ...\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2836.586426\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2927.115967\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2946.700846\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2935.845825\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2917.975195\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2900.282756\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2920.191685\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2917.737122\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2933.331543\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2922.552832\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2914.018044\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2919.390727\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2926.074238\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2920.213658\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2905.993604\n",
      "Epoch: 17/1000... Step: 544... Loss: 2770.955811... Val Loss: 2892.946716\n",
      "Validation loss decreased (3032.547653 --> 2892.946716).  Saving model ...\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2701.843018\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2789.849731\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2809.159180\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2798.473022\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2781.164307\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2764.311483\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2783.643694\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2781.190125\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2796.339952\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2785.890332\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2777.644775\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2782.923442\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2789.398531\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2783.680751\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2769.836100\n",
      "Epoch: 18/1000... Step: 576... Loss: 2638.076660... Val Loss: 2757.124969\n",
      "Validation loss decreased (2892.946716 --> 2757.124969).  Saving model ...\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2570.792236\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2656.282837\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2675.317546\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2664.799805\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2648.051465\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2632.035970\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2650.793039\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2648.340424\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2663.046902\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2652.925513\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2644.968439\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2650.153280\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2656.420523\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2650.845128\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2637.374870\n",
      "Epoch: 19/1000... Step: 608... Loss: 2508.886719... Val Loss: 2624.998611\n",
      "Validation loss decreased (2757.124969 --> 2624.998611).  Saving model ...\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2443.363770\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2526.344482\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2545.105306\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2534.755432\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2518.566016\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2503.385905\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2521.569231\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2519.117523\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2533.381673\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2523.587720\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2515.918390\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2521.009664\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2527.069580\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2521.636265\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2508.539502\n",
      "Epoch: 20/1000... Step: 640... Loss: 2383.316406... Val Loss: 2496.497269\n",
      "Validation loss decreased (2624.998611 --> 2496.497269).  Saving model ...\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2319.498291\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2399.974487\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2418.461751\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2408.279480\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2392.647998\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2378.301432\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2395.912179\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2393.461517\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2407.284315\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2397.817114\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2390.434948\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2395.432882\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2401.285889\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2395.994332\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2383.270264\n",
      "Epoch: 21/1000... Step: 672... Loss: 2261.307373... Val Loss: 2371.561386\n",
      "Validation loss decreased (2496.497269 --> 2371.561386).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2199.147705\n",
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2277.123901\n",
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2295.338460\n",
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2285.323547\n",
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2270.248828\n",
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2256.734334\n",
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2273.773612\n",
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2271.323883\n",
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2284.706136\n",
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2275.565039\n",
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2268.469482\n",
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2273.374207\n",
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2279.020714\n",
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2273.870640\n",
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2261.518571\n",
      "Epoch: 22/1000... Step: 704... Loss: 2142.810547... Val Loss: 2250.142395\n",
      "Validation loss decreased (2371.561386 --> 2250.142395).  Saving model ...\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2082.270020\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2157.750366\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2175.692383\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2165.844421\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2151.325586\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2138.641642\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2155.110456\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2152.661682\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2165.604112\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2156.788599\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2149.979159\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2154.790873\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2160.231201\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2155.222395\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2143.241699\n",
      "Epoch: 23/1000... Step: 736... Loss: 2027.783691... Val Loss: 2132.197701\n",
      "Validation loss decreased (2250.142395 --> 2132.197701).  Saving model ...\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 1968.828735\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 2041.817200\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 2059.487264\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 2049.806061\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 2035.842212\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 2023.987651\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 2039.886771\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 2037.438950\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 2049.942234\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 2041.451819\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 2034.928101\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 2039.646901\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 2044.881376\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 2040.013611\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 2028.403719\n",
      "Epoch: 24/1000... Step: 768... Loss: 1916.192261... Val Loss: 2017.691368\n",
      "Validation loss decreased (2132.197701 --> 2017.691368).  Saving model ...\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1858.792847\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1929.292603\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1946.691040\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1937.176422\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1923.766870\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1912.740499\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1928.070748\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1925.623871\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1937.688626\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1929.522864\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1923.284479\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1927.910502\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1932.939416\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1928.212507\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1916.972933\n",
      "Epoch: 25/1000... Step: 800... Loss: 1808.004150... Val Loss: 1906.591789\n",
      "Validation loss decreased (2017.691368 --> 1906.591789).  Saving model ...\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1752.135864\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1820.150085\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1837.277140\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1827.928802\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1815.072778\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1804.873617\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1819.635690\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1817.189804\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1828.816583\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1820.975073\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1815.021651\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1819.555023\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1824.378578\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1819.792367\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1808.922648\n",
      "Epoch: 26/1000... Step: 832... Loss: 1703.193359... Val Loss: 1798.872292\n",
      "Validation loss decreased (1906.591789 --> 1798.872292).  Saving model ...\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1648.833984\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1714.365540\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1731.221598\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1722.039398\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1709.736304\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1700.363464\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1714.558001\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1712.113037\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1723.302273\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1715.784644\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1710.115867\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1714.556702\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1719.175180\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1714.729483\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1704.229191\n",
      "Epoch: 27/1000... Step: 864... Loss: 1601.735474... Val Loss: 1694.509254\n",
      "Validation loss decreased (1798.872292 --> 1694.509254).  Saving model ...\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1548.865845\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1611.917175\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1628.502482\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1619.486237\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1607.735547\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1599.188131\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1612.815744\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1610.371750\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1621.123942\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1613.929858\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1608.545432\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1612.893789\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1617.307392\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1613.002058\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1602.870817\n",
      "Epoch: 28/1000... Step: 896... Loss: 1503.610596... Val Loss: 1593.480988\n",
      "Validation loss decreased (1694.509254 --> 1593.480988).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1452.213867\n",
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1512.787415\n",
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1529.102295\n",
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1520.251892\n",
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1509.053125\n",
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1501.330363\n",
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1514.391532\n",
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1511.948486\n",
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1522.264025\n",
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1515.393188\n",
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1510.292847\n",
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1514.548838\n",
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1518.757737\n",
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1514.592669\n",
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1504.830151\n",
      "Epoch: 29/1000... Step: 928... Loss: 1408.800537... Val Loss: 1495.770088\n",
      "Validation loss decreased (1593.480988 --> 1495.770088).  Saving model ...\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1358.862183\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1416.959839\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1433.004395\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1424.319672\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1413.672241\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1406.773438\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1419.268677\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1416.826584\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1426.705838\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1420.157983\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1415.341486\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1419.505188\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1423.509578\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1419.484637\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1410.090511\n",
      "Epoch: 30/1000... Step: 960... Loss: 1317.289429... Val Loss: 1401.359932\n",
      "Validation loss decreased (1495.770088 --> 1401.359932).  Saving model ...\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1268.795044\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1324.418884\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1340.193481\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1331.674286\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1321.577832\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1315.502340\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1327.432077\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1324.990936\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1334.434258\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1328.209106\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1323.676236\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1327.747721\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1331.547758\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1327.662816\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1318.636792\n",
      "Epoch: 31/1000... Step: 992... Loss: 1229.062134... Val Loss: 1310.235466\n",
      "Validation loss decreased (1401.359932 --> 1310.235466).  Saving model ...\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1182.000854\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1235.152771\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1250.657552\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1242.303772\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1232.757886\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1227.505107\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1238.869768\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1236.429565\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1245.437283\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1239.534619\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1235.285156\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1239.264486\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1242.860314\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1239.115295\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1230.457113\n",
      "Epoch: 32/1000... Step: 1024... Loss: 1144.106445... Val Loss: 1222.384773\n",
      "Validation loss decreased (1310.235466 --> 1222.384773).  Saving model ...\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1098.467529\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1149.149109\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1164.384196\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1156.195740\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1147.200024\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1142.769368\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1153.569336\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1151.130081\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1159.702474\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1154.122083\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1150.155839\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1154.043091\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1157.434852\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1153.829660\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1145.539062\n",
      "Epoch: 33/1000... Step: 1056... Loss: 1062.410278... Val Loss: 1137.795494\n",
      "Validation loss decreased (1222.384773 --> 1137.795494).  Saving model ...\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1018.184570\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1066.397400\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1081.362996\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1073.339752\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1064.893872\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1061.284851\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1071.520473\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1069.082153\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1077.219496\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1071.961169\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1068.277976\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1072.073191\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1075.261014\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1071.795541\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1063.872294\n",
      "Epoch: 34/1000... Step: 1088... Loss: 983.964111... Val Loss: 1056.457294\n",
      "Validation loss decreased (1137.795494 --> 1056.457294).  Saving model ...\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 941.142334\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 986.887817\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 1001.584066\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 993.725922\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 985.829565\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 983.041728\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 992.713327\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 990.275970\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 997.978529\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 993.042078\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 989.641768\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 993.345011\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 996.329008\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 993.003191\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 985.447095\n",
      "Epoch: 35/1000... Step: 1120... Loss: 908.757874... Val Loss: 978.360474\n",
      "Validation loss decreased (1056.457294 --> 978.360474).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 867.332642\n",
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 910.612030\n",
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 925.039124\n",
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 917.346024\n",
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 909.998889\n",
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 908.031759\n",
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 917.139648\n",
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 914.703239\n",
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 921.971225\n",
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 917.356476\n",
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 914.238897\n",
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 917.850204\n",
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 920.630493\n",
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 917.444257\n",
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 910.255115\n",
      "Epoch: 36/1000... Step: 1152... Loss: 836.783386... Val Loss: 903.496700\n",
      "Validation loss decreased (978.360474 --> 903.496700).  Saving model ...\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 796.747925\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 837.562561\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 851.720601\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 844.192413\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 837.394238\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 836.247416\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 844.791870\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 842.356407\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 849.190043\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 844.896832\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 842.061845\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 845.581278\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 848.157950\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 845.111219\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 838.288843\n",
      "Epoch: 37/1000... Step: 1184... Loss: 768.032715... Val Loss: 831.858459\n",
      "Validation loss decreased (903.496700 --> 831.858459).  Saving model ...\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 729.381470\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 767.732452\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 781.621562\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 774.258240\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 768.008740\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 767.681834\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 775.663103\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 773.228584\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 779.628079\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 775.656262\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 773.103726\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 776.531316\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 778.904475\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 775.997179\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 769.541394\n",
      "Epoch: 38/1000... Step: 1216... Loss: 702.499756... Val Loss: 763.438889\n",
      "Validation loss decreased (831.858459 --> 763.438889).  Saving model ...\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 665.225952\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 701.114349\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 714.734639\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 707.536118\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 701.835083\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 702.327738\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 709.746111\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 707.312538\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 713.278083\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 709.627509\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 707.357300\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 710.693090\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 712.862821\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 710.094910\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 704.005554\n",
      "Epoch: 39/1000... Step: 1248... Loss: 640.177246... Val Loss: 698.230793\n",
      "Validation loss decreased (763.438889 --> 698.230793).  Saving model ...\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 604.276001\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 637.702881\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 651.054484\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 644.020676\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 638.867871\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 640.179749\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 647.035427\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 644.602798\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 650.134569\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 646.805115\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 644.817133\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 648.061162\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 650.027560\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 647.398965\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 641.675887\n",
      "Epoch: 40/1000... Step: 1280... Loss: 581.059570... Val Loss: 636.228725\n",
      "Validation loss decreased (698.230793 --> 636.228725).  Saving model ...\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 546.526123\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 577.492371\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 590.575358\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 583.706207\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 579.101428\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 581.232229\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 587.525426\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 585.093750\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 590.191915\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 587.183466\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 585.477589\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 588.629888\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 590.393010\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 587.903687\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 582.546753\n",
      "Epoch: 41/1000... Step: 1312... Loss: 525.141663... Val Loss: 577.427074\n",
      "Validation loss decreased (636.228725 --> 577.427074).  Saving model ...\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 491.971558\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 520.478149\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 533.292643\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 526.588089\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 522.531134\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 525.480545\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 531.211474\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 528.780735\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 533.445452\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 530.757880\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 529.334007\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 532.394618\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 533.954557\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 531.604455\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 526.613527\n",
      "Epoch: 42/1000... Step: 1344... Loss: 472.418060... Val Loss: 521.821209\n",
      "Validation loss decreased (577.427074 --> 521.821209).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 440.607941\n",
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 466.655640\n",
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 479.201752\n",
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 472.661743\n",
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 469.152417\n",
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 472.920166\n",
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 478.089011\n",
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 475.659214\n",
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 479.890628\n",
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 477.523828\n",
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 476.381866\n",
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 479.350812\n",
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 480.707628\n",
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 478.496691\n",
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 473.871651\n",
      "Epoch: 43/1000... Step: 1376... Loss: 422.885162... Val Loss: 469.406578\n",
      "Validation loss decreased (521.821209 --> 469.406578).  Saving model ...\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 392.430542\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 416.020142\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 428.297933\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 421.922417\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 418.960553\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 423.546382\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 428.153329\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 425.724480\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 429.522725\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 427.476590\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 426.616447\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 429.493759\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 430.647517\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 428.575710\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 424.316437\n",
      "Epoch: 44/1000... Step: 1408... Loss: 376.538116... Val Loss: 420.178507\n",
      "Validation loss decreased (469.406578 --> 420.178507).  Saving model ...\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 347.436096\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 368.568329\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 380.577881\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 374.366821\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 371.952258\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 377.355906\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 381.401119\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 378.973221\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 382.338433\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 380.612872\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 380.034468\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 382.820178\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 383.770940\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 381.838215\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 377.944602\n",
      "Epoch: 45/1000... Step: 1440... Loss: 333.373383... Val Loss: 374.133720\n",
      "Validation loss decreased (420.178507 --> 374.133720).  Saving model ...\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 305.620819\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 324.296402\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 336.037781\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 329.991096\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 328.123663\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 334.344889\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 337.828530\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 335.401569\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 338.333872\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 336.928796\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 336.632050\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 339.326180\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 340.074003\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 338.280326\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 334.752266\n",
      "Epoch: 46/1000... Step: 1472... Loss: 293.387329... Val Loss: 331.268337\n",
      "Validation loss decreased (374.133720 --> 331.268337).  Saving model ...\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 266.981323\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 283.200912\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 294.674194\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 288.791862\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 287.471429\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 294.509984\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 297.432212\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 295.006203\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 297.505714\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 296.421033\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 296.405862\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 299.008441\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 299.553385\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 297.898719\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 294.736116\n",
      "Epoch: 47/1000... Step: 1504... Loss: 256.576965... Val Loss: 291.579057\n",
      "Validation loss decreased (331.268337 --> 291.579057).  Saving model ...\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 231.514862\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 245.279099\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 256.484355\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 250.766323\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 249.992743\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 257.848424\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 260.209377\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 257.784315\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 259.851144\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 259.086775\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 259.353101\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 261.864150\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 262.206265\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 260.690575\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 257.893334\n",
      "Epoch: 48/1000... Step: 1536... Loss: 222.938599... Val Loss: 255.063058\n",
      "Validation loss decreased (291.579057 --> 255.063058).  Saving model ...\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 199.218216\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 210.527725\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 221.465007\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 215.911255\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 215.684363\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 224.356964\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 226.156780\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 223.732672\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 225.366928\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 224.922791\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 225.470544\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 227.890074\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 228.029422\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 226.652681\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 224.220709\n",
      "Epoch: 49/1000... Step: 1568... Loss: 192.470215... Val Loss: 221.717136\n",
      "Validation loss decreased (255.063058 --> 221.717136).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 170.089035\n",
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 178.944626\n",
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 189.613897\n",
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 184.224468\n",
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 184.544080\n",
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 194.033333\n",
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 195.272204\n",
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 192.849110\n",
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 194.050930\n",
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 193.926855\n",
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 194.755877\n",
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 197.083899\n",
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 197.020612\n",
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 195.782779\n",
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 193.715970\n",
      "Epoch: 50/1000... Step: 1600... Loss: 165.168976... Val Loss: 191.539063\n",
      "Validation loss decreased (221.717136 --> 191.539063).  Saving model ...\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 144.124084\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 150.526207\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 160.927607\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 155.702332\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 156.568384\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 166.874041\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 167.552015\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 165.129858\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 165.899292\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 166.095306\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 167.205584\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 169.442136\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 169.176162\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 168.077233\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 166.375532\n",
      "Epoch: 51/1000... Step: 1632... Loss: 141.032227... Val Loss: 164.525157\n",
      "Validation loss decreased (191.539063 --> 164.525157).  Saving model ...\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 121.321930\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 125.271301\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 135.404760\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 130.343723\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 131.756015\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 142.878009\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 142.995252\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 140.574034\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 140.911223\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 141.427188\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 142.818703\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 144.963791\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 144.495198\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 143.535146\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 142.198440\n",
      "Epoch: 52/1000... Step: 1664... Loss: 120.057648... Val Loss: 140.674484\n",
      "Validation loss decreased (164.525157 --> 140.674484).  Saving model ...\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 101.678360\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 103.176552\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 113.041667\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 108.145035\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 110.103120\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 122.040735\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 121.597566\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 119.177395\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 119.082567\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 119.918182\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 121.590634\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 123.644211\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 122.973201\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 122.152065\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 121.180193\n",
      "Epoch: 53/1000... Step: 1696... Loss: 102.243698... Val Loss: 119.982504\n",
      "Validation loss decreased (140.674484 --> 119.982504).  Saving model ...\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 85.099892\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 84.198898\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 93.774251\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 89.051207\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 91.542238\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 104.257236\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 103.263671\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 100.849315\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 100.330467\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 101.475455\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 103.419802\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 105.380725\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 104.514094\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 103.835228\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 103.224640\n",
      "Epoch: 54/1000... Step: 1728... Loss: 87.585426... Val Loss: 102.348312\n",
      "Validation loss decreased (119.982504 --> 102.348312).  Saving model ...\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 61.799847\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 66.774683\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 73.666531\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 69.434594\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 69.277203\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 69.883462\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 70.237598\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 69.716016\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 70.614919\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 71.849684\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 72.065898\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 73.326568\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 74.601446\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 74.922788\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 74.238722\n",
      "Epoch: 55/1000... Step: 1760... Loss: 78.019684... Val Loss: 74.758896\n",
      "Validation loss decreased (102.348312 --> 74.758896).  Saving model ...\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 57.360786\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 60.029510\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 65.965856\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 62.079468\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 61.421348\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 60.267491\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 60.811938\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 60.648891\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 61.654375\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 62.491287\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 62.387171\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 63.502490\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 64.214831\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 64.799032\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 63.985232\n",
      "Epoch: 56/1000... Step: 1792... Loss: 72.575821... Val Loss: 63.275960\n",
      "Validation loss decreased (74.758896 --> 63.275960).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 46.564339\n",
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 50.769115\n",
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 56.253118\n",
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 52.875123\n",
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 53.394943\n",
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 51.837417\n",
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 52.334042\n",
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 51.986085\n",
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 52.850482\n",
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 53.520275\n",
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 53.504884\n",
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 55.374404\n",
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 56.196272\n",
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 56.860626\n",
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 56.439837\n",
      "Epoch: 57/1000... Step: 1824... Loss: 68.129448... Val Loss: 56.195556\n",
      "Validation loss decreased (63.275960 --> 56.195556).  Saving model ...\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 42.645813\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 46.179321\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 49.970229\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 46.770383\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 47.237191\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 45.822177\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 46.202197\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 45.940623\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 46.708039\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 47.499229\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 47.615892\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 49.118542\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 49.931639\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 50.859037\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 50.467606\n",
      "Epoch: 58/1000... Step: 1856... Loss: 67.086670... Val Loss: 50.586548\n",
      "Validation loss decreased (56.195556 --> 50.586548).  Saving model ...\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 43.412605\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 44.288963\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 48.082509\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 44.826206\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 45.099485\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 44.171415\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 44.477414\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 44.468716\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 45.067640\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 47.578894\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 48.416607\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 49.191399\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 50.562494\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 51.347748\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 51.219530\n",
      "Epoch: 59/1000... Step: 1888... Loss: 63.028217... Val Loss: 53.075966\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 36.293137\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 37.597940\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 41.266023\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 38.507845\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 38.502734\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 37.593086\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 37.803206\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 37.841244\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 38.478953\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 39.986103\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 40.474487\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 41.294498\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 42.187946\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 43.149708\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 42.663271\n",
      "Epoch: 60/1000... Step: 1920... Loss: 59.112999... Val Loss: 43.139590\n",
      "Validation loss decreased (50.586548 --> 43.139590).  Saving model ...\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 30.859142\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 32.978825\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 36.767832\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 33.923819\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 34.609367\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 33.441991\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 33.350437\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 33.092229\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 33.514321\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 34.057160\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 34.020501\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 35.679118\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 36.265268\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 37.316433\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 37.110544\n",
      "Epoch: 61/1000... Step: 1952... Loss: 57.277077... Val Loss: 36.643931\n",
      "Validation loss decreased (43.139590 --> 36.643931).  Saving model ...\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 26.184313\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 28.549933\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 31.732599\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 29.334527\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 30.169027\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 29.167688\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 29.078648\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 28.922251\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 29.312920\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 29.917072\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 29.954167\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 31.506643\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 32.152881\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 33.261536\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 33.114745\n",
      "Epoch: 62/1000... Step: 1984... Loss: 54.894009... Val Loss: 33.014165\n",
      "Validation loss decreased (36.643931 --> 33.014165).  Saving model ...\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 25.162485\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 26.166579\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 29.076207\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 26.778392\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 27.174910\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 26.323443\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 26.116779\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 25.922974\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 26.240878\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 26.913425\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 26.939029\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 28.231338\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 28.797829\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 29.724041\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 29.550815\n",
      "Epoch: 63/1000... Step: 2016... Loss: 53.323120... Val Loss: 29.133521\n",
      "Validation loss decreased (33.014165 --> 29.133521).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 21.485376\n",
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 22.549453\n",
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 25.807210\n",
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 23.823348\n",
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 24.871218\n",
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 24.103345\n",
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 23.728960\n",
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 23.458899\n",
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 23.656759\n",
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 24.127360\n",
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 24.157078\n",
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 25.740319\n",
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 26.224074\n",
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 27.115482\n",
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 27.217059\n",
      "Epoch: 64/1000... Step: 2048... Loss: 52.555939... Val Loss: 27.045382\n",
      "Validation loss decreased (29.133521 --> 27.045382).  Saving model ...\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 19.741310\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 20.091111\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 23.151497\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 21.363869\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 22.180207\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 21.552589\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 21.160238\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 20.888972\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 21.075082\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 21.594045\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 21.571790\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 22.997797\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 23.514840\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 24.269587\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 24.357572\n",
      "Epoch: 65/1000... Step: 2080... Loss: 49.544323... Val Loss: 24.236311\n",
      "Validation loss decreased (27.045382 --> 24.236311).  Saving model ...\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 16.981127\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 17.492441\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 20.762730\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 19.237739\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 20.302834\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 19.704444\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 19.322090\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 18.964466\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 19.094187\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 19.541573\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 19.549346\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 21.059183\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 21.588622\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 22.266799\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 22.499386\n",
      "Epoch: 66/1000... Step: 2112... Loss: 46.979378... Val Loss: 22.627049\n",
      "Validation loss decreased (24.236311 --> 22.627049).  Saving model ...\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 16.986866\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 16.618988\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 19.576161\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 18.071435\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 18.963278\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 18.579750\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 18.022776\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 17.664994\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 17.716886\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 18.193885\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 18.206787\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 19.592482\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 20.028651\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 20.747016\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 20.985426\n",
      "Epoch: 67/1000... Step: 2144... Loss: 47.617996... Val Loss: 20.949251\n",
      "Validation loss decreased (22.627049 --> 20.949251).  Saving model ...\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 14.397684\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 14.598930\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 17.292527\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 16.161560\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 16.986352\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 16.587940\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 16.167254\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 15.867163\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 15.939638\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 16.443324\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 16.478028\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 17.728529\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 18.305072\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 18.980341\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 19.168919\n",
      "Epoch: 68/1000... Step: 2176... Loss: 44.396122... Val Loss: 19.305620\n",
      "Validation loss decreased (20.949251 --> 19.305620).  Saving model ...\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 14.068703\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 13.798107\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 15.863762\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 15.041975\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 15.878776\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 15.972243\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 15.588105\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 15.303989\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 15.374096\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 16.086470\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 16.306120\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 17.403939\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 17.879439\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 18.521779\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 18.690951\n",
      "Epoch: 69/1000... Step: 2208... Loss: 46.498291... Val Loss: 18.887290\n",
      "Validation loss decreased (19.305620 --> 18.887290).  Saving model ...\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 13.137010\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 12.932350\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 15.249302\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 14.388759\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 15.120281\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 14.888601\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 14.444974\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 14.082210\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 14.123197\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 14.640444\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 14.712352\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 15.837491\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 16.391017\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 17.035582\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 17.256906\n",
      "Epoch: 70/1000... Step: 2240... Loss: 44.218208... Val Loss: 17.456442\n",
      "Validation loss decreased (18.887290 --> 17.456442).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 14.177811\n",
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 13.589571\n",
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 15.360492\n",
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 14.751396\n",
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 15.298413\n",
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 15.418476\n",
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 15.140484\n",
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 14.975950\n",
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 15.053904\n",
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 15.870684\n",
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 16.244459\n",
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 17.164985\n",
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 17.781756\n",
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 18.084889\n",
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 18.315804\n",
      "Epoch: 71/1000... Step: 2272... Loss: 42.617912... Val Loss: 19.023541\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 13.762217\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 12.677809\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 15.121815\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 14.154891\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 14.904406\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 14.608754\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 13.927667\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 13.440836\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 13.395308\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 13.864867\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 13.815365\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 15.013319\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 15.429474\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 16.109855\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 16.473998\n",
      "Epoch: 72/1000... Step: 2304... Loss: 43.702778... Val Loss: 16.562770\n",
      "Validation loss decreased (17.456442 --> 16.562770).  Saving model ...\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 14.146942\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 13.334252\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 14.873223\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 14.475645\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 14.971783\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 15.150072\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 14.785238\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 14.707669\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 14.775457\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 15.578502\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 15.851250\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 16.797683\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 17.587446\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 17.862071\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 18.181101\n",
      "Epoch: 73/1000... Step: 2336... Loss: 42.621155... Val Loss: 18.943030\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 14.081071\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 12.569063\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 15.109455\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 14.103304\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 15.075455\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 14.738207\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 13.861298\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 13.323764\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 13.155504\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 13.639848\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 13.664686\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 14.940821\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 15.308120\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 16.127238\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 16.625742\n",
      "Epoch: 74/1000... Step: 2368... Loss: 45.941841... Val Loss: 16.702287\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 11.165965\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 10.651488\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 12.342045\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 11.940067\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 12.719816\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 12.775063\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 12.178404\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 11.767701\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 11.783820\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 12.312450\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 12.472523\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 13.455613\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 13.932149\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 14.550010\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 14.820456\n",
      "Epoch: 75/1000... Step: 2400... Loss: 41.903404... Val Loss: 15.117070\n",
      "Validation loss decreased (16.562770 --> 15.117070).  Saving model ...\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 11.852159\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 10.767393\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 12.112863\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 11.948740\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 12.709360\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 12.886553\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 12.247881\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 11.919037\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 11.920969\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 12.477946\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 12.606996\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 13.541831\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 13.955057\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 14.466138\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 14.783175\n",
      "Epoch: 76/1000... Step: 2432... Loss: 46.218025... Val Loss: 15.060844\n",
      "Validation loss decreased (15.117070 --> 15.060844).  Saving model ...\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 10.886871\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 10.275365\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 11.880868\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 11.913844\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 12.621001\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 12.752312\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 12.355090\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 12.023000\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 11.998833\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 12.626481\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 12.833623\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 13.726670\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 14.261943\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 14.691008\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 15.003331\n",
      "Epoch: 77/1000... Step: 2464... Loss: 36.929230... Val Loss: 15.624979\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 12.583766\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 11.021899\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 12.432767\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 12.154421\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 12.951782\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 13.075750\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 12.255057\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 11.764257\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 11.668709\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 12.077332\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 12.093673\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 13.115315\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 13.412075\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 14.098023\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 14.505422\n",
      "Epoch: 78/1000... Step: 2496... Loss: 48.854813... Val Loss: 14.477284\n",
      "Validation loss decreased (15.060844 --> 14.477284).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 11.378613\n",
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 10.297481\n",
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 12.060430\n",
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 11.727375\n",
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 12.738504\n",
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 12.767508\n",
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 11.903075\n",
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 11.311563\n",
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 11.220529\n",
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 11.542669\n",
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 11.636497\n",
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 12.798031\n",
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 13.106122\n",
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 13.843995\n",
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 14.314397\n",
      "Epoch: 79/1000... Step: 2528... Loss: 44.395473... Val Loss: 14.406863\n",
      "Validation loss decreased (14.477284 --> 14.406863).  Saving model ...\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 10.678872\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 9.699830\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 11.150852\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 11.078708\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 11.975954\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 12.169545\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 11.385206\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 10.908753\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 10.853909\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 11.229467\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 11.336305\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 12.354055\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 12.710083\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 13.301556\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 13.723373\n",
      "Epoch: 80/1000... Step: 2560... Loss: 41.581585... Val Loss: 13.972372\n",
      "Validation loss decreased (14.406863 --> 13.972372).  Saving model ...\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 11.587809\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 10.167645\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 11.776978\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 11.537218\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 12.409344\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 12.457150\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 11.588756\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 11.021686\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 10.930175\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 11.214338\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 11.272645\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 12.304337\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 12.595960\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 13.244185\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 13.697594\n",
      "Epoch: 81/1000... Step: 2592... Loss: 41.042160... Val Loss: 13.770183\n",
      "Validation loss decreased (13.972372 --> 13.770183).  Saving model ...\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 10.173100\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 9.236882\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 10.724236\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 10.684349\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 11.574981\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 11.745983\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 10.944056\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 10.424892\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 10.348681\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 10.760191\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 10.948656\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 11.771563\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 12.127388\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 12.773941\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 13.144362\n",
      "Epoch: 82/1000... Step: 2624... Loss: 40.029400... Val Loss: 13.521508\n",
      "Validation loss decreased (13.770183 --> 13.521508).  Saving model ...\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 11.243412\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 9.904312\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 11.570377\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 11.269201\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 12.244926\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 12.225264\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 11.293862\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 10.677271\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 10.550711\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 10.769661\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 10.812175\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 11.903657\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 12.203545\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 12.895708\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 13.417985\n",
      "Epoch: 83/1000... Step: 2656... Loss: 38.542076... Val Loss: 13.526290\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 10.569285\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 9.357415\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 10.853442\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 10.762096\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 11.583048\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 11.663850\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 10.849284\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 10.328093\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 10.245634\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 10.549390\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 10.708985\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 11.534883\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 11.880842\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 12.485307\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 12.876550\n",
      "Epoch: 84/1000... Step: 2688... Loss: 34.245598... Val Loss: 13.113266\n",
      "Validation loss decreased (13.521508 --> 13.113266).  Saving model ...\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 10.392781\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 9.166758\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 10.478714\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 10.576927\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 11.400590\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 11.561491\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 10.730492\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 10.222739\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 10.116398\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 10.504492\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 10.717941\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 11.432618\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 11.795648\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 12.402142\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 12.789407\n",
      "Epoch: 85/1000... Step: 2720... Loss: 37.649078... Val Loss: 13.145792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 11.069336\n",
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 9.466470\n",
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 10.938828\n",
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 10.912420\n",
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 11.776676\n",
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 12.017489\n",
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 11.053114\n",
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 10.436769\n",
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 10.287263\n",
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 10.585313\n",
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 10.717424\n",
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 11.582790\n",
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 11.855213\n",
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 12.501637\n",
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 12.986121\n",
      "Epoch: 86/1000... Step: 2752... Loss: 40.723263... Val Loss: 13.125729\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 10.874154\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 9.320512\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 10.786261\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 10.791111\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 11.676544\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 11.815684\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 10.876198\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 10.311051\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 10.164961\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 10.399455\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 10.544100\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 11.409582\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 11.686868\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 12.281395\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 12.771449\n",
      "Epoch: 87/1000... Step: 2784... Loss: 33.035435... Val Loss: 12.955900\n",
      "Validation loss decreased (13.113266 --> 12.955900).  Saving model ...\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 10.597988\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 9.149266\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 10.609317\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 10.748536\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 11.524732\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 11.686425\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 10.813807\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 10.279101\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 10.091614\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 10.387982\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 10.553094\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 11.331935\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 11.655676\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 12.219369\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 12.677178\n",
      "Epoch: 88/1000... Step: 2816... Loss: 31.015114... Val Loss: 12.920109\n",
      "Validation loss decreased (12.955900 --> 12.920109).  Saving model ...\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 11.028786\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 9.310061\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 10.855905\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 10.791122\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 11.825010\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 11.976466\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 10.944337\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 10.288043\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 10.116378\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 10.285291\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 10.391368\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 11.396646\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 11.638862\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 12.283550\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 12.875332\n",
      "Epoch: 89/1000... Step: 2848... Loss: 35.143860... Val Loss: 13.021795\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 12.126318\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 10.042302\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 11.603736\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 11.416680\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 12.529349\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 12.373106\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 11.354733\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 10.703897\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 10.517188\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 10.514187\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 10.562047\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 11.627080\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 11.828419\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 12.467338\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 13.088487\n",
      "Epoch: 90/1000... Step: 2880... Loss: 31.655977... Val Loss: 13.117337\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 10.831040\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 9.158875\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 10.730199\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 10.591666\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 11.479642\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 11.587836\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 10.605694\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 10.024455\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 9.859196\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 10.002168\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 10.111625\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 11.017220\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 11.293728\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 11.868409\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 12.400776\n",
      "Epoch: 91/1000... Step: 2912... Loss: 24.988596... Val Loss: 12.588217\n",
      "Validation loss decreased (12.920109 --> 12.588217).  Saving model ...\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 10.473259\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 8.908111\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 10.457418\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 10.436969\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 11.472807\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 11.606195\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 10.585974\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 9.947906\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 9.764108\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 9.923465\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 10.098286\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 11.034310\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 11.301253\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 11.932323\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 12.513490\n",
      "Epoch: 92/1000... Step: 2944... Loss: 30.489853... Val Loss: 12.735936\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 10.572567\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 9.062317\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 10.479204\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 10.388700\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 11.350939\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 11.225640\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 10.254916\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 9.666572\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 9.514439\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 9.472303\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 9.675881\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 10.599636\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 10.897318\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 11.452758\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 12.007818\n",
      "Epoch: 93/1000... Step: 2976... Loss: 26.676788... Val Loss: 12.284838\n",
      "Validation loss decreased (12.588217 --> 12.284838).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 12.477000\n",
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 9.909484\n",
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 11.199385\n",
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 11.353187\n",
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 12.213154\n",
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 12.381500\n",
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 11.332588\n",
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 10.743640\n",
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 10.525610\n",
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 10.675370\n",
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 10.740729\n",
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 11.591343\n",
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 11.755366\n",
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 12.311021\n",
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 12.913914\n",
      "Epoch: 94/1000... Step: 3008... Loss: 35.929478... Val Loss: 12.984166\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 11.350127\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 9.482741\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 10.907106\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 10.743086\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 11.709548\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 11.501547\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 10.473225\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 9.852455\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 9.650263\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 9.547452\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 9.714868\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 10.675209\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 10.937214\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 11.499759\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 12.107088\n",
      "Epoch: 95/1000... Step: 3040... Loss: 25.594067... Val Loss: 12.299038\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 12.346666\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 9.895957\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 11.316161\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 11.285611\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 12.338774\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 12.498700\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 11.337153\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 10.635624\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 10.363321\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 10.439738\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 10.453322\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 11.405347\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 11.584161\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 12.189697\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 12.866834\n",
      "Epoch: 96/1000... Step: 3072... Loss: 32.644749... Val Loss: 12.961044\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 11.338318\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 9.327987\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 10.733125\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 10.794407\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 12.009194\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 11.766104\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 10.773822\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 10.149856\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 9.925128\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 9.788980\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 9.865248\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 10.890961\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 11.107025\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 11.621016\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 12.275670\n",
      "Epoch: 97/1000... Step: 3104... Loss: 32.540749... Val Loss: 12.358413\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 11.803356\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 9.494612\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 10.599795\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 10.754239\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 11.700418\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 11.621859\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 10.588780\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 9.964904\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 9.752615\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 9.741931\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 9.872321\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 10.778777\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 10.982677\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 11.585407\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 12.217151\n",
      "Epoch: 98/1000... Step: 3136... Loss: 31.477936... Val Loss: 12.331579\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 11.559552\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 9.517462\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 10.491793\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 10.659840\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 11.360074\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 11.496447\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 10.562044\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 10.204380\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 9.999205\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 10.130621\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 10.295028\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 10.977205\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 11.282020\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 11.679419\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 12.226540\n",
      "Epoch: 99/1000... Step: 3168... Loss: 21.709711... Val Loss: 12.576828\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 11.095503\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 9.026668\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 10.138876\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 10.355630\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 11.375568\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 11.187181\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 10.189100\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 9.624142\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 9.435239\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 9.367368\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 9.533081\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 10.387146\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 10.633142\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 11.190194\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 11.816756\n",
      "Epoch: 100/1000... Step: 3200... Loss: 29.835531... Val Loss: 12.037522\n",
      "Validation loss decreased (12.284838 --> 12.037522).  Saving model ...\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 10.689648\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 9.108651\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 10.311410\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 10.112217\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 10.845887\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 10.481464\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 9.619637\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 9.237808\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 9.075114\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 8.974815\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 9.168373\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 9.925321\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 10.298545\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 10.698752\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 11.195447\n",
      "Epoch: 101/1000... Step: 3232... Loss: 13.182405... Val Loss: 11.574247\n",
      "Validation loss decreased (12.037522 --> 11.574247).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 11.530201\n",
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 9.279280\n",
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 10.270989\n",
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 10.631548\n",
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 11.682289\n",
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 11.525012\n",
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 10.512216\n",
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 9.989461\n",
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 9.785719\n",
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 9.764575\n",
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 9.925402\n",
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 10.723402\n",
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 10.947478\n",
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 11.527849\n",
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 12.166717\n",
      "Epoch: 102/1000... Step: 3264... Loss: 34.775780... Val Loss: 12.306689\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 10.421515\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 9.074234\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 10.456796\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 10.121990\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 10.868668\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 10.464564\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 9.668490\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 9.429666\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 9.241031\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 9.107936\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 9.220083\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 10.005634\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 10.409794\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 10.774686\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 11.281530\n",
      "Epoch: 103/1000... Step: 3296... Loss: 8.413757... Val Loss: 11.657314\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 10.740239\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 8.753795\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 9.940820\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 10.003321\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 11.103825\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 10.821538\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 9.829993\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 9.249348\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 9.042140\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 8.947868\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 9.090165\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 9.958042\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 10.214007\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 10.793049\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 11.434610\n",
      "Epoch: 104/1000... Step: 3328... Loss: 26.734619... Val Loss: 11.605160\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 10.439533\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 8.854605\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 10.092172\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 9.832676\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 10.555214\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 10.194859\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 9.392277\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 9.199683\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 9.038678\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 8.946563\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 9.081190\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 9.795512\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 10.181927\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 10.542111\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 11.044187\n",
      "Epoch: 105/1000... Step: 3360... Loss: 10.169902... Val Loss: 11.442248\n",
      "Validation loss decreased (11.574247 --> 11.442248).  Saving model ...\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 11.188748\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 9.043720\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 10.312937\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 10.228248\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 11.377155\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 11.141788\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 10.083032\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 9.472792\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 9.217033\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 9.199107\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 9.305813\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 10.207322\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 10.427202\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 11.069202\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 11.745493\n",
      "Epoch: 106/1000... Step: 3392... Loss: 30.159712... Val Loss: 11.813402\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 9.908368\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 8.412214\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 9.916324\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 9.462070\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 10.339875\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 10.009411\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 9.111722\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 8.669140\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 8.509956\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 8.418691\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 8.546074\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 9.380354\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 9.742082\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 10.230180\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 10.756872\n",
      "Epoch: 107/1000... Step: 3424... Loss: 13.465655... Val Loss: 11.076499\n",
      "Validation loss decreased (11.442248 --> 11.076499).  Saving model ...\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 11.164116\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 9.210291\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 10.338509\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 10.155701\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 11.280080\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 10.996150\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 9.949236\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 9.335045\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 9.070920\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 9.002734\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 9.157717\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 10.064674\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 10.296180\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 11.002385\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 11.648855\n",
      "Epoch: 108/1000... Step: 3456... Loss: 28.516331... Val Loss: 11.709562\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 9.627957\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 8.320744\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 9.877162\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 9.318660\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 10.188356\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 9.863532\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 8.999973\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 8.588646\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 8.443958\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 8.351323\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 8.486781\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 9.319551\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 9.693504\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 10.188736\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 10.674671\n",
      "Epoch: 109/1000... Step: 3488... Loss: 11.589929... Val Loss: 11.004557\n",
      "Validation loss decreased (11.076499 --> 11.004557).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 11.262446\n",
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 9.350401\n",
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 10.373761\n",
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 10.276260\n",
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 11.468551\n",
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 11.190610\n",
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 10.111669\n",
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 9.495526\n",
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 9.203838\n",
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 9.120669\n",
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 9.288124\n",
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 10.181907\n",
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 10.403778\n",
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 11.137306\n",
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 11.810951\n",
      "Epoch: 110/1000... Step: 3520... Loss: 31.645990... Val Loss: 11.873223\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 9.461773\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 8.312837\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 9.959481\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 9.300038\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 10.191881\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 9.868137\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 9.037896\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 8.708477\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 8.572648\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 8.460100\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 8.563032\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 9.399868\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 9.775393\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 10.237803\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 10.696692\n",
      "Epoch: 111/1000... Step: 3552... Loss: 9.036584... Val Loss: 11.013527\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 10.302530\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 8.814748\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 9.876487\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 9.585582\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 10.748071\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 10.444319\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 9.448040\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 8.862724\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 8.625817\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 8.525030\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 8.704632\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 9.566884\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 9.849420\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 10.559036\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 11.159209\n",
      "Epoch: 112/1000... Step: 3584... Loss: 24.257996... Val Loss: 11.284172\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 9.657816\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 8.037913\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 9.312908\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 9.000103\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 10.004059\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 9.699614\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 8.815365\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 8.359606\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 8.199873\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 8.113117\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 8.238841\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 9.025873\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 9.367221\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 9.889563\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 10.445372\n",
      "Epoch: 113/1000... Step: 3616... Loss: 15.953963... Val Loss: 10.718077\n",
      "Validation loss decreased (11.004557 --> 10.718077).  Saving model ...\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 11.520136\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 9.439237\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 10.527404\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 10.324907\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 11.496374\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 11.154784\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 10.057440\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 9.436091\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 9.134201\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 9.043202\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 9.208693\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 10.104059\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 10.318151\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 11.067715\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 11.756558\n",
      "Epoch: 114/1000... Step: 3648... Loss: 31.214834... Val Loss: 11.795474\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 9.757012\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 8.440088\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 10.247299\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 9.437883\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 10.447972\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 10.093856\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 9.191090\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 8.776316\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 8.583919\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 8.433191\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 8.476660\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 9.391591\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 9.730388\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 10.232441\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 10.754189\n",
      "Epoch: 115/1000... Step: 3680... Loss: 10.684068... Val Loss: 10.995867\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 9.408410\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 8.262723\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 9.680214\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 9.103165\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 9.986301\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 9.730428\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 8.933777\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 8.743219\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 8.570377\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 8.459657\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 8.548866\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 9.302957\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 9.713248\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 10.132082\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 10.595273\n",
      "Epoch: 116/1000... Step: 3712... Loss: 8.018027... Val Loss: 10.993558\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 11.609287\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 9.578199\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 11.245427\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 10.549994\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 11.966421\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 11.558284\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 10.422087\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 9.706678\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 9.380744\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 9.263949\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 9.416293\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 10.505906\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 10.694153\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 11.448271\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 12.181910\n",
      "Epoch: 117/1000... Step: 3744... Loss: 24.625753... Val Loss: 12.242639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 9.713120\n",
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 8.329367\n",
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 9.887081\n",
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 9.045474\n",
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 9.910118\n",
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 9.668542\n",
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 8.782252\n",
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 8.268954\n",
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 8.128159\n",
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 8.080436\n",
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 8.207082\n",
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 9.083522\n",
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 9.408392\n",
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 10.017702\n",
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 10.490505\n",
      "Epoch: 118/1000... Step: 3776... Loss: 15.838518... Val Loss: 10.682328\n",
      "Validation loss decreased (10.718077 --> 10.682328).  Saving model ...\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 10.111932\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 8.238337\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 9.453434\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 9.142911\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 10.063050\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 9.748806\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 8.807066\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 8.322307\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 8.120247\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 8.017536\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 8.160229\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 8.952768\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 9.274667\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 9.830473\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 10.429017\n",
      "Epoch: 119/1000... Step: 3808... Loss: 15.467959... Val Loss: 10.711036\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 10.890707\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 9.067677\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 10.027552\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 9.651380\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 10.723969\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 10.398927\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 9.362432\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 8.765916\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 8.500952\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 8.419412\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 8.588615\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 9.414289\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 9.685663\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 10.441138\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 11.062638\n",
      "Epoch: 120/1000... Step: 3840... Loss: 26.875256... Val Loss: 11.132450\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 10.064093\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 8.487101\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 10.380761\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 9.440421\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 10.588826\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 10.226421\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 9.251563\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 8.705410\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 8.479685\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 8.365621\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 8.412457\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 9.384232\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 9.694181\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 10.283632\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 10.870093\n",
      "Epoch: 121/1000... Step: 3872... Loss: 13.595817... Val Loss: 11.078962\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 9.905184\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 8.202343\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 9.465508\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 8.909932\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 9.739588\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 9.526222\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 8.699419\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 8.409548\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 8.235819\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 8.156465\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 8.224130\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 8.939674\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 9.289972\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 9.777491\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 10.285367\n",
      "Epoch: 122/1000... Step: 3904... Loss: 11.079931... Val Loss: 10.502007\n",
      "Validation loss decreased (10.682328 --> 10.502007).  Saving model ...\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 11.205935\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 9.284452\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 10.360019\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 9.896293\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 11.051189\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 10.701916\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 9.626116\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 8.992475\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 8.701094\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 8.624824\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 8.772816\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 9.651310\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 9.896264\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 10.676159\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 11.338274\n",
      "Epoch: 123/1000... Step: 3936... Loss: 28.362158... Val Loss: 11.376843\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 9.793232\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 8.512274\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 10.629163\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 9.437856\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 10.567793\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 10.220787\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 9.318610\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 8.802137\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 8.642597\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 8.550924\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 8.615608\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 9.619459\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 9.917556\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 10.512361\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 11.007391\n",
      "Epoch: 124/1000... Step: 3968... Loss: 12.471812... Val Loss: 11.183676\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 9.505576\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 8.049401\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 9.463463\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 8.826367\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 9.826702\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 9.581699\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 8.667889\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 8.188039\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 7.980982\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 7.869077\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 7.989765\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 8.823434\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 9.165135\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 9.744550\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 10.284710\n",
      "Epoch: 125/1000... Step: 4000... Loss: 12.574514... Val Loss: 10.557083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 10.366731\n",
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 8.635685\n",
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 9.719696\n",
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 9.225833\n",
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 10.321929\n",
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 10.011628\n",
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 9.023628\n",
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 8.432458\n",
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 8.192689\n",
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 8.086563\n",
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 8.227387\n",
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 9.052021\n",
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 9.345749\n",
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 10.049856\n",
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 10.658835\n",
      "Epoch: 126/1000... Step: 4032... Loss: 20.528206... Val Loss: 10.757658\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 10.070816\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 8.281356\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 9.427474\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 8.906175\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 9.911462\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 9.619779\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 8.699904\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 8.195193\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 7.997648\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 7.950419\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 8.057828\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 8.817787\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 9.143427\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 9.781374\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 10.355574\n",
      "Epoch: 127/1000... Step: 4064... Loss: 19.122147... Val Loss: 10.490899\n",
      "Validation loss decreased (10.502007 --> 10.490899).  Saving model ...\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 10.970014\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 8.936193\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 9.946591\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 9.505855\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 10.537187\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 10.194824\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 9.167745\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 8.576202\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 8.325591\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 8.308244\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 8.436424\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 9.232944\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 9.505303\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 10.245747\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 10.880527\n",
      "Epoch: 128/1000... Step: 4096... Loss: 27.213030... Val Loss: 10.949059\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 10.594067\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 8.754282\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 10.618792\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 9.645609\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 10.844988\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 10.467878\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 9.437356\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 8.818790\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 8.552368\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 8.478470\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 8.565992\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 9.559427\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 9.837114\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 10.498783\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 11.130054\n",
      "Epoch: 129/1000... Step: 4128... Loss: 15.324530... Val Loss: 11.326202\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 9.321445\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 7.995631\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 9.786389\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 8.672209\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 9.693223\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 9.463295\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 8.639522\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 8.164007\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 8.060356\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 8.063302\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 8.122897\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 9.012199\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 9.341777\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 9.952155\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 10.407259\n",
      "Epoch: 130/1000... Step: 4160... Loss: 13.627955... Val Loss: 10.584261\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 10.732240\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 9.157462\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 10.038705\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 9.476225\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 10.606209\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 10.349088\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 9.318010\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 8.685972\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 8.410638\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 8.345350\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 8.502200\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 9.351623\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 9.615337\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 10.419934\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 11.027697\n",
      "Epoch: 131/1000... Step: 4192... Loss: 26.669737... Val Loss: 11.076115\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 9.067209\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 7.853772\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 9.723329\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 8.657667\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 9.672402\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 9.475623\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 8.722603\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 8.383691\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 8.319562\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 8.253686\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 8.322473\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 9.204256\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 9.507263\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 10.047295\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 10.460825\n",
      "Epoch: 132/1000... Step: 4224... Loss: 8.113997... Val Loss: 10.635219\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 8.549067\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 7.498025\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 9.499784\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 8.431237\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 9.410961\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 9.278193\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 8.696209\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 8.449304\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 8.527316\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 8.534661\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 8.709366\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 9.580248\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 9.894291\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 10.401715\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 10.693676\n",
      "Epoch: 133/1000... Step: 4256... Loss: 7.516906... Val Loss: 10.913386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 8.766268\n",
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 7.538620\n",
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 8.999665\n",
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 8.043726\n",
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 8.983383\n",
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 8.886013\n",
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 8.127468\n",
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 7.687793\n",
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 7.654534\n",
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 7.774884\n",
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 7.933736\n",
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 8.659687\n",
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 9.012424\n",
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 9.638353\n",
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 10.008967\n",
      "Epoch: 134/1000... Step: 4288... Loss: 16.153807... Val Loss: 10.262642\n",
      "Validation loss decreased (10.490899 --> 10.262642).  Saving model ...\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 8.909278\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 7.589395\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 8.931786\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 8.230815\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 9.200217\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 9.076235\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 8.265183\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 7.837902\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 7.739784\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 7.818883\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 7.957395\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 8.664894\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 9.002427\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 9.583332\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 10.020871\n",
      "Epoch: 135/1000... Step: 4320... Loss: 15.151062... Val Loss: 10.304609\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 8.880761\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 7.598782\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 8.942050\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 8.194869\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 9.192680\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 9.066723\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 8.237080\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 7.800618\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 7.678854\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 7.722169\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 7.838521\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 8.550749\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 8.897621\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 9.487251\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 9.941068\n",
      "Epoch: 136/1000... Step: 4352... Loss: 13.581634... Val Loss: 10.222948\n",
      "Validation loss decreased (10.262642 --> 10.222948).  Saving model ...\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 8.792680\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 7.547503\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 8.957981\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 8.199243\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 9.167055\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 9.040208\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 8.250801\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 7.865616\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 7.765995\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 7.767202\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 7.903093\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 8.612828\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 8.947451\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 9.507511\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 9.916996\n",
      "Epoch: 137/1000... Step: 4384... Loss: 10.885747... Val Loss: 10.197961\n",
      "Validation loss decreased (10.222948 --> 10.197961).  Saving model ...\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 8.939059\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 7.605928\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 8.956809\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 8.202714\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 9.154433\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 9.040664\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 8.225848\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 7.803258\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 7.695517\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 7.743957\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 7.876075\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 8.566500\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 8.901587\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 9.480820\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 9.905945\n",
      "Epoch: 138/1000... Step: 4416... Loss: 12.637788... Val Loss: 10.179649\n",
      "Validation loss decreased (10.197961 --> 10.179649).  Saving model ...\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 8.917471\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 7.606807\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 9.024605\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 8.243227\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 9.249050\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 9.118803\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 8.289723\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 7.833719\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 7.712424\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 7.689690\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 7.809491\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 8.547485\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 8.860145\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 9.448746\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 9.884784\n",
      "Epoch: 139/1000... Step: 4448... Loss: 12.485861... Val Loss: 10.112179\n",
      "Validation loss decreased (10.179649 --> 10.112179).  Saving model ...\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 8.995667\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 7.698430\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 9.009098\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 8.332477\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 9.311165\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 9.164608\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 8.316689\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 7.921777\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 7.754981\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 7.730813\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 7.840465\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 8.545881\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 8.888785\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 9.452793\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 9.924800\n",
      "Epoch: 140/1000... Step: 4480... Loss: 10.957262... Val Loss: 10.219601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 8.935721\n",
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 7.605284\n",
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 9.051002\n",
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 8.261157\n",
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 9.267681\n",
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 9.135533\n",
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 8.320120\n",
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 7.856659\n",
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 7.751026\n",
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 7.706725\n",
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 7.841588\n",
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 8.587957\n",
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 8.878123\n",
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 9.468853\n",
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 9.886908\n",
      "Epoch: 141/1000... Step: 4512... Loss: 11.413824... Val Loss: 10.084998\n",
      "Validation loss decreased (10.112179 --> 10.084998).  Saving model ...\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 9.214052\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 7.721607\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 9.026836\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 8.404975\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 9.542589\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 9.519320\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 8.640712\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 8.139166\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 7.960326\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 8.098728\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 8.177934\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 8.913104\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 9.183571\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 9.766547\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 10.297874\n",
      "Epoch: 142/1000... Step: 4544... Loss: 21.890553... Val Loss: 10.424213\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 10.838705\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 8.819539\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 9.838717\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 9.579468\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 10.693345\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 10.559352\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 9.484798\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 8.797617\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 8.541687\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 8.609747\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 8.727082\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 9.516601\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 9.726652\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 10.450325\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 11.123312\n",
      "Epoch: 143/1000... Step: 4576... Loss: 30.681927... Val Loss: 11.241863\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 9.160673\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 7.847194\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 9.194787\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 8.530096\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 9.449929\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 9.321139\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 8.508412\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 8.264692\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 8.067856\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 8.077588\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 8.157292\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 8.818490\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 9.173367\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 9.673028\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 10.147958\n",
      "Epoch: 144/1000... Step: 4608... Loss: 9.385379... Val Loss: 10.446047\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 9.843515\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 8.285214\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 9.663493\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 8.900019\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 10.100191\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 9.887034\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 8.939987\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 8.303180\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 8.097227\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 8.008158\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 8.113411\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 8.966493\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 9.208886\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 9.911916\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 10.470550\n",
      "Epoch: 145/1000... Step: 4640... Loss: 17.078413... Val Loss: 10.527243\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 8.633035\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 7.587667\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 9.431362\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 8.382209\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 9.389188\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 9.234849\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 8.505593\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 8.163319\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 8.080826\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 8.020942\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 8.125469\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 8.906639\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 9.225877\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 9.771321\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 10.130710\n",
      "Epoch: 146/1000... Step: 4672... Loss: 7.650197... Val Loss: 10.337664\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 9.022053\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 7.662492\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 9.107431\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 8.207155\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 9.205413\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 9.080470\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 8.259866\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 7.798362\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 7.672386\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 7.706057\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 7.795050\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 8.511387\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 8.832620\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 9.440938\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 9.867195\n",
      "Epoch: 147/1000... Step: 4704... Loss: 13.341372... Val Loss: 10.058700\n",
      "Validation loss decreased (10.084998 --> 10.058700).  Saving model ...\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 8.969184\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 7.702376\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 9.189097\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 8.362546\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 9.435873\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 9.266778\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 8.402362\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 7.900512\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 7.736073\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 7.645311\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 7.757707\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 8.539191\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 8.824460\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 9.440210\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 9.900587\n",
      "Epoch: 148/1000... Step: 4736... Loss: 10.556502... Val Loss: 10.084574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 9.406488\n",
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 7.892499\n",
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 9.303916\n",
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 8.503726\n",
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 9.685078\n",
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 9.578039\n",
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 8.705489\n",
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 8.118868\n",
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 7.968849\n",
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 8.015859\n",
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 8.113977\n",
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 8.914611\n",
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 9.153004\n",
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 9.817216\n",
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 10.319570\n",
      "Epoch: 149/1000... Step: 4768... Loss: 18.088945... Val Loss: 10.380372\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 10.900379\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 8.951732\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 9.789726\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 9.616131\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 10.732758\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 10.647581\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 9.595805\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 8.904259\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 8.637367\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 8.657791\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 8.835530\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 9.570187\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 9.775654\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 10.505657\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 11.147488\n",
      "Epoch: 150/1000... Step: 4800... Loss: 30.385260... Val Loss: 11.242713\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 9.712291\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 8.081703\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 9.284166\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 8.734614\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 9.646836\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 9.606011\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 8.699362\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 8.382132\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 8.131979\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 8.140160\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 8.183856\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 8.856292\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 9.163165\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 9.694065\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 10.239097\n",
      "Epoch: 151/1000... Step: 4832... Loss: 9.955626... Val Loss: 10.477117\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 10.927196\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 9.066551\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 9.815884\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 9.518796\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 10.653401\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 10.450566\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 9.432870\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 8.778864\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 8.508486\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 8.431352\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 8.604245\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 9.325372\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 9.567591\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 10.297397\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 10.921423\n",
      "Epoch: 152/1000... Step: 4864... Loss: 23.838831... Val Loss: 10.971371\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 9.030240\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 7.736400\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 9.233523\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 8.243572\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 9.276200\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 9.091949\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 8.290330\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 7.901686\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 7.750633\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 7.708972\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 7.744382\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 8.492856\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 8.825719\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 9.409611\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 9.867421\n",
      "Epoch: 153/1000... Step: 4896... Loss: 9.079186... Val Loss: 10.050578\n",
      "Validation loss decreased (10.058700 --> 10.050578).  Saving model ...\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 8.284435\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 7.381479\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 9.028493\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 8.067891\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 9.228790\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 9.065649\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 8.314852\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 7.947387\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 7.836684\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 7.758867\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 7.883288\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 8.674276\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 8.974342\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 9.546714\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 9.954503\n",
      "Epoch: 154/1000... Step: 4928... Loss: 8.377100... Val Loss: 10.176066\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 8.651277\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 7.445786\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 8.962160\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 7.954800\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 8.943126\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 8.816191\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 8.058465\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 7.620001\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 7.532470\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 7.563951\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 7.694033\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 8.396275\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 8.719670\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 9.343457\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 9.698918\n",
      "Epoch: 155/1000... Step: 4960... Loss: 10.707904... Val Loss: 9.915379\n",
      "Validation loss decreased (10.050578 --> 9.915379).  Saving model ...\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 8.743566\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 7.529927\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 8.946157\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 8.092550\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 9.083220\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 8.948355\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 8.142294\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 7.710670\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 7.568484\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 7.549972\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 7.680915\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 8.379037\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 8.694071\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 9.300196\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 9.693597\n",
      "Epoch: 156/1000... Step: 4992... Loss: 9.918809... Val Loss: 9.931877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 8.841402\n",
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 7.520612\n",
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 8.903582\n",
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 8.093042\n",
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 9.093987\n",
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 8.960275\n",
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 8.158890\n",
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 7.771441\n",
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 7.633530\n",
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 7.606622\n",
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 7.717530\n",
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 8.405381\n",
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 8.709013\n",
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 9.295255\n",
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 9.717688\n",
      "Epoch: 157/1000... Step: 5024... Loss: 8.337795... Val Loss: 9.944182\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 8.899260\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 7.536367\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 8.908148\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 8.129760\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 9.183095\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 9.038610\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 8.224669\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 7.801881\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 7.650771\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 7.601378\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 7.700248\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 8.411157\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 8.694980\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 9.284320\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 9.731193\n",
      "Epoch: 158/1000... Step: 5056... Loss: 10.013865... Val Loss: 9.919620\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 9.395067\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 7.838651\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 9.028269\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 8.412969\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 9.581541\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 9.586202\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 8.692227\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 8.140148\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 7.939633\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 8.022198\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 8.108348\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 8.827949\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 9.074049\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 9.697568\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 10.228769\n",
      "Epoch: 159/1000... Step: 5088... Loss: 19.735893... Val Loss: 10.332456\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 10.858593\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 8.838263\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 9.803921\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 9.538706\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 10.641444\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 10.476058\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 9.423566\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 8.720925\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 8.472192\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 8.441961\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 8.573806\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 9.329355\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 9.534636\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 10.250931\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 10.904764\n",
      "Epoch: 160/1000... Step: 5120... Loss: 25.677179... Val Loss: 11.013554\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 9.381497\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 7.895406\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 9.198794\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 8.553612\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 9.439871\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 9.314390\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 8.536982\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 8.514161\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 8.291011\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 8.256157\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 8.274334\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 8.896533\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 9.206600\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 9.658311\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 10.163856\n",
      "Epoch: 161/1000... Step: 5152... Loss: 5.142776... Val Loss: 10.408058\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 10.509113\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 8.845967\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 9.737731\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 9.269600\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 10.459642\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 10.247700\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 9.258423\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 8.603703\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 8.335829\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 8.231897\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 8.396811\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 9.132261\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 9.370194\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 10.101432\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 10.690685\n",
      "Epoch: 162/1000... Step: 5184... Loss: 19.109308... Val Loss: 10.730059\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 9.089301\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 7.715301\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 9.299388\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 8.322517\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 9.458152\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 9.243778\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 8.421359\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 7.996391\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 7.838825\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 7.747898\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 7.786758\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 8.603625\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 8.868871\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 9.466187\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 9.959301\n",
      "Epoch: 163/1000... Step: 5216... Loss: 8.433702... Val Loss: 10.084842\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 10.591700\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 8.922560\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 9.989899\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 9.301251\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 10.597606\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 10.526441\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 9.516977\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 8.825795\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 8.545113\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 8.570590\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 8.683247\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 9.487645\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 9.699336\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 10.444843\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 11.057517\n",
      "Epoch: 164/1000... Step: 5248... Loss: 24.490330... Val Loss: 11.023298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 11.274365\n",
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 9.182651\n",
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 9.997449\n",
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 9.738625\n",
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 10.842537\n",
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 10.717291\n",
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 9.661208\n",
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 8.956018\n",
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 8.679483\n",
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 8.654709\n",
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 8.815784\n",
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 9.536777\n",
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 9.737193\n",
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 10.474093\n",
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 11.118352\n",
      "Epoch: 165/1000... Step: 5280... Loss: 24.901190... Val Loss: 11.158002\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 9.242574\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 7.804450\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 9.193892\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 8.341391\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 9.346815\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 9.188738\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 8.337624\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 7.982640\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 7.772023\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 7.725931\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 7.761149\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 8.474842\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 8.790816\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 9.366683\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 9.869031\n",
      "Epoch: 166/1000... Step: 5312... Loss: 7.641677... Val Loss: 10.082437\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 10.403293\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 8.717447\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 9.660605\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 9.182498\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 10.398391\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 10.212654\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 9.227178\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 8.574850\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 8.312865\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 8.234930\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 8.407498\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 9.155737\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 9.385072\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 10.109491\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 10.715010\n",
      "Epoch: 167/1000... Step: 5344... Loss: 19.074108... Val Loss: 10.770150\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 9.083907\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 7.689022\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 9.189672\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 8.189321\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 9.274339\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 9.096488\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 8.284070\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 7.882245\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 7.712297\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 7.646240\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 7.679178\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 8.441036\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 8.729223\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 9.333431\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 9.805404\n",
      "Epoch: 168/1000... Step: 5376... Loss: 8.243101... Val Loss: 9.947125\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 10.675236\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 8.951941\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 9.825354\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 9.334384\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 10.558891\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 10.526557\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 9.507300\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 8.824859\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 8.544949\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 8.571186\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 8.735922\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 9.477602\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 9.691666\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 10.436398\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 11.051764\n",
      "Epoch: 169/1000... Step: 5408... Loss: 25.116737... Val Loss: 11.075968\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 10.891804\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 8.987536\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 10.052003\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 9.443681\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 10.653601\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 10.571173\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 9.524001\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 8.804545\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 8.508646\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 8.477957\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 8.565938\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 9.366675\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 9.576608\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 10.326820\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 10.954089\n",
      "Epoch: 170/1000... Step: 5440... Loss: 22.025896... Val Loss: 10.954654\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 9.836074\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 8.102611\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 9.360330\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 8.695724\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 9.817278\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 9.626176\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 8.705930\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 8.179614\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 7.949422\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 7.857739\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 7.961267\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 8.728200\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 8.963272\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 9.592081\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 10.172080\n",
      "Epoch: 171/1000... Step: 5472... Loss: 11.358506... Val Loss: 10.278538\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 10.270194\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 8.491059\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 9.639372\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 8.888452\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 10.083059\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 10.017589\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 9.087647\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 8.458011\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 8.226669\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 8.239912\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 8.317577\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 9.081111\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 9.300159\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 9.993994\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 10.556871\n",
      "Epoch: 172/1000... Step: 5504... Loss: 19.889828... Val Loss: 10.537685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 11.081587\n",
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 9.039715\n",
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 9.782228\n",
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 9.565907\n",
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 10.678931\n",
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 10.621058\n",
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 9.582782\n",
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 8.891731\n",
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 8.633330\n",
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 8.623761\n",
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 8.805267\n",
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 9.499596\n",
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 9.699850\n",
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 10.422815\n",
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 11.068030\n",
      "Epoch: 173/1000... Step: 5536... Loss: 24.147573... Val Loss: 11.141425\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 9.595219\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 7.939552\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 9.213441\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 8.475544\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 9.440226\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 9.475631\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 8.581825\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 8.243542\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 7.994231\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 7.990889\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 8.003381\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 8.684588\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 8.963665\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 9.510001\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 10.046961\n",
      "Epoch: 174/1000... Step: 5568... Loss: 7.831238... Val Loss: 10.233006\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 11.388394\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 9.391724\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 10.096138\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 9.819646\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 10.961432\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 10.770225\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 9.728105\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 9.027838\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 8.747520\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 8.642044\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 8.834065\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 9.555751\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 9.757292\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 10.493719\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 11.147774\n",
      "Epoch: 175/1000... Step: 5600... Loss: 19.409973... Val Loss: 11.169424\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 10.167035\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 8.247329\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 9.634006\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 8.826863\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 9.928969\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 9.734855\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 8.779361\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 8.231461\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 8.000135\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 7.977106\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 7.999204\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 8.800127\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 9.038176\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 9.682555\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 10.291110\n",
      "Epoch: 176/1000... Step: 5632... Loss: 10.986094... Val Loss: 10.404450\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 10.501747\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 8.765953\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 9.675245\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 8.919574\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 10.025001\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 9.935355\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 8.991377\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 8.358871\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 8.110913\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 8.092411\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 8.235402\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 8.930997\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 9.181067\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 9.928723\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 10.454692\n",
      "Epoch: 177/1000... Step: 5664... Loss: 18.978348... Val Loss: 10.442996\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 10.210467\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 8.358174\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 9.755176\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 8.846838\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 10.071303\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 9.881215\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 8.940366\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 8.288169\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 8.072342\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 8.025845\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 8.069952\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 8.920740\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 9.135620\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 9.851913\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 10.436634\n",
      "Epoch: 178/1000... Step: 5696... Loss: 14.044751... Val Loss: 10.455386\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 9.997021\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 8.120017\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 9.312845\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 8.660219\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 9.664849\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 9.564224\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 8.652036\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 8.161297\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 7.931835\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 7.863871\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 7.977327\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 8.679343\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 8.917111\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 9.518415\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 10.083276\n",
      "Epoch: 179/1000... Step: 5728... Loss: 10.988646... Val Loss: 10.186795\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 10.651686\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 8.705754\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 9.908713\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 9.074939\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 10.280743\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 10.270844\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 9.295684\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 8.603283\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 8.360850\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 8.406606\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 8.431074\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 9.241053\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 9.444900\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 10.177140\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 10.771496\n",
      "Epoch: 180/1000... Step: 5760... Loss: 18.523390... Val Loss: 10.732981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 11.546029\n",
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 9.312161\n",
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 10.010649\n",
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 9.820204\n",
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 10.891832\n",
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 10.793693\n",
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 9.757749\n",
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 9.071677\n",
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 8.808471\n",
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 8.707803\n",
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 8.908292\n",
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 9.598930\n",
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 9.790481\n",
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 10.475895\n",
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 11.149901\n",
      "Epoch: 181/1000... Step: 5792... Loss: 18.718559... Val Loss: 11.171314\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 10.803009\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 8.703732\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 9.938508\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 9.128601\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 10.248943\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 10.074728\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 9.080254\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 8.437932\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 8.193708\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 8.225813\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 8.249564\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 9.060890\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 9.279696\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 9.993408\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 10.636678\n",
      "Epoch: 182/1000... Step: 5824... Loss: 14.784657... Val Loss: 10.663214\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 9.830691\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 8.080569\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 9.293613\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 8.625484\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 9.608694\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 9.456235\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 8.547355\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 8.119385\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 7.874764\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 7.780785\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 7.861498\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 8.578457\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 8.831916\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 9.418724\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 9.975860\n",
      "Epoch: 183/1000... Step: 5856... Loss: 8.702055... Val Loss: 10.121577\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 10.773592\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 9.053598\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 9.877821\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 9.136143\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 10.319179\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 10.217203\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 9.248619\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 8.591106\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 8.339467\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 8.359825\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 8.468003\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 9.187038\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 9.426240\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 10.203953\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 10.777517\n",
      "Epoch: 184/1000... Step: 5888... Loss: 19.292742... Val Loss: 10.725211\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 10.664983\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 8.887848\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 9.654191\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 9.088076\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 10.190809\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 10.029171\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 9.069642\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 8.435229\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 8.179225\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 8.100776\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 8.241731\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 8.915621\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 9.168415\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 9.904295\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 10.460796\n",
      "Epoch: 185/1000... Step: 5920... Loss: 15.266496... Val Loss: 10.471156\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 9.378660\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 7.849389\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 9.226340\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 8.332719\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 9.417870\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 9.214240\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 8.360709\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 7.949882\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 7.738797\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 7.627290\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 7.662630\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 8.444758\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 8.703295\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 9.303558\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 9.835252\n",
      "Epoch: 186/1000... Step: 5952... Loss: 7.843679... Val Loss: 9.955584\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 10.502935\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 8.878305\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 9.760195\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 8.899091\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 9.989936\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 9.821501\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 8.890453\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 8.269284\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 8.027430\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 7.974305\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 8.102644\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 8.786483\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 9.050026\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 9.823896\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 10.326513\n",
      "Epoch: 187/1000... Step: 5984... Loss: 16.094776... Val Loss: 10.294137\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 9.259322\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 7.794438\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 9.287332\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 8.271569\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 9.397681\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 9.188520\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 8.361338\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 7.953504\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 7.763989\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 7.659483\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 7.692412\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 8.504513\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 8.754879\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 9.363490\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 9.882975\n",
      "Epoch: 188/1000... Step: 6016... Loss: 8.035425... Val Loss: 9.987844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 10.080748\n",
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 8.439371\n",
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 9.437115\n",
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 8.532325\n",
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 9.578312\n",
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 9.421231\n",
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 8.547818\n",
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 7.989325\n",
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 7.775968\n",
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 7.734115\n",
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 7.858323\n",
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 8.532304\n",
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 8.801656\n",
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 9.517092\n",
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 10.001680\n",
      "Epoch: 189/1000... Step: 6048... Loss: 14.167773... Val Loss: 10.002722\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 9.625751\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 8.029378\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 9.222808\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 8.292189\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 9.510383\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 9.357288\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 8.507969\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 7.951660\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 7.760057\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 7.708389\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 7.782840\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 8.553429\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 8.806251\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 9.501634\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 10.032443\n",
      "Epoch: 190/1000... Step: 6080... Loss: 12.536174... Val Loss: 10.043596\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 9.716911\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 8.002874\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 9.150695\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 8.357818\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 9.408964\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 9.251992\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 8.390742\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 7.888864\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 7.686563\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 7.627689\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 7.719750\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 8.423340\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 8.678427\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 9.312396\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 9.840413\n",
      "Epoch: 191/1000... Step: 6112... Loss: 11.571418... Val Loss: 9.915505\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 9.833340\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 8.220008\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 9.165827\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 8.311065\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 9.443402\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 9.337269\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 8.486453\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 7.931928\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 7.733267\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 7.705733\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 7.833108\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 8.499425\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 8.775329\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 9.493866\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 9.984582\n",
      "Epoch: 192/1000... Step: 6144... Loss: 13.575274... Val Loss: 10.008648\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 9.538908\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 7.926196\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 9.231123\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 8.386688\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 9.485541\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 9.286797\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 8.423727\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 7.992272\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 7.771109\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 7.650803\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 7.694618\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 8.470404\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 8.710625\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 9.307669\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 9.864462\n",
      "Epoch: 193/1000... Step: 6176... Loss: 8.547120... Val Loss: 9.963393\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 10.287171\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 8.634379\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 9.498361\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 8.633053\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 9.713539\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 9.598705\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 8.704366\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 8.105856\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 7.884979\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 7.852162\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 7.999189\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 8.660208\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 8.927139\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 9.684053\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 10.170153\n",
      "Epoch: 194/1000... Step: 6208... Loss: 15.300204... Val Loss: 10.165739\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 9.324349\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 7.858284\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 9.264384\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 8.268173\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 9.349349\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 9.144725\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 8.311474\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 7.943337\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 7.730635\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 7.614538\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 7.644955\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 8.429127\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 8.682734\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 9.267762\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 9.799935\n",
      "Epoch: 195/1000... Step: 6240... Loss: 7.483335... Val Loss: 9.922754\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 10.231220\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 8.537318\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 9.459958\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 8.546663\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 9.568983\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 9.430619\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 8.558077\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 7.992083\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 7.777529\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 7.730879\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 7.858212\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 8.523173\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 8.789269\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 9.514931\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 10.000962\n",
      "Epoch: 196/1000... Step: 6272... Loss: 13.420497... Val Loss: 10.000320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 9.363899\n",
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 7.814742\n",
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 9.158489\n",
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 8.203954\n",
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 9.336327\n",
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 9.149411\n",
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 8.316659\n",
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 7.866809\n",
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 7.672008\n",
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 7.573332\n",
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 7.623824\n",
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 8.405318\n",
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 8.648896\n",
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 9.269378\n",
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 9.802487\n",
      "Epoch: 197/1000... Step: 6304... Loss: 8.628401... Val Loss: 9.890215\n",
      "Validation loss decreased (9.915379 --> 9.890215).  Saving model ...\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 9.596501\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 7.950080\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 9.007772\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 8.054418\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 9.051727\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 8.955390\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 8.154834\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 7.669296\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 7.499244\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 7.494661\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 7.627103\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 8.260917\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 8.539980\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 9.210626\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 9.653382\n",
      "Epoch: 198/1000... Step: 6336... Loss: 12.154868... Val Loss: 9.712688\n",
      "Validation loss decreased (9.890215 --> 9.712688).  Saving model ...\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 9.743508\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 8.102630\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 9.138764\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 8.255646\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 9.441925\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 9.336165\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 8.483471\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 7.940772\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 7.738253\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 7.698552\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 7.785809\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 8.506306\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 8.765796\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 9.452345\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 9.990972\n",
      "Epoch: 199/1000... Step: 6368... Loss: 12.982438... Val Loss: 10.005755\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 9.959580\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 8.144511\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 9.144189\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 8.462037\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 9.471503\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 9.314161\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 8.462834\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 7.995286\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 7.792213\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 7.722343\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 7.822110\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 8.480092\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 8.725939\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 9.337874\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 9.884529\n",
      "Epoch: 200/1000... Step: 6400... Loss: 10.260665... Val Loss: 9.950350\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 10.327772\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 8.662680\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 9.427474\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 8.648738\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 9.754622\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 9.643564\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 8.739640\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 8.145231\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 7.921647\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 7.887624\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 8.045104\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 8.681810\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 8.951748\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 9.703851\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 10.213911\n",
      "Epoch: 201/1000... Step: 6432... Loss: 14.948067... Val Loss: 10.234716\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 9.492052\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 7.922816\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 9.192666\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 8.358788\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 9.413703\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 9.205082\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 8.361754\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 8.024908\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 7.801674\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 7.676796\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 7.710737\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 8.461693\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 8.702547\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 9.257541\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 9.827254\n",
      "Epoch: 202/1000... Step: 6464... Loss: 7.434499... Val Loss: 9.952458\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 10.457436\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 8.773340\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 9.557721\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 8.679192\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 9.753030\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 9.627046\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 8.729568\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 8.138268\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 7.913528\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 7.875497\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 8.023704\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 8.681361\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 8.942678\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 9.696975\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 10.199967\n",
      "Epoch: 203/1000... Step: 6496... Loss: 15.124533... Val Loss: 10.186620\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 9.958035\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 8.281095\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 9.307293\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 8.461130\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 9.620945\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 9.513842\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 8.636678\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 8.072742\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 7.853675\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 7.790867\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 7.878452\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 8.595017\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 8.843096\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 9.534668\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 10.081694\n",
      "Epoch: 204/1000... Step: 6528... Loss: 11.885774... Val Loss: 10.094014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 9.675793\n",
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 7.867896\n",
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 8.948448\n",
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 8.178038\n",
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 9.079021\n",
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 8.928631\n",
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 8.134938\n",
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 7.795432\n",
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 7.626841\n",
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 7.610017\n",
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 7.701383\n",
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 8.289426\n",
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 8.559241\n",
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 9.129633\n",
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 9.627791\n",
      "Epoch: 205/1000... Step: 6560... Loss: 7.769568... Val Loss: 9.752194\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 11.011438\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 9.355815\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 10.016510\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 9.290639\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 10.494627\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 10.407061\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 9.403563\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 8.725792\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 8.460405\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 8.400460\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 8.540211\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 9.231575\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 9.470309\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 10.285189\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 10.869077\n",
      "Epoch: 206/1000... Step: 6592... Loss: 19.679848... Val Loss: 10.851034\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 9.883770\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 8.086562\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 9.113817\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 8.484271\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 9.437297\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 9.251532\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 8.418647\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 8.134813\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 7.921618\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 7.815749\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 7.872218\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 8.527406\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 8.765728\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 9.296679\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 9.883973\n",
      "Epoch: 207/1000... Step: 6624... Loss: 7.410593... Val Loss: 10.012139\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 10.964994\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 9.210831\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 9.980083\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 9.277041\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 10.518469\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 10.397365\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 9.393913\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 8.709060\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 8.448108\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 8.367518\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 8.506436\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 9.244370\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 9.461644\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 10.246528\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 10.865203\n",
      "Epoch: 208/1000... Step: 6656... Loss: 18.512548... Val Loss: 10.850186\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 10.047371\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 8.182847\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 9.527301\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 8.674803\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 9.759271\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 9.614493\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 8.665821\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 8.139532\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 7.897609\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 7.817101\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 7.869372\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 8.656490\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 8.886426\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 9.525614\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 10.117199\n",
      "Epoch: 209/1000... Step: 6688... Loss: 8.376050... Val Loss: 10.253537\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 9.438571\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 7.698925\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 8.923951\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 7.968775\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 8.790518\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 8.701841\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 7.932026\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 7.638130\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 7.477963\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 7.481722\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 7.576496\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 8.162048\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 8.451591\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 9.029840\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 9.470977\n",
      "Epoch: 210/1000... Step: 6720... Loss: 6.540064... Val Loss: 9.645585\n",
      "Validation loss decreased (9.712688 --> 9.645585).  Saving model ...\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 11.196825\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 9.326408\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 10.120723\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 9.348507\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 10.531775\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 10.403757\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 9.393756\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 8.704568\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 8.444750\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 8.371323\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 8.489287\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 9.224997\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 9.446398\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 10.250641\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 10.861199\n",
      "Epoch: 211/1000... Step: 6752... Loss: 18.120140... Val Loss: 10.826913\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 10.268836\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 8.187021\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 9.214376\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 8.669437\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 9.535036\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 9.451269\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 8.528299\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 8.096617\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 7.890709\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 7.866460\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 7.980157\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 8.615412\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 8.868098\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 9.449958\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 10.031664\n",
      "Epoch: 212/1000... Step: 6784... Loss: 7.750009... Val Loss: 10.227243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 10.526687\n",
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 8.468542\n",
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 9.844351\n",
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 8.917824\n",
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 10.051901\n",
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 9.884838\n",
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 8.892264\n",
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 8.279832\n",
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 8.031894\n",
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 7.968202\n",
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 8.024631\n",
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 8.861566\n",
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 9.074384\n",
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 9.757101\n",
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 10.392365\n",
      "Epoch: 213/1000... Step: 6816... Loss: 9.399905... Val Loss: 10.503342\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 10.210209\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 8.122316\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 9.286438\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 8.559970\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 9.298175\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 9.258784\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 8.353781\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 7.926173\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 7.733568\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 7.777799\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 7.888610\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 8.473364\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 8.752528\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 9.362393\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 9.884089\n",
      "Epoch: 214/1000... Step: 6848... Loss: 7.845044... Val Loss: 10.102556\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 11.032056\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 8.757473\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 9.700511\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 9.292684\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 10.297787\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 10.213504\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 9.197641\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 8.599902\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 8.381100\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 8.296792\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 8.420140\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 9.134780\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 9.331803\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 9.968643\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 10.652200\n",
      "Epoch: 215/1000... Step: 6880... Loss: 10.261275... Val Loss: 10.783186\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 10.892552\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 8.723268\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 10.192458\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 9.223450\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 10.320417\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 10.120502\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 9.060244\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 8.438132\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 8.155922\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 8.093730\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 8.136087\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 8.998536\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 9.231207\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 9.927843\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 10.597452\n",
      "Epoch: 216/1000... Step: 6912... Loss: 9.721453... Val Loss: 10.776418\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 10.680058\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 8.552038\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 9.783717\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 8.862496\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 9.735989\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 9.557405\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 8.667185\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 8.180582\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 7.959317\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 7.934774\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 7.987485\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 8.660144\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 8.883156\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 9.525328\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 10.101509\n",
      "Epoch: 217/1000... Step: 6944... Loss: 11.464381... Val Loss: 10.109937\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 9.210403\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 7.627980\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 9.115244\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 7.992506\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 9.149513\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 9.190166\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 8.337647\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 7.868288\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 7.674425\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 7.716741\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 7.750718\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 8.504290\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 8.760154\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 9.390000\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 9.893590\n",
      "Epoch: 218/1000... Step: 6976... Loss: 8.272161... Val Loss: 10.009743\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 11.471604\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 9.514424\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 10.093740\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 9.471507\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 10.474532\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 10.345251\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 9.367063\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 8.711446\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 8.468738\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 8.368834\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 8.540706\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 9.197999\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 9.416707\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 10.158485\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 10.766014\n",
      "Epoch: 219/1000... Step: 7008... Loss: 12.874620... Val Loss: 10.746844\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 10.306551\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 8.448197\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 9.698091\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 8.794989\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 9.939890\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 9.722315\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 8.766013\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 8.253308\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 8.004465\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 7.887120\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 7.906852\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 8.734313\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 8.946176\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 9.601804\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 10.236168\n",
      "Epoch: 220/1000... Step: 7040... Loss: 8.189122... Val Loss: 10.326355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 9.624026\n",
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 7.882764\n",
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 8.949373\n",
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 8.001848\n",
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 8.811638\n",
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 8.771068\n",
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 7.981590\n",
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 7.675918\n",
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 7.513478\n",
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 7.514441\n",
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 7.623753\n",
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 8.200147\n",
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 8.483238\n",
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 9.056746\n",
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 9.526187\n",
      "Epoch: 221/1000... Step: 7072... Loss: 6.951799... Val Loss: 9.678104\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 11.102877\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 9.134063\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 10.046191\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 9.043977\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 10.263676\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 10.296106\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 9.301812\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 8.615103\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 8.362775\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 8.394545\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 8.437297\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 9.201669\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 9.422015\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 10.212075\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 10.825177\n",
      "Epoch: 222/1000... Step: 7104... Loss: 17.038092... Val Loss: 10.751644\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 9.546943\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 8.198390\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 9.178651\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 8.186894\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 9.134753\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 8.965112\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 8.177741\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 7.832628\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 7.620777\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 7.539111\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 7.666449\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 8.271652\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 8.529010\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 9.134022\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 9.599916\n",
      "Epoch: 223/1000... Step: 7136... Loss: 8.522872... Val Loss: 9.633165\n",
      "Validation loss decreased (9.645585 --> 9.633165).  Saving model ...\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 10.670167\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 8.789286\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 9.770966\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 8.821085\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 10.012176\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 10.031621\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 9.077188\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 8.420005\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 8.181606\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 8.196589\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 8.255862\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 8.999855\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 9.225117\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 9.983579\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 10.568918\n",
      "Epoch: 224/1000... Step: 7168... Loss: 14.542538... Val Loss: 10.516645\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 9.447411\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 8.102202\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 9.139459\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 8.225410\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 9.180427\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 8.994165\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 8.206406\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 7.902507\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 7.689310\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 7.596527\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 7.686101\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 8.315976\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 8.558216\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 9.114769\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 9.613164\n",
      "Epoch: 225/1000... Step: 7200... Loss: 8.270494... Val Loss: 9.664661\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 11.189568\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 9.229806\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 9.995649\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 9.174896\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 10.303855\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 10.245635\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 9.262475\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 8.594991\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 8.356198\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 8.313873\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 8.408658\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 9.120024\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 9.348688\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 10.130016\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 10.746915\n",
      "Epoch: 226/1000... Step: 7232... Loss: 13.821984... Val Loss: 10.703523\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 9.691760\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 7.908421\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 8.869081\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 8.212063\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 9.051071\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 8.879405\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 8.081808\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 7.883081\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 7.698587\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 7.654478\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 7.757853\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 8.324246\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 8.598713\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 9.117112\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 9.653722\n",
      "Epoch: 227/1000... Step: 7264... Loss: 6.185310... Val Loss: 9.845100\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 10.733321\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 8.883426\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 9.603562\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 8.824585\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 9.915092\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 9.774594\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 8.852332\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 8.259092\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 8.048178\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 7.980529\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 8.111455\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 8.768223\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 9.015918\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 9.744175\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 10.332980\n",
      "Epoch: 228/1000... Step: 7296... Loss: 11.244799... Val Loss: 10.327380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 9.635393\n",
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 7.973262\n",
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 8.901539\n",
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 8.218047\n",
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 9.141678\n",
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 8.953331\n",
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 8.158511\n",
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 7.934840\n",
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 7.743231\n",
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 7.668741\n",
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 7.743935\n",
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 8.350192\n",
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 8.607673\n",
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 9.121002\n",
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 9.684081\n",
      "Epoch: 229/1000... Step: 7328... Loss: 6.673092... Val Loss: 9.834222\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 10.300646\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 8.590480\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 9.290899\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 8.463898\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 9.516948\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 9.367540\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 8.505756\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 7.999594\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 7.799626\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 7.725617\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 7.873195\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 8.486883\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 8.746202\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 9.426938\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 9.978312\n",
      "Epoch: 230/1000... Step: 7360... Loss: 9.544519... Val Loss: 10.004853\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 9.714313\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 8.178917\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 9.081264\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 8.312613\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 9.377001\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 9.201595\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 8.373573\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 7.974507\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 7.771883\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 7.666442\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 7.758280\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 8.408912\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 8.649824\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 9.231743\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 9.803939\n",
      "Epoch: 231/1000... Step: 7392... Loss: 8.204773... Val Loss: 9.878359\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 9.440518\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 7.792496\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 8.751290\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 7.800690\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 8.792226\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 8.658417\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 7.891105\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 7.512747\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 7.359826\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 7.379463\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 7.531631\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 8.073058\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 8.368327\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 9.009811\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 9.467798\n",
      "Epoch: 232/1000... Step: 7424... Loss: 8.108550... Val Loss: 9.566980\n",
      "Validation loss decreased (9.633165 --> 9.566980).  Saving model ...\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 10.050387\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 8.283827\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 9.231774\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 8.475603\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 9.596019\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 9.415484\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 8.523936\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 8.060733\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 7.853877\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 7.743700\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 7.809718\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 8.530590\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 8.757092\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 9.364631\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 10.003027\n",
      "Epoch: 233/1000... Step: 7456... Loss: 8.267855... Val Loss: 10.070676\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 9.310987\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 7.643075\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 8.666469\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 7.793825\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 8.702279\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 8.593514\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 7.818044\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 7.528215\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 7.376525\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 7.384870\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 7.507613\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 8.047903\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 8.359150\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 8.941758\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 9.414753\n",
      "Epoch: 234/1000... Step: 7488... Loss: 5.568922... Val Loss: 9.637330\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 10.038514\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 8.524908\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 9.235771\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 8.409553\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 9.500720\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 9.319439\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 8.468879\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 7.998158\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 7.798804\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 7.703637\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 7.851285\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 8.456898\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 8.712935\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 9.359230\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 9.922580\n",
      "Epoch: 235/1000... Step: 7520... Loss: 9.196288... Val Loss: 9.966139\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 9.921474\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 8.251234\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 9.094434\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 8.270895\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 9.368948\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 9.203527\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 8.368332\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 7.945072\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 7.747795\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 7.658644\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 7.741737\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 8.407968\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 8.650599\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 9.263390\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 9.858852\n",
      "Epoch: 236/1000... Step: 7552... Loss: 8.193230... Val Loss: 9.899144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 9.662741\n",
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 8.037793\n",
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 8.816699\n",
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 8.091676\n",
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 9.047367\n",
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 8.888375\n",
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 8.120239\n",
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 7.848522\n",
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 7.676285\n",
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 7.620535\n",
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 7.737484\n",
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 8.294283\n",
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 8.549574\n",
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 9.074002\n",
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 9.624916\n",
      "Epoch: 237/1000... Step: 7584... Loss: 6.674721... Val Loss: 9.751441\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 11.354996\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 9.520912\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 9.851365\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 9.294651\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 10.432157\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 10.319736\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 9.352703\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 8.708882\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 8.485716\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 8.404037\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 8.614843\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 9.219230\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 9.446736\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 10.202551\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 10.819436\n",
      "Epoch: 238/1000... Step: 7616... Loss: 14.258085... Val Loss: 10.826856\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 10.334267\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 8.614136\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 9.319908\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 8.713310\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 9.759832\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 9.701521\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 8.831281\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 8.385890\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 8.176855\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 8.085350\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 8.200486\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 8.811888\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 9.027601\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 9.604018\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 10.224150\n",
      "Epoch: 239/1000... Step: 7648... Loss: 8.940389... Val Loss: 10.273963\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 11.010583\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 9.110900\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 9.758949\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 9.002904\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 10.006055\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 9.884336\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 8.945763\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 8.337158\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 8.126058\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 8.083396\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 8.258123\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 8.819322\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 9.065150\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 9.818838\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 10.371499\n",
      "Epoch: 240/1000... Step: 7680... Loss: 11.312983... Val Loss: 10.374163\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 9.973438\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 8.192312\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 9.007944\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 8.490735\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 9.459050\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 9.295344\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 8.447725\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 8.110885\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 7.941664\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 7.870321\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 7.984164\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 8.548135\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 8.791291\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 9.320550\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 9.915934\n",
      "Epoch: 241/1000... Step: 7712... Loss: 6.844325... Val Loss: 10.085407\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 9.926031\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 8.115550\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 9.084353\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 8.318462\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 9.303568\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 9.126641\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 8.274674\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 7.880927\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 7.704308\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 7.627639\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 7.722354\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 8.323972\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 8.578168\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 9.177052\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 9.745206\n",
      "Epoch: 242/1000... Step: 7744... Loss: 6.895467... Val Loss: 9.860499\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 9.552882\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 8.010532\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 8.908621\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 8.115532\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 9.031873\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 8.845182\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 8.090873\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 7.838570\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 7.673053\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 7.620630\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 7.722957\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 8.255409\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 8.515317\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 9.015989\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 9.528002\n",
      "Epoch: 243/1000... Step: 7776... Loss: 6.337357... Val Loss: 9.666158\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 9.968155\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 8.276601\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 9.086020\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 8.208457\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 9.244258\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 9.085605\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 8.251059\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 7.791528\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 7.625354\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 7.581570\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 7.756015\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 8.295181\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 8.569255\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 9.222479\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 9.741270\n",
      "Epoch: 244/1000... Step: 7808... Loss: 8.056383... Val Loss: 9.825749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 10.471430\n",
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 8.479626\n",
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 9.424260\n",
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 8.752426\n",
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 9.865145\n",
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 9.708252\n",
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 8.780682\n",
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 8.262681\n",
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 8.056370\n",
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 7.955708\n",
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 8.021790\n",
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 8.725617\n",
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 8.930684\n",
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 9.537378\n",
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 10.204505\n",
      "Epoch: 245/1000... Step: 7840... Loss: 8.369362... Val Loss: 10.259281\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 10.266726\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 8.081521\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 8.858799\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 8.336602\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 9.194826\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 9.182264\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 8.300766\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 7.880002\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 7.741481\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 7.822181\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 7.974374\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 8.470412\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 8.747218\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 9.368529\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 9.938624\n",
      "Epoch: 246/1000... Step: 7872... Loss: 6.768572... Val Loss: 10.160789\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 10.611240\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 8.524726\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 9.264247\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 9.156262\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 10.150748\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 9.997662\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 9.079632\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 8.612641\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 8.462543\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 8.322690\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 8.487864\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 9.060546\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 9.278129\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 9.775674\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 10.464513\n",
      "Epoch: 247/1000... Step: 7904... Loss: 7.384823... Val Loss: 10.702611\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 10.601493\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 8.503055\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 9.679086\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 8.762155\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 9.933803\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 9.954367\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 8.948112\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 8.389914\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 8.152931\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 8.148281\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 8.153680\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 8.927686\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 9.133200\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 9.780936\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 10.472466\n",
      "Epoch: 248/1000... Step: 7936... Loss: 8.045818... Val Loss: 10.547660\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 10.273416\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 8.302742\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 9.131962\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 8.500465\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 9.299577\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 9.148314\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 8.308203\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 7.983634\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 7.810210\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 7.742819\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 7.852572\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 8.408473\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 8.649354\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 9.163772\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 9.743118\n",
      "Epoch: 249/1000... Step: 7968... Loss: 6.275381... Val Loss: 9.895495\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 10.502731\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 8.519496\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 9.294342\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 8.683007\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 9.729849\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 9.610401\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 8.685929\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 8.183084\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 7.999091\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 7.892878\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 8.024643\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 8.655757\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 8.881878\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 9.516436\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 10.173235\n",
      "Epoch: 250/1000... Step: 8000... Loss: 7.367979... Val Loss: 10.275543\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 10.778756\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 8.637852\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 9.380682\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 8.849497\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 9.796830\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 9.655101\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 8.756199\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 8.291481\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 8.097151\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 8.005737\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 8.101726\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 8.719093\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 8.923978\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 9.492907\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 10.149185\n",
      "Epoch: 251/1000... Step: 8032... Loss: 7.924960... Val Loss: 10.214237\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 10.115008\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 8.034378\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 8.653016\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 8.173600\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 9.076503\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 8.995680\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 8.177711\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 7.890844\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 7.746439\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 7.757475\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 7.935228\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 8.419715\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 8.695723\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 9.257813\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 9.842074\n",
      "Epoch: 252/1000... Step: 8064... Loss: 6.011241... Val Loss: 10.061583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 10.372189\n",
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 8.399569\n",
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 9.176562\n",
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 8.944825\n",
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 9.975031\n",
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 9.838521\n",
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 8.910629\n",
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 8.460449\n",
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 8.301441\n",
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 8.165509\n",
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 8.327846\n",
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 8.902833\n",
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 9.134032\n",
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 9.661520\n",
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 10.345363\n",
      "Epoch: 253/1000... Step: 8096... Loss: 6.967514... Val Loss: 10.582166\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 10.184188\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 8.461612\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 9.570919\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 8.584490\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 9.763257\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 9.742278\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 8.784437\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 8.375908\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 8.112255\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 8.071957\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 8.031323\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 8.807758\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 9.015427\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 9.582388\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 10.269356\n",
      "Epoch: 254/1000... Step: 8128... Loss: 6.880881... Val Loss: 10.339139\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 10.937907\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 8.967206\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 9.650295\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 9.136404\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 9.991611\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 9.810321\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 8.889258\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 8.417136\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 8.240897\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 8.090874\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 8.230368\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 8.813359\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 9.022330\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 9.562437\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 10.216502\n",
      "Epoch: 255/1000... Step: 8160... Loss: 6.740532... Val Loss: 10.344492\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 10.718325\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 8.581703\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 9.361842\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 8.754661\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 9.796510\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 9.741540\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 8.782626\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 8.199486\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 8.023083\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 7.958920\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 8.099912\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 8.720793\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 8.952858\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 9.642347\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 10.293698\n",
      "Epoch: 256/1000... Step: 8192... Loss: 7.937809... Val Loss: 10.401441\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 10.497964\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 8.324571\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 9.005916\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 8.602426\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 9.507075\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 9.435066\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 8.563068\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 8.191292\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 8.041805\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 8.002709\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 8.148656\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 8.664218\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 8.904678\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 9.442219\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 10.089574\n",
      "Epoch: 257/1000... Step: 8224... Loss: 7.450680... Val Loss: 10.250972\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 10.491621\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 8.406436\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 9.001269\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 8.825313\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 9.860020\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 9.855549\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 8.930629\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 8.486333\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 8.337661\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 8.255230\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 8.442950\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 8.962396\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 9.213323\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 9.762769\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 10.451955\n",
      "Epoch: 258/1000... Step: 8256... Loss: 6.533033... Val Loss: 10.722668\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 10.974441\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 9.145318\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 10.171148\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 9.271122\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 10.522223\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 10.558731\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 9.494333\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 8.951200\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 8.650521\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 8.581582\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 8.531908\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 9.343804\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 9.493235\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 10.060627\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 10.835623\n",
      "Epoch: 259/1000... Step: 8288... Loss: 8.368443... Val Loss: 10.853342\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 11.760752\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 9.426569\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 10.158083\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 9.679360\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 10.623440\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 10.563325\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 9.541608\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 8.944207\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 8.726423\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 8.597900\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 8.727991\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 9.353579\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 9.524588\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 10.114265\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 10.835164\n",
      "Epoch: 260/1000... Step: 8320... Loss: 8.066831... Val Loss: 10.906260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 11.242660\n",
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 8.993374\n",
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 9.703902\n",
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 9.077803\n",
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 10.133758\n",
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 10.194949\n",
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 9.185402\n",
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 8.549988\n",
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 8.353960\n",
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 8.308134\n",
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 8.421141\n",
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 9.048452\n",
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 9.261179\n",
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 9.972770\n",
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 10.656248\n",
      "Epoch: 261/1000... Step: 8352... Loss: 8.462616... Val Loss: 10.732010\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 11.064944\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 8.986034\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 9.390131\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 9.092812\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 10.057699\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 9.949608\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 9.081276\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 8.679180\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 8.507724\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 8.399124\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 8.574299\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 9.080944\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 9.277415\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 9.793256\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 10.507024\n",
      "Epoch: 262/1000... Step: 8384... Loss: 8.706071... Val Loss: 10.566793\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 10.602300\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 8.555912\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 8.955040\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 8.816286\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 9.884662\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 9.870995\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 8.989559\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 8.570455\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 8.432277\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 8.362676\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 8.579180\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 9.068014\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 9.295895\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 9.823407\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 10.527926\n",
      "Epoch: 263/1000... Step: 8416... Loss: 7.147907... Val Loss: 10.737324\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 11.013145\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 9.342726\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 10.117417\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 9.179062\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 10.507781\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 10.479301\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 9.439864\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 8.843443\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 8.560750\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 8.438800\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 8.458700\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 9.224032\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 9.389887\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 10.009395\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 10.767140\n",
      "Epoch: 264/1000... Step: 8448... Loss: 11.619510... Val Loss: 10.701979\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 11.231684\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 9.166349\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 10.270541\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 9.233829\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 10.485452\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 10.299535\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 9.238987\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 8.653824\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 8.367048\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 8.239178\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 8.250798\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 9.046631\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 9.236606\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 9.901949\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 10.624119\n",
      "Epoch: 265/1000... Step: 8480... Loss: 8.247396... Val Loss: 10.682663\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 11.235158\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 8.897620\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 9.427045\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 9.131331\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 10.103142\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 10.249072\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 9.281281\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 8.801712\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 8.619259\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 8.550205\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 8.676583\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 9.203592\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 9.402069\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 9.907699\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 10.617126\n",
      "Epoch: 266/1000... Step: 8512... Loss: 6.912637... Val Loss: 10.758425\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 10.579412\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 8.884244\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 9.430198\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 8.942701\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 10.144489\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 10.161640\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 9.207996\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 8.681513\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 8.479235\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 8.304097\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 8.442821\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 9.048175\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 9.234453\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 9.804628\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 10.535968\n",
      "Epoch: 267/1000... Step: 8544... Loss: 7.430839... Val Loss: 10.644529\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 11.559833\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 9.632272\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 9.980132\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 9.445611\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 10.470392\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 10.392245\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 9.453752\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 9.012651\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 8.776723\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 8.599089\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 8.706682\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 9.287868\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 9.446668\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 9.990772\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 10.740150\n",
      "Epoch: 268/1000... Step: 8576... Loss: 7.851337... Val Loss: 10.714632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 11.030662\n",
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 9.362413\n",
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 9.522553\n",
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 9.140024\n",
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 10.230750\n",
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 10.135989\n",
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 9.245661\n",
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 8.747021\n",
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 8.535011\n",
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 8.378985\n",
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 8.560824\n",
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 9.121015\n",
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 9.291750\n",
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 9.828819\n",
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 10.526948\n",
      "Epoch: 269/1000... Step: 8608... Loss: 8.202366... Val Loss: 10.552389\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 10.949587\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 9.244143\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 9.324164\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 9.026313\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 10.162879\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 10.156533\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 9.276450\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 8.850741\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 8.648420\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 8.565568\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 8.816846\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 9.290510\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 9.486804\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 10.076255\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 10.774347\n",
      "Epoch: 270/1000... Step: 8640... Loss: 8.453897... Val Loss: 10.890568\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 11.377934\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 9.821609\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 9.814693\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 9.430772\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 10.543039\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 10.584608\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 9.685982\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 9.193732\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 8.971519\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 8.811968\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 8.994920\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 9.513223\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 9.673163\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 10.203356\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 10.951974\n",
      "Epoch: 271/1000... Step: 8672... Loss: 8.312953... Val Loss: 10.940583\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 10.877611\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 9.514416\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 9.451049\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 9.209976\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 10.523235\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 10.486662\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 9.579813\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 9.095409\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 8.873844\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 8.747486\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 9.002384\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 9.516846\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 9.705993\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 10.292763\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 11.029107\n",
      "Epoch: 272/1000... Step: 8704... Loss: 9.240463... Val Loss: 11.199082\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 10.693816\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 9.676276\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 9.689304\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 9.273246\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 10.420749\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 10.358377\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 9.456580\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 8.951273\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 8.741515\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 8.540819\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 8.743251\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 9.272323\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 9.436478\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 9.961860\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 10.678385\n",
      "Epoch: 273/1000... Step: 8736... Loss: 8.358658... Val Loss: 10.761813\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 11.867724\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 10.139856\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 10.092812\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 9.668676\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 10.732653\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 10.707590\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 9.761135\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 9.222211\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 8.983023\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 8.816002\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 8.992876\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 9.533267\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 9.685854\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 10.265695\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 10.993589\n",
      "Epoch: 274/1000... Step: 8768... Loss: 8.541567... Val Loss: 10.942597\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 11.045417\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 9.619357\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 9.484855\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 9.366202\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 10.640930\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 10.635782\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 9.728740\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 9.220037\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 9.005754\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 8.879311\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 9.148195\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 9.644082\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 9.818128\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 10.385171\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 11.126759\n",
      "Epoch: 275/1000... Step: 8800... Loss: 8.873401... Val Loss: 11.301866\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 10.920182\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 9.846522\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 9.758881\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 9.397813\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 10.503331\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 10.423751\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 9.528797\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 9.057119\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 8.852432\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 8.662253\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 8.876806\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 9.366297\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 9.531701\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 10.051633\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 10.775573\n",
      "Epoch: 276/1000... Step: 8832... Loss: 7.756573... Val Loss: 10.858112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 11.788600\n",
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 10.111888\n",
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 10.039187\n",
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 9.633628\n",
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 10.695541\n",
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 10.635055\n",
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 9.706011\n",
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 9.208446\n",
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 8.979302\n",
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 8.800488\n",
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 8.959016\n",
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 9.487454\n",
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 9.639951\n",
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 10.192491\n",
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 10.938553\n",
      "Epoch: 277/1000... Step: 8864... Loss: 7.890692... Val Loss: 10.901550\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 11.105879\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 9.581124\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 9.436977\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 9.265482\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 10.441876\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 10.405600\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 9.529715\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 9.087342\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 8.885101\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 8.761397\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 8.997334\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 9.466270\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 9.642497\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 10.173674\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 10.907614\n",
      "Epoch: 278/1000... Step: 8896... Loss: 7.620289... Val Loss: 11.018035\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 11.203642\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 9.561676\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 9.536112\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 9.354253\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 10.467238\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 10.414141\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 9.516183\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 9.042696\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 8.860731\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 8.708227\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 8.916132\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 9.397882\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 9.573392\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 10.102010\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 10.845258\n",
      "Epoch: 279/1000... Step: 8928... Loss: 7.088500... Val Loss: 10.933490\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 11.283748\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 9.665188\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 9.609756\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 9.235668\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 10.332128\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 10.331822\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 9.436457\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 9.003084\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 8.803763\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 8.658641\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 8.827608\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 9.319929\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 9.499302\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 10.040769\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 10.788247\n",
      "Epoch: 280/1000... Step: 8960... Loss: 7.048485... Val Loss: 10.810212\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 11.414116\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 9.383252\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 9.368937\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 9.227195\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 10.233506\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 10.229928\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 9.352184\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 8.936883\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 8.770869\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 8.668461\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 8.859497\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 9.305445\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 9.496046\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 10.004650\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 10.739498\n",
      "Epoch: 281/1000... Step: 8992... Loss: 6.467466... Val Loss: 10.824793\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 10.938637\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 9.442815\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 9.328597\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 9.344456\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 10.459087\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 10.333432\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 9.479355\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 9.079365\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 8.933313\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 8.792555\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 9.041390\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 9.475344\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 9.680744\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 10.138122\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 10.893990\n",
      "Epoch: 282/1000... Step: 9024... Loss: 6.360370... Val Loss: 11.088263\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 10.474895\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 10.536651\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 10.493902\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 9.354930\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 10.665293\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 10.480063\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 9.442631\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 8.909863\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 8.581842\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 8.408650\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 8.384057\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 9.105648\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 9.264596\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 9.874313\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 10.606432\n",
      "Epoch: 283/1000... Step: 9056... Loss: 10.003081... Val Loss: 10.444948\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 11.161192\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 9.402405\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 9.406605\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 9.153323\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 10.305151\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 10.218839\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 9.309290\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 8.860337\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 8.677882\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 8.505902\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 8.676956\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 9.188794\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 9.361678\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 9.874655\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 10.618305\n",
      "Epoch: 284/1000... Step: 9088... Loss: 6.959687... Val Loss: 10.683259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 11.221755\n",
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 10.356756\n",
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 9.930270\n",
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 9.487281\n",
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 10.666828\n",
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 10.739998\n",
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 9.832507\n",
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 9.389651\n",
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 9.155051\n",
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 8.988074\n",
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 9.177464\n",
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 9.643589\n",
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 9.795196\n",
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 10.313717\n",
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 11.072331\n",
      "Epoch: 285/1000... Step: 9120... Loss: 7.725259... Val Loss: 11.092320\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 11.598900\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 10.141721\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 9.794751\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 9.553558\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 10.712500\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 10.652213\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 9.749442\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 9.274927\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 9.047515\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 8.907981\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 9.149025\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 9.600957\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 9.766109\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 10.338966\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 11.063162\n",
      "Epoch: 286/1000... Step: 9152... Loss: 8.651266... Val Loss: 11.092075\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 11.072266\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 10.709777\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 10.099839\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 9.647736\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 10.746374\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 10.695837\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 9.815375\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 9.437371\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 9.212048\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 9.014447\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 9.206086\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 9.635932\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 9.790439\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 10.249752\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 11.016697\n",
      "Epoch: 287/1000... Step: 9184... Loss: 6.622803... Val Loss: 11.047631\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 11.855608\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 10.374466\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 10.130860\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 9.655057\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 10.798403\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 10.694853\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 9.744114\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 9.170046\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 8.924975\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 8.756688\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 8.937648\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 9.450278\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 9.619160\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 10.256960\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 10.958217\n",
      "Epoch: 288/1000... Step: 9216... Loss: 9.022012... Val Loss: 10.895256\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 10.770807\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 9.814463\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 9.437986\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 9.152654\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 10.347893\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 10.220965\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 9.372038\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 9.001726\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 8.814488\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 8.675092\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 8.910299\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 9.326469\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 9.524897\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 10.011527\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 10.743106\n",
      "Epoch: 289/1000... Step: 9248... Loss: 6.167240... Val Loss: 10.883236\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 11.249660\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 9.732795\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 9.685078\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 9.142962\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 10.289852\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 10.215137\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 9.289939\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 8.809265\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 8.609803\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 8.439246\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 8.568323\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 9.094839\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 9.277900\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 9.850309\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 10.593679\n",
      "Epoch: 290/1000... Step: 9280... Loss: 6.869771... Val Loss: 10.581739\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 11.383283\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 9.445108\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 9.263747\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 9.098392\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 10.192980\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 10.223718\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 9.351587\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 8.940841\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 8.773448\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 8.679664\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 8.873976\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 9.301039\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 9.493142\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 10.010856\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 10.750444\n",
      "Epoch: 291/1000... Step: 9312... Loss: 6.305733... Val Loss: 10.820949\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 11.166458\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 9.954313\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 9.554658\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 9.520992\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 10.669800\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 10.489901\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 9.641011\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 9.239458\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 9.076456\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 8.896404\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 9.130575\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 9.563485\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 9.748447\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 10.195417\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 10.974282\n",
      "Epoch: 292/1000... Step: 9344... Loss: 6.712088... Val Loss: 11.090281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 10.724685\n",
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 10.921286\n",
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 10.398711\n",
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 9.522373\n",
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 10.816209\n",
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 10.724153\n",
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 9.659898\n",
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 9.109586\n",
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 8.809026\n",
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 8.652995\n",
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 8.671525\n",
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 9.323762\n",
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 9.475100\n",
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 10.086532\n",
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 10.865124\n",
      "Epoch: 293/1000... Step: 9376... Loss: 10.054910... Val Loss: 10.710555\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 10.581676\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 9.458417\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 9.274239\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 8.926280\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 10.166258\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 10.122044\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 9.209168\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 8.711922\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 8.526518\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 8.377483\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 8.572395\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 9.051313\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 9.230950\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 9.762266\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 10.463124\n",
      "Epoch: 294/1000... Step: 9408... Loss: 7.345847... Val Loss: 10.567714\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 11.257604\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 10.578609\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 10.098261\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 9.395723\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 10.521091\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 10.579592\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 9.634961\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 9.189518\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 8.932455\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 8.771685\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 8.867123\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 9.359943\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 9.507111\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 10.033563\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 10.775178\n",
      "Epoch: 295/1000... Step: 9440... Loss: 7.114164... Val Loss: 10.692390\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 11.323072\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 10.370811\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 9.961695\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 9.611590\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 10.963615\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 10.871787\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 9.904319\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 9.321328\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 9.072063\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 8.838443\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 9.026242\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 9.559450\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 9.700120\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 10.285950\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 11.026172\n",
      "Epoch: 296/1000... Step: 9472... Loss: 9.510404... Val Loss: 11.018230\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 10.811351\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 10.921709\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 10.123306\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 9.604642\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 10.804207\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 10.767876\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 9.867998\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 9.513739\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 9.269104\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 9.049281\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 9.237682\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 9.639545\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 9.800398\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 10.261153\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 11.015385\n",
      "Epoch: 297/1000... Step: 9504... Loss: 6.921859... Val Loss: 11.097043\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 11.788322\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 11.052893\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 10.486873\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 9.777125\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 10.898102\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 10.869946\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 9.919553\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 9.407047\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 9.126714\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 8.901520\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 9.042252\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 9.519688\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 9.664058\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 10.246427\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 10.962567\n",
      "Epoch: 298/1000... Step: 9536... Loss: 8.074269... Val Loss: 10.841168\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 10.369353\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 10.958562\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 10.088775\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 9.390423\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 10.672717\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 10.557512\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 9.649432\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 9.293516\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 9.017803\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 8.800183\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 8.968216\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 9.386820\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 9.555578\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 10.014559\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 10.711215\n",
      "Epoch: 299/1000... Step: 9568... Loss: 7.001912... Val Loss: 10.807100\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 11.375244\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 10.742812\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 10.282312\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 9.682778\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 10.867625\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 10.757785\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 9.802762\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 9.263169\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 8.992518\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 8.738825\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 8.896081\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 9.385426\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 9.517357\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 10.092926\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 10.783575\n",
      "Epoch: 300/1000... Step: 9600... Loss: 8.498171... Val Loss: 10.735062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 11.467264\n",
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 10.992009\n",
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 10.221189\n",
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 9.784723\n",
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 10.861868\n",
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 10.807422\n",
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 9.878124\n",
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 9.444737\n",
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 9.223772\n",
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 9.008845\n",
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 9.170741\n",
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 9.585809\n",
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 9.726767\n",
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 10.186034\n",
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 10.935074\n",
      "Epoch: 301/1000... Step: 9632... Loss: 5.224190... Val Loss: 10.947228\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 12.064801\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 10.603501\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 10.152487\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 9.750829\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 10.772904\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 10.811285\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 9.875001\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 9.400829\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 9.179657\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 8.992713\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 9.150086\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 9.569573\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 9.717402\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 10.256001\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 10.999271\n",
      "Epoch: 302/1000... Step: 9664... Loss: 7.045146... Val Loss: 10.969922\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 11.079683\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 10.375481\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 9.735424\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 9.548806\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 10.702287\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 10.533181\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 9.670016\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 9.308635\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 9.129631\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 8.955260\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 9.180245\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 9.555635\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 9.740778\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 10.171421\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 10.927643\n",
      "Epoch: 303/1000... Step: 9696... Loss: 5.913849... Val Loss: 11.037945\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 9.473770\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 11.365673\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 10.637309\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 9.452618\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 10.715283\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 10.324793\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 9.273991\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 8.760544\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 8.432814\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 8.185737\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 8.149598\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 8.825802\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 8.989122\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 9.677978\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 10.382913\n",
      "Epoch: 304/1000... Step: 9728... Loss: 10.059352... Val Loss: 10.201929\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 11.477013\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 9.862581\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 9.579830\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 9.257187\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 10.458102\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 10.361732\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 9.446220\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 8.976568\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 8.773733\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 8.615770\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 8.789035\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 9.228358\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 9.406923\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 9.980908\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 10.716851\n",
      "Epoch: 305/1000... Step: 9760... Loss: 7.285309... Val Loss: 10.709537\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 11.173476\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 10.531164\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 9.899490\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 9.373087\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 10.553135\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 10.557217\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 9.646996\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 9.213329\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 9.008847\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 8.822830\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 8.973876\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 9.410419\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 9.582314\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 10.089006\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 10.869308\n",
      "Epoch: 306/1000... Step: 9792... Loss: 5.684718... Val Loss: 10.842032\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 11.974823\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 10.311528\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 9.897576\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 9.563126\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 10.677855\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 10.650226\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 9.715249\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 9.201153\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 8.985838\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 8.837005\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 8.994007\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 9.432142\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 9.595096\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 10.174119\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 10.910956\n",
      "Epoch: 307/1000... Step: 9824... Loss: 7.303554... Val Loss: 10.852508\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 11.159994\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 10.036497\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 9.503196\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 9.386109\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 10.601743\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 10.498710\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 9.620608\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 9.216753\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 9.043584\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 8.861765\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 9.080687\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 9.465130\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 9.663643\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 10.143003\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 10.901223\n",
      "Epoch: 308/1000... Step: 9856... Loss: 5.653599... Val Loss: 11.031814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 10.749331\n",
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 11.725408\n",
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 10.642546\n",
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 9.808933\n",
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 11.028429\n",
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 10.924152\n",
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 9.901201\n",
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 9.406155\n",
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 9.101172\n",
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 8.866087\n",
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 8.888007\n",
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 9.458015\n",
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 9.573920\n",
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 10.070254\n",
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 10.863144\n",
      "Epoch: 309/1000... Step: 9888... Loss: 9.163854... Val Loss: 10.703291\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 11.860432\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 10.735662\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 10.168253\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 9.780269\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 10.955657\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 10.909245\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 9.934184\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 9.376029\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 9.132166\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 8.954917\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 9.140500\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 9.581071\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 9.725675\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 10.330338\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 11.038635\n",
      "Epoch: 310/1000... Step: 9920... Loss: 8.202869... Val Loss: 10.988474\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 11.052549\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 11.184216\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 10.321038\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 9.616264\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 10.820084\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 10.927433\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 9.973223\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 9.585295\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 9.312755\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 9.103889\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 9.230317\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 9.638176\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 9.785365\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 10.307168\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 11.085177\n",
      "Epoch: 311/1000... Step: 9952... Loss: 7.171629... Val Loss: 11.030180\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 11.619127\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 10.771637\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 10.223206\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 9.654600\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 10.699907\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 10.599650\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 9.638311\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 9.111827\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 8.870773\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 8.683936\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 8.825729\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 9.251369\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 9.404328\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 9.981461\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 10.656173\n",
      "Epoch: 312/1000... Step: 9984... Loss: 8.086139... Val Loss: 10.597090\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 11.697317\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 11.322637\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 10.336253\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 9.842578\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 10.851236\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 10.818049\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 9.893250\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 9.536314\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 9.301504\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 9.059776\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 9.185447\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 9.588580\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 9.739551\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 10.187676\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 10.959144\n",
      "Epoch: 313/1000... Step: 10016... Loss: 5.538437... Val Loss: 10.925563\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 11.117691\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 11.028857\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 10.141173\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 9.591623\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 10.531867\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 10.334632\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 9.429223\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 9.155644\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 8.900689\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 8.660838\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 8.748911\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 9.159193\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 9.308343\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 9.735264\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 10.451838\n",
      "Epoch: 314/1000... Step: 10048... Loss: 5.924790... Val Loss: 10.415821\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 11.789682\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 11.154180\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 10.283227\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 9.840023\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 10.976826\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 10.881161\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 9.938999\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 9.479214\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 9.239671\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 9.003132\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 9.117631\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 9.584966\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 9.721631\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 10.217469\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 11.020143\n",
      "Epoch: 315/1000... Step: 10080... Loss: 8.197039... Val Loss: 10.903067\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 10.945796\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 11.035489\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 10.147258\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 9.723841\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 10.886986\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 10.617177\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 9.675535\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 9.300976\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 9.076653\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 8.884225\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 9.097056\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 9.449908\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 9.657201\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 10.184976\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 10.936444\n",
      "Epoch: 316/1000... Step: 10112... Loss: 7.199542... Val Loss: 10.950391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 10.639927\n",
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 9.938881\n",
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 9.988661\n",
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 8.921822\n",
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 10.398841\n",
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 10.115382\n",
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 9.094854\n",
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 8.609939\n",
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 8.336755\n",
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 8.145299\n",
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 8.097101\n",
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 8.870084\n",
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 9.038646\n",
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 9.629801\n",
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 10.409959\n",
      "Epoch: 317/1000... Step: 10144... Loss: 7.231367... Val Loss: 10.389914\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 10.255755\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 9.470671\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 9.111890\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 8.617936\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 9.807123\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 9.917902\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 9.027734\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 8.739758\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 8.564522\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 8.425729\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 8.519201\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 8.994958\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 9.248897\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 9.689018\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 10.445883\n",
      "Epoch: 318/1000... Step: 10176... Loss: 4.205742... Val Loss: 10.559771\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 11.488130\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 10.600334\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 9.853725\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 9.672972\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 10.774609\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 10.666798\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 9.773622\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 9.412340\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 9.223754\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 8.998194\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 9.186927\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 9.561212\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 9.739087\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 10.178429\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 10.955240\n",
      "Epoch: 319/1000... Step: 10208... Loss: 5.477193... Val Loss: 11.013472\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 10.543752\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 12.313941\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 10.920444\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 10.025517\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 11.240011\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 11.006239\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 9.973439\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 9.494407\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 9.151104\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 8.824555\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 8.832234\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 9.401618\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 9.499756\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 9.981420\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 10.768321\n",
      "Epoch: 320/1000... Step: 10240... Loss: 9.702824... Val Loss: 10.587376\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 10.601394\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 10.099507\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 9.433571\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 9.087708\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 10.407187\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 10.269253\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 9.379916\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 8.983961\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 8.750548\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 8.546512\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 8.720660\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 9.153606\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 9.326446\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 9.817760\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 10.546612\n",
      "Epoch: 321/1000... Step: 10272... Loss: 6.794912... Val Loss: 10.577318\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 11.171083\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 11.012165\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 10.164520\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 9.575116\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 10.763322\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 10.721467\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 9.773123\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 9.307818\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 9.054231\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 8.787119\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 8.887507\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 9.344466\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 9.478551\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 9.967269\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 10.721396\n",
      "Epoch: 322/1000... Step: 10304... Loss: 6.728418... Val Loss: 10.662270\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 10.672399\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 11.830535\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 10.550558\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 9.961671\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 11.198453\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 11.005231\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 10.024088\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 9.691923\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 9.389130\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 9.073316\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 9.200979\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 9.613832\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 9.739252\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 10.187563\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 10.940364\n",
      "Epoch: 323/1000... Step: 10336... Loss: 5.412062... Val Loss: 10.969796\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 11.104353\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 12.005329\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 10.868470\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 10.063503\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 11.316981\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 11.157880\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 10.154284\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 9.617591\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 9.280976\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 8.951489\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 9.013062\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 9.556069\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 9.650957\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 10.182732\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 10.943467\n",
      "Epoch: 324/1000... Step: 10368... Loss: 8.045975... Val Loss: 10.804270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 11.977663\n",
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 11.049150\n",
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 10.271822\n",
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 9.893922\n",
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 11.122571\n",
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 11.104161\n",
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 10.129832\n",
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 9.519813\n",
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 9.288046\n",
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 9.092353\n",
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 9.276706\n",
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 9.684865\n",
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 9.816413\n",
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 10.383359\n",
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 11.112902\n",
      "Epoch: 325/1000... Step: 10400... Loss: 7.074653... Val Loss: 11.047931\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 11.330095\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 12.110187\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 10.827435\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 10.097780\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 11.229695\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 11.120265\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 10.141687\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 9.722485\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 9.424103\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 9.128058\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 9.244856\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 9.631475\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 9.752250\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 10.245805\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 10.972540\n",
      "Epoch: 326/1000... Step: 10432... Loss: 6.785556... Val Loss: 10.904265\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 12.160153\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 11.681278\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 10.660483\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 10.118882\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 11.180836\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 11.091733\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 10.117274\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 9.633159\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 9.369881\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 9.122712\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 9.229683\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 9.649731\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 9.777261\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 10.283944\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 11.028981\n",
      "Epoch: 327/1000... Step: 10464... Loss: 6.168172... Val Loss: 10.932318\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 11.119242\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 12.508362\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 11.118144\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 10.186306\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 11.305382\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 11.034796\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 10.008441\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 9.560836\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 9.221211\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 8.891750\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 8.909913\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 9.423753\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 9.516547\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 9.980526\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 10.711073\n",
      "Epoch: 328/1000... Step: 10496... Loss: 6.543921... Val Loss: 10.587502\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 12.256078\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 12.310967\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 11.047545\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 10.542925\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 11.680093\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 11.646034\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 10.644278\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 10.101065\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 9.802941\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 9.512476\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 9.619051\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 10.044663\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 10.124554\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 10.609128\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 11.381864\n",
      "Epoch: 329/1000... Step: 10528... Loss: 6.607506... Val Loss: 11.246538\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 10.037795\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 12.868099\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 11.158699\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 10.204455\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 11.390894\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 11.000046\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 9.997009\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 9.501558\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 9.118634\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 8.747320\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 8.746046\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 9.253827\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 9.314783\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 9.685143\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 10.355757\n",
      "Epoch: 330/1000... Step: 10560... Loss: 6.961973... Val Loss: 10.258407\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 11.291332\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 12.713126\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 11.136557\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 10.539134\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 11.682155\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 11.517281\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 10.550063\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 10.091213\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 9.758243\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 9.430169\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 9.542898\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 9.931027\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 9.993296\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 10.386614\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 11.116712\n",
      "Epoch: 331/1000... Step: 10592... Loss: 6.615064... Val Loss: 11.009092\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 10.363856\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 13.256732\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 11.487314\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 10.546297\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 11.779209\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 11.495433\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 10.474268\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 9.878583\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 9.490144\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 9.116811\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 9.211525\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 9.605823\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 9.660175\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 10.086497\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 10.739023\n",
      "Epoch: 332/1000... Step: 10624... Loss: 7.950161... Val Loss: 10.647602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 11.874277\n",
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 13.054760\n",
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 11.472733\n",
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 11.010307\n",
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 12.261718\n",
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 12.053156\n",
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 11.048389\n",
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 10.466294\n",
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 10.135248\n",
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 9.772207\n",
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 9.915851\n",
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 10.327990\n",
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 10.378764\n",
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 10.816118\n",
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 11.574638\n",
      "Epoch: 333/1000... Step: 10656... Loss: 7.562973... Val Loss: 11.468123\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 9.587508\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 13.655270\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 11.758634\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 10.475049\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 11.567952\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 11.112773\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 10.059572\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 9.527424\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 9.099957\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 8.721807\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 8.678084\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 9.177806\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 9.205820\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 9.590451\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 10.232518\n",
      "Epoch: 334/1000... Step: 10688... Loss: 6.721767... Val Loss: 10.095144\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 10.519808\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 14.101355\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 11.995227\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 11.003731\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 11.951430\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 11.629015\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 10.616772\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 10.186174\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 9.796753\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 9.418297\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 9.444579\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 9.787352\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 9.811943\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 10.121771\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 10.773777\n",
      "Epoch: 335/1000... Step: 10720... Loss: 5.783407... Val Loss: 10.671601\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 11.003077\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 13.697274\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 11.817248\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 11.129925\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 12.161472\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 11.868479\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 10.888801\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 10.453759\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 10.079340\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 9.715366\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 9.825965\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 10.130630\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 10.163062\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 10.489714\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 11.162840\n",
      "Epoch: 336/1000... Step: 10752... Loss: 6.254996... Val Loss: 11.113911\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 11.419592\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 14.095663\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 12.138489\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 11.344760\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 12.223081\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 12.010497\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 11.022665\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 10.610228\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 10.229164\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 9.877045\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 9.959280\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 10.229803\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 10.242499\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 10.566789\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 11.241838\n",
      "Epoch: 337/1000... Step: 10784... Loss: 5.987707... Val Loss: 11.119272\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 10.914855\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 14.100049\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 12.083980\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 11.245243\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 12.170980\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 11.892055\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 10.906324\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 10.477004\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 10.094653\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 9.725715\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 9.809737\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 10.084562\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 10.109204\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 10.414361\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 11.055729\n",
      "Epoch: 338/1000... Step: 10816... Loss: 5.882015... Val Loss: 10.991712\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 10.781756\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 14.965621\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 12.639867\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 11.681806\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 12.502729\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 12.124464\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 11.106217\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 10.697908\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 10.279968\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 9.886457\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 9.926368\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 10.197052\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 10.196144\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 10.466112\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 11.111236\n",
      "Epoch: 339/1000... Step: 10848... Loss: 5.804289... Val Loss: 11.010593\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 10.503416\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 14.997859\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 12.657025\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 11.585714\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 12.475127\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 12.068100\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 11.017262\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 10.501913\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 10.062073\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 9.625739\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 9.643298\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 9.990341\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 9.976952\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 10.282325\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 10.931463\n",
      "Epoch: 340/1000... Step: 10880... Loss: 6.429754... Val Loss: 10.789654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 9.234338\n",
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 15.520877\n",
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 12.929256\n",
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 11.399725\n",
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 12.286293\n",
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 11.691421\n",
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 10.576125\n",
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 10.033414\n",
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 9.547345\n",
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 9.092864\n",
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 9.002810\n",
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 9.438723\n",
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 9.414842\n",
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 9.695829\n",
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 10.289288\n",
      "Epoch: 341/1000... Step: 10912... Loss: 5.958553... Val Loss: 10.147270\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 9.510686\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 16.057490\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 13.310563\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 11.744800\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 12.607595\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 11.990842\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 10.842968\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 10.309218\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 9.796013\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 9.317887\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 9.188318\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 9.639446\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 9.599006\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 9.857332\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 10.468244\n",
      "Epoch: 342/1000... Step: 10944... Loss: 5.498672... Val Loss: 10.307290\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 11.180410\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 15.877308\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 13.314918\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 12.276000\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 12.995360\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 12.576399\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 11.502923\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 11.088641\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 10.645612\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 10.198670\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 10.211089\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 10.479286\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 10.447436\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 10.694331\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 11.360666\n",
      "Epoch: 343/1000... Step: 10976... Loss: 5.007969... Val Loss: 11.230517\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 9.136866\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 15.827795\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 13.200379\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 11.580960\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 12.402388\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 11.791431\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 10.619085\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 10.072559\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 9.565729\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 9.104147\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 8.972536\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 9.383573\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 9.357187\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 9.621294\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 10.207867\n",
      "Epoch: 344/1000... Step: 11008... Loss: 5.009592... Val Loss: 10.090890\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 9.208164\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 18.346934\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 14.813706\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 12.923952\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 13.591895\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 12.697306\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 11.478195\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 10.973502\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 10.365681\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 9.806284\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 9.615785\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 9.984795\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 9.904578\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 10.064714\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 10.612425\n",
      "Epoch: 345/1000... Step: 11040... Loss: 4.968195... Val Loss: 10.458802\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 9.961247\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 17.768829\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 14.586114\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 13.036445\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 13.792301\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 13.044227\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 11.813152\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 11.186074\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 10.567939\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 10.016411\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 9.880973\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 10.237932\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 10.107963\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 10.334339\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 10.960735\n",
      "Epoch: 346/1000... Step: 11072... Loss: 6.651845... Val Loss: 10.755266\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 10.767288\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 17.643719\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 14.568955\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 13.077591\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 13.815229\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 13.182960\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 11.974850\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 11.362491\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 10.799365\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 10.265113\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 10.180345\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 10.549211\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 10.439283\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 10.702638\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 11.366210\n",
      "Epoch: 347/1000... Step: 11104... Loss: 6.315401... Val Loss: 11.148253\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 13.423985\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 17.927123\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 14.972362\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 13.892488\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 14.533793\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 14.159654\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 12.988814\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 12.366314\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 11.930073\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 11.423252\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 11.434496\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 11.716192\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 11.642666\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 11.930502\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 12.652968\n",
      "Epoch: 348/1000... Step: 11136... Loss: 5.401278... Val Loss: 12.446311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 11.444767\n",
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 18.537332\n",
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 15.029131\n",
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 13.405422\n",
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 13.972798\n",
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 13.501761\n",
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 12.259184\n",
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 11.741150\n",
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 11.248148\n",
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 10.719717\n",
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 10.587862\n",
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 10.962342\n",
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 10.895793\n",
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 11.103093\n",
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 11.783173\n",
      "Epoch: 349/1000... Step: 11168... Loss: 2.949079... Val Loss: 11.610176\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 10.185714\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 20.107079\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 16.104148\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 14.192766\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 14.546741\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 13.704588\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 12.461786\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 11.980598\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 11.366268\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 10.766183\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 10.628208\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 10.819224\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 10.697876\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 10.832906\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 11.367496\n",
      "Epoch: 350/1000... Step: 11200... Loss: 4.719330... Val Loss: 11.231787\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 9.081410\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 19.336173\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 15.484809\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 13.461095\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 14.087145\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 13.140389\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 11.829772\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 11.263290\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 10.621339\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 10.020481\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 9.789586\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 10.258019\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 10.125803\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 10.287486\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 10.898683\n",
      "Epoch: 351/1000... Step: 11232... Loss: 4.732683... Val Loss: 10.748860\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 9.525214\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 20.416683\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 16.296079\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 14.149956\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 14.574788\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 13.558852\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 12.252615\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 11.693361\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 10.995375\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 10.375053\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 10.162627\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 10.461507\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 10.313062\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 10.455991\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 11.014161\n",
      "Epoch: 352/1000... Step: 11264... Loss: 5.256079... Val Loss: 10.794853\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 10.516335\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 18.866112\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 15.331531\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 13.718077\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 14.204090\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 13.483817\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 12.244846\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 11.714658\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 11.121887\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 10.549901\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 10.438741\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 10.656045\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 10.533802\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 10.723450\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 11.335583\n",
      "Epoch: 353/1000... Step: 11296... Loss: 4.816288... Val Loss: 11.161841\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 9.081549\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 18.207077\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 14.750097\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 12.841971\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 13.444870\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 12.651825\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 11.415326\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 10.903604\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 10.327478\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 9.782396\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 9.627739\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 9.943802\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 9.869136\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 10.053559\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 10.611679\n",
      "Epoch: 354/1000... Step: 11328... Loss: 4.170699... Val Loss: 10.498251\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 10.086490\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 20.316268\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 16.121392\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 14.205922\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 14.733314\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 13.796205\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 12.515653\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 11.937715\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 11.254682\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 10.624145\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 10.399792\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 10.783278\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 10.607001\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 10.732827\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 11.344933\n",
      "Epoch: 355/1000... Step: 11360... Loss: 5.277958... Val Loss: 11.100822\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 10.402699\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 18.083838\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 14.824426\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 13.271857\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 13.909947\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 13.266666\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 12.038729\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 11.402921\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 10.824613\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 10.282951\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 10.189502\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 10.449137\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 10.327705\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 10.560491\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 11.174113\n",
      "Epoch: 356/1000... Step: 11392... Loss: 5.788423... Val Loss: 10.979499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 8.782090\n",
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 18.671025\n",
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 15.078831\n",
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 13.084951\n",
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 13.612228\n",
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 12.726381\n",
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 11.494601\n",
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 11.060257\n",
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 10.461985\n",
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 9.925324\n",
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 9.759997\n",
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 9.971543\n",
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 9.914928\n",
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 10.061194\n",
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 10.558602\n",
      "Epoch: 357/1000... Step: 11424... Loss: 3.668766... Val Loss: 10.491979\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 11.583394\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 19.551354\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 15.781488\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 14.294164\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 14.753122\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 14.129607\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 12.901394\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 12.357093\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 11.762761\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 11.147066\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 11.050674\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 11.325470\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 11.187345\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 11.366264\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 12.012581\n",
      "Epoch: 358/1000... Step: 11456... Loss: 4.092545... Val Loss: 11.806992\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 11.397715\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 19.785117\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 15.914152\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 14.089866\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 14.528697\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 13.902199\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 12.615636\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 12.003713\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 11.440247\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 10.861497\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 10.718785\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 11.064056\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 10.932995\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 11.135831\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 11.784906\n",
      "Epoch: 359/1000... Step: 11488... Loss: 3.009734... Val Loss: 11.557816\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 10.985515\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 19.332718\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 15.664766\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 13.763950\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 14.235450\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 13.542612\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 12.250668\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 11.623775\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 11.052886\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 10.479041\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 10.348275\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 10.674189\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 10.565805\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 10.803141\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 11.410757\n",
      "Epoch: 360/1000... Step: 11520... Loss: 3.227215... Val Loss: 11.209854\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 9.749589\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 21.271214\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 16.770053\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 14.541249\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 14.896240\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 13.890433\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 12.594776\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 12.006043\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 11.298342\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 10.682758\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 10.454659\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 10.701666\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 10.499201\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 10.635499\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 11.165271\n",
      "Epoch: 361/1000... Step: 11552... Loss: 5.202724... Val Loss: 10.885331\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 10.623591\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 18.411600\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 14.988050\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 13.288407\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 13.956277\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 13.392589\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 12.110823\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 11.455793\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 10.904787\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 10.360986\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 10.222144\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 10.598259\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 10.474258\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 10.730589\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 11.375608\n",
      "Epoch: 362/1000... Step: 11584... Loss: 4.719193... Val Loss: 11.182068\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 9.919046\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 20.297845\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 16.178731\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 14.019511\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 14.484127\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 13.534511\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 12.186115\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 11.547876\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 10.880785\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 10.272211\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 10.053838\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 10.408214\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 10.248588\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 10.418462\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 10.997741\n",
      "Epoch: 363/1000... Step: 11616... Loss: 4.916317... Val Loss: 10.791640\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 9.105988\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 21.024817\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 16.563285\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 14.338225\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 14.802747\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 13.717142\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 12.351250\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 11.796276\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 11.100443\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 10.453814\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 10.203864\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 10.491551\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 10.345676\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 10.458650\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 11.001934\n",
      "Epoch: 364/1000... Step: 11648... Loss: 3.936383... Val Loss: 10.847714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 11.388441\n",
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 19.899563\n",
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 16.058087\n",
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 14.421851\n",
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 14.879491\n",
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 14.231353\n",
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 12.958847\n",
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 12.335394\n",
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 11.710061\n",
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 11.096194\n",
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 10.987793\n",
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 11.221349\n",
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 11.060842\n",
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 11.257363\n",
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 11.885082\n",
      "Epoch: 365/1000... Step: 11680... Loss: 4.889412... Val Loss: 11.667935\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 11.335437\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 21.207719\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 16.839767\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 14.966310\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 15.504387\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 14.772677\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 13.443365\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 12.698238\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 12.051853\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 11.407716\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 11.266181\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 11.656910\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 11.443113\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 11.644087\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 12.328112\n",
      "Epoch: 366/1000... Step: 11712... Loss: 4.939618... Val Loss: 12.047187\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 10.732538\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 21.229095\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 16.808059\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 14.724507\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 15.135925\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 14.240173\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 12.925450\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 12.257745\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 11.559530\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 10.931234\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 10.715584\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 11.063051\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 10.837737\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 11.001725\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 11.615891\n",
      "Epoch: 367/1000... Step: 11744... Loss: 5.586285... Val Loss: 11.315153\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 10.188023\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 19.676153\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 15.848486\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 13.997509\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 14.437021\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 13.649549\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 12.382288\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 11.958550\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 11.372564\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 10.810159\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 10.699153\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 10.776936\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 10.693216\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 10.844725\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 11.370893\n",
      "Epoch: 368/1000... Step: 11776... Loss: 3.437567... Val Loss: 11.301529\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 8.886977\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 20.047314\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 15.903944\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 13.763303\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 14.205267\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 13.222059\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 11.905052\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 11.356606\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 10.686388\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 10.062301\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 9.842316\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 10.166783\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 10.032160\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 10.176705\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 10.727052\n",
      "Epoch: 369/1000... Step: 11808... Loss: 3.967421... Val Loss: 10.590925\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 10.030993\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 20.393490\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 16.256644\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 14.199841\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 14.866550\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 13.897153\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 12.522614\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 11.766185\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 11.052394\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 10.436055\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 10.191857\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 10.656880\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 10.457023\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 10.645416\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 11.281432\n",
      "Epoch: 370/1000... Step: 11840... Loss: 6.185070... Val Loss: 11.046487\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 8.295769\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 21.108881\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 17.049256\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 14.594948\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 15.085537\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 13.739842\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 12.408579\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 12.034283\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 11.314615\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 10.755946\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 10.485012\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 10.572072\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 10.577997\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 10.625321\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 10.982769\n",
      "Epoch: 371/1000... Step: 11872... Loss: 3.089899... Val Loss: 11.081875\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 11.638593\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 20.771477\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 16.683791\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 14.667287\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 14.963716\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 14.255456\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 12.908807\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 12.429675\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 11.789238\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 11.154921\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 10.969739\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 11.352699\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 11.203153\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 11.400605\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 12.093793\n",
      "Epoch: 372/1000... Step: 11904... Loss: 9.708473... Val Loss: 11.869273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 10.199771\n",
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 19.128044\n",
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 15.552553\n",
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 13.408915\n",
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 13.724474\n",
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 12.906053\n",
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 11.612168\n",
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 11.028868\n",
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 10.461330\n",
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 9.947319\n",
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 9.753389\n",
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 10.133063\n",
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 10.035006\n",
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 10.249229\n",
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 10.853315\n",
      "Epoch: 373/1000... Step: 11936... Loss: 3.337965... Val Loss: 10.659611\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 9.509348\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 20.291616\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 16.204904\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 13.978670\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 14.626094\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 13.597483\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 12.219469\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 11.544382\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 10.830037\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 10.220733\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 9.949658\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 10.416232\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 10.214332\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 10.413088\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 11.042687\n",
      "Epoch: 374/1000... Step: 11968... Loss: 5.967026... Val Loss: 10.810751\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 9.869243\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 21.199066\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 16.751485\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 14.523425\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 14.932376\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 13.959146\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 12.581384\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 12.006134\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 11.325553\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 10.676828\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 10.437214\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 10.729496\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 10.556288\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 10.713809\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 11.316673\n",
      "Epoch: 375/1000... Step: 12000... Loss: 4.375066... Val Loss: 11.101236\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 8.653153\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 21.329884\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 16.898439\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 14.461149\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 14.742052\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 13.515091\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 12.172298\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 11.733151\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 11.018954\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 10.388454\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 10.127943\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 10.244184\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 10.156098\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 10.235607\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 10.667835\n",
      "Epoch: 376/1000... Step: 12032... Loss: 3.558754... Val Loss: 10.585923\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 12.219021\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 23.052862\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 18.172862\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 16.251296\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 16.533736\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 15.729768\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 14.328286\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 13.806634\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 13.111137\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 12.348795\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 12.154222\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 12.461126\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 12.226771\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 12.344603\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 13.055692\n",
      "Epoch: 377/1000... Step: 12064... Loss: 3.713396... Val Loss: 12.809643\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 12.050181\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 22.363796\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 17.770687\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 15.913062\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 16.278302\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 15.466599\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 14.129362\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 13.438946\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 12.750248\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 12.071996\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 11.931197\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 12.136701\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 11.894970\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 12.061490\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 12.709662\n",
      "Epoch: 378/1000... Step: 12096... Loss: 5.446990... Val Loss: 12.392634\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 12.871686\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 21.028214\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 17.068709\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 15.583150\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 16.094074\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 15.638496\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 14.330267\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 13.719239\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 13.181517\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 12.553108\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 12.530894\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 12.730939\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 12.577427\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 12.769440\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 13.473249\n",
      "Epoch: 379/1000... Step: 12128... Loss: 4.096425... Val Loss: 13.321685\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 9.717091\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 24.935839\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 19.217089\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 16.577760\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 16.614127\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 15.261474\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 13.857801\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 13.537640\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 12.656049\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 11.911803\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 11.585520\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 11.654639\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 11.373795\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 11.377405\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 11.919700\n",
      "Epoch: 380/1000... Step: 12160... Loss: 3.970235... Val Loss: 11.632656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 9.750513\n",
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 18.074783\n",
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 14.768229\n",
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 12.932666\n",
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 13.560388\n",
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 12.899571\n",
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 11.612685\n",
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 10.991787\n",
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 10.436898\n",
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 9.899186\n",
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 9.759626\n",
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 10.067328\n",
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 9.967592\n",
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 10.233809\n",
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 10.824664\n",
      "Epoch: 381/1000... Step: 12192... Loss: 4.258037... Val Loss: 10.688056\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 11.409346\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 21.994769\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 17.387506\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 15.420305\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 15.831566\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 15.009419\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 13.650143\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 12.932258\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 12.253375\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 11.596470\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 11.429081\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 11.773985\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 11.533640\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 11.731424\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 12.414125\n",
      "Epoch: 382/1000... Step: 12224... Loss: 4.869131... Val Loss: 12.092197\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 11.076053\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 20.529425\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 16.476080\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 14.587486\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 15.110563\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 14.432088\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 13.114597\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 12.432850\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 11.799481\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 11.170610\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 11.047818\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 11.355035\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 11.151631\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 11.394356\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 12.059028\n",
      "Epoch: 383/1000... Step: 12256... Loss: 4.564161... Val Loss: 11.810359\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 11.517706\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 22.245038\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 17.589676\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 15.631218\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 15.988914\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 15.194438\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 13.854513\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 13.183538\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 12.511899\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 11.864658\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 11.734173\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 11.945851\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 11.718847\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 11.912787\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 12.539514\n",
      "Epoch: 384/1000... Step: 12288... Loss: 4.971920... Val Loss: 12.242370\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 9.185575\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 19.787544\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 15.707783\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 13.548517\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 14.062073\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 13.211993\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 11.889179\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 11.271694\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 10.632407\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 10.036500\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 9.839010\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 10.292566\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 10.124756\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 10.350144\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 10.969629\n",
      "Epoch: 385/1000... Step: 12320... Loss: 4.108223... Val Loss: 10.774517\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 8.396018\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 20.607599\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 16.336694\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 13.934974\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 14.287456\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 13.130148\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 11.790532\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 11.330585\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 10.653474\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 10.064735\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 9.823518\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 9.991863\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 9.925782\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 10.053571\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 10.514205\n",
      "Epoch: 386/1000... Step: 12352... Loss: 3.150891... Val Loss: 10.432962\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 11.099125\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 22.359243\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 17.635681\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 15.333714\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 15.489274\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 14.584141\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 13.188279\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 12.618323\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 11.936487\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 11.246664\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 11.044693\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 11.283065\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 11.114209\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 11.274183\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 11.851068\n",
      "Epoch: 387/1000... Step: 12384... Loss: 2.663737... Val Loss: 11.634915\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 9.137268\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 23.804714\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 18.406055\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 15.735145\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 15.906659\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 14.544530\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 13.141808\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 12.565731\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 11.724768\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 11.023521\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 10.713851\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 10.848856\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 10.596224\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 10.659299\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 11.120709\n",
      "Epoch: 388/1000... Step: 12416... Loss: 4.079250... Val Loss: 10.840868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 11.345659\n",
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 20.187545\n",
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 16.266086\n",
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 14.429249\n",
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 14.935871\n",
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 14.310374\n",
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 12.986668\n",
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 12.342926\n",
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 11.767303\n",
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 11.144442\n",
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 11.038718\n",
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 11.245682\n",
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 11.099234\n",
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 11.343071\n",
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 11.933101\n",
      "Epoch: 389/1000... Step: 12448... Loss: 3.039910... Val Loss: 11.742778\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 11.484534\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 21.872194\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 17.360124\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 15.351883\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 15.749551\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 15.029237\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 13.665988\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 12.911358\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 12.260548\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 11.600503\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 11.469286\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 11.771842\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 11.557212\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 11.792260\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 12.442705\n",
      "Epoch: 390/1000... Step: 12480... Loss: 3.915545... Val Loss: 12.172167\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 10.939983\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 22.451666\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 17.640985\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 15.609764\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 16.034293\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 15.230828\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 13.850351\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 13.102306\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 12.449031\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 11.768344\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 11.646156\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 11.862002\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 11.641863\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 11.829544\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 12.448452\n",
      "Epoch: 391/1000... Step: 12512... Loss: 3.717632... Val Loss: 12.227503\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 11.552146\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 22.289291\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 17.579839\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 15.636493\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 15.990534\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 15.260475\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 13.903342\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 13.179026\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 12.548322\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 11.868478\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 11.751292\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 12.025924\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 11.811820\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 12.010497\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 12.649501\n",
      "Epoch: 392/1000... Step: 12544... Loss: 3.335365... Val Loss: 12.415984\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 10.770265\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 21.712111\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 17.236592\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 15.083192\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 15.528846\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 14.632612\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 13.204512\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 12.452216\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 11.749116\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 11.115608\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 10.913681\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 11.268912\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 11.018892\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 11.230876\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 11.876091\n",
      "Epoch: 393/1000... Step: 12576... Loss: 5.156363... Val Loss: 11.628075\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 9.167076\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 21.460043\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 16.824804\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 14.406891\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 14.881309\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 13.686682\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 12.263371\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 11.563745\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 10.810246\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 10.176665\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 9.902844\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 10.245090\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 10.036827\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 10.202412\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 10.745625\n",
      "Epoch: 394/1000... Step: 12608... Loss: 5.014001... Val Loss: 10.512966\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 11.232488\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 20.518225\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 16.534729\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 14.583635\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 14.998571\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 14.295947\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 12.957240\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 12.343191\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 11.689336\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 11.059428\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 10.925768\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 11.064616\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 10.900187\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 11.137981\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 11.705616\n",
      "Epoch: 395/1000... Step: 12640... Loss: 3.705889... Val Loss: 11.485866\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 11.587948\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 23.347092\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 18.238813\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 16.183151\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 16.500672\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 15.753339\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 14.374090\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 13.613089\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 12.946369\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 12.232009\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 12.098768\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 12.439382\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 12.181749\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 12.351997\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 13.018898\n",
      "Epoch: 396/1000... Step: 12672... Loss: 3.801194... Val Loss: 12.748136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 9.183768\n",
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 22.940969\n",
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 17.831293\n",
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 15.186687\n",
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 15.435167\n",
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 14.141817\n",
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 12.750402\n",
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 12.091456\n",
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 11.267238\n",
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 10.580149\n",
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 10.294372\n",
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 10.510191\n",
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 10.255439\n",
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 10.383192\n",
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 10.873153\n",
      "Epoch: 397/1000... Step: 12704... Loss: 4.836452... Val Loss: 10.568265\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 10.882787\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 19.533309\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 15.879714\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 13.844738\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 14.326039\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 13.650230\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 12.295571\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 11.650509\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 11.056333\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 10.504083\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 10.350881\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 10.597348\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 10.467138\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 10.761388\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 11.347871\n",
      "Epoch: 398/1000... Step: 12736... Loss: 3.567226... Val Loss: 11.155419\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 11.411556\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 21.991438\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 17.310818\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 15.302955\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 15.747315\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 15.002291\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 13.640491\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 12.873553\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 12.252811\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 11.590982\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 11.447991\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 11.775327\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 11.556001\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 11.769064\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 12.415638\n",
      "Epoch: 399/1000... Step: 12768... Loss: 3.127353... Val Loss: 12.143437\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 10.297284\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 20.812356\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 16.591038\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 14.536208\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 14.977100\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 14.184093\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 12.849906\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 12.166711\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 11.522417\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 10.894839\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 10.778389\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 10.920693\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 10.737288\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 10.947649\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 11.500726\n",
      "Epoch: 400/1000... Step: 12800... Loss: 3.823506... Val Loss: 11.303295\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 8.754185\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 23.098475\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 17.835177\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 15.158715\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 15.374176\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 14.062140\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 12.668490\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 12.055646\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 11.257874\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 10.583410\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 10.291627\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 10.512642\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 10.281085\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 10.385267\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 10.871912\n",
      "Epoch: 401/1000... Step: 12832... Loss: 3.742540... Val Loss: 10.600735\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 9.741471\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 20.412548\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 16.128087\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 13.818892\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 14.266298\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 13.347439\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 11.988710\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 11.352730\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 10.748764\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 10.126575\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 9.919797\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 10.221851\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 10.105284\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 10.314641\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 10.836499\n",
      "Epoch: 402/1000... Step: 12864... Loss: 2.177506... Val Loss: 10.670264\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 8.656172\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 21.511605\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 16.907032\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 14.410685\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 14.860998\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 13.624989\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 12.166634\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 11.460597\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 10.721787\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 10.073939\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 9.826229\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 10.086218\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 9.925846\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 10.093876\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 10.608909\n",
      "Epoch: 403/1000... Step: 12896... Loss: 4.325365... Val Loss: 10.444389\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 11.051102\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 20.508996\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 16.476068\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 14.441005\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 14.951666\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 14.247022\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 12.867655\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 12.121302\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 11.477489\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 10.848507\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 10.709235\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 10.947918\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 10.799353\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 11.089478\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 11.686591\n",
      "Epoch: 404/1000... Step: 12928... Loss: 4.661644... Val Loss: 11.479285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 9.865923\n",
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 22.082808\n",
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 17.385743\n",
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 15.024151\n",
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 15.233247\n",
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 14.133909\n",
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 12.748337\n",
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 12.202371\n",
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 11.543410\n",
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 10.889056\n",
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 10.680854\n",
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 10.744880\n",
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 10.622089\n",
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 10.719459\n",
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 11.201975\n",
      "Epoch: 405/1000... Step: 12960... Loss: 2.819411... Val Loss: 11.071045\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 11.035049\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 21.152163\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 16.806802\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 14.949150\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 15.313583\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 14.550947\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 13.227533\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 12.618416\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 11.964448\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 11.302688\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 11.165627\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 11.350040\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 11.152213\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 11.346036\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 11.967964\n",
      "Epoch: 406/1000... Step: 12992... Loss: 3.783077... Val Loss: 11.741256\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 11.320218\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 22.529686\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 17.787127\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 15.682797\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 16.079448\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 15.272065\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 13.902244\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 13.111360\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 12.426644\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 11.733960\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 11.596384\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 11.874262\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 11.626283\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 11.849872\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 12.495533\n",
      "Epoch: 407/1000... Step: 13024... Loss: 4.704555... Val Loss: 12.187756\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 13.077429\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 20.701651\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 16.966935\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 15.550168\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 16.077282\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 15.684650\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 14.399046\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 13.757505\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 13.270463\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 12.661807\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 12.708284\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 12.677169\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 12.575517\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 12.832757\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 13.463789\n",
      "Epoch: 408/1000... Step: 13056... Loss: 4.254832... Val Loss: 13.358398\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 9.085059\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 22.181369\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 17.214101\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 14.827187\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 14.995726\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 13.954704\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 12.636445\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 12.123017\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 11.403062\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 10.770852\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 10.540207\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 10.814056\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 10.585295\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 10.708244\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 11.284906\n",
      "Epoch: 409/1000... Step: 13088... Loss: 5.065262... Val Loss: 11.028731\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 10.689034\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 17.784801\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 14.803067\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 12.916587\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 13.577051\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 12.989624\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 11.709507\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 10.952001\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 10.402207\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 9.871901\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 9.780114\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 10.075711\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 9.979929\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 10.348105\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 10.911463\n",
      "Epoch: 410/1000... Step: 13120... Loss: 5.610114... Val Loss: 10.733273\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 8.119999\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 21.924330\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 17.263457\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 14.482331\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 14.811159\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 13.363998\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 11.947571\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 11.378769\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 10.659641\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 10.020110\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 9.705248\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 9.910293\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 9.845191\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 9.950175\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 10.350039\n",
      "Epoch: 411/1000... Step: 13152... Loss: 3.055554... Val Loss: 10.253864\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 11.319571\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 22.315639\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 17.621598\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 15.557497\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 16.070863\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 15.174670\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 13.734238\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 12.987858\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 12.290738\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 11.587233\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 11.416292\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 11.874283\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 11.622896\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 11.840923\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 12.564105\n",
      "Epoch: 412/1000... Step: 13184... Loss: 4.925695... Val Loss: 12.333485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 8.250771\n",
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 22.095070\n",
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 17.313533\n",
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 14.570754\n",
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 14.857149\n",
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 13.452210\n",
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 12.047807\n",
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 11.489370\n",
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 10.737114\n",
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 10.091555\n",
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 9.777920\n",
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 9.920615\n",
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 9.777643\n",
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 9.878549\n",
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 10.285713\n",
      "Epoch: 413/1000... Step: 13216... Loss: 3.203022... Val Loss: 10.123620\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 11.162777\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 23.466795\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 18.367034\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 16.147231\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 16.448056\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 15.474734\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 14.038108\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 13.430244\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 12.669363\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 11.896487\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 11.676133\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 11.964177\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 11.681203\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 11.825003\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 12.482411\n",
      "Epoch: 414/1000... Step: 13248... Loss: 4.057162... Val Loss: 12.226322\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 9.810395\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 19.325934\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 15.820445\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 13.694312\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 14.191079\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 13.427895\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 12.093246\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 11.420238\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 10.854140\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 10.311981\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 10.170474\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 10.262254\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 10.149552\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 10.389441\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 10.887649\n",
      "Epoch: 415/1000... Step: 13280... Loss: 5.164466... Val Loss: 10.759272\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 8.072447\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 23.190160\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 17.900843\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 15.196157\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 15.468526\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 13.969684\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 12.505747\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 11.937084\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 11.135870\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 10.430242\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 10.096281\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 10.261485\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 10.101846\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 10.172127\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 10.621288\n",
      "Epoch: 416/1000... Step: 13312... Loss: 3.833257... Val Loss: 10.537698\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 11.933961\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 23.393897\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 18.350931\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 16.316287\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 16.581426\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 15.700586\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 14.319903\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 13.628466\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 12.937361\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 12.199311\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 12.020532\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 12.262091\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 11.990425\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 12.148229\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 12.777911\n",
      "Epoch: 417/1000... Step: 13344... Loss: 3.504163... Val Loss: 12.499102\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 11.317602\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 23.548175\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 18.373606\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 16.279215\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 16.575744\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 15.718512\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 14.324466\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 13.611288\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 12.933533\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 12.191057\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 12.041288\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 12.314260\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 12.048341\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 12.215355\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 12.877989\n",
      "Epoch: 418/1000... Step: 13376... Loss: 3.282963... Val Loss: 12.630342\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 12.599559\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 24.234264\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 19.004291\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 16.846877\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 17.087189\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 16.311368\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 14.885439\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 14.136772\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 13.457640\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 12.758888\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 12.555683\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 12.892613\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 12.560417\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 12.719985\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 13.412557\n",
      "Epoch: 419/1000... Step: 13408... Loss: 4.670442... Val Loss: 13.051088\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 10.053572\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 23.689786\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 18.425917\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 15.969437\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 16.071926\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 14.859754\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 13.465047\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 13.079070\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 12.317531\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 11.595935\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 11.320802\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 11.288228\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 11.082782\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 11.118371\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 11.576522\n",
      "Epoch: 420/1000... Step: 13440... Loss: 2.728429... Val Loss: 11.418190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 10.539902\n",
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 19.589670\n",
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 15.825636\n",
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 13.823358\n",
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 14.290588\n",
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 13.601339\n",
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 12.289867\n",
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 11.636553\n",
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 11.048579\n",
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 10.450060\n",
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 10.305057\n",
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 10.462618\n",
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 10.320548\n",
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 10.587438\n",
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 11.145017\n",
      "Epoch: 421/1000... Step: 13472... Loss: 3.479615... Val Loss: 10.922034\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 10.017447\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 22.423061\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 17.499888\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 15.360471\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 15.603173\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 14.638808\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 13.294317\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 12.654134\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 11.931897\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 11.217610\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 11.076271\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 11.315936\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 11.093433\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 11.246877\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 11.850999\n",
      "Epoch: 422/1000... Step: 13504... Loss: 3.900495... Val Loss: 11.636010\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 10.611505\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 21.105360\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 16.742704\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 14.763378\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 15.108627\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 14.289209\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 12.955989\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 12.326917\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 11.670750\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 11.023024\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 10.879237\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 11.060311\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 10.858704\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 11.077457\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 11.690982\n",
      "Epoch: 423/1000... Step: 13536... Loss: 3.886600... Val Loss: 11.457221\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 13.145838\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 20.078945\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 16.691744\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 15.137238\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 15.643073\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 15.460522\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 14.165183\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 13.517540\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 13.098498\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 12.555686\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 12.575179\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 12.616215\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 12.510296\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 12.835707\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 13.504958\n",
      "Epoch: 424/1000... Step: 13568... Loss: 4.319552... Val Loss: 13.356611\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 10.186147\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 22.753918\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 17.719083\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 15.508203\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 15.716753\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 14.794509\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 13.411581\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 12.814379\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 12.098162\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 11.372844\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 11.201660\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 11.466549\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 11.231441\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 11.396875\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 12.029623\n",
      "Epoch: 425/1000... Step: 13600... Loss: 4.214051... Val Loss: 11.816574\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 8.192842\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 22.149699\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 17.241145\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 14.535298\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 15.025097\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 13.525292\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 12.058310\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 11.363939\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 10.549225\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 9.874582\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 9.559879\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 9.886796\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 9.649035\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 9.782480\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 10.272481\n",
      "Epoch: 426/1000... Step: 13632... Loss: 4.968292... Val Loss: 10.065342\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 12.330118\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 24.225526\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 18.979509\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 16.928179\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 17.120632\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 16.218082\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 14.832432\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 14.239287\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 13.509214\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 12.744043\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 12.564072\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 12.772046\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 12.470391\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 12.615205\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 13.272519\n",
      "Epoch: 427/1000... Step: 13664... Loss: 4.543848... Val Loss: 12.965934\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 13.078702\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 20.776450\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 17.184784\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 15.653438\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 16.259597\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 15.899866\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 14.586794\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 13.877946\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 13.377763\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 12.786667\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 12.791425\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 12.812253\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 12.619526\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 12.942726\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 13.609078\n",
      "Epoch: 428/1000... Step: 13696... Loss: 4.927632... Val Loss: 13.397908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 8.875736\n",
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 24.157650\n",
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 18.610833\n",
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 15.875193\n",
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 15.828799\n",
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 14.512320\n",
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 13.063190\n",
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 12.723318\n",
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 11.947860\n",
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 11.232638\n",
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 10.920269\n",
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 10.962151\n",
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 10.751650\n",
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 10.814043\n",
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 11.341340\n",
      "Epoch: 429/1000... Step: 13728... Loss: 2.688434... Val Loss: 11.176624\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 10.440807\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 19.994010\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 16.056500\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 14.242243\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 14.688693\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 13.941859\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 12.668104\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 12.079712\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 11.455198\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 10.838262\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 10.739220\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 10.809539\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 10.639193\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 10.879262\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 11.427306\n",
      "Epoch: 430/1000... Step: 13760... Loss: 3.461921... Val Loss: 11.237584\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 12.166806\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 25.143942\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 19.506972\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 17.258708\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 17.253521\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 16.324164\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 14.907409\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 14.314424\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 13.602094\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 12.838240\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 12.648737\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 12.927901\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 12.612926\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 12.747528\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 13.453316\n",
      "Epoch: 431/1000... Step: 13792... Loss: 4.253771... Val Loss: 13.116694\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 8.490421\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 21.690010\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 16.936425\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 14.453160\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 14.798613\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 13.475452\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 12.081384\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 11.520368\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 10.807206\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 10.134719\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 9.882798\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 10.009471\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 9.841034\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 9.977590\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 10.461051\n",
      "Epoch: 432/1000... Step: 13824... Loss: 3.331860... Val Loss: 10.307827\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 10.161706\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 19.572676\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 15.710093\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 13.705176\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 14.179449\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 13.411438\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 12.103482\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 11.490447\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 10.878618\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 10.287637\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 10.113126\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 10.316796\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 10.160343\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 10.447662\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 11.031242\n",
      "Epoch: 433/1000... Step: 13856... Loss: 3.844751... Val Loss: 10.810864\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 10.576304\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 20.404099\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 16.426779\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 14.385818\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 14.777791\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 14.041017\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 12.714558\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 12.084420\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 11.500204\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 10.874782\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 10.764342\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 10.904632\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 10.758120\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 11.018886\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 11.598131\n",
      "Epoch: 434/1000... Step: 13888... Loss: 3.548600... Val Loss: 11.415601\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 10.461808\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 20.927756\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 16.632801\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 14.671052\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 15.003341\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 14.222401\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 12.906579\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 12.313416\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 11.699939\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 11.063071\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 10.948635\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 11.092446\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 10.928074\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 11.145740\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 11.729741\n",
      "Epoch: 435/1000... Step: 13920... Loss: 3.232861... Val Loss: 11.562937\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 10.815728\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 20.644747\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 16.608330\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 14.528410\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 14.907504\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 14.156007\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 12.809560\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 12.161394\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 11.529967\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 10.910451\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 10.770624\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 10.966327\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 10.785882\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 11.064109\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 11.653345\n",
      "Epoch: 436/1000... Step: 13952... Loss: 3.582568... Val Loss: 11.437517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 9.932824\n",
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 21.917994\n",
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 17.144367\n",
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 14.943714\n",
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 15.321309\n",
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 14.328426\n",
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 12.958056\n",
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 12.228194\n",
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 11.553444\n",
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 10.875266\n",
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 10.712520\n",
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 10.912097\n",
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 10.691887\n",
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 10.910041\n",
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 11.466451\n",
      "Epoch: 437/1000... Step: 13984... Loss: 3.934348... Val Loss: 11.241504\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 10.483459\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 21.170489\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 16.813533\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 14.762977\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 15.115730\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 14.283499\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 12.938513\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 12.272914\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 11.624113\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 10.988376\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 10.853677\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 10.980194\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 10.780330\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 11.034479\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 11.605276\n",
      "Epoch: 438/1000... Step: 14016... Loss: 4.272530... Val Loss: 11.384133\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 13.621888\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 22.952071\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 18.524914\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 16.729754\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 16.899742\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 16.418120\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 15.081055\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 14.526441\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 13.946713\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 13.271114\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 13.225651\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 13.213228\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 13.022872\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 13.267243\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 13.897104\n",
      "Epoch: 439/1000... Step: 14048... Loss: 4.837301... Val Loss: 13.688587\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 12.410233\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 21.217688\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 17.341225\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 15.701957\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 16.120014\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 15.683469\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 14.379349\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 13.740155\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 13.216249\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 12.617763\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 12.602812\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 12.619494\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 12.417117\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 12.705180\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 13.373049\n",
      "Epoch: 440/1000... Step: 14080... Loss: 4.176257... Val Loss: 13.175781\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 8.211876\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 21.390099\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 16.602270\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 14.001325\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 14.379897\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 13.075085\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 11.686868\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 11.083598\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 10.320959\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 9.679065\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 9.371159\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 9.807851\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 9.577780\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 9.773623\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 10.331246\n",
      "Epoch: 441/1000... Step: 14112... Loss: 4.364035... Val Loss: 10.087297\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 9.408423\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 18.995775\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 15.228689\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 13.056248\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 13.398424\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 12.541514\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 11.281304\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 10.757483\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 10.173640\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 9.624936\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 9.456256\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 9.578297\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 9.503248\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 9.764811\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 10.233914\n",
      "Epoch: 442/1000... Step: 14144... Loss: 2.482459... Val Loss: 10.093083\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 10.360711\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 20.799296\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 16.503201\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 14.511278\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 14.884311\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 14.064183\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 12.737260\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 12.063091\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 11.429595\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 10.793482\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 10.671876\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 10.833556\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 10.654898\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 10.914263\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 11.497718\n",
      "Epoch: 443/1000... Step: 14176... Loss: 4.368397... Val Loss: 11.295310\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 12.200918\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 22.181972\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 17.763329\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 15.778169\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 15.951683\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 15.280192\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 13.930544\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 13.469216\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 12.865403\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 12.198040\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 12.088926\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 12.034463\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 11.886023\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 12.100537\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 12.676360\n",
      "Epoch: 444/1000... Step: 14208... Loss: 4.192125... Val Loss: 12.506659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 8.490941\n",
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 24.568694\n",
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 18.754869\n",
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 15.883322\n",
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 15.922556\n",
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 14.276508\n",
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 12.779075\n",
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 12.379759\n",
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 11.572263\n",
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 10.808252\n",
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 10.409502\n",
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 10.642669\n",
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 10.399968\n",
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 10.426939\n",
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 10.969158\n",
      "Epoch: 445/1000... Step: 14240... Loss: 2.966755... Val Loss: 10.743629\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 10.200157\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 18.219031\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 14.910286\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 13.162656\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 13.733688\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 13.094610\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 11.888253\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 11.243915\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 10.668438\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 10.107161\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 10.044366\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 10.144537\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 10.013768\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 10.319786\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 10.853110\n",
      "Epoch: 446/1000... Step: 14272... Loss: 4.607265... Val Loss: 10.653747\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 11.146191\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 22.698048\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 17.955123\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 15.862762\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 15.911559\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 15.055302\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 13.718808\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 13.349824\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 12.665938\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 11.977423\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 11.825117\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 11.770138\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 11.567955\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 11.697253\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 12.281275\n",
      "Epoch: 447/1000... Step: 14304... Loss: 3.460462... Val Loss: 12.088829\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 7.909278\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 17.974612\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 14.331367\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 12.228764\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 12.952260\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 12.003978\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 10.721628\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 10.052721\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 9.452107\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 8.945332\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 8.734134\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 9.134010\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 8.995347\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 9.298358\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 9.857670\n",
      "Epoch: 448/1000... Step: 14336... Loss: 4.654396... Val Loss: 9.678133\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 10.863008\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 21.832337\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 17.158670\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 14.878123\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 15.114409\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 14.160697\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 12.794759\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 12.112804\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 11.462531\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 10.793428\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 10.596703\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 10.856996\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 10.682922\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 10.906841\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 11.501131\n",
      "Epoch: 449/1000... Step: 14368... Loss: 2.881242... Val Loss: 11.282555\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 9.998196\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 19.610112\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 15.786307\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 13.896219\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 14.383698\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 13.625557\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 12.342902\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 11.698870\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 11.087403\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 10.499494\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 10.388020\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 10.474745\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 10.299611\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 10.572634\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 11.134856\n",
      "Epoch: 450/1000... Step: 14400... Loss: 4.073641... Val Loss: 10.931542\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 8.552021\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 23.145008\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 17.781799\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 15.022989\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 15.122452\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 13.649776\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 12.229806\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 11.838891\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 11.006179\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 10.271047\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 9.909071\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 10.156246\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 9.929500\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 10.008005\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 10.545829\n",
      "Epoch: 451/1000... Step: 14432... Loss: 3.338278... Val Loss: 10.299487\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 9.809902\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 18.053211\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 14.701876\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 12.804830\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 13.409212\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 12.734414\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 11.485227\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 10.868179\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 10.305531\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 9.766766\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 9.650758\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 9.759304\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 9.644009\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 9.972319\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 10.487825\n",
      "Epoch: 452/1000... Step: 14464... Loss: 3.592493... Val Loss: 10.292515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 10.511240\n",
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 23.473262\n",
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 18.164301\n",
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 15.862522\n",
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 16.033948\n",
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 15.036853\n",
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 13.645530\n",
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 13.040462\n",
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 12.296099\n",
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 11.540472\n",
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 11.335495\n",
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 11.560063\n",
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 11.295281\n",
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 11.461929\n",
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 12.089350\n",
      "Epoch: 453/1000... Step: 14496... Loss: 3.871163... Val Loss: 11.817866\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 10.624846\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 21.191312\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 16.890724\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 14.853180\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 15.279929\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 14.365303\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 12.990910\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 12.288451\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 11.614861\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 10.982443\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 10.829383\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 10.976923\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 10.748124\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 10.992148\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 11.581604\n",
      "Epoch: 454/1000... Step: 14528... Loss: 5.476619... Val Loss: 11.352311\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 10.367513\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 19.839112\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 15.973514\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 13.971047\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 14.451007\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 13.725010\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 12.406526\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 11.763000\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 11.161506\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 10.572519\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 10.436438\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 10.576927\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 10.401400\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 10.716057\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 11.311366\n",
      "Epoch: 455/1000... Step: 14560... Loss: 3.991777... Val Loss: 11.088722\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 8.137461\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 23.372298\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 18.161981\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 15.125216\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 15.472181\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 13.677410\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 12.163766\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 11.576570\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 10.752476\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 10.037100\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 9.637939\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 9.859615\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 9.749657\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 9.844513\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 10.225237\n",
      "Epoch: 456/1000... Step: 14592... Loss: 3.280321... Val Loss: 10.096046\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 11.743549\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 22.786091\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 17.853370\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 15.872665\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 16.178094\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 15.385110\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 14.025805\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 13.431975\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 12.777535\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 12.068616\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 11.911120\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 11.968030\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 11.732652\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 11.931060\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 12.527749\n",
      "Epoch: 457/1000... Step: 14624... Loss: 4.594089... Val Loss: 12.262062\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 8.828972\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 17.703141\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 14.381663\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 12.345804\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 13.139558\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 12.283404\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 10.951819\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 10.179580\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 9.605652\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 9.124295\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 8.957940\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 9.308470\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 9.171871\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 9.535627\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 10.111704\n",
      "Epoch: 458/1000... Step: 14656... Loss: 5.514443... Val Loss: 9.922786\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 11.805225\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 20.543190\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 16.587238\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 14.697131\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 15.188778\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 14.473906\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 13.113589\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 12.450742\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 11.833306\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 11.240882\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 11.097372\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 11.395172\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 11.208467\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 11.551108\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 12.260158\n",
      "Epoch: 459/1000... Step: 14688... Loss: 5.732034... Val Loss: 12.034269\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 8.493040\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 20.208328\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 15.804320\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 13.503514\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 13.971655\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 12.671995\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 11.338764\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 10.744800\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 10.057957\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 9.433415\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 9.178765\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 9.502091\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 9.319319\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 9.482275\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 10.013613\n",
      "Epoch: 460/1000... Step: 14720... Loss: 3.473906... Val Loss: 9.824676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 10.615484\n",
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 20.870683\n",
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 16.451164\n",
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 14.556539\n",
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 14.902450\n",
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 14.051593\n",
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 12.743347\n",
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 12.163022\n",
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 11.539212\n",
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 10.875875\n",
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 10.746951\n",
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 10.885517\n",
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 10.709013\n",
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 10.927799\n",
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 11.522657\n",
      "Epoch: 461/1000... Step: 14752... Loss: 2.932769... Val Loss: 11.322039\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 11.350659\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 19.206991\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 15.963924\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 13.966514\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 14.624803\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 13.914335\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 12.542971\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 11.749503\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 11.127578\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 10.683738\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 10.507254\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 10.875756\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 10.644903\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 11.032602\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 11.713750\n",
      "Epoch: 462/1000... Step: 14784... Loss: 6.740582... Val Loss: 11.488546\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 8.289165\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 21.420827\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 16.618174\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 14.073605\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 14.515840\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 13.023551\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 11.610777\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 11.024918\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 10.283634\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 9.600069\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 9.305289\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 9.566528\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 9.415918\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 9.558728\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 10.050099\n",
      "Epoch: 463/1000... Step: 14816... Loss: 3.448334... Val Loss: 9.885062\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 10.937855\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 21.557062\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 16.957070\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 15.095497\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 15.522155\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 14.637678\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 13.313311\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 12.724412\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 12.083404\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 11.406632\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 11.250731\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 11.598197\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 11.339631\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 11.528384\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 12.232232\n",
      "Epoch: 464/1000... Step: 14848... Loss: 4.642910... Val Loss: 11.996437\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 10.691403\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 19.943080\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 16.290485\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 14.306728\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 14.792334\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 14.073488\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 12.774497\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 12.191880\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 11.551773\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 10.951580\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 10.821572\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 10.912243\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 10.698945\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 10.975148\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 11.561788\n",
      "Epoch: 465/1000... Step: 14880... Loss: 3.647724... Val Loss: 11.332534\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 10.926394\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 20.042077\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 16.226073\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 14.322660\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 14.851180\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 14.095939\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 12.788635\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 12.150539\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 11.542974\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 10.928747\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 10.822243\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 10.927907\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 10.750666\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 11.062240\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 11.656338\n",
      "Epoch: 466/1000... Step: 14912... Loss: 4.887424... Val Loss: 11.448748\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 7.058809\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 20.303540\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 16.301261\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 13.760612\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 14.547284\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 12.964155\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 11.709025\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 11.335816\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 10.650227\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 10.126805\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 9.790380\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 9.819220\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 9.889833\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 9.966999\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 10.263139\n",
      "Epoch: 467/1000... Step: 14944... Loss: 1.834814... Val Loss: 10.451084\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 10.168226\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 19.475897\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 15.547036\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 13.671509\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 14.176496\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 13.370922\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 12.116957\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 11.507011\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 10.866645\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 10.271267\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 10.107436\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 10.320698\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 10.119960\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 10.410530\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 11.034205\n",
      "Epoch: 468/1000... Step: 14976... Loss: 4.735856... Val Loss: 10.777150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 8.557214\n",
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 20.826277\n",
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 16.246391\n",
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 13.898723\n",
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 14.318702\n",
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 12.911144\n",
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 11.540437\n",
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 10.972825\n",
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 10.196099\n",
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 9.556413\n",
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 9.273740\n",
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 9.540347\n",
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 9.325734\n",
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 9.499202\n",
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 10.041961\n",
      "Epoch: 469/1000... Step: 15008... Loss: 4.984231... Val Loss: 9.786563\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 11.020136\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 21.259643\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 16.725409\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 14.935557\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 15.472380\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 14.637063\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 13.352689\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 12.723908\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 12.073356\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 11.363241\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 11.226165\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 11.482358\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 11.245621\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 11.483896\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 12.146610\n",
      "Epoch: 470/1000... Step: 15040... Loss: 5.078078... Val Loss: 11.900527\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 11.356872\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 19.087593\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 15.835309\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 13.894381\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 14.451841\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 13.763927\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 12.453651\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 11.776456\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 11.158664\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 10.649111\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 10.505045\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 10.668880\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 10.473821\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 10.844670\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 11.466369\n",
      "Epoch: 471/1000... Step: 15072... Loss: 5.394742... Val Loss: 11.219411\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 8.839125\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 19.366217\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 15.652363\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 13.756284\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 14.343388\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 13.420855\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 12.157046\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 11.774089\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 11.156712\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 10.584428\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 10.455516\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 10.348902\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 10.251867\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 10.428480\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 10.935053\n",
      "Epoch: 472/1000... Step: 15104... Loss: 3.007411... Val Loss: 10.926326\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 8.908218\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 19.771953\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 15.591794\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 13.430191\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 14.040084\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 12.831319\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 11.499896\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 10.890824\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 10.132047\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 9.534303\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 9.261374\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 9.621589\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 9.397163\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 9.653631\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 10.275047\n",
      "Epoch: 473/1000... Step: 15136... Loss: 6.000576... Val Loss: 10.006422\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 9.916273\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 18.137847\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 14.759968\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 12.977319\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 13.523420\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 12.793939\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 11.577194\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 11.012598\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 10.462032\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 9.919140\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 9.812858\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 9.890865\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 9.760187\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 10.060079\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 10.597164\n",
      "Epoch: 474/1000... Step: 15168... Loss: 3.104445... Val Loss: 10.414305\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 10.858114\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 19.198461\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 15.671483\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 13.835389\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 14.319079\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 13.606498\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 12.346248\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 11.787148\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 11.178066\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 10.596904\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 10.483500\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 10.609799\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 10.439968\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 10.755880\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 11.352187\n",
      "Epoch: 475/1000... Step: 15200... Loss: 4.305501... Val Loss: 11.139093\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 7.012897\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 18.234985\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 14.637226\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 12.430238\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 13.115371\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 11.821112\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 10.572670\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 10.095390\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 9.464755\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 8.938724\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 8.696732\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 8.749409\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 8.755513\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 8.952302\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 9.331868\n",
      "Epoch: 476/1000... Step: 15232... Loss: 3.120694... Val Loss: 9.353782\n",
      "Validation loss decreased (9.566980 --> 9.353782).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 9.468563\n",
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 18.734427\n",
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 14.963791\n",
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 13.202225\n",
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 13.707009\n",
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 12.876505\n",
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 11.660249\n",
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 11.136596\n",
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 10.542115\n",
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 9.968400\n",
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 9.843728\n",
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 9.989287\n",
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 9.821469\n",
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 10.081913\n",
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 10.673009\n",
      "Epoch: 477/1000... Step: 15264... Loss: 3.673259... Val Loss: 10.477841\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 11.354634\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 19.320647\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 15.956479\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 14.006335\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 14.491595\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 13.747910\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 12.463574\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 11.858996\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 11.224255\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 10.655020\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 10.516998\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 10.664595\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 10.481953\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 10.827038\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 11.432271\n",
      "Epoch: 478/1000... Step: 15296... Loss: 4.949965... Val Loss: 11.193631\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 7.031937\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 19.259872\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 15.686790\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 13.331303\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 14.174337\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 12.653699\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 11.411102\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 11.051282\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 10.396282\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 9.868866\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 9.577334\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 9.579714\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 9.695903\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 9.788396\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 10.112831\n",
      "Epoch: 479/1000... Step: 15328... Loss: 2.133073... Val Loss: 10.353049\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 9.489996\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 19.390624\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 15.340905\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 13.571597\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 14.047629\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 13.164359\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 11.938683\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 11.393979\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 10.780545\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 10.170408\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 10.043544\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 10.242251\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 10.058408\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 10.297940\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 10.921289\n",
      "Epoch: 480/1000... Step: 15360... Loss: 3.694075... Val Loss: 10.712800\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 8.558609\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 18.331966\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 14.860722\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 12.764464\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 13.375100\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 12.363131\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 11.093254\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 10.591822\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 9.995088\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 9.409526\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 9.213416\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 9.323540\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 9.250335\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 9.491517\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 10.001614\n",
      "Epoch: 481/1000... Step: 15392... Loss: 3.518518... Val Loss: 9.899675\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 9.484130\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 19.589097\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 15.496701\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 13.635159\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 14.156991\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 13.166851\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 11.915071\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 11.314332\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 10.674046\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 10.052033\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 9.895091\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 10.186269\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 9.966621\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 10.185825\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 10.790453\n",
      "Epoch: 482/1000... Step: 15424... Loss: 4.541005... Val Loss: 10.574793\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 9.959657\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 17.229988\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 14.191369\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 12.442785\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 13.230931\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 12.507469\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 11.238928\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 10.523311\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 9.943298\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 9.483904\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 9.340212\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 9.658333\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 9.517020\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 9.915407\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 10.557024\n",
      "Epoch: 483/1000... Step: 15456... Loss: 6.169111... Val Loss: 10.364340\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 9.697421\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 17.669053\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 14.504742\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 12.693161\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 13.417807\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 12.651425\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 11.385199\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 10.754082\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 10.185046\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 9.665333\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 9.544182\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 9.847658\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 9.731583\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 10.074598\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 10.730727\n",
      "Epoch: 484/1000... Step: 15488... Loss: 4.028543... Val Loss: 10.588216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 10.259396\n",
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 19.488305\n",
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 15.924726\n",
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 14.144926\n",
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 14.539382\n",
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 13.703689\n",
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 12.476322\n",
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 12.124423\n",
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 11.530537\n",
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 10.972353\n",
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 10.861655\n",
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 10.715728\n",
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 10.629650\n",
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 10.821637\n",
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 11.338346\n",
      "Epoch: 485/1000... Step: 15520... Loss: 3.329533... Val Loss: 11.271253\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 10.610329\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 18.810719\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 15.303726\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 13.666157\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 14.312588\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 13.603518\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 12.368467\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 11.764582\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 11.160726\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 10.583711\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 10.478020\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 10.697502\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 10.486030\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 10.801494\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 11.468998\n",
      "Epoch: 486/1000... Step: 15552... Loss: 5.143663... Val Loss: 11.237138\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 8.501339\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 18.085020\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 14.725488\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 12.935843\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 13.574457\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 12.635399\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 11.429470\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 11.075734\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 10.485068\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 9.942854\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 9.814889\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 9.741775\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 9.683011\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 9.872100\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 10.373003\n",
      "Epoch: 487/1000... Step: 15584... Loss: 2.856626... Val Loss: 10.361948\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 9.820996\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 17.828621\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 14.584564\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 12.825109\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 13.492447\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 12.798423\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 11.576744\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 10.947643\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 10.350220\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 9.812681\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 9.687485\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 9.977309\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 9.788393\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 10.124994\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 10.759177\n",
      "Epoch: 488/1000... Step: 15616... Loss: 4.223340... Val Loss: 10.538503\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 9.596547\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 18.367828\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 14.865268\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 12.920998\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 13.585555\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 12.709625\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 11.436011\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 10.788549\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 10.183364\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 9.617040\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 9.459353\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 9.770198\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 9.623534\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 9.929816\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 10.517236\n",
      "Epoch: 489/1000... Step: 15648... Loss: 3.703207... Val Loss: 10.360199\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 9.365580\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 18.501322\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 14.911311\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 13.113906\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 13.720892\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 12.876217\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 11.634941\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 11.048248\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 10.442375\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 9.867900\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 9.755696\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 9.847741\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 9.713677\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 9.999904\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 10.547377\n",
      "Epoch: 490/1000... Step: 15680... Loss: 4.506119... Val Loss: 10.400306\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 10.422680\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 19.512137\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 15.719903\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 13.749289\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 14.299721\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 13.442174\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 12.151798\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 11.561260\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 10.911559\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 10.307905\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 10.153879\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 10.354151\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 10.171004\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 10.488351\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 11.107683\n",
      "Epoch: 491/1000... Step: 15712... Loss: 4.100369... Val Loss: 10.897602\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 7.760749\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 20.182983\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 16.091690\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 13.649010\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 14.210772\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 12.642354\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 11.291548\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 10.756960\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 10.073113\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 9.491336\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 9.200442\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 9.317687\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 9.305128\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 9.437930\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 9.803034\n",
      "Epoch: 492/1000... Step: 15744... Loss: 2.830220... Val Loss: 9.739193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 8.660512\n",
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 18.238024\n",
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 14.440769\n",
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 12.727525\n",
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 13.379692\n",
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 12.562015\n",
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 11.348571\n",
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 10.764769\n",
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 10.196447\n",
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 9.646245\n",
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 9.528610\n",
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 10.007209\n",
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 9.833528\n",
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 10.095843\n",
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 10.732502\n",
      "Epoch: 493/1000... Step: 15776... Loss: 3.608602... Val Loss: 10.594512\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 10.762346\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 18.639940\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 15.243824\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 13.569313\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 14.157952\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 13.493305\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 12.260789\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 11.607682\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 11.038596\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 10.490744\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 10.423391\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 10.478392\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 10.329188\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 10.680151\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 11.256673\n",
      "Epoch: 494/1000... Step: 15808... Loss: 5.344621... Val Loss: 11.053599\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 12.809134\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 20.493595\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 16.868202\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 15.521092\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 16.093690\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 15.748910\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 14.539875\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 13.988224\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 13.485797\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 12.887572\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 12.926048\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 12.823213\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 12.641238\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 12.953521\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 13.610403\n",
      "Epoch: 495/1000... Step: 15840... Loss: 5.196379... Val Loss: 13.445253\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 10.230828\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 19.247179\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 15.721463\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 14.022094\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 14.430283\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 13.725725\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 12.495189\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 12.000905\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 11.440070\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 10.899861\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 10.820300\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 10.725906\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 10.554214\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 10.815004\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 11.398951\n",
      "Epoch: 496/1000... Step: 15872... Loss: 3.987767... Val Loss: 11.224955\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 13.366934\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 21.142095\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 17.347778\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 16.028423\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 16.599138\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 16.214920\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 15.017372\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 14.425972\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 13.921048\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 13.257532\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 13.325113\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 13.232104\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 13.086886\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 13.389065\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 14.052509\n",
      "Epoch: 497/1000... Step: 15904... Loss: 5.353886... Val Loss: 13.935316\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 9.828014\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 18.381213\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 14.802866\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 13.118973\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 13.642609\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 12.965707\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 11.767756\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 11.265830\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 10.704003\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 10.139494\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 10.040589\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 10.282406\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 10.123241\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 10.417496\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 11.055619\n",
      "Epoch: 498/1000... Step: 15936... Loss: 4.437814... Val Loss: 10.884064\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 7.122487\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 19.613347\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 16.142280\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 13.797307\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 14.762988\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 13.050628\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 11.761557\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 11.416137\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 10.777646\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 10.240238\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 9.939250\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 10.053979\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 10.208010\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 10.255391\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 10.577506\n",
      "Epoch: 499/1000... Step: 15968... Loss: 1.752009... Val Loss: 10.847400\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 9.903445\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 20.103311\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 15.956789\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 14.048990\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 14.315450\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 13.474905\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 12.245266\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 11.878806\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 11.211447\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 10.580904\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 10.418317\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 10.501246\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 10.302151\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 10.499939\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 11.097285\n",
      "Epoch: 500/1000... Step: 16000... Loss: 3.642293... Val Loss: 10.895619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 9.615649\n",
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 17.517992\n",
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 14.397058\n",
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 12.632686\n",
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 13.501990\n",
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 12.775365\n",
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 11.524367\n",
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 10.812117\n",
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 10.230279\n",
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 9.699469\n",
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 9.577241\n",
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 9.822470\n",
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 9.641239\n",
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 10.035958\n",
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 10.654525\n",
      "Epoch: 501/1000... Step: 16032... Loss: 4.913225... Val Loss: 10.442876\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 7.569258\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 19.712843\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 15.738858\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 13.381350\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 14.125129\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 12.577310\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 11.238023\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 10.726337\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 9.986008\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 9.324724\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 9.009938\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 9.263871\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 9.163302\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 9.318986\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 9.805985\n",
      "Epoch: 502/1000... Step: 16064... Loss: 3.509675... Val Loss: 9.751973\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 9.045066\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 19.412583\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 15.399584\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 13.260771\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 13.784929\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 12.617445\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 11.263945\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 10.676973\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 9.960682\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 9.413807\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 9.155731\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 9.343484\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 9.158227\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 9.430787\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 10.012130\n",
      "Epoch: 503/1000... Step: 16096... Loss: 5.177646... Val Loss: 9.816747\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 8.055033\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 21.135154\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 16.285857\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 13.677705\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 14.231389\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 12.648311\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 11.235723\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 10.648939\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 9.893101\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 9.211179\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 8.873612\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 9.250636\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 9.108709\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 9.261546\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 9.736764\n",
      "Epoch: 504/1000... Step: 16128... Loss: 3.629255... Val Loss: 9.583935\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 8.240949\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 21.861821\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 16.881114\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 14.388247\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 14.703952\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 13.131270\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 11.765734\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 11.389591\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 10.599296\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 9.905040\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 9.553320\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 9.595699\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 9.447913\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 9.521175\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 9.964557\n",
      "Epoch: 505/1000... Step: 16160... Loss: 2.686019... Val Loss: 9.820991\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 9.482865\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 18.918093\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 15.136939\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 13.285222\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 13.985230\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 13.090619\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 11.830750\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 11.219919\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 10.623716\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 10.045551\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 9.904540\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 10.082161\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 9.894307\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 10.175659\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 10.771906\n",
      "Epoch: 506/1000... Step: 16192... Loss: 4.170258... Val Loss: 10.576424\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 11.214024\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 18.582599\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 15.453407\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 13.428923\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 14.158679\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 13.418450\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 12.114280\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 11.402240\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 10.774333\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 10.287020\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 10.123154\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 10.366241\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 10.188088\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 10.627517\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 11.240022\n",
      "Epoch: 507/1000... Step: 16224... Loss: 6.016992... Val Loss: 11.008463\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 7.444751\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 19.898126\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 15.693041\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 13.187487\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 13.966613\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 12.361401\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 10.981333\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 10.419795\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 9.691379\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 9.044238\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 8.705032\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 8.941019\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 8.857057\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 9.058506\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 9.510293\n",
      "Epoch: 508/1000... Step: 16256... Loss: 3.576259... Val Loss: 9.432568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 9.649570\n",
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 18.019193\n",
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 14.721422\n",
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 12.540076\n",
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 13.217382\n",
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 12.296803\n",
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 11.005213\n",
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 10.352546\n",
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 9.750857\n",
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 9.278452\n",
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 9.059721\n",
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 9.270885\n",
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 9.133976\n",
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 9.525555\n",
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 10.063723\n",
      "Epoch: 509/1000... Step: 16288... Loss: 4.231841... Val Loss: 9.865972\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 7.860337\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 22.646065\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 17.266840\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 14.408440\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 14.904306\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 13.154045\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 11.673880\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 11.072051\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 10.273657\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 9.547283\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 9.162112\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 9.408889\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 9.265494\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 9.399935\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 9.816682\n",
      "Epoch: 510/1000... Step: 16320... Loss: 3.164794... Val Loss: 9.670491\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 15.310736\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 22.508354\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 18.625484\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 17.046371\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 17.429452\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 17.116670\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 15.798881\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 15.204142\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 14.699045\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 14.085545\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 14.062236\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 14.071735\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 13.849294\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 14.218529\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 14.904434\n",
      "Epoch: 511/1000... Step: 16352... Loss: 5.554612... Val Loss: 14.657990\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 6.452575\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 19.086265\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 15.416786\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 13.081033\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 14.022647\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 12.435275\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 11.094430\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 10.608131\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 9.909652\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 9.313644\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 9.059214\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 9.081339\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 9.074666\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 9.205727\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 9.539214\n",
      "Epoch: 512/1000... Step: 16384... Loss: 2.393672... Val Loss: 9.635427\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 9.979558\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 18.935330\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 15.279698\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 13.398468\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 13.951118\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 13.096214\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 11.863640\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 11.332816\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 10.672143\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 10.084826\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 9.923800\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 10.037559\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 9.835488\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 10.151865\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 10.742233\n",
      "Epoch: 513/1000... Step: 16416... Loss: 4.490338... Val Loss: 10.491524\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 9.975307\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 17.931242\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 14.644936\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 12.956548\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 13.637350\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 12.917432\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 11.714789\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 11.088371\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 10.526209\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 9.995059\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 9.907762\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 9.991406\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 9.833409\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 10.193258\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 10.768595\n",
      "Epoch: 514/1000... Step: 16448... Loss: 4.741685... Val Loss: 10.551492\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 7.592786\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 18.036172\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 14.386728\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 12.191793\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 12.882746\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 11.765241\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 10.521556\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 10.011581\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 9.412717\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 8.855775\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 8.600592\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 8.817818\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 8.735488\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 8.965354\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 9.435327\n",
      "Epoch: 515/1000... Step: 16480... Loss: 3.158241... Val Loss: 9.330313\n",
      "Validation loss decreased (9.353782 --> 9.330313).  Saving model ...\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 9.988754\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 19.573229\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 15.427720\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 13.716448\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 14.552527\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 13.532915\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 12.275863\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 11.532115\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 10.896229\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 10.262304\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 10.150831\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 10.574747\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 10.344645\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 10.624432\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 11.252345\n",
      "Epoch: 516/1000... Step: 16512... Loss: 5.517472... Val Loss: 11.052048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 10.749780\n",
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 16.499926\n",
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 13.951109\n",
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 12.207434\n",
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 13.072290\n",
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 12.416474\n",
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 11.134187\n",
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 10.411230\n",
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 9.857365\n",
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 9.542116\n",
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 9.351632\n",
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 9.652094\n",
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 9.544669\n",
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 10.069422\n",
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 10.727042\n",
      "Epoch: 517/1000... Step: 16544... Loss: 7.711607... Val Loss: 10.563992\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 8.396170\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 18.192411\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 14.767539\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 13.003196\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 13.658492\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 12.653158\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 11.472667\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 11.075102\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 10.463547\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 9.910813\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 9.777364\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 9.697597\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 9.665210\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 9.864629\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 10.370394\n",
      "Epoch: 518/1000... Step: 16576... Loss: 3.132323... Val Loss: 10.380347\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 10.120775\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 17.605715\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 14.406113\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 12.775793\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 13.471832\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 12.728920\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 11.540803\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 10.940324\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 10.349267\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 9.834356\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 9.716840\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 9.922440\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 9.740756\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 10.100393\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 10.730094\n",
      "Epoch: 519/1000... Step: 16608... Loss: 5.206646... Val Loss: 10.499872\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 7.214744\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 20.364986\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 16.088411\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 13.680786\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 14.488794\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 12.695062\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 11.376736\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 10.999323\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 10.200941\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 9.502325\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 9.143427\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 9.338395\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 9.275264\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 9.351441\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 9.755178\n",
      "Epoch: 520/1000... Step: 16640... Loss: 2.724342... Val Loss: 9.740122\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 8.375475\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 19.495538\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 15.145732\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 12.887110\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 13.740106\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 12.324503\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 10.979769\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 10.431935\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 9.713032\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 9.104986\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 8.761900\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 9.264718\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 9.034235\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 9.227474\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 9.810220\n",
      "Epoch: 521/1000... Step: 16672... Loss: 4.361403... Val Loss: 9.594919\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 13.110112\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 21.210181\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 17.266651\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 15.858780\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 16.284721\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 15.794745\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 14.619341\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 14.132666\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 13.627606\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 13.003898\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 12.998212\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 12.986472\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 12.763507\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 13.011452\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 13.677726\n",
      "Epoch: 522/1000... Step: 16704... Loss: 5.234464... Val Loss: 13.444621\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 8.867029\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 21.492366\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 16.788598\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 14.633042\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 15.133724\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 13.650428\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 12.318777\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 11.875622\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 11.155031\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 10.434582\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 10.112175\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 10.304767\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 10.189421\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 10.261208\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 10.823570\n",
      "Epoch: 523/1000... Step: 16736... Loss: 3.299954... Val Loss: 10.773010\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 9.107439\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 16.631115\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 13.663037\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 11.959640\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 12.863084\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 12.101589\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 10.887412\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 10.224473\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 9.687654\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 9.197955\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 9.064347\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 9.211021\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 9.070100\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 9.478485\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 10.039989\n",
      "Epoch: 524/1000... Step: 16768... Loss: 5.008195... Val Loss: 9.846544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 10.191587\n",
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 19.171204\n",
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 15.364489\n",
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 13.524423\n",
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 14.206362\n",
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 13.248829\n",
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 11.992971\n",
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 11.448779\n",
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 10.796862\n",
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 10.170102\n",
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 10.029256\n",
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 10.234574\n",
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 10.064413\n",
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 10.366693\n",
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 11.010107\n",
      "Epoch: 525/1000... Step: 16800... Loss: 4.233020... Val Loss: 10.818680\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 7.402156\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 19.021376\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 15.521137\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 13.241824\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 14.051580\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 12.411753\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 11.176939\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 10.800433\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 10.161825\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 9.569518\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 9.252569\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 9.340080\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 9.425915\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 9.520599\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 9.853782\n",
      "Epoch: 526/1000... Step: 16832... Loss: 1.960948... Val Loss: 9.907076\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 11.082671\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 19.515371\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 15.655709\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 13.742328\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 14.342844\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 13.575448\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 12.286073\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 11.801134\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 11.240304\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 10.602766\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 10.427082\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 10.695914\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 10.552474\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 10.874587\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 11.545685\n",
      "Epoch: 527/1000... Step: 16864... Loss: 3.109286... Val Loss: 11.344672\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 9.837361\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 17.411882\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 14.225709\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 12.607378\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 13.314799\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 12.508588\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 11.324217\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 10.745024\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 10.162382\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 9.639798\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 9.525625\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 9.622121\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 9.465765\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 9.801246\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 10.383872\n",
      "Epoch: 528/1000... Step: 16896... Loss: 4.938553... Val Loss: 10.191855\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 9.742503\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 17.654204\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 14.387187\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 12.867488\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 13.556475\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 12.823745\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 11.662727\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 11.170670\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 10.616384\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 10.068390\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 9.989189\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 10.030162\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 9.879613\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 10.185627\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 10.777201\n",
      "Epoch: 529/1000... Step: 16928... Loss: 4.576797... Val Loss: 10.597233\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 7.796619\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 20.574341\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 16.216155\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 13.645839\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 14.367463\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 12.583725\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 11.240947\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 10.897456\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 10.132235\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 9.422182\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 9.011837\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 9.251230\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 9.227155\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 9.344985\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 9.734451\n",
      "Epoch: 530/1000... Step: 16960... Loss: 2.802593... Val Loss: 9.647105\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 8.404232\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 19.581981\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 15.125659\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 12.998089\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 13.561585\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 12.196873\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 10.908824\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 10.479208\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 9.784146\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 9.145769\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 8.825273\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 9.094111\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 8.972081\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 9.097134\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 9.606657\n",
      "Epoch: 531/1000... Step: 16992... Loss: 3.277487... Val Loss: 9.448726\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 9.683177\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 19.511398\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 15.189249\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 13.554732\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 14.348649\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 13.275455\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 12.055417\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 11.494487\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 10.863354\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 10.187260\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 10.059460\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 10.422750\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 10.240514\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 10.445591\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 11.091685\n",
      "Epoch: 532/1000... Step: 17024... Loss: 4.216920... Val Loss: 10.913122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 7.682240\n",
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 19.272420\n",
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 15.012640\n",
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 12.904584\n",
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 13.384939\n",
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 11.857451\n",
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 10.605828\n",
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 10.283011\n",
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 9.574524\n",
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 8.984655\n",
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 8.642967\n",
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 8.777422\n",
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 8.676564\n",
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 8.812996\n",
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 9.271415\n",
      "Epoch: 533/1000... Step: 17056... Loss: 3.139723... Val Loss: 9.133184\n",
      "Validation loss decreased (9.330313 --> 9.133184).  Saving model ...\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 8.318609\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 19.309497\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 14.940515\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 12.885172\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 13.472603\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 12.247035\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 10.989896\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 10.654311\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 9.989521\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 9.333553\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 9.043179\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 9.241261\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 9.139138\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 9.286758\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 9.832146\n",
      "Epoch: 534/1000... Step: 17088... Loss: 3.063561... Val Loss: 9.713770\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 7.769274\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 17.881942\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 14.076728\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 12.096173\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 12.939439\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 11.614067\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 10.361509\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 9.814589\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 9.159845\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 8.557402\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 8.320956\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 8.683256\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 8.570128\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 8.787425\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 9.317906\n",
      "Epoch: 535/1000... Step: 17120... Loss: 4.227490... Val Loss: 9.197632\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 8.832954\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 22.492403\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 17.244429\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 14.890643\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 15.187115\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 13.528824\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 12.137304\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 11.842367\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 11.056629\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 10.300950\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 9.904496\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 9.953770\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 9.722955\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 9.761831\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 10.272627\n",
      "Epoch: 536/1000... Step: 17152... Loss: 3.264282... Val Loss: 10.061801\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 13.113111\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 20.831677\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 16.835641\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 15.463937\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 16.294434\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 15.645759\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 14.400581\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 13.896135\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 13.342596\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 12.683868\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 12.625643\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 12.946799\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 12.693669\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 12.987685\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 13.801295\n",
      "Epoch: 537/1000... Step: 17184... Loss: 6.139349... Val Loss: 13.604221\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 8.726786\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 18.726822\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 14.832706\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 13.001824\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 13.456244\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 12.247796\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 11.048787\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 10.811807\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 10.163605\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 9.540764\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 9.293602\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 9.345539\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 9.251960\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 9.415714\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 10.009615\n",
      "Epoch: 538/1000... Step: 17216... Loss: 3.314792... Val Loss: 9.884914\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 9.474751\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 17.462309\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 14.227112\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 12.635647\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 13.356043\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 12.477938\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 11.330498\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 10.818864\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 10.227094\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 9.654144\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 9.540776\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 9.604590\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 9.426687\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 9.707272\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 10.293204\n",
      "Epoch: 539/1000... Step: 17248... Loss: 4.412349... Val Loss: 10.083834\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 9.315805\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 17.519950\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 13.984308\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 12.326937\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 13.058253\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 12.045024\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 10.851188\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 10.315611\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 9.691287\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 9.148127\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 8.965782\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 9.188594\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 9.024970\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 9.319915\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 9.944567\n",
      "Epoch: 540/1000... Step: 17280... Loss: 5.292374... Val Loss: 9.729111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 8.106804\n",
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 17.462543\n",
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 13.775237\n",
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 11.888257\n",
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 12.536946\n",
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 11.289004\n",
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 10.071269\n",
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 9.701568\n",
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 9.051710\n",
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 8.484840\n",
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 8.226369\n",
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 8.443476\n",
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 8.348516\n",
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 8.586530\n",
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 9.155398\n",
      "Epoch: 541/1000... Step: 17312... Loss: 3.786192... Val Loss: 8.993778\n",
      "Validation loss decreased (9.133184 --> 8.993778).  Saving model ...\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 11.359912\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 19.051851\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 15.414351\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 13.867415\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 14.627738\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 13.886566\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 12.724211\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 12.089188\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 11.544306\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 10.913138\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 10.848272\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 11.056327\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 10.856039\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 11.181460\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 11.798230\n",
      "Epoch: 542/1000... Step: 17344... Loss: 4.528635... Val Loss: 11.576769\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 9.770332\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 17.706104\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 14.302887\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 12.764170\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 13.569778\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 12.696665\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 11.536811\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 10.994761\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 10.408511\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 9.831935\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 9.739853\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 9.880359\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 9.736418\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 10.037604\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 10.675485\n",
      "Epoch: 543/1000... Step: 17376... Loss: 3.897298... Val Loss: 10.489131\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 7.340920\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 17.139784\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 13.701668\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 11.677220\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 12.704358\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 11.414281\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 10.202627\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 9.795890\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 9.146864\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 8.542136\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 8.261521\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 8.752045\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 8.624409\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 8.867631\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 9.433466\n",
      "Epoch: 544/1000... Step: 17408... Loss: 3.859106... Val Loss: 9.334496\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 7.751107\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 19.139102\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 15.263850\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 13.050899\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 13.681220\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 12.017580\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 10.733498\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 10.342105\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 9.610334\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 8.959753\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 8.638451\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 8.777949\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 8.770965\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 8.896254\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 9.276251\n",
      "Epoch: 545/1000... Step: 17440... Loss: 3.925227... Val Loss: 9.215916\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 10.784897\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 19.773170\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 15.655789\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 13.938841\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 14.656966\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 13.704555\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 12.493037\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 11.909338\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 11.296806\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 10.663911\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 10.500249\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 10.896526\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 10.653233\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 10.913075\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 11.599813\n",
      "Epoch: 546/1000... Step: 17472... Loss: 4.015749... Val Loss: 11.361124\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 7.817735\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 16.342637\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 13.331662\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 11.630984\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 12.453054\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 11.340044\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 10.210274\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 9.719017\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 9.184967\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 8.656181\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 8.487290\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 8.548389\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 8.544802\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 8.780669\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 9.266529\n",
      "Epoch: 547/1000... Step: 17504... Loss: 4.193666... Val Loss: 9.234190\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 7.185887\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 15.673517\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 12.586689\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 10.663256\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 11.558702\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 10.480015\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 9.341388\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 8.894738\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 8.374101\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 7.882950\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 7.626822\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 7.955273\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 7.942929\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 8.270858\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 8.776716\n",
      "Epoch: 548/1000... Step: 17536... Loss: 3.302128... Val Loss: 8.675659\n",
      "Validation loss decreased (8.993778 --> 8.675659).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 9.326515\n",
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 16.371273\n",
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 13.996461\n",
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 12.407427\n",
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 13.063839\n",
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 12.251733\n",
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 11.229342\n",
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 10.695994\n",
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 10.230656\n",
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 9.747590\n",
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 9.691685\n",
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 9.596246\n",
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 9.604335\n",
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 9.868642\n",
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 10.370481\n",
      "Epoch: 549/1000... Step: 17568... Loss: 4.411329... Val Loss: 10.402431\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 9.533226\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 16.074392\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 13.139981\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 11.578372\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 12.450686\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 11.560861\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 10.372840\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 9.836811\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 9.268167\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 8.846765\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 8.656228\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 8.974887\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 8.838402\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 9.223088\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 9.879559\n",
      "Epoch: 550/1000... Step: 17600... Loss: 4.973529... Val Loss: 9.702146\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 8.018923\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 15.959677\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 12.852379\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 11.158829\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 11.839084\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 10.725589\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 9.608798\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 9.221848\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 8.630981\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 8.106118\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 7.915072\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 8.091863\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 8.064254\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 8.333772\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 8.866347\n",
      "Epoch: 551/1000... Step: 17632... Loss: 3.812713... Val Loss: 8.754575\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 7.737402\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 17.811497\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 13.894664\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 11.971809\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 12.759372\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 11.436566\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 10.202451\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 9.694809\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 9.073109\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 8.471707\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 8.233397\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 8.598688\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 8.505718\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 8.725057\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 9.264523\n",
      "Epoch: 552/1000... Step: 17664... Loss: 3.774186... Val Loss: 9.136229\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 8.397466\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 15.488558\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 12.624333\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 11.103474\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 11.989285\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 11.240216\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 10.109831\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 9.557658\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 9.115309\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 8.656909\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 8.527104\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 8.732169\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 8.665644\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 9.029275\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 9.592431\n",
      "Epoch: 553/1000... Step: 17696... Loss: 3.792857... Val Loss: 9.482592\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 6.621582\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 17.213320\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 13.849036\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 11.966593\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 12.824590\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 11.372664\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 10.223150\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 9.954935\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 9.331701\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 8.754589\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 8.498850\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 8.673022\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 8.732764\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 8.851007\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 9.297473\n",
      "Epoch: 554/1000... Step: 17728... Loss: 1.718664... Val Loss: 9.372680\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 11.225310\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 17.603478\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 14.410528\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 13.150917\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 13.897793\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 13.318642\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 12.236899\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 11.786520\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 11.250176\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 10.680507\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 10.639017\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 10.881273\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 10.718619\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 11.020114\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 11.706245\n",
      "Epoch: 555/1000... Step: 17760... Loss: 3.671178... Val Loss: 11.517514\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 6.951805\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 17.062908\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 14.261681\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 12.291998\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 13.394541\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 11.819466\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 10.631395\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 10.251953\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 9.673400\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 9.178556\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 8.935366\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 9.071627\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 9.258572\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 9.376238\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 9.691697\n",
      "Epoch: 556/1000... Step: 17792... Loss: 1.835358... Val Loss: 9.906965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 8.683042\n",
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 19.169797\n",
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 14.979657\n",
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 13.181173\n",
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 13.704872\n",
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 12.483528\n",
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 11.293150\n",
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 11.143414\n",
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 10.511525\n",
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 9.813253\n",
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 9.557954\n",
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 9.760774\n",
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 9.685351\n",
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 9.788301\n",
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 10.406688\n",
      "Epoch: 557/1000... Step: 17824... Loss: 3.289255... Val Loss: 10.314247\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 7.392373\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 15.347536\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 12.762871\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 10.700960\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 11.720562\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 10.445879\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 9.304429\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 8.772078\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 8.269056\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 7.820830\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 7.513050\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 7.804894\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 7.859333\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 8.216795\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 8.659495\n",
      "Epoch: 558/1000... Step: 17856... Loss: 4.595933... Val Loss: 8.568907\n",
      "Validation loss decreased (8.675659 --> 8.568907).  Saving model ...\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 10.433339\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 17.316589\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 14.110745\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 12.771597\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 13.605015\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 12.971840\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 11.878328\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 11.329367\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 10.869710\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 10.292715\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 10.278535\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 10.443405\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 10.331975\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 10.652897\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 11.313646\n",
      "Epoch: 559/1000... Step: 17888... Loss: 3.044094... Val Loss: 11.155392\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 7.458611\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 16.299802\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 13.072553\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 11.294852\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 12.017530\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 10.785338\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 9.661318\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 9.312276\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 8.696751\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 8.155428\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 7.932885\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 8.051598\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 8.001908\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 8.238302\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 8.733241\n",
      "Epoch: 560/1000... Step: 17920... Loss: 3.039128... Val Loss: 8.632709\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 8.791945\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 17.783873\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 14.153945\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 12.273957\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 13.054205\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 11.817880\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 10.622222\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 10.192136\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 9.621047\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 8.990540\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 8.736797\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 9.198098\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 9.125950\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 9.321752\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 9.920124\n",
      "Epoch: 561/1000... Step: 17952... Loss: 1.670384... Val Loss: 9.831859\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 10.293015\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 15.519182\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 13.164281\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 11.669971\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 12.565628\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 11.917284\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 10.767014\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 10.113828\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 9.590653\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 9.196158\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 9.079868\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 9.292507\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 9.166441\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 9.623900\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 10.242423\n",
      "Epoch: 562/1000... Step: 17984... Loss: 6.659871... Val Loss: 10.058869\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 6.996197\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 17.963234\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 14.874084\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 12.826821\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 13.622172\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 12.067702\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 10.983245\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 10.777100\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 10.161537\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 9.611566\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 9.318019\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 9.353806\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 9.590887\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 9.678831\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 10.039874\n",
      "Epoch: 563/1000... Step: 18016... Loss: 1.546641... Val Loss: 10.303398\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 10.475748\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 17.995028\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 14.424149\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 12.887459\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 13.643710\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 12.881134\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 11.732460\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 11.171856\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 10.679742\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 10.086058\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 9.995606\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 10.243841\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 10.142621\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 10.431706\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 11.099225\n",
      "Epoch: 564/1000... Step: 18048... Loss: 2.537616... Val Loss: 10.927975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 8.795737\n",
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 16.159829\n",
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 13.398277\n",
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 11.729871\n",
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 12.454303\n",
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 11.429765\n",
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 10.303738\n",
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 9.910110\n",
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 9.295083\n",
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 8.786079\n",
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 8.620323\n",
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 8.637674\n",
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 8.547236\n",
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 8.846918\n",
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 9.369898\n",
      "Epoch: 565/1000... Step: 18080... Loss: 4.149591... Val Loss: 9.235483\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 7.821292\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 15.189336\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 12.712247\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 11.241485\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 12.250028\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 11.322477\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 10.212002\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 9.711452\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 9.244642\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 8.728643\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 8.616682\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 8.657335\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 8.635141\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 8.918986\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 9.467038\n",
      "Epoch: 566/1000... Step: 18112... Loss: 3.526603... Val Loss: 9.438790\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 8.687975\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 14.600914\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 12.299129\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 10.776957\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 11.777946\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 10.996192\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 9.872794\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 9.276332\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 8.789540\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 8.359931\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 8.236914\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 8.480905\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 8.396573\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 8.828687\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 9.408310\n",
      "Epoch: 567/1000... Step: 18144... Loss: 4.498506... Val Loss: 9.268458\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 6.372649\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 16.735711\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 13.708432\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 11.867771\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 12.910801\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 11.340476\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 10.153113\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 9.811143\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 9.180564\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 8.625029\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 8.392000\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 8.567119\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 8.638734\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 8.758705\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 9.141117\n",
      "Epoch: 568/1000... Step: 18176... Loss: 1.913716... Val Loss: 9.274591\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 8.056233\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 17.144039\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 13.612938\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 11.826593\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 12.523477\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 11.332708\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 10.195555\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 9.890723\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 9.289932\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 8.688625\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 8.457756\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 8.639970\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 8.591772\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 8.800184\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 9.337795\n",
      "Epoch: 569/1000... Step: 18208... Loss: 3.445792... Val Loss: 9.262657\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 10.947996\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 18.233541\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 14.752305\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 13.265288\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 14.174921\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 13.369092\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 12.215037\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 11.608358\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 11.041263\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 10.436906\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 10.347119\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 10.640212\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 10.483675\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 10.814694\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 11.493265\n",
      "Epoch: 570/1000... Step: 18240... Loss: 3.757394... Val Loss: 11.314573\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 8.383590\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 17.874356\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 14.167202\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 12.377127\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 12.940967\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 11.738433\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 10.584405\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 10.358079\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 9.745466\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 9.140891\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 8.881213\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 9.021325\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 8.939558\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 9.112308\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 9.670251\n",
      "Epoch: 571/1000... Step: 18272... Loss: 3.335073... Val Loss: 9.582317\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 10.127791\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 16.918595\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 13.862170\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 12.555919\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 13.481972\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 12.699093\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 11.587013\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 10.999692\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 10.432806\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 9.876361\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 9.818511\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 10.004859\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 9.847992\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 10.193501\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 10.834466\n",
      "Epoch: 572/1000... Step: 18304... Loss: 5.755672... Val Loss: 10.666226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 6.349558\n",
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 15.933717\n",
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 13.080319\n",
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 11.362994\n",
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 12.470130\n",
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 11.010363\n",
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 9.853544\n",
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 9.506301\n",
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 8.888791\n",
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 8.311011\n",
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 8.081095\n",
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 8.172219\n",
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 8.159712\n",
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 8.322775\n",
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 8.753147\n",
      "Epoch: 573/1000... Step: 18336... Loss: 2.214665... Val Loss: 8.787920\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 9.438128\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 16.192833\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 13.404901\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 11.718585\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 12.629427\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 11.679988\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 10.539468\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 10.104566\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 9.521097\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 8.968848\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 8.808810\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 8.939331\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 8.826231\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 9.175599\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 9.756363\n",
      "Epoch: 574/1000... Step: 18368... Loss: 2.985879... Val Loss: 9.579314\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 6.610168\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 16.583741\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 13.370512\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 11.843700\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 12.744019\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 11.379301\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 10.234673\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 9.934059\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 9.309131\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 8.758954\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 8.597518\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 8.723145\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 8.709229\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 8.836101\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 9.309973\n",
      "Epoch: 575/1000... Step: 18400... Loss: 1.831629... Val Loss: 9.348842\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 7.528071\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 18.218872\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 14.223537\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 12.355864\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 13.100954\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 11.697471\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 10.494312\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 10.183118\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 9.532515\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 8.888897\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 8.609232\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 8.932983\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 8.835199\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 8.945041\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 9.468561\n",
      "Epoch: 576/1000... Step: 18432... Loss: 2.663868... Val Loss: 9.382220\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 7.413229\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 15.579379\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 12.513282\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 10.780606\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 11.771543\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 10.498542\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 9.342525\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 8.777296\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 8.161653\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 7.639189\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 7.438915\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 7.643112\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 7.570364\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 7.857927\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 8.303605\n",
      "Epoch: 577/1000... Step: 18464... Loss: 4.318345... Val Loss: 8.147605\n",
      "Validation loss decreased (8.568907 --> 8.147605).  Saving model ...\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 11.976146\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 19.281920\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 15.822940\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 14.449263\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 15.089246\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 14.325057\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 13.211013\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 12.830035\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 12.252439\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 11.618199\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 11.559926\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 11.593396\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 11.363702\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 11.635623\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 12.284417\n",
      "Epoch: 578/1000... Step: 18496... Loss: 5.860935... Val Loss: 12.060206\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 6.908285\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 14.921077\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 12.275297\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 10.605114\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 11.507500\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 10.343385\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 9.225958\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 8.760141\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 8.253055\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 7.786025\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 7.581982\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 7.684540\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 7.681644\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 7.977725\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 8.448634\n",
      "Epoch: 579/1000... Step: 18528... Loss: 3.742082... Val Loss: 8.389515\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 7.476702\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 16.934450\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 13.407550\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 11.488480\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 12.516970\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 11.050170\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 9.797656\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 9.396480\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 8.693176\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 8.096798\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 7.807086\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 8.308596\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 8.194030\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 8.436161\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 8.988813\n",
      "Epoch: 580/1000... Step: 18560... Loss: 4.453987... Val Loss: 8.871815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 14.539132\n",
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 20.457370\n",
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 17.064926\n",
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 16.168081\n",
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 16.794140\n",
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 16.400632\n",
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 15.397305\n",
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 14.980260\n",
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 14.486698\n",
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 13.837774\n",
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 13.913451\n",
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 13.729425\n",
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 13.528824\n",
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 13.821248\n",
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 14.530527\n",
      "Epoch: 581/1000... Step: 18592... Loss: 7.351013... Val Loss: 14.385896\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 7.202230\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 17.198346\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 13.937335\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 11.987574\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 12.831862\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 11.321639\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 10.172201\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 9.849653\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 9.297727\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 8.723664\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 8.414462\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 8.673896\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 8.741269\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 8.875216\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 9.261166\n",
      "Epoch: 582/1000... Step: 18624... Loss: 1.730796... Val Loss: 9.248416\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 9.798234\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 18.213459\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 14.618159\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 12.652883\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 13.441972\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 12.284376\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 11.053187\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 10.634481\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 10.014314\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 9.389887\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 9.105451\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 9.523213\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 9.394651\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 9.622812\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 10.240378\n",
      "Epoch: 583/1000... Step: 18656... Loss: 2.315915... Val Loss: 10.112793\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 8.778603\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 14.017153\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 12.086574\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 10.472473\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 11.491999\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 10.665898\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 9.586553\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 8.999862\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 8.507853\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 8.139769\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 8.002680\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 8.150482\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 8.076188\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 8.536114\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 9.036765\n",
      "Epoch: 584/1000... Step: 18688... Loss: 5.767966... Val Loss: 8.886591\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 8.379011\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 15.561430\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 12.745135\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 11.404403\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 12.425069\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 11.642153\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 10.605148\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 10.153624\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 9.655509\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 9.100615\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 9.034676\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 9.240461\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 9.167809\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 9.466624\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 10.056469\n",
      "Epoch: 585/1000... Step: 18720... Loss: 2.758437... Val Loss: 10.011642\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 7.115678\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 15.944149\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 13.338272\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 11.872671\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 12.836039\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 11.551487\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 10.472904\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 10.257732\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 9.657434\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 9.162782\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 9.001289\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 8.986115\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 8.979537\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 9.104038\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 9.556891\n",
      "Epoch: 586/1000... Step: 18752... Loss: 2.791058... Val Loss: 9.675557\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 13.671844\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 21.885569\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 17.544633\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 16.515281\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 17.043822\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 16.508311\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 15.516059\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 15.172441\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 14.626042\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 13.877338\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 13.921575\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 13.951938\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 13.732122\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 13.935960\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 14.694098\n",
      "Epoch: 587/1000... Step: 18784... Loss: 5.987617... Val Loss: 14.563922\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 13.475186\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 18.807059\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 16.063004\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 15.066936\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 15.634764\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 15.330661\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 14.247563\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 13.937054\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 13.442769\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 12.910971\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 12.924405\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 12.836193\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 12.639047\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 12.986774\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 13.747337\n",
      "Epoch: 588/1000... Step: 18816... Loss: 5.363853... Val Loss: 13.566917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 8.490903\n",
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 14.775439\n",
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 12.581569\n",
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 11.242905\n",
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 11.935169\n",
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 11.110287\n",
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 10.092026\n",
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 9.707983\n",
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 9.225985\n",
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 8.766491\n",
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 8.655277\n",
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 8.617099\n",
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 8.572405\n",
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 8.874462\n",
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 9.434281\n",
      "Epoch: 589/1000... Step: 18848... Loss: 3.656291... Val Loss: 9.381143\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 7.230621\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 16.751205\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 13.350563\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 11.478300\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 12.287845\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 10.760359\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 9.572260\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 9.292014\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 8.617521\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 8.011591\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 7.718766\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 7.964409\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 7.947931\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 8.142956\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 8.600564\n",
      "Epoch: 590/1000... Step: 18880... Loss: 3.312142... Val Loss: 8.527161\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 13.382206\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 18.447336\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 15.227882\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 14.359264\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 15.311070\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 14.989167\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 13.988784\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 13.489491\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 13.060555\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 12.454846\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 12.484525\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 12.735282\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 12.542005\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 12.872449\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 13.631853\n",
      "Epoch: 591/1000... Step: 18912... Loss: 5.829135... Val Loss: 13.433526\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 8.782529\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 14.668666\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 12.932528\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 11.735419\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 12.467879\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 11.725685\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 10.739547\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 10.383489\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 9.914075\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 9.458429\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 9.403152\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 9.262308\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 9.243453\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 9.488506\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 10.073402\n",
      "Epoch: 592/1000... Step: 18944... Loss: 3.577272... Val Loss: 10.080911\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 8.883610\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 14.059104\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 11.842935\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 10.582949\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 11.697717\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 10.987002\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 9.939726\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 9.405365\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 8.937335\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 8.467300\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 8.400182\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 8.656641\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 8.591115\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 9.016982\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 9.627171\n",
      "Epoch: 593/1000... Step: 18976... Loss: 4.253844... Val Loss: 9.514043\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 7.919326\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 18.307071\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 15.610281\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 13.641110\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 14.658802\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 12.942640\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 11.821435\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 11.705371\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 11.105411\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 10.467601\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 10.140656\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 10.331175\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 10.650453\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 10.679342\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 11.003512\n",
      "Epoch: 594/1000... Step: 19008... Loss: 1.416082... Val Loss: 11.295284\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 7.247342\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 14.129441\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 11.571399\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 10.040691\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 10.978183\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 10.137294\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 9.078973\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 8.614227\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 8.155727\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 7.692076\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 7.537290\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 7.775315\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 7.776782\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 8.090936\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 8.602607\n",
      "Epoch: 595/1000... Step: 19040... Loss: 2.579479... Val Loss: 8.512034\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 7.400984\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 15.700016\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 12.621022\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 10.866941\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 11.804888\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 10.475429\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 9.367574\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 8.901384\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 8.315959\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 7.743999\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 7.523318\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 7.779649\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 7.778601\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 8.003240\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 8.466463\n",
      "Epoch: 596/1000... Step: 19072... Loss: 3.933529... Val Loss: 8.437343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 7.928806\n",
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 14.240547\n",
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 11.854490\n",
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 10.706250\n",
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 11.837267\n",
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 11.068919\n",
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 10.014441\n",
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 9.480347\n",
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 9.033068\n",
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 8.593841\n",
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 8.540396\n",
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 8.592705\n",
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 8.535401\n",
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 8.895569\n",
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 9.509135\n",
      "Epoch: 597/1000... Step: 19104... Loss: 4.597963... Val Loss: 9.485701\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 7.387329\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 18.000748\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 14.213498\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 12.354408\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 12.930370\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 11.338229\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 10.145226\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 10.115064\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 9.383213\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 8.732935\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 8.407435\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 8.560401\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 8.547388\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 8.661240\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 9.167018\n",
      "Epoch: 598/1000... Step: 19136... Loss: 3.091787... Val Loss: 9.070465\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 13.761643\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 20.053001\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 16.657120\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 15.418405\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 16.037820\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 15.502282\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 14.450721\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 13.976295\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 13.466094\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 12.822148\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 12.785892\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 12.812081\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 12.587658\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 12.883332\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 13.534194\n",
      "Epoch: 599/1000... Step: 19168... Loss: 6.453903... Val Loss: 13.358149\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 8.939821\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 15.886657\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 12.935049\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 11.872177\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 12.928511\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 12.132521\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 11.144740\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 10.652228\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 10.149896\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 9.590737\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 9.601246\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 9.778835\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 9.659185\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 9.945077\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 10.559039\n",
      "Epoch: 600/1000... Step: 19200... Loss: 3.558462... Val Loss: 10.435094\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 6.338191\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 15.681770\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 12.709603\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 11.143202\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 12.192404\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 10.820923\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 9.670011\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 9.238022\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 8.686697\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 8.143695\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 7.970987\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 8.189854\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 8.214266\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 8.389164\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 8.848646\n",
      "Epoch: 601/1000... Step: 19232... Loss: 1.800116... Val Loss: 8.875465\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 7.370151\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 15.558759\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 12.498117\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 10.685685\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 11.546516\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 10.279054\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 9.132287\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 8.671095\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 8.078706\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 7.573549\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 7.322678\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 7.556037\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 7.524529\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 7.829723\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 8.328530\n",
      "Epoch: 602/1000... Step: 19264... Loss: 3.933455... Val Loss: 8.212610\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 10.486519\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 16.977985\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 13.997942\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 12.785997\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 13.597929\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 12.851623\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 11.832995\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 11.424491\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 10.892572\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 10.324666\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 10.304943\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 10.348838\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 10.224250\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 10.512087\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 11.106788\n",
      "Epoch: 603/1000... Step: 19296... Loss: 4.581010... Val Loss: 10.974492\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 9.311179\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 16.163763\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 13.628936\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 12.037623\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 12.917422\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 12.000034\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 10.896291\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 10.444805\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 9.960344\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 9.396422\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 9.249903\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 9.200160\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 9.162272\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 9.468490\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 10.065623\n",
      "Epoch: 604/1000... Step: 19328... Loss: 5.109921... Val Loss: 10.029081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 7.464688\n",
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 15.140804\n",
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 12.213474\n",
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 10.497853\n",
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 11.532406\n",
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 10.300514\n",
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 9.118838\n",
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 8.652993\n",
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 8.046095\n",
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 7.553980\n",
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 7.315591\n",
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 7.719239\n",
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 7.663864\n",
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 8.007358\n",
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 8.561668\n",
      "Epoch: 605/1000... Step: 19360... Loss: 5.028015... Val Loss: 8.429240\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 7.517153\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 17.065572\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 14.719372\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 12.939882\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 13.986308\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 12.377682\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 11.299697\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 11.131416\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 10.574151\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 10.085092\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 9.830910\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 10.052086\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 10.422797\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 10.462564\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 10.809329\n",
      "Epoch: 606/1000... Step: 19392... Loss: 1.123601... Val Loss: 11.233938\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 9.243435\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 16.354720\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 13.416856\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 11.615603\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 12.309353\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 11.416116\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 10.311123\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 9.944129\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 9.488803\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 8.919651\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 8.729622\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 8.714053\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 8.740403\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 9.024724\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 9.558896\n",
      "Epoch: 607/1000... Step: 19424... Loss: 1.829371... Val Loss: 9.444170\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 7.299190\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 15.335685\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 12.659765\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 10.928305\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 11.793852\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 10.673681\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 9.607799\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 9.191772\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 8.605884\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 8.043958\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 7.842048\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 8.000487\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 7.996151\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 8.238914\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 8.710123\n",
      "Epoch: 608/1000... Step: 19456... Loss: 3.354916... Val Loss: 8.673340\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 9.729639\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 17.366343\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 13.965667\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 12.593817\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 13.495898\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 12.566856\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 11.502354\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 10.998457\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 10.419024\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 9.789067\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 9.730554\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 10.065614\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 9.945387\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 10.203867\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 10.846645\n",
      "Epoch: 609/1000... Step: 19488... Loss: 4.334911... Val Loss: 10.709176\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 10.211329\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 15.354464\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 13.016896\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 11.609385\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 12.614691\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 11.856400\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 10.757320\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 10.146029\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 9.628047\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 9.188963\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 9.084987\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 9.245287\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 9.116326\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 9.571478\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 10.191149\n",
      "Epoch: 610/1000... Step: 19520... Loss: 5.562935... Val Loss: 9.994906\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 6.775934\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 16.689075\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 13.639768\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 11.775443\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 12.478276\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 10.983273\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 9.898932\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 9.654902\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 9.082300\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 8.567982\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 8.316536\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 8.457099\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 8.604886\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 8.746115\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 9.096907\n",
      "Epoch: 611/1000... Step: 19552... Loss: 1.503800... Val Loss: 9.178885\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 8.936676\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 15.241039\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 12.751145\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 11.582618\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 12.372251\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 11.663251\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 10.660468\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 10.313829\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 9.854491\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 9.379179\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 9.356811\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 9.301858\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 9.241602\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 9.506666\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 10.086362\n",
      "Epoch: 612/1000... Step: 19584... Loss: 3.436823... Val Loss: 10.009145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 6.842572\n",
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 16.861037\n",
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 13.808964\n",
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 12.006053\n",
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 12.837431\n",
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 11.245894\n",
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 10.149915\n",
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 9.981106\n",
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 9.333388\n",
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 8.713270\n",
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 8.415949\n",
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 8.653096\n",
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 8.806628\n",
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 8.915137\n",
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 9.301109\n",
      "Epoch: 613/1000... Step: 19616... Loss: 1.865520... Val Loss: 9.393432\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 14.643003\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 17.871628\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 15.508998\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 14.555799\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 15.324195\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 15.143141\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 14.116318\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 13.568255\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 13.178124\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 12.710859\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 12.761782\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 12.745989\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 12.554099\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 13.003956\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 13.733388\n",
      "Epoch: 614/1000... Step: 19648... Loss: 6.472481... Val Loss: 13.558697\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 8.025872\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 15.838044\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 12.779935\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 11.314000\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 12.069218\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 11.005878\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 9.952514\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 9.642602\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 9.083540\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 8.513180\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 8.363512\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 8.511198\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 8.467204\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 8.715404\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 9.278545\n",
      "Epoch: 615/1000... Step: 19680... Loss: 4.142853... Val Loss: 9.201028\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 11.389585\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 18.200490\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 14.730452\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 13.529850\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 14.608315\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 13.799206\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 12.727418\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 12.141769\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 11.570538\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 10.937371\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 10.902736\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 11.240904\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 11.069383\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 11.394430\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 12.095240\n",
      "Epoch: 616/1000... Step: 19712... Loss: 4.514966... Val Loss: 11.935217\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 6.665566\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 15.181383\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 12.869840\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 11.195756\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 12.197662\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 10.755275\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 9.732231\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 9.529529\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 9.015015\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 8.472090\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 8.205383\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 8.253453\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 8.377885\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 8.521953\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 8.903396\n",
      "Epoch: 617/1000... Step: 19744... Loss: 1.196728... Val Loss: 8.997206\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 7.026488\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 15.517497\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 12.830162\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 11.150918\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 12.254758\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 10.773446\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 9.633398\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 9.228716\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 8.692202\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 8.138627\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 7.902383\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 8.258358\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 8.395855\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 8.563554\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 8.903747\n",
      "Epoch: 618/1000... Step: 19776... Loss: 1.992569... Val Loss: 8.979831\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 9.928510\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 16.172060\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 13.175979\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 11.796414\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 12.547100\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 11.613201\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 10.576853\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 10.263022\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 9.700684\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 9.169265\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 8.995848\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 9.127585\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 9.018204\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 9.274186\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 9.876459\n",
      "Epoch: 619/1000... Step: 19808... Loss: 4.050920... Val Loss: 9.722278\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 6.731369\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 16.524907\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 13.680689\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 11.949543\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 12.930542\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 11.416389\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 10.293418\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 10.107633\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 9.507837\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 8.889921\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 8.581851\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 8.788265\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 8.941934\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 9.039395\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 9.446662\n",
      "Epoch: 620/1000... Step: 19840... Loss: 1.625406... Val Loss: 9.568669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 9.269992\n",
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 16.520888\n",
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 13.539653\n",
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 12.422659\n",
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 13.122018\n",
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 12.222441\n",
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 11.257675\n",
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 11.046352\n",
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 10.524700\n",
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 9.986298\n",
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 9.952035\n",
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 9.833970\n",
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 9.799429\n",
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 9.972634\n",
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 10.540890\n",
      "Epoch: 621/1000... Step: 19872... Loss: 3.668048... Val Loss: 10.523452\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 9.419635\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 18.493941\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 15.408685\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 13.299844\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 14.363528\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 12.915509\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 11.802762\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 11.381126\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 10.907713\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 10.356113\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 9.956095\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 10.458199\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 10.540635\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 10.765931\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 11.132764\n",
      "Epoch: 622/1000... Step: 19904... Loss: 8.181684... Val Loss: 10.984244\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 6.574907\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 12.724183\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 10.651225\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 9.283204\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 10.337589\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 9.484873\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 8.523645\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 8.129645\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 7.784539\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 7.333491\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 7.237675\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 7.517272\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 7.660306\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 7.976895\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 8.470436\n",
      "Epoch: 623/1000... Step: 19936... Loss: 1.699602... Val Loss: 8.501964\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 7.565025\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 16.074637\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 12.894479\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 11.184685\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 11.996297\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 10.874782\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 9.792646\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 9.365586\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 8.822904\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 8.277130\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 8.083088\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 8.201669\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 8.213405\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 8.405636\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 8.912786\n",
      "Epoch: 624/1000... Step: 19968... Loss: 2.739137... Val Loss: 8.862123\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 9.657187\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 15.305458\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 12.522883\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 11.388804\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 12.475981\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 11.609875\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 10.593062\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 10.065654\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 9.553106\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 8.998166\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 8.934070\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 9.230766\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 9.107002\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 9.407559\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 10.001843\n",
      "Epoch: 625/1000... Step: 20000... Loss: 4.937011... Val Loss: 9.850224\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 7.396175\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 14.111843\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 11.456692\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 10.042928\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 10.939919\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 9.767801\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 8.706400\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 8.338733\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 7.784678\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 7.303049\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 7.120234\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 7.344546\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 7.335131\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 7.617692\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 8.138110\n",
      "Epoch: 626/1000... Step: 20032... Loss: 3.498865... Val Loss: 8.004577\n",
      "Validation loss decreased (8.147605 --> 8.004577).  Saving model ...\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 6.208336\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 13.514750\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 11.581734\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 10.109089\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 11.211103\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 9.949042\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 8.960567\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 8.649070\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 8.224444\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 7.762214\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 7.582944\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 7.760391\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 8.004944\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 8.191501\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 8.578401\n",
      "Epoch: 627/1000... Step: 20064... Loss: 1.664409... Val Loss: 8.747695\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 7.027617\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 15.098299\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 12.490055\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 10.579070\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 11.644265\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 10.351776\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 9.246739\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 8.794181\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 8.289349\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 7.783001\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 7.486214\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 7.732638\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 7.796181\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 8.139572\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 8.543647\n",
      "Epoch: 628/1000... Step: 20096... Loss: 5.680767... Val Loss: 8.437606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 6.842441\n",
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 12.740310\n",
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 10.808386\n",
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 9.654283\n",
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 10.828968\n",
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 9.869881\n",
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 8.894864\n",
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 8.454328\n",
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 7.952571\n",
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 7.490901\n",
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 7.429501\n",
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 7.695808\n",
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 7.598141\n",
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 7.886376\n",
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 8.400793\n",
      "Epoch: 629/1000... Step: 20128... Loss: 3.824998... Val Loss: 8.315658\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 10.498802\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 16.106187\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 13.232557\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 12.091333\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 13.282157\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 12.543163\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 11.503048\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 10.955231\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 10.474506\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 9.898709\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 9.869103\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 10.234769\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 10.087395\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 10.442705\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 11.138228\n",
      "Epoch: 630/1000... Step: 20160... Loss: 5.957596... Val Loss: 11.020335\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 7.105424\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 16.083974\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 13.776535\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 12.229272\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 13.272099\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 11.739240\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 10.695862\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 10.576040\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 10.067929\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 9.566780\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 9.313075\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 9.519288\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 9.805870\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 9.882372\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 10.237955\n",
      "Epoch: 631/1000... Step: 20192... Loss: 1.048247... Val Loss: 10.534464\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 9.088146\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 14.252291\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 11.758131\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 10.660744\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 11.842129\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 11.230820\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 10.223631\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 9.681113\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 9.248281\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 8.756370\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 8.720577\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 8.968853\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 8.907913\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 9.261233\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 9.887183\n",
      "Epoch: 632/1000... Step: 20224... Loss: 3.178581... Val Loss: 9.727974\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 6.564322\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 15.219791\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 12.847334\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 11.524343\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 12.543827\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 10.991163\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 9.884148\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 9.678534\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 9.092951\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 8.549375\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 8.331642\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 8.556009\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 8.654800\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 8.753084\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 9.147259\n",
      "Epoch: 633/1000... Step: 20256... Loss: 1.867570... Val Loss: 9.265215\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 9.460358\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 15.588374\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 12.678337\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 11.371808\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 12.237143\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 11.281490\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 10.262986\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 9.940534\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 9.413646\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 8.872399\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 8.720448\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 8.915072\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 8.837645\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 9.072368\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 9.702336\n",
      "Epoch: 634/1000... Step: 20288... Loss: 3.436754... Val Loss: 9.561700\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 6.482295\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 15.032853\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 12.884065\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 11.389576\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 12.389596\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 10.976379\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 9.943741\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 9.760557\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 9.234277\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 8.714062\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 8.494132\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 8.585646\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 8.801983\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 8.906077\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 9.318505\n",
      "Epoch: 635/1000... Step: 20320... Loss: 1.413501... Val Loss: 9.515576\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 11.058096\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 15.752781\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 13.344551\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 12.515879\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 13.323319\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 12.997104\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 12.166534\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 11.961054\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 11.492957\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 10.961931\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 11.058636\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 11.115603\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 10.999535\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 11.233881\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 11.887030\n",
      "Epoch: 636/1000... Step: 20352... Loss: 3.420686... Val Loss: 11.760599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 6.357612\n",
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 13.913640\n",
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 11.524662\n",
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 10.049025\n",
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 11.073875\n",
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 9.787743\n",
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 8.741513\n",
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 8.395475\n",
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 7.848759\n",
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 7.333660\n",
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 7.144327\n",
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 7.291310\n",
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 7.335563\n",
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 7.549273\n",
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 7.963766\n",
      "Epoch: 637/1000... Step: 20384... Loss: 2.432413... Val Loss: 7.918847\n",
      "Validation loss decreased (8.004577 --> 7.918847).  Saving model ...\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 7.981345\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 16.007711\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 12.836483\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 11.338175\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 12.076357\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 10.849398\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 9.845121\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 9.626767\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 9.041595\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 8.398346\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 8.179805\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 8.382673\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 8.418044\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 8.574564\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 9.098211\n",
      "Epoch: 638/1000... Step: 20416... Loss: 3.913221... Val Loss: 9.108896\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 7.016722\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 13.398505\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 11.572151\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 9.923544\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 11.128643\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 9.997650\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 9.120866\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 8.714913\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 8.390737\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 7.983643\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 7.738071\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 7.992385\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 8.211812\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 8.546312\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 8.916892\n",
      "Epoch: 639/1000... Step: 20448... Loss: 4.938628... Val Loss: 8.916807\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 7.556871\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 13.994666\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 11.394100\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 10.004220\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 10.834044\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 9.850394\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 8.833656\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 8.478507\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 8.022936\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 7.538926\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 7.390496\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 7.707105\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 7.753879\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 8.044077\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 8.598783\n",
      "Epoch: 640/1000... Step: 20480... Loss: 2.479313... Val Loss: 8.498163\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 7.406889\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 12.532802\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 10.671924\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 9.370033\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 10.370629\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 9.427179\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 8.447570\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 7.959464\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 7.499815\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 7.091883\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 6.994527\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 7.155555\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 7.141707\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 7.513463\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 8.016658\n",
      "Epoch: 641/1000... Step: 20512... Loss: 4.234824... Val Loss: 7.926470\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 7.998651\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 14.680963\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 11.894523\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 10.741171\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 11.631904\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 10.673503\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 9.729224\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 9.460055\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 8.947048\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 8.396996\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 8.335630\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 8.435514\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 8.402948\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 8.627749\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 9.186422\n",
      "Epoch: 642/1000... Step: 20544... Loss: 2.635162... Val Loss: 9.089797\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 10.505850\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 14.390519\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 12.349850\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 11.278498\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 12.460254\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 11.742086\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 10.686315\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 10.070113\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 9.580865\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 9.187918\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 9.101784\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 9.304490\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 9.152086\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 9.593643\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 10.263719\n",
      "Epoch: 643/1000... Step: 20576... Loss: 6.867618... Val Loss: 10.073568\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 6.695638\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 13.986324\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 11.506780\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 9.975459\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 11.013327\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 9.705584\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 8.647007\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 8.327712\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 7.767418\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 7.225042\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 7.015970\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 7.273604\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 7.305671\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 7.539160\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 8.001473\n",
      "Epoch: 644/1000... Step: 20608... Loss: 2.477669... Val Loss: 7.912348\n",
      "Validation loss decreased (7.918847 --> 7.912348).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 8.453229\n",
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 14.156662\n",
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 11.600536\n",
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 10.283602\n",
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 11.263227\n",
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 10.290908\n",
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 9.315388\n",
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 8.926957\n",
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 8.472507\n",
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 7.937881\n",
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 7.799747\n",
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 8.132400\n",
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 8.138763\n",
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 8.393043\n",
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 8.988302\n",
      "Epoch: 645/1000... Step: 20640... Loss: 1.538084... Val Loss: 8.917643\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 7.344778\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 14.061558\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 11.935103\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 10.131033\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 11.155160\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 10.033835\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 9.134301\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 8.653735\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 8.262787\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 7.905296\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 7.619300\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 7.858181\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 7.983958\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 8.367342\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 8.731164\n",
      "Epoch: 646/1000... Step: 20672... Loss: 6.579969... Val Loss: 8.633411\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 7.429834\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 14.035233\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 11.600394\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 10.314169\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 11.233108\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 10.178432\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 9.121001\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 8.753092\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 8.237565\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 7.732251\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 7.624459\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 7.825063\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 7.796106\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 8.084227\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 8.673933\n",
      "Epoch: 647/1000... Step: 20704... Loss: 4.114530... Val Loss: 8.564078\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 10.577995\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 16.493603\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 13.306077\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 12.294808\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 13.447341\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 12.700152\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 11.755234\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 11.274762\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 10.812880\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 10.214543\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 10.210317\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 10.601532\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 10.462775\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 10.725484\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 11.408083\n",
      "Epoch: 648/1000... Step: 20736... Loss: 3.087020... Val Loss: 11.261472\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 7.754538\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 14.803375\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 11.993001\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 10.519670\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 11.671151\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 10.265141\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 9.108033\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 8.642082\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 8.007973\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 7.483241\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 7.248493\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 7.598436\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 7.522542\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 7.794419\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 8.336744\n",
      "Epoch: 649/1000... Step: 20768... Loss: 4.035786... Val Loss: 8.203128\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 9.509394\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 16.060428\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 13.114036\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 12.191867\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 13.118323\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 12.289008\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 11.398779\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 11.209782\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 10.707753\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 10.115460\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 10.096880\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 10.189173\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 10.094716\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 10.267619\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 10.883434\n",
      "Epoch: 650/1000... Step: 20800... Loss: 5.156306... Val Loss: 10.849380\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 8.920640\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 13.840986\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 11.729114\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 10.478318\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 11.537254\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 10.603521\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 9.539923\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 9.052800\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 8.524943\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 8.122601\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 7.992097\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 8.128615\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 8.024631\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 8.413900\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 9.017600\n",
      "Epoch: 651/1000... Step: 20832... Loss: 5.820518... Val Loss: 8.870302\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 6.402301\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 13.733761\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 11.118828\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 9.900501\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 10.952083\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 9.672259\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 8.658421\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 8.379750\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 7.837751\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 7.319361\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 7.151243\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 7.553649\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 7.573266\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 7.774899\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 8.295403\n",
      "Epoch: 652/1000... Step: 20864... Loss: 2.357522... Val Loss: 8.295732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 7.640047\n",
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 14.673584\n",
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 11.830088\n",
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 10.304616\n",
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 11.403789\n",
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 10.119354\n",
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 9.039588\n",
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 8.630167\n",
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 8.045355\n",
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 7.505088\n",
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 7.279943\n",
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 7.671432\n",
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 7.656374\n",
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 7.891984\n",
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 8.417057\n",
      "Epoch: 653/1000... Step: 20896... Loss: 3.477980... Val Loss: 8.306742\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 6.877482\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 13.572896\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 11.986041\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 10.611827\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 11.641757\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 10.359608\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 9.421069\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 9.202750\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 8.813194\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 8.353204\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 8.159167\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 8.272218\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 8.552958\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 8.682748\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 9.073325\n",
      "Epoch: 654/1000... Step: 20928... Loss: 1.561724... Val Loss: 9.250503\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 8.602022\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 16.142147\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 12.970873\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 11.522897\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 12.365803\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 11.187597\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 10.161592\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 9.907680\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 9.300762\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 8.681858\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 8.489423\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 8.673661\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 8.582690\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 8.805359\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 9.402868\n",
      "Epoch: 655/1000... Step: 20960... Loss: 5.858903... Val Loss: 9.324537\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 7.308706\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 16.202856\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 12.896057\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 11.181630\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 12.008173\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 10.563397\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 9.451856\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 9.150298\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 8.496303\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 7.865731\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 7.581266\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 7.829543\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 7.820123\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 8.003953\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 8.488921\n",
      "Epoch: 656/1000... Step: 20992... Loss: 3.077930... Val Loss: 8.420048\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 7.708490\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 12.332210\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 10.493047\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 9.190985\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 10.274601\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 9.384042\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 8.442440\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 7.998653\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 7.607887\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 7.225265\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 7.115340\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 7.368610\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 7.381664\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 7.756068\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 8.249438\n",
      "Epoch: 657/1000... Step: 21024... Loss: 3.679919... Val Loss: 8.173772\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 6.602012\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 13.958450\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 11.398607\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 10.033529\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 10.914158\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 9.633227\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 8.708335\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 8.605626\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 8.013087\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 7.489787\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 7.331113\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 7.421084\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 7.468237\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 7.660289\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 8.093723\n",
      "Epoch: 658/1000... Step: 21056... Loss: 2.576915... Val Loss: 8.096579\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 8.470308\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 13.705644\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 11.309229\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 9.868197\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 10.989781\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 9.941754\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 8.887286\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 8.410201\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 7.940557\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 7.509301\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 7.333596\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 7.847442\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 7.787675\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 8.140471\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 8.717009\n",
      "Epoch: 659/1000... Step: 21088... Loss: 2.832906... Val Loss: 8.590050\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 7.608243\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 14.162189\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 12.152122\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 10.461982\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 11.921267\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 10.636153\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 9.507562\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 8.917713\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 8.506300\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 8.027089\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 7.763000\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 8.480192\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 8.528986\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 8.886871\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 9.327148\n",
      "Epoch: 660/1000... Step: 21120... Loss: 7.653552... Val Loss: 9.253427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 10.880840\n",
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 14.372723\n",
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 12.095504\n",
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 11.295595\n",
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 12.719594\n",
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 12.286574\n",
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 11.370255\n",
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 10.893242\n",
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 10.537132\n",
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 10.018930\n",
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 10.045996\n",
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 10.338419\n",
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 10.294149\n",
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 10.701493\n",
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 11.434023\n",
      "Epoch: 661/1000... Step: 21152... Loss: 3.603355... Val Loss: 11.266981\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 8.051128\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 14.545307\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 12.092371\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 10.976637\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 11.678911\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 10.627547\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 9.728643\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 9.778096\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 9.260834\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 8.737362\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 8.594209\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 8.593520\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 8.591959\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 8.731630\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 9.294629\n",
      "Epoch: 662/1000... Step: 21184... Loss: 3.069174... Val Loss: 9.278005\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 6.445948\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 13.112833\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 10.876556\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 9.601657\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 11.097519\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 9.755735\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 8.602269\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 8.109371\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 7.527710\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 7.003703\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 6.808127\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 7.436408\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 7.312613\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 7.607303\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 8.146897\n",
      "Epoch: 663/1000... Step: 21216... Loss: 4.737096... Val Loss: 8.054538\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 6.255375\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 10.941537\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 9.808050\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 8.777599\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 9.881022\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 8.941644\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 8.057176\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 7.696494\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 7.397677\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 7.026787\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 6.957144\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 7.091499\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 7.259959\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 7.523159\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 8.014917\n",
      "Epoch: 664/1000... Step: 21248... Loss: 2.171396... Val Loss: 8.089381\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 11.756429\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 17.708026\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 14.175911\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 13.467892\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 14.614543\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 13.989929\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 13.064522\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 12.879012\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 12.320561\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 11.611286\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 11.583846\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 11.896259\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 11.692545\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 11.923644\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 12.726981\n",
      "Epoch: 665/1000... Step: 21280... Loss: 5.208147... Val Loss: 12.557805\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 7.119979\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 13.482695\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 11.219932\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 9.761796\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 11.079673\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 9.678987\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 8.552753\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 8.070419\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 7.484025\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 6.972695\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 6.744271\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 7.150419\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 7.120322\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 7.425635\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 7.918132\n",
      "Epoch: 666/1000... Step: 21312... Loss: 4.240077... Val Loss: 7.827514\n",
      "Validation loss decreased (7.912348 --> 7.827514).  Saving model ...\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 6.673804\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 13.031530\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 10.873651\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 9.557307\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 10.688180\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 9.386903\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 8.370324\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 8.103608\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 7.577915\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 7.054568\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 6.831275\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 7.019689\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 7.059918\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 7.316724\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 7.787197\n",
      "Epoch: 667/1000... Step: 21344... Loss: 2.139873... Val Loss: 7.721469\n",
      "Validation loss decreased (7.827514 --> 7.721469).  Saving model ...\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 7.680411\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 12.510382\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 10.481893\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 9.412046\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 10.419632\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 9.524311\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 8.646710\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 8.439500\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 8.002223\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 7.539509\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 7.412683\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 7.677738\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 7.678422\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 7.935167\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 8.505400\n",
      "Epoch: 668/1000... Step: 21376... Loss: 3.367539... Val Loss: 8.439324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 9.793714\n",
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 14.604830\n",
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 12.171190\n",
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 11.194829\n",
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 12.459477\n",
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 11.665916\n",
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 10.749669\n",
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 10.272174\n",
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 9.753568\n",
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 9.203949\n",
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 9.181752\n",
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 9.405108\n",
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 9.285628\n",
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 9.637269\n",
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 10.272162\n",
      "Epoch: 669/1000... Step: 21408... Loss: 4.847063... Val Loss: 10.142302\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 6.493388\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 13.576844\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 11.601760\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 10.306584\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 11.138237\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 9.911497\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 9.015614\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 8.949727\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 8.489225\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 8.072302\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 7.882409\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 8.016369\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 8.239900\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 8.397659\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 8.807028\n",
      "Epoch: 670/1000... Step: 21440... Loss: 1.410745... Val Loss: 8.981576\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 8.307911\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 11.484864\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 10.007336\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 9.110781\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 10.529313\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 10.234846\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 9.304277\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 8.724139\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 8.458484\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 8.016428\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 8.041888\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 8.286954\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 8.277325\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 8.694897\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 9.277844\n",
      "Epoch: 671/1000... Step: 21472... Loss: 2.520444... Val Loss: 9.196986\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 6.356680\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 12.320093\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 11.028349\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 10.108067\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 11.105266\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 9.964077\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 9.117528\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 8.949446\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 8.469101\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 8.067975\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 8.042585\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 8.144961\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 8.348633\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 8.498253\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 8.940790\n",
      "Epoch: 672/1000... Step: 21504... Loss: 2.082818... Val Loss: 9.161694\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 7.178625\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 11.857461\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 10.151994\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 8.952311\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 10.172314\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 9.314368\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 8.343659\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 7.962675\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 7.557515\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 7.118713\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 6.915465\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 7.589881\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 7.586536\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 7.842289\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 8.412122\n",
      "Epoch: 673/1000... Step: 21536... Loss: 2.185425... Val Loss: 8.337798\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 6.037340\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 11.919570\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 10.601860\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 9.491172\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 10.586401\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 9.429484\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 8.562769\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 8.382221\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 7.976050\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 7.539678\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 7.413776\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 7.504738\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 7.747198\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 7.931335\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 8.369949\n",
      "Epoch: 674/1000... Step: 21568... Loss: 1.440434... Val Loss: 8.593247\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 8.187181\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 12.353513\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 10.470216\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 9.365979\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 10.631748\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 9.815929\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 8.867858\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 8.489408\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 8.067235\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 7.629400\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 7.498843\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 7.919378\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 7.834508\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 8.171749\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 8.797553\n",
      "Epoch: 675/1000... Step: 21600... Loss: 2.486887... Val Loss: 8.673515\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 6.467376\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 13.056263\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 11.291471\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 10.448904\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 11.525586\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 10.332060\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 9.482090\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 9.438972\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 8.913935\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 8.444414\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 8.370677\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 8.455704\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 8.607453\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 8.727005\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 9.272850\n",
      "Epoch: 676/1000... Step: 21632... Loss: 1.533053... Val Loss: 9.545748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 8.117482\n",
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 13.298901\n",
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 10.998785\n",
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 9.933966\n",
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 11.067800\n",
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 9.922143\n",
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 8.894783\n",
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 8.515846\n",
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 7.973224\n",
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 7.531684\n",
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 7.341857\n",
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 7.596203\n",
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 7.462634\n",
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 7.749649\n",
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 8.366847\n",
      "Epoch: 677/1000... Step: 21664... Loss: 4.792032... Val Loss: 8.195150\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 9.591039\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 13.564740\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 11.302246\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 10.240076\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 11.455192\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 10.684553\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 9.778885\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 9.340185\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 8.950397\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 8.448794\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 8.399784\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 8.723755\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 8.694872\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 9.056797\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 9.741133\n",
      "Epoch: 678/1000... Step: 21696... Loss: 2.647869... Val Loss: 9.645877\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 7.050691\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 14.226584\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 11.459141\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 10.197818\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 11.229068\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 9.788012\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 8.728351\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 8.480871\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 7.873924\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 7.303472\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 7.068505\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 7.395132\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 7.371811\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 7.564264\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 8.065762\n",
      "Epoch: 679/1000... Step: 21728... Loss: 2.943638... Val Loss: 7.998351\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 6.487405\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 13.142962\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 10.912739\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 9.658599\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 10.661696\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 9.339816\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 8.393035\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 8.279566\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 7.725864\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 7.201501\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 6.991073\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 7.129882\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 7.162952\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 7.366483\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 7.828444\n",
      "Epoch: 680/1000... Step: 21760... Loss: 1.876914... Val Loss: 7.785912\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 6.378635\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 12.058445\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 11.067577\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 10.036040\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 11.154735\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 9.987119\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 9.151758\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 9.096134\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 8.737640\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 8.329509\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 8.190593\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 8.378809\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 8.723574\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 8.828131\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 9.266406\n",
      "Epoch: 681/1000... Step: 21792... Loss: 1.225552... Val Loss: 9.614171\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 9.476613\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 14.477420\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 11.810307\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 10.680353\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 11.617948\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 10.780393\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 9.839175\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 9.643164\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 9.140860\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 8.622431\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 8.467598\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 8.776746\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 8.689093\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 8.914436\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 9.574489\n",
      "Epoch: 682/1000... Step: 21824... Loss: 1.960364... Val Loss: 9.418154\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 6.390006\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 12.192965\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 10.315680\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 9.107342\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 10.177696\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 8.991753\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 8.050389\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 7.744451\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 7.233776\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 6.767838\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 6.623328\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 6.746655\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 6.788006\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 7.063206\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 7.530964\n",
      "Epoch: 683/1000... Step: 21856... Loss: 4.202934... Val Loss: 7.504756\n",
      "Validation loss decreased (7.721469 --> 7.504756).  Saving model ...\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 8.434162\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 12.102939\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 10.417720\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 9.426705\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 10.850934\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 9.973112\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 9.030533\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 8.476718\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 8.052070\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 7.635392\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 7.584181\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 7.887672\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 7.800470\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 8.224607\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 8.815212\n",
      "Epoch: 684/1000... Step: 21888... Loss: 5.414742... Val Loss: 8.652934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 6.849936\n",
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 13.536524\n",
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 11.910735\n",
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 10.497397\n",
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 11.668686\n",
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 10.459953\n",
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 9.455575\n",
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 9.178104\n",
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 8.778240\n",
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 8.211686\n",
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 7.980675\n",
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 8.191034\n",
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 8.419888\n",
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 8.550341\n",
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 8.983242\n",
      "Epoch: 685/1000... Step: 21920... Loss: 1.755142... Val Loss: 9.114772\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 8.800845\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 13.709795\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 11.487450\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 10.399748\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 11.722883\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 10.638208\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 9.588742\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 9.153271\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 8.626924\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 8.167229\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 8.007415\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 8.193449\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 8.023687\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 8.343820\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 8.964670\n",
      "Epoch: 686/1000... Step: 21952... Loss: 5.370828... Val Loss: 8.790952\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 7.927811\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 13.660244\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 11.307166\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 10.047577\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 11.282855\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 9.951984\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 8.816833\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 8.373400\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 7.819672\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 7.343368\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 7.141228\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 7.628357\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 7.603271\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 7.911220\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 8.461770\n",
      "Epoch: 687/1000... Step: 21984... Loss: 4.151638... Val Loss: 8.324287\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 7.502467\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 12.390121\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 10.557301\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 9.529561\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 10.360038\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 9.363571\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 8.529831\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 8.415812\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 7.960193\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 7.482048\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 7.353382\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 7.404695\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 7.493192\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 7.716858\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 8.274401\n",
      "Epoch: 688/1000... Step: 22016... Loss: 2.927325... Val Loss: 8.275131\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 11.935012\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 15.942792\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 13.458037\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 12.548510\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 13.667829\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 13.032375\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 12.105050\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 11.725399\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 11.252033\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 10.697605\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 10.679785\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 10.915422\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 10.708438\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 11.041070\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 11.743714\n",
      "Epoch: 689/1000... Step: 22048... Loss: 6.840119... Val Loss: 11.587401\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 7.977301\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 12.867783\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 10.821620\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 9.763887\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 10.821382\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 9.859358\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 8.962521\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 8.663240\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 8.216835\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 7.710718\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 7.577914\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 7.806194\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 7.784807\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 8.054940\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 8.682132\n",
      "Epoch: 690/1000... Step: 22080... Loss: 4.798065... Val Loss: 8.656627\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 9.902207\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 13.167070\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 11.376073\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 10.226917\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 11.572351\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 10.771209\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 9.794994\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 9.306749\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 8.881750\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 8.442052\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 8.366117\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 8.609296\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 8.544869\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 8.988037\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 9.648001\n",
      "Epoch: 691/1000... Step: 22112... Loss: 3.858808... Val Loss: 9.529278\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 6.171035\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 11.917911\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 10.031949\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 8.921867\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 10.051697\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 8.808421\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 7.868014\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 7.641706\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 7.176610\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 6.702265\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 6.530950\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 6.728729\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 6.755339\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 7.025632\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 7.490608\n",
      "Epoch: 692/1000... Step: 22144... Loss: 2.165877... Val Loss: 7.461472\n",
      "Validation loss decreased (7.504756 --> 7.461472).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 6.075541\n",
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 11.222173\n",
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 10.327623\n",
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 9.334277\n",
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 10.427676\n",
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 9.319428\n",
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 8.516207\n",
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 8.438157\n",
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 8.103819\n",
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 7.750117\n",
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 7.647823\n",
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 7.838321\n",
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 8.170526\n",
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 8.313812\n",
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 8.727109\n",
      "Epoch: 693/1000... Step: 22176... Loss: 0.737820... Val Loss: 9.035157\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 7.416405\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 12.937576\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 11.122247\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 9.685349\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 10.828473\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 9.758121\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 8.880971\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 8.652140\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 8.273801\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 7.862538\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 7.606080\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 8.089495\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 8.199711\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 8.503292\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 8.949636\n",
      "Epoch: 694/1000... Step: 22208... Loss: 6.602963... Val Loss: 8.856218\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 7.312122\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 12.233793\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 10.573349\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 9.545140\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 10.444886\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 9.444475\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 8.603645\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 8.423981\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 7.956860\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 7.466660\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 7.369056\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 7.336061\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 7.438566\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 7.671732\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 8.211806\n",
      "Epoch: 695/1000... Step: 22240... Loss: 3.162351... Val Loss: 8.250281\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 6.728178\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 12.387733\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 10.255987\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 8.990243\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 10.308222\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 9.126456\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 8.104057\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 7.715072\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 7.208943\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 6.718808\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 6.535782\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 7.022580\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 7.003630\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 7.314415\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 7.866178\n",
      "Epoch: 696/1000... Step: 22272... Loss: 3.769779... Val Loss: 7.769493\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 7.021597\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 13.424926\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 11.734985\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 10.669045\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 11.856121\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 10.611542\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 9.719329\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 9.684479\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 9.280582\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 8.739065\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 8.520032\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 8.942948\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 9.230720\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 9.318433\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 9.751137\n",
      "Epoch: 697/1000... Step: 22304... Loss: 1.202708... Val Loss: 9.978929\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 6.159867\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 12.811357\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 10.684436\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 9.551760\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 10.598660\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 9.410555\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 8.451233\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 8.350623\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 7.804712\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 7.279795\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 7.068588\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 7.311832\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 7.360453\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 7.534720\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 8.032435\n",
      "Epoch: 698/1000... Step: 22336... Loss: 2.267864... Val Loss: 8.006475\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 6.196501\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 11.753206\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 10.110260\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 9.090251\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 10.253880\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 8.939543\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 7.945596\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 7.628272\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 7.130891\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 6.655236\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 6.508629\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 6.846300\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 6.847417\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 7.079825\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 7.494911\n",
      "Epoch: 699/1000... Step: 22368... Loss: 3.112822... Val Loss: 7.475393\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 6.434694\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 12.271764\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 10.279791\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 9.025552\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 10.169878\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 8.916339\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 7.939758\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 7.665572\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 7.179395\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 6.696384\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 6.488210\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 6.699245\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 6.737742\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 7.052791\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 7.501491\n",
      "Epoch: 700/1000... Step: 22400... Loss: 3.097561... Val Loss: 7.438192\n",
      "Validation loss decreased (7.461472 --> 7.438192).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 12.539524\n",
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 15.320121\n",
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 13.461594\n",
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 13.076967\n",
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 13.895429\n",
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 13.467682\n",
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 12.693416\n",
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 12.542325\n",
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 12.171101\n",
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 11.721326\n",
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 11.836251\n",
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 11.686220\n",
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 11.508287\n",
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 11.764075\n",
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 12.458102\n",
      "Epoch: 701/1000... Step: 22432... Loss: 6.825254... Val Loss: 12.317164\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 7.629661\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 13.016119\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 10.952893\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 9.976856\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 10.844553\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 9.846674\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 9.025856\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 8.916388\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 8.452658\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 7.926930\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 7.780919\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 7.985078\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 8.013679\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 8.195729\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 8.784997\n",
      "Epoch: 702/1000... Step: 22464... Loss: 3.554848... Val Loss: 8.814678\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 7.276501\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 14.331478\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 11.906680\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 10.541427\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 11.708610\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 10.242103\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 9.102880\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 8.753802\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 8.190477\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 7.614188\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 7.341381\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 7.791432\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 7.787548\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 8.022215\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 8.477644\n",
      "Epoch: 703/1000... Step: 22496... Loss: 3.659731... Val Loss: 8.365320\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 12.067429\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 14.933363\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 12.587684\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 11.795123\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 12.806245\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 12.185704\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 11.406522\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 11.113497\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 10.746116\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 10.233426\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 10.230655\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 10.334484\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 10.219006\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 10.542795\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 11.256032\n",
      "Epoch: 704/1000... Step: 22528... Loss: 3.902667... Val Loss: 11.119674\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 6.857245\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 13.370781\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 12.147308\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 11.217059\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 12.307180\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 10.932916\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 10.066443\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 10.238322\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 9.792995\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 9.311255\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 9.073749\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 9.256750\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 9.625413\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 9.677574\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 10.122498\n",
      "Epoch: 705/1000... Step: 22560... Loss: 1.157431... Val Loss: 10.569970\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 12.735723\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 18.525598\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 15.316572\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 14.545477\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 15.173846\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 14.955785\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 14.249404\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 14.006414\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 13.530375\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 12.883502\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 12.996424\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 12.929849\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 12.651942\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 12.867953\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 13.610675\n",
      "Epoch: 706/1000... Step: 22592... Loss: 14.114450... Val Loss: 13.586073\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 8.761672\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 13.293369\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 11.562252\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 10.710562\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 11.332487\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 10.409080\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 9.631384\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 9.496135\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 9.017672\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 8.561093\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 8.527052\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 8.464682\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 8.460781\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 8.645372\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 9.205033\n",
      "Epoch: 707/1000... Step: 22624... Loss: 4.455660... Val Loss: 9.154018\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 8.006598\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 11.805985\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 10.111255\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 9.117428\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 10.449054\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 9.603471\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 8.712008\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 8.317188\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 7.932196\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 7.442915\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 7.376167\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 7.845596\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 7.833477\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 8.191970\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 8.857930\n",
      "Epoch: 708/1000... Step: 22656... Loss: 1.902424... Val Loss: 8.811049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 7.345212\n",
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 12.542579\n",
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 10.360086\n",
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 9.245295\n",
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 10.218820\n",
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 9.027677\n",
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 8.082981\n",
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 7.883846\n",
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 7.403108\n",
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 6.958834\n",
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 6.713457\n",
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 7.056916\n",
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 7.008877\n",
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 7.251729\n",
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 7.816179\n",
      "Epoch: 709/1000... Step: 22688... Loss: 2.082783... Val Loss: 7.689938\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 6.563672\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 13.389476\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 11.391323\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 10.367520\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 11.315864\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 9.977432\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 9.102784\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 9.225357\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 8.715644\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 8.171636\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 7.926005\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 8.134399\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 8.278183\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 8.379461\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 8.841538\n",
      "Epoch: 710/1000... Step: 22720... Loss: 1.114258... Val Loss: 8.944872\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 8.364709\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 11.032574\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 9.794968\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 8.879929\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 10.295193\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 9.429933\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 8.491224\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 7.940281\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 7.547684\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 7.237508\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 7.122533\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 7.425371\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 7.374980\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 7.803848\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 8.415070\n",
      "Epoch: 711/1000... Step: 22752... Loss: 5.505289... Val Loss: 8.266329\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 9.088045\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 12.385228\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 10.514190\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 9.666857\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 11.072401\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 10.286291\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 9.430605\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 9.034183\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 8.611503\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 8.106765\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 8.085235\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 8.439287\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 8.386261\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 8.748818\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 9.432980\n",
      "Epoch: 712/1000... Step: 22784... Loss: 2.550456... Val Loss: 9.339469\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 6.890247\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 12.065122\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 10.090813\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 8.972909\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 10.156803\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 8.926593\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 7.950862\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 7.624130\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 7.135419\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 6.665752\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 6.466317\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 6.946613\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 6.952937\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 7.234196\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 7.785930\n",
      "Epoch: 713/1000... Step: 22816... Loss: 3.162854... Val Loss: 7.699136\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 7.007777\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 13.006908\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 11.609219\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 10.655544\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 11.733826\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 10.382123\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 9.506475\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 9.589155\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 9.159090\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 8.616724\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 8.366020\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 8.556140\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 8.814938\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 8.917150\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 9.344036\n",
      "Epoch: 714/1000... Step: 22848... Loss: 0.838158... Val Loss: 9.554129\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 14.301991\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 18.311991\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 15.281055\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 15.090043\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 15.949864\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 15.741008\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 15.058439\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 15.181215\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 14.725901\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 14.023842\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 14.181719\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 14.180612\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 13.942396\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 14.081571\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 14.921846\n",
      "Epoch: 715/1000... Step: 22880... Loss: 7.965010... Val Loss: 14.728040\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 6.444629\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 10.779005\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 10.447610\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 9.785177\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 10.779484\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 9.665704\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 8.812970\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 8.731184\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 8.398837\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 8.046630\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 7.946879\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 8.033193\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 8.160859\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 8.313646\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 8.739922\n",
      "Epoch: 716/1000... Step: 22912... Loss: 2.417163... Val Loss: 8.885617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 11.701571\n",
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 15.316678\n",
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 12.872826\n",
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 12.255712\n",
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 13.060906\n",
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 12.716338\n",
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 11.984612\n",
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 11.897208\n",
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 11.517256\n",
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 10.939421\n",
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 11.004429\n",
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 11.085122\n",
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 10.982742\n",
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 11.178316\n",
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 11.889564\n",
      "Epoch: 717/1000... Step: 22944... Loss: 5.135436... Val Loss: 11.775447\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 5.857498\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 10.425439\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 9.837825\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 8.794287\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 9.999388\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 9.092294\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 8.256481\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 7.909471\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 7.620081\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 7.303133\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 7.280573\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 7.422984\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 7.746946\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 7.947003\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 8.376587\n",
      "Epoch: 718/1000... Step: 22976... Loss: 2.577853... Val Loss: 8.602057\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 11.770122\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 17.062796\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 14.346414\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 13.811253\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 14.413280\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 13.687086\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 12.842735\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 12.936018\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 12.314817\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 11.650150\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 11.627541\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 11.549942\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 11.302452\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 11.435529\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 12.187996\n",
      "Epoch: 719/1000... Step: 23008... Loss: 9.362214... Val Loss: 11.995057\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 5.264543\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 10.541396\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 9.351902\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 8.431744\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 9.928797\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 8.727323\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 7.800285\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 7.546476\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 7.063018\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 6.568087\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 6.454757\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 6.647341\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 6.680100\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 6.923687\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 7.405924\n",
      "Epoch: 720/1000... Step: 23040... Loss: 1.998959... Val Loss: 7.423430\n",
      "Validation loss decreased (7.438192 --> 7.423430).  Saving model ...\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 6.640227\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 11.754317\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 9.950644\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 8.788433\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 10.249421\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 9.021866\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 8.027904\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 7.642014\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 7.119330\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 6.618955\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 6.440767\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 6.711720\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 6.707799\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 7.042571\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 7.517205\n",
      "Epoch: 721/1000... Step: 23072... Loss: 2.900073... Val Loss: 7.439825\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 6.791739\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 12.343641\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 10.831400\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 9.767820\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 10.703990\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 9.456405\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 8.578704\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 8.658612\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 8.209883\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 7.704618\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 7.482833\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 7.624556\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 7.798877\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 7.971636\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 8.409462\n",
      "Epoch: 722/1000... Step: 23104... Loss: 1.222579... Val Loss: 8.428606\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 8.976060\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 11.963474\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 10.191967\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 9.583363\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 11.207883\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 10.616376\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 9.838464\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 9.459241\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 9.185401\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 8.639921\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 8.702479\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 8.944817\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 8.944308\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 9.284197\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 9.944221\n",
      "Epoch: 723/1000... Step: 23136... Loss: 1.877785... Val Loss: 9.819462\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 7.185618\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 12.266669\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 10.397746\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 9.214894\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 10.608551\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 9.338872\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 8.261242\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 7.904037\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 7.367361\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 6.895554\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 6.664433\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 7.154994\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 7.138690\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 7.452047\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 8.004514\n",
      "Epoch: 724/1000... Step: 23168... Loss: 3.590671... Val Loss: 7.892449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 6.569017\n",
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 8.794855\n",
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 8.301378\n",
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 7.600990\n",
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 9.080590\n",
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 8.330159\n",
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 7.464744\n",
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 6.933254\n",
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 6.758581\n",
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 6.496729\n",
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 6.478702\n",
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 6.853317\n",
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 6.851343\n",
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 7.248451\n",
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 7.772733\n",
      "Epoch: 725/1000... Step: 23200... Loss: 3.923111... Val Loss: 7.687206\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 7.163156\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 11.706439\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 9.975158\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 9.139661\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 10.551960\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 9.491574\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 8.479906\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 8.217784\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 7.868099\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 7.377810\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 7.212604\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 7.442845\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 7.405579\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 7.663190\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 8.230318\n",
      "Epoch: 726/1000... Step: 23232... Loss: 4.001672... Val Loss: 8.239234\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 6.360156\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 11.370093\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 10.085380\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 9.142630\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 10.288698\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 9.203398\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 8.360898\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 8.306458\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 7.836592\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 7.345835\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 7.237752\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 7.249073\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 7.356936\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 7.545305\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 8.044027\n",
      "Epoch: 727/1000... Step: 23264... Loss: 2.670347... Val Loss: 8.147180\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 6.822133\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 10.160262\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 9.516005\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 8.320025\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 9.661946\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 8.658316\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 7.876979\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 7.533960\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 7.298404\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 6.959356\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 6.791069\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 7.090774\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 7.273563\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 7.647456\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 8.087736\n",
      "Epoch: 728/1000... Step: 23296... Loss: 4.902614... Val Loss: 8.073199\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 7.556475\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 12.168953\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 10.291899\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 9.320204\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 10.342841\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 9.300232\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 8.400140\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 8.295069\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 7.836320\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 7.341281\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 7.206987\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 7.538593\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 7.520984\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 7.758215\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 8.425328\n",
      "Epoch: 729/1000... Step: 23328... Loss: 3.186664... Val Loss: 8.356221\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 6.096941\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 11.276472\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 9.729228\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 8.642012\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 9.927028\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 8.678696\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 7.727551\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 7.438660\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 6.977996\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 6.485316\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 6.314207\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 6.681488\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 6.753776\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 7.035806\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 7.505446\n",
      "Epoch: 730/1000... Step: 23360... Loss: 2.377274... Val Loss: 7.462398\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 6.261730\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 10.684136\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 9.226711\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 8.268949\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 9.328674\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 8.225351\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 7.403203\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 7.261623\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 6.826285\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 6.413959\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 6.268799\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 6.433384\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 6.516616\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 6.782201\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 7.263672\n",
      "Epoch: 731/1000... Step: 23392... Loss: 1.784423... Val Loss: 7.244536\n",
      "Validation loss decreased (7.423430 --> 7.244536).  Saving model ...\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 7.485946\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 11.970715\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 10.871292\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 10.246203\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 10.997373\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 10.034196\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 9.305633\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 9.477130\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 9.027292\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 8.592343\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 8.548197\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 8.592264\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 8.740912\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 8.846632\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 9.449352\n",
      "Epoch: 732/1000... Step: 23424... Loss: 3.383661... Val Loss: 9.629383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 6.842829\n",
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 11.622034\n",
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 9.841694\n",
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 9.189643\n",
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 10.368209\n",
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 9.315551\n",
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 8.415477\n",
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 8.353958\n",
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 7.834057\n",
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 7.315987\n",
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 7.230480\n",
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 7.460075\n",
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 7.353053\n",
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 7.582111\n",
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 8.224894\n",
      "Epoch: 733/1000... Step: 23456... Loss: 2.565790... Val Loss: 8.115801\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 7.625470\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 12.180892\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 10.407615\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 9.362918\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 10.275776\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 9.206037\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 8.391527\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 8.218196\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 7.755807\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 7.237845\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 7.082487\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 7.285350\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 7.339516\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 7.558732\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 8.114129\n",
      "Epoch: 734/1000... Step: 23488... Loss: 3.863195... Val Loss: 8.111488\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 6.122586\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 12.430559\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 10.501294\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 9.473616\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 10.581709\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 9.274236\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 8.329455\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 8.211781\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 7.714823\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 7.178668\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 6.977220\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 7.315699\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 7.423381\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 7.600947\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 8.049301\n",
      "Epoch: 735/1000... Step: 23520... Loss: 1.821585... Val Loss: 8.069793\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 7.582176\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 13.028128\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 11.949904\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 10.893013\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 11.929460\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 10.726324\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 9.896771\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 9.874903\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 9.515769\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 9.038405\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 8.844098\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 9.109427\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 9.501244\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 9.620725\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 9.956002\n",
      "Epoch: 736/1000... Step: 23552... Loss: 0.624143... Val Loss: 10.206351\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 6.001202\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 11.556196\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 9.905620\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 8.926416\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 10.114838\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 8.860056\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 7.925308\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 7.714305\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 7.217364\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 6.712527\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 6.550620\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 6.932177\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 7.009627\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 7.230222\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 7.709440\n",
      "Epoch: 737/1000... Step: 23584... Loss: 2.431946... Val Loss: 7.714422\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 6.355121\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 10.844851\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 9.246662\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 8.252239\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 9.680220\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 8.503037\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 7.577867\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 7.309590\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 6.848926\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 6.354157\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 6.166342\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 6.551870\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 6.570059\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 6.839100\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 7.360411\n",
      "Epoch: 738/1000... Step: 23616... Loss: 1.675121... Val Loss: 7.345513\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 6.118912\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 10.054956\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 9.082847\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 8.479566\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 9.838007\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 8.830839\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 7.842511\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 7.674329\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 7.287922\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 6.860541\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 6.763318\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 6.911325\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 6.860567\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 7.144890\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 7.637029\n",
      "Epoch: 739/1000... Step: 23648... Loss: 4.850269... Val Loss: 7.560640\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 9.653009\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 12.400760\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 10.903621\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 9.878542\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 11.696401\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 10.635383\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 9.608821\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 8.997672\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 8.481798\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 8.083518\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 8.013408\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 8.426061\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 8.290602\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 8.716887\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 9.374824\n",
      "Epoch: 740/1000... Step: 23680... Loss: 6.325813... Val Loss: 9.249307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 7.162209\n",
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 10.756514\n",
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 9.295928\n",
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 8.317125\n",
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 9.486195\n",
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 8.488344\n",
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 7.613087\n",
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 7.486636\n",
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 7.114191\n",
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 6.700336\n",
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 6.540806\n",
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 6.892106\n",
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 6.890432\n",
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 7.203641\n",
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 7.799876\n",
      "Epoch: 741/1000... Step: 23712... Loss: 2.056823... Val Loss: 7.699729\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 6.709435\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 10.492862\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 9.048346\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 8.407535\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 9.718392\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 8.732517\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 7.932597\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 7.765314\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 7.371795\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 6.959797\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 6.907178\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 7.120074\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 7.066696\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 7.306391\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 7.822400\n",
      "Epoch: 742/1000... Step: 23744... Loss: 2.402859... Val Loss: 7.817987\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 7.391966\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 12.023155\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 10.359390\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 9.264336\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 10.298378\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 9.160935\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 8.266561\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 8.061915\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 7.658559\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 7.139922\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 6.962497\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 7.195623\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 7.320231\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 7.504391\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 8.034281\n",
      "Epoch: 743/1000... Step: 23776... Loss: 2.393531... Val Loss: 8.071367\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 5.801810\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 11.283165\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 10.022167\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 9.294784\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 10.701818\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 9.426708\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 8.522141\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 8.414404\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 7.959924\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 7.509861\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 7.392398\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 7.873105\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 8.089937\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 8.232515\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 8.633868\n",
      "Epoch: 744/1000... Step: 23808... Loss: 1.221185... Val Loss: 8.937086\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 7.513546\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 11.031881\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 10.262801\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 9.618324\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 10.760645\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 9.809500\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 8.800025\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 8.647080\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 8.116449\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 7.684747\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 7.564375\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 7.836491\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 7.639347\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 7.843393\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 8.410166\n",
      "Epoch: 745/1000... Step: 23840... Loss: 7.455554... Val Loss: 8.253774\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 6.344465\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 9.699072\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 8.878893\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 8.287635\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 9.606258\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 8.735020\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 7.803516\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 7.736038\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 7.291114\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 6.860721\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 6.798439\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 6.900025\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 6.776593\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 7.094616\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 7.645306\n",
      "Epoch: 746/1000... Step: 23872... Loss: 5.677239... Val Loss: 7.479866\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 8.904649\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 11.790173\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 10.418070\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 9.573293\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 10.854704\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 10.006310\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 9.076286\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 9.049073\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 8.637915\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 8.146222\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 8.053512\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 8.344840\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 8.150313\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 8.362226\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 8.998702\n",
      "Epoch: 747/1000... Step: 23904... Loss: 4.073338... Val Loss: 8.766510\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 7.909509\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 11.616332\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 10.692225\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 9.573718\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 11.101021\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 10.036555\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 9.146996\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 9.200722\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 8.913499\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 8.381267\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 8.130862\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 8.505691\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 8.533764\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 8.691733\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 9.141137\n",
      "Epoch: 748/1000... Step: 23936... Loss: 7.434927... Val Loss: 8.979689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 8.267401\n",
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 11.627845\n",
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 10.114819\n",
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 9.538843\n",
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 10.982247\n",
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 10.202689\n",
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 9.369132\n",
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 9.373357\n",
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 8.872289\n",
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 8.320284\n",
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 8.319617\n",
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 8.580718\n",
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 8.411288\n",
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 8.640192\n",
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 9.329728\n",
      "Epoch: 749/1000... Step: 23968... Loss: 6.138638... Val Loss: 9.117040\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 10.909914\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 12.875479\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 11.447321\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 10.735002\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 12.214667\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 11.536639\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 10.793464\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 10.714003\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 10.289347\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 9.770259\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 9.781985\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 9.925797\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 9.700806\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 9.909316\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 10.579833\n",
      "Epoch: 750/1000... Step: 24000... Loss: 5.643595... Val Loss: 10.321267\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 8.327461\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 10.525701\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 9.527921\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 8.616748\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 10.037850\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 9.186359\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 8.308522\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 8.202286\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 7.803128\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 7.404818\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 7.332525\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 7.484295\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 7.371388\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 7.725810\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 8.310121\n",
      "Epoch: 751/1000... Step: 24032... Loss: 4.839446... Val Loss: 8.079573\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 6.877667\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 11.224905\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 10.674804\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 9.874383\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 10.930062\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 9.855795\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 9.030146\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 9.346200\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 8.950352\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 8.466211\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 8.271004\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 8.276165\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 8.389499\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 8.466210\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 8.875982\n",
      "Epoch: 752/1000... Step: 24064... Loss: 3.026236... Val Loss: 8.901236\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 11.277802\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 14.074262\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 12.322838\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 11.890363\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 12.927525\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 12.561232\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 11.874393\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 12.273344\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 11.840807\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 11.212725\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 11.294612\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 11.384040\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 11.085217\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 11.148672\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 11.877589\n",
      "Epoch: 753/1000... Step: 24096... Loss: 6.626700... Val Loss: 11.612995\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 12.934147\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 13.841899\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 12.547724\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 12.082872\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 13.384360\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 13.258275\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 12.379429\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 11.989175\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 11.776899\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 11.404234\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 11.531695\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 11.454620\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 11.325649\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 11.822014\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 12.516899\n",
      "Epoch: 754/1000... Step: 24128... Loss: 7.731362... Val Loss: 12.256173\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 7.903434\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 11.118077\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 10.654761\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 9.644052\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 10.628310\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 9.636088\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 8.883944\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 9.211430\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 8.882496\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 8.409732\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 8.256664\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 8.264786\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 8.345604\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 8.380462\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 8.802740\n",
      "Epoch: 755/1000... Step: 24160... Loss: 1.613939... Val Loss: 8.839141\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 9.094820\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 11.904222\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 10.525730\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 9.773492\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 11.077797\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 10.550061\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 9.740509\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 9.708530\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 9.332630\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 8.800871\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 8.907922\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 8.925824\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 8.803883\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 8.966231\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 9.680706\n",
      "Epoch: 756/1000... Step: 24192... Loss: 3.974746... Val Loss: 9.419611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 7.843789\n",
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 11.128383\n",
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 9.496636\n",
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 8.818084\n",
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 10.312168\n",
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 9.440009\n",
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 8.601394\n",
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 8.505090\n",
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 8.067322\n",
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 7.562632\n",
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 7.566447\n",
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 7.872317\n",
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 7.739874\n",
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 7.898832\n",
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 8.543125\n",
      "Epoch: 757/1000... Step: 24224... Loss: 4.366459... Val Loss: 8.363729\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 8.412207\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 10.964411\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 9.707832\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 8.582335\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 10.074968\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 8.984311\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 7.994472\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 7.827349\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 7.436796\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 7.021621\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 6.798940\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 7.327493\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 7.285307\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 7.489172\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 8.088883\n",
      "Epoch: 758/1000... Step: 24256... Loss: 4.478024... Val Loss: 7.907066\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 6.454346\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 10.086465\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 8.821372\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 8.127715\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 9.638422\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 8.645816\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 7.766558\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 7.643706\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 7.169381\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 6.681897\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 6.631709\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 6.963970\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 6.855541\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 7.104193\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 7.727250\n",
      "Epoch: 759/1000... Step: 24288... Loss: 5.177401... Val Loss: 7.566658\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 7.286968\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 11.383984\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 10.091401\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 9.297045\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 10.432340\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 9.460097\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 8.592474\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 8.662808\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 8.180643\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 7.675977\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 7.548910\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 7.654826\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 7.521913\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 7.640728\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 8.227932\n",
      "Epoch: 760/1000... Step: 24320... Loss: 6.646428... Val Loss: 8.091549\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 7.347110\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 11.087067\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 9.560334\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 8.757820\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 10.568839\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 9.378629\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 8.360160\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 8.270905\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 7.710005\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 7.211965\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 7.042663\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 7.789145\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 7.598203\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 7.766074\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 8.447712\n",
      "Epoch: 761/1000... Step: 24352... Loss: 7.435764... Val Loss: 8.300868\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 7.460877\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 10.537223\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 9.286204\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 8.244874\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 9.708647\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 8.561775\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 7.667889\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 7.632563\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 7.194597\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 6.718325\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 6.501878\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 7.036792\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 6.930196\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 7.076546\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 7.586841\n",
      "Epoch: 762/1000... Step: 24384... Loss: 5.082078... Val Loss: 7.385918\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 12.200865\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 14.231420\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 12.295956\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 11.643550\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 13.126439\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 12.504994\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 11.721527\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 11.901040\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 11.558398\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 10.964888\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 10.972653\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 11.180033\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 10.896745\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 11.048046\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 11.831840\n",
      "Epoch: 763/1000... Step: 24416... Loss: 7.759462... Val Loss: 11.553715\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 8.266976\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 12.745536\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 11.298480\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 10.429837\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 11.494730\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 10.444229\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 9.588498\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 9.875640\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 9.351762\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 8.753352\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 8.639028\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 8.532656\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 8.429976\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 8.450114\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 9.015232\n",
      "Epoch: 764/1000... Step: 24448... Loss: 5.468220... Val Loss: 8.971119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 14.113582\n",
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 15.250913\n",
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 13.654159\n",
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 13.145329\n",
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 14.558546\n",
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 14.408435\n",
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 13.650127\n",
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 13.558216\n",
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 13.126795\n",
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 12.564385\n",
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 12.617631\n",
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 12.705721\n",
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 12.347460\n",
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 12.620452\n",
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 13.344634\n",
      "Epoch: 765/1000... Step: 24480... Loss: 12.548031... Val Loss: 13.006677\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 9.704413\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 11.737808\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 10.366028\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 9.834400\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 11.083293\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 10.357099\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 9.617418\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 9.741898\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 9.291095\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 8.765153\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 8.707199\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 8.841976\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 8.585818\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 8.728257\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 9.358483\n",
      "Epoch: 766/1000... Step: 24512... Loss: 9.349406... Val Loss: 9.137884\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 7.453482\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 10.449368\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 9.131100\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 8.476002\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 10.151534\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 9.274558\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 8.405176\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 8.222948\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 7.775749\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 7.293015\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 7.219439\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 7.561306\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 7.371751\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 7.635836\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 8.287790\n",
      "Epoch: 767/1000... Step: 24544... Loss: 6.688075... Val Loss: 8.097232\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 6.457934\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 9.498802\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 8.589251\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 7.591209\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 9.084679\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 8.191794\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 7.353220\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 7.217350\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 6.898938\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 6.475866\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 6.345800\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 6.620700\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 6.610019\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 6.824914\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 7.303383\n",
      "Epoch: 768/1000... Step: 24576... Loss: 3.774638... Val Loss: 7.193318\n",
      "Validation loss decreased (7.244536 --> 7.193318).  Saving model ...\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 6.430215\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 10.759177\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 9.493155\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 8.598647\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 10.106771\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 8.912464\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 7.928684\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 7.841605\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 7.287345\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 6.782632\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 6.635151\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 6.937927\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 6.796078\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 6.995606\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 7.555465\n",
      "Epoch: 769/1000... Step: 24608... Loss: 6.030797... Val Loss: 7.361302\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 7.724236\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 10.826203\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 9.272625\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 8.813688\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 10.283197\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 9.351479\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 8.565613\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 8.341713\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 7.911598\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 7.441155\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 7.437721\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 7.705911\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 7.556900\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 7.870499\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 8.510197\n",
      "Epoch: 770/1000... Step: 24640... Loss: 4.361053... Val Loss: 8.360454\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 6.111015\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 11.068335\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 9.684654\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 8.916166\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 9.888376\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 8.680870\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 7.851985\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 7.867012\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 7.438065\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 6.987191\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 6.847653\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 7.078242\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 7.232318\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 7.412643\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 7.883440\n",
      "Epoch: 771/1000... Step: 24672... Loss: 2.038177... Val Loss: 7.954812\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 8.178144\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 11.311794\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 9.533752\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 8.862775\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 10.125139\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 9.072985\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 8.213596\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 7.897662\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 7.467863\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 7.058583\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 6.943865\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 7.341730\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 7.229557\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 7.520240\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 8.159878\n",
      "Epoch: 772/1000... Step: 24704... Loss: 4.608927... Val Loss: 8.008633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 5.858067\n",
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 10.525045\n",
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 9.139990\n",
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 8.364403\n",
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 9.510771\n",
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 8.379242\n",
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 7.551370\n",
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 7.491251\n",
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 7.089045\n",
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 6.583860\n",
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 6.435720\n",
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 6.748317\n",
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 6.893018\n",
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 7.094473\n",
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 7.647591\n",
      "Epoch: 773/1000... Step: 24736... Loss: 1.414886... Val Loss: 7.690154\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 6.273850\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 10.441597\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 8.829930\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 7.899162\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 9.023314\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 7.936440\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 7.085510\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 6.833839\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 6.439217\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 6.014659\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 5.873060\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 6.234741\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 6.312457\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 6.619550\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 7.139460\n",
      "Epoch: 774/1000... Step: 24768... Loss: 2.983008... Val Loss: 7.105702\n",
      "Validation loss decreased (7.193318 --> 7.105702).  Saving model ...\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 8.148172\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 10.123984\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 9.076957\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 8.644637\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 10.364988\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 9.704341\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 8.899712\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 8.504281\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 8.210872\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 7.825894\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 7.963065\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 8.115796\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 7.989473\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 8.397425\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 8.966056\n",
      "Epoch: 775/1000... Step: 24800... Loss: 4.806613... Val Loss: 8.710461\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 6.785131\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 10.866560\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 10.214859\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 9.444014\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 10.364888\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 9.345871\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 8.503064\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 8.635976\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 8.241297\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 7.833618\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 7.685243\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 7.794531\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 7.954344\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 8.175759\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 8.653673\n",
      "Epoch: 776/1000... Step: 24832... Loss: 2.887621... Val Loss: 8.648545\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 13.243641\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 16.357395\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 13.642938\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 13.382951\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 14.706116\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 14.407327\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 13.814963\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 14.120353\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 13.739313\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 13.024259\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 13.302788\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 13.395950\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 13.119951\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 13.122507\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 13.941362\n",
      "Epoch: 777/1000... Step: 24864... Loss: 4.917348... Val Loss: 13.616993\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 6.385383\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 10.464025\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 10.158353\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 9.216034\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 10.376201\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 9.364720\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 8.571546\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 8.383317\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 8.094272\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 7.736743\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 7.674011\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 7.686458\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 7.982504\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 8.155517\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 8.535914\n",
      "Epoch: 778/1000... Step: 24896... Loss: 2.315404... Val Loss: 8.754541\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 8.052963\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 10.017131\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 8.884854\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 8.321827\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 9.898684\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 9.346389\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 8.668964\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 8.147340\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 7.921216\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 7.482551\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 7.613751\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 7.797955\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 7.806117\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 8.196206\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 8.832917\n",
      "Epoch: 779/1000... Step: 24928... Loss: 1.887917... Val Loss: 8.729932\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 5.926401\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 10.195387\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 9.118442\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 8.353734\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 9.504345\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 8.411722\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 7.581208\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 7.466964\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 7.064245\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 6.589383\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 6.472656\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 6.688212\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 6.829519\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 7.039328\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 7.544300\n",
      "Epoch: 780/1000... Step: 24960... Loss: 1.657600... Val Loss: 7.583574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 10.063534\n",
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 12.083940\n",
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 10.866074\n",
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 10.227516\n",
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 11.307582\n",
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 10.551353\n",
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 9.859572\n",
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 9.566073\n",
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 9.146656\n",
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 8.709600\n",
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 8.681780\n",
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 8.797012\n",
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 8.651833\n",
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 8.949698\n",
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 9.599210\n",
      "Epoch: 781/1000... Step: 24992... Loss: 8.332712... Val Loss: 9.454416\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 6.497014\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 10.486690\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 9.934843\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 9.195345\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 10.129602\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 8.999429\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 8.320869\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 8.507681\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 8.125534\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 7.798974\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 7.696265\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 7.814210\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 8.213953\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 8.347334\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 8.832192\n",
      "Epoch: 782/1000... Step: 25024... Loss: 1.649675... Val Loss: 9.182210\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 7.328640\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 11.090299\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 10.138765\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 9.528059\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 10.624153\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 9.632223\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 8.775969\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 8.801175\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 8.382884\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 7.892723\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 7.784985\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 7.835208\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 7.800974\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 7.957401\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 8.562846\n",
      "Epoch: 783/1000... Step: 25056... Loss: 4.578957... Val Loss: 8.568844\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 8.248358\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 11.417276\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 9.563289\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 9.195427\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 10.507043\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 9.767688\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 9.133831\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 9.072470\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 8.680303\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 8.115242\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 8.170716\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 8.367235\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 8.357369\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 8.586592\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 9.288812\n",
      "Epoch: 784/1000... Step: 25088... Loss: 2.769127... Val Loss: 9.194294\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 7.195996\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 11.172861\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 9.440096\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 8.615716\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 9.853691\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 8.638977\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 7.692418\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 7.433181\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 6.921287\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 6.472921\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 6.290341\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 6.621445\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 6.547430\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 6.799077\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 7.368910\n",
      "Epoch: 785/1000... Step: 25120... Loss: 3.450379... Val Loss: 7.247110\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 6.576130\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 9.142846\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 8.163445\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 7.539519\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 8.931260\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 8.044820\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 7.257629\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 6.939814\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 6.636066\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 6.226894\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 6.197475\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 6.601753\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 6.612527\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 6.948809\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 7.584215\n",
      "Epoch: 786/1000... Step: 25152... Loss: 3.401225... Val Loss: 7.533650\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 8.499756\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 10.886757\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 9.326985\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 8.711057\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 10.098712\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 9.376496\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 8.682546\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 8.447361\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 8.139568\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 7.670522\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 7.686744\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 7.846549\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 7.805357\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 8.134646\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 8.732686\n",
      "Epoch: 787/1000... Step: 25184... Loss: 3.132140... Val Loss: 8.604390\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 11.128496\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 12.444446\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 11.090496\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 10.279251\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 11.758627\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 11.054189\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 10.206711\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 9.713636\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 9.323823\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 8.933262\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 8.935898\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 9.339814\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 9.190546\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 9.659972\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 10.390115\n",
      "Epoch: 788/1000... Step: 25216... Loss: 5.446009... Val Loss: 10.282792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 6.573559\n",
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 9.475654\n",
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 8.886735\n",
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 8.564607\n",
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 9.745011\n",
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 8.819972\n",
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 8.071392\n",
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 8.011769\n",
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 7.655372\n",
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 7.320255\n",
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 7.349453\n",
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 7.384663\n",
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 7.401354\n",
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 7.582348\n",
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 8.111030\n",
      "Epoch: 789/1000... Step: 25248... Loss: 2.463110... Val Loss: 8.205742\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 5.773873\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 10.437771\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 9.188024\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 8.382812\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 9.754515\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 8.608447\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 7.777930\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 7.711433\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 7.254216\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 6.751030\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 6.597870\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 6.974589\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 7.136950\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 7.331157\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 7.802017\n",
      "Epoch: 790/1000... Step: 25280... Loss: 1.740527... Val Loss: 7.903842\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 7.240188\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 10.028489\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 9.088825\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 8.816432\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 10.387093\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 9.515327\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 8.599851\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 8.293145\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 7.796778\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 7.378886\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 7.406705\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 7.654989\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 7.430127\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 7.740873\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 8.421050\n",
      "Epoch: 791/1000... Step: 25312... Loss: 8.060513... Val Loss: 8.250552\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 13.288149\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 13.854330\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 12.250648\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 12.200571\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 13.635309\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 13.465310\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 12.862296\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 12.618686\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 12.378655\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 11.747060\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 11.993054\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 12.019202\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 11.838853\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 12.209698\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 13.086360\n",
      "Epoch: 792/1000... Step: 25344... Loss: 8.249008... Val Loss: 12.963064\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 6.010108\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 8.716652\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 8.324220\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 7.699015\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 8.729827\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 7.880726\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 7.193986\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 7.280896\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 6.977122\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 6.652354\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 6.561251\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 6.702749\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 7.015446\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 7.241344\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 7.762572\n",
      "Epoch: 793/1000... Step: 25376... Loss: 1.769830... Val Loss: 7.970817\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 14.041423\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 14.937331\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 13.291743\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 13.012295\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 14.266122\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 13.975453\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 13.288068\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 12.932489\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 12.594412\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 12.038965\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 12.178796\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 12.238613\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 11.993659\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 12.406388\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 13.126673\n",
      "Epoch: 794/1000... Step: 25408... Loss: 10.265707... Val Loss: 12.937451\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 6.674829\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 10.372568\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 10.141212\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 9.526659\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 10.493750\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 9.390412\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 8.682218\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 8.886540\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 8.548037\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 8.266457\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 8.179673\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 8.339188\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 8.751151\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 8.878112\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 9.297371\n",
      "Epoch: 795/1000... Step: 25440... Loss: 0.727487... Val Loss: 9.702467\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 8.928293\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 11.854086\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 10.776075\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 10.371458\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 11.361237\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 10.897289\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 10.076270\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 9.801574\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 9.401992\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 8.956441\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 9.138116\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 9.197006\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 9.028547\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 9.295253\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 9.994575\n",
      "Epoch: 796/1000... Step: 25472... Loss: 4.736742... Val Loss: 9.789490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 6.028861\n",
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 10.774390\n",
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 9.596078\n",
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 8.692585\n",
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 10.428967\n",
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 9.168060\n",
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 8.104894\n",
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 7.811319\n",
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 7.284431\n",
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 6.739884\n",
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 6.559336\n",
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 6.854658\n",
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 6.821745\n",
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 7.092692\n",
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 7.640196\n",
      "Epoch: 797/1000... Step: 25504... Loss: 3.563239... Val Loss: 7.580730\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 6.075807\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 9.214640\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 8.626140\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 8.111093\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 9.452752\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 8.600290\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 7.700222\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 7.388131\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 7.001736\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 6.661514\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 6.782519\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 6.948386\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 6.962440\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 7.219781\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 7.699132\n",
      "Epoch: 798/1000... Step: 25536... Loss: 2.986242... Val Loss: 7.575255\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 5.635494\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 8.441805\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 8.230691\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 8.054936\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 9.570004\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 8.657748\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 7.785700\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 7.481297\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 7.125282\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 6.734655\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 6.796468\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 6.872787\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 6.866370\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 7.114654\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 7.734516\n",
      "Epoch: 799/1000... Step: 25568... Loss: 2.361987... Val Loss: 7.815746\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 6.265845\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 10.719853\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 9.244162\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 8.591295\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 10.163068\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 8.974880\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 7.941131\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 7.728987\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 7.154319\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 6.628604\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 6.456790\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 6.964607\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 6.801080\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 7.059548\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 7.604870\n",
      "Epoch: 800/1000... Step: 25600... Loss: 5.753058... Val Loss: 7.450156\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 9.628822\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 11.015671\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 9.592978\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 8.943963\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 10.366385\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 9.678323\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 8.940205\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 8.625109\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 8.362674\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 7.897984\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 7.904718\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 7.996912\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 7.990375\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 8.386644\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 9.071976\n",
      "Epoch: 801/1000... Step: 25632... Loss: 2.276331... Val Loss: 8.957690\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 7.849700\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 10.382912\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 9.063260\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 8.447332\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 9.898318\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 8.911650\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 8.121602\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 7.789783\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 7.362902\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 6.933460\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 6.891481\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 7.111973\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 6.993742\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 7.340290\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 7.921386\n",
      "Epoch: 802/1000... Step: 25664... Loss: 4.878870... Val Loss: 7.807181\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 6.660307\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 8.700947\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 7.970718\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 7.378141\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 8.854092\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 8.032125\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 7.207228\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 6.816002\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 6.572569\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 6.250171\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 6.216323\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 6.372572\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 6.400649\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 6.823744\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 7.354418\n",
      "Epoch: 803/1000... Step: 25696... Loss: 3.958529... Val Loss: 7.302614\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 6.812429\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 11.061966\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 9.744070\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 9.196982\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 10.148623\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 8.984976\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 8.151775\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 8.398683\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 7.892399\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 7.370878\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 7.184707\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 7.483582\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 7.411444\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 7.548845\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 8.224971\n",
      "Epoch: 804/1000... Step: 25728... Loss: 2.441197... Val Loss: 8.098097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 7.738776\n",
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 9.542583\n",
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 8.737608\n",
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 8.174820\n",
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 9.629222\n",
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 8.855659\n",
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 8.095021\n",
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 7.853790\n",
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 7.413734\n",
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 7.031156\n",
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 7.079206\n",
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 7.176013\n",
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 7.031593\n",
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 7.414734\n",
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 7.941758\n",
      "Epoch: 805/1000... Step: 25760... Loss: 4.895915... Val Loss: 7.690826\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 6.228345\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 9.970568\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 9.374131\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 8.733183\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 9.984044\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 8.896218\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 8.045328\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 7.878665\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 7.524988\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 7.058430\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 6.923818\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 7.247253\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 7.495756\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 7.724245\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 8.147560\n",
      "Epoch: 806/1000... Step: 25792... Loss: 1.713926... Val Loss: 8.298629\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 11.864208\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 13.633302\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 11.965429\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 11.547154\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 12.582812\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 12.046716\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 11.452125\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 11.377296\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 11.071911\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 10.546826\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 10.626245\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 10.587303\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 10.415693\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 10.704212\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 11.415459\n",
      "Epoch: 807/1000... Step: 25824... Loss: 6.493591... Val Loss: 11.288903\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 6.139247\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 8.620154\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 8.281801\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 7.595544\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 8.687350\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 7.810939\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 7.101548\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 6.969415\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 6.723478\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 6.405806\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 6.356599\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 6.447517\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 6.711075\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 6.970908\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 7.446334\n",
      "Epoch: 808/1000... Step: 25856... Loss: 2.511487... Val Loss: 7.582905\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 6.914006\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 9.127338\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 8.355470\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 7.830822\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 9.649109\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 8.873907\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 8.093619\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 7.874532\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 7.422196\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 6.968468\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 6.979022\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 7.239528\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 7.058073\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 7.340629\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 7.933385\n",
      "Epoch: 809/1000... Step: 25888... Loss: 5.949879... Val Loss: 7.711690\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 6.353961\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 8.515002\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 7.921775\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 7.376209\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 9.100644\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 8.245014\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 7.363373\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 7.218062\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 6.867373\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 6.443315\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 6.379503\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 7.063639\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 6.942316\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 7.149731\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 7.800712\n",
      "Epoch: 810/1000... Step: 25920... Loss: 4.292113... Val Loss: 7.632640\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 10.985257\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 14.577137\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 14.623591\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 13.731565\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 14.791325\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 13.532758\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 12.654860\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 13.052749\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 12.616397\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 12.033378\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 11.789352\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 11.868245\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 12.115468\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 12.068370\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 12.403033\n",
      "Epoch: 811/1000... Step: 25952... Loss: 4.053018... Val Loss: 12.649893\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 8.030157\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 9.495644\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 8.850502\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 8.274370\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 9.653222\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 9.313445\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 8.548946\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 8.464131\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 8.100750\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 7.650116\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 7.639714\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 7.908427\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 7.734586\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 7.962863\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 8.575104\n",
      "Epoch: 812/1000... Step: 25984... Loss: 8.914049... Val Loss: 8.332796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 7.340745\n",
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 9.768190\n",
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 8.694372\n",
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 7.867169\n",
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 9.552583\n",
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 8.601528\n",
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 7.785403\n",
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 7.622728\n",
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 7.181629\n",
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 6.711301\n",
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 6.590446\n",
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 6.705282\n",
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 6.552688\n",
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 6.753315\n",
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 7.263573\n",
      "Epoch: 813/1000... Step: 26016... Loss: 7.827389... Val Loss: 7.130815\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 8.468664\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 9.716949\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 8.812778\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 8.231508\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 10.021643\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 9.284009\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 8.544344\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 8.396125\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 8.029187\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 7.581768\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 7.586806\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 7.826866\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 7.671086\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 7.938564\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 8.571107\n",
      "Epoch: 814/1000... Step: 26048... Loss: 7.330469... Val Loss: 8.355678\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 8.885199\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 10.264960\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 9.049868\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 8.665070\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 10.366045\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 9.683715\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 8.972659\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 8.883249\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 8.521736\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 8.086382\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 8.113680\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 8.444119\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 8.274111\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 8.511900\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 9.176289\n",
      "Epoch: 815/1000... Step: 26080... Loss: 5.412363... Val Loss: 8.942605\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 7.384087\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 10.033700\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 9.765724\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 9.057014\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 10.108149\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 9.103476\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 8.431272\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 8.669471\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 8.346883\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 7.932537\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 7.787301\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 7.805811\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 7.957620\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 8.079577\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 8.517246\n",
      "Epoch: 816/1000... Step: 26112... Loss: 2.378001... Val Loss: 8.506527\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 8.416192\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 10.100975\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 8.884135\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 8.268250\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 9.946243\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 8.942977\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 8.089650\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 8.136798\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 7.731554\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 7.167650\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 6.956663\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 7.532310\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 7.398133\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 7.504133\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 8.050394\n",
      "Epoch: 817/1000... Step: 26144... Loss: 3.358397... Val Loss: 7.892367\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 7.797461\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 10.162675\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 10.187911\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 9.202831\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 10.812915\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 9.882139\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 9.055454\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 8.689463\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 8.549758\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 8.132778\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 7.982475\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 8.318895\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 8.591422\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 8.982863\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 9.363531\n",
      "Epoch: 818/1000... Step: 26176... Loss: 6.708807... Val Loss: 9.358238\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 6.686951\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 8.456522\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 7.719371\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 7.082880\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 8.430445\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 7.712708\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 6.960151\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 6.725150\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 6.459482\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 6.055413\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 6.009858\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 6.174483\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 6.308787\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 6.705187\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 7.288006\n",
      "Epoch: 819/1000... Step: 26208... Loss: 2.085282... Val Loss: 7.245868\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 7.023309\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 8.838791\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 7.796802\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 7.396324\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 8.813438\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 7.997448\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 7.299331\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 7.088161\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 6.760635\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 6.376546\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 6.385525\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 6.662568\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 6.647777\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 6.981522\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 7.559075\n",
      "Epoch: 820/1000... Step: 26240... Loss: 2.634700... Val Loss: 7.499714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 7.005669\n",
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 8.895012\n",
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 8.026457\n",
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 7.507129\n",
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 9.150877\n",
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 8.317528\n",
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 7.546954\n",
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 7.279598\n",
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 6.861133\n",
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 6.446660\n",
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 6.464083\n",
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 6.641407\n",
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 6.529845\n",
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 6.939313\n",
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 7.540395\n",
      "Epoch: 821/1000... Step: 26272... Loss: 3.931151... Val Loss: 7.309292\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 7.382879\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 9.412279\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 8.224950\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 7.605456\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 9.322268\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 8.294075\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 7.427740\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 7.101465\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 6.672942\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 6.287417\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 6.169663\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 6.835954\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 6.774272\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 7.149616\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 7.826566\n",
      "Epoch: 822/1000... Step: 26304... Loss: 3.556160... Val Loss: 7.706275\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 6.848033\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 9.868536\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 9.235024\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 8.667190\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 9.618326\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 8.567196\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 7.877309\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 7.883662\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 7.487565\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 7.012914\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 6.933884\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 7.008139\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 7.159688\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 7.358092\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 7.967492\n",
      "Epoch: 823/1000... Step: 26336... Loss: 3.310228... Val Loss: 8.037046\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 6.686216\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 10.234735\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 9.461254\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 8.843538\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 10.082564\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 8.979310\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 8.150495\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 8.157261\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 7.761189\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 7.295788\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 7.133434\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 7.392293\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 7.468243\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 7.641606\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 8.123195\n",
      "Epoch: 824/1000... Step: 26368... Loss: 3.299754... Val Loss: 8.041888\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 8.272964\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 11.812053\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 11.171615\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 10.328587\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 11.453260\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 10.504617\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 9.730256\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 9.980717\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 9.573405\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 9.084789\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 8.966025\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 9.225441\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 9.312952\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 9.317670\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 9.813075\n",
      "Epoch: 825/1000... Step: 26400... Loss: 3.442225... Val Loss: 9.903870\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 6.804370\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 8.036744\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 7.843788\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 7.091764\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 8.914905\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 8.358067\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 7.555450\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 7.393829\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 7.075032\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 6.618737\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 6.607867\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 6.771649\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 6.671105\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 6.926377\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 7.555209\n",
      "Epoch: 826/1000... Step: 26432... Loss: 5.776149... Val Loss: 7.384162\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 6.765395\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 9.580292\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 8.943981\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 8.139629\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 9.847488\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 8.815343\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 7.900104\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 7.735624\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 7.383590\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 6.866802\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 6.675415\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 7.022708\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 7.059996\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 7.265755\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 7.771641\n",
      "Epoch: 827/1000... Step: 26464... Loss: 3.723841... Val Loss: 7.662322\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 7.577498\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 9.122467\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 8.230140\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 7.572119\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 9.459213\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 8.582284\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 7.797178\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 7.745811\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 7.376509\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 6.890828\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 6.803863\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 7.212149\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 7.054833\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 7.280398\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 7.929478\n",
      "Epoch: 828/1000... Step: 26496... Loss: 3.549742... Val Loss: 7.770103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 6.834566\n",
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 9.068188\n",
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 8.402464\n",
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 7.754097\n",
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 9.208592\n",
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 8.202620\n",
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 7.465292\n",
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 7.563916\n",
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 7.169475\n",
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 6.703781\n",
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 6.617946\n",
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 6.716399\n",
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 6.669298\n",
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 6.806302\n",
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 7.292612\n",
      "Epoch: 829/1000... Step: 26528... Loss: 3.526489... Val Loss: 7.267121\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 5.895890\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 9.307762\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 8.401865\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 7.929963\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 9.181778\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 8.100163\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 7.268626\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 7.277305\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 6.837419\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 6.398800\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 6.302075\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 6.526136\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 6.514647\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 6.707182\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 7.237990\n",
      "Epoch: 830/1000... Step: 26560... Loss: 3.891948... Val Loss: 7.148388\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 6.541892\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 8.518242\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 7.792798\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 6.980276\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 8.424983\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 7.502997\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 6.705528\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 6.433616\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 6.097914\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 5.762315\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 5.711844\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 6.005694\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 5.978951\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 6.274891\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 6.793129\n",
      "Epoch: 831/1000... Step: 26592... Loss: 5.681133... Val Loss: 6.630958\n",
      "Validation loss decreased (7.105702 --> 6.630958).  Saving model ...\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 6.402046\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 9.681550\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 9.042099\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 8.442609\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 9.445462\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 8.374977\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 7.619639\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 7.697326\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 7.407104\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 7.028715\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 6.959405\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 7.171457\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 7.486830\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 7.669271\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 8.325898\n",
      "Epoch: 832/1000... Step: 26624... Loss: 1.713692... Val Loss: 8.585526\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 6.188670\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 9.759575\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 8.665571\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 7.837406\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 9.357320\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 8.269005\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 7.323541\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 7.167502\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 6.697394\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 6.229418\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 6.071375\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 6.410413\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 6.329570\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 6.585233\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 7.112418\n",
      "Epoch: 833/1000... Step: 26656... Loss: 5.440318... Val Loss: 6.906384\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 6.811499\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 8.442187\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 7.874739\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 7.257021\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 8.997353\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 8.011626\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 7.150462\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 6.876490\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 6.505620\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 6.124309\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 6.077231\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 6.555534\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 6.412774\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 6.634745\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 7.213383\n",
      "Epoch: 834/1000... Step: 26688... Loss: 5.943567... Val Loss: 7.048748\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 13.566259\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 14.207636\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 12.644850\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 12.274728\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 13.677274\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 13.248145\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 12.417873\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 12.206939\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 11.954233\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 11.506495\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 11.628549\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 11.753339\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 11.527772\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 11.993621\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 12.748136\n",
      "Epoch: 835/1000... Step: 26720... Loss: 9.625484... Val Loss: 12.409779\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 6.616016\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 8.831585\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 7.968436\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 7.327463\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 8.802434\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 7.817013\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 6.910457\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 6.669197\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 6.249778\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 5.874319\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 5.800477\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 5.980000\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 5.924159\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 6.367213\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 6.979089\n",
      "Epoch: 836/1000... Step: 26752... Loss: 3.992758... Val Loss: 6.788997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 7.197532\n",
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 10.996648\n",
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 10.298779\n",
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 9.662077\n",
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 11.276488\n",
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 10.176536\n",
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 9.363482\n",
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 9.423535\n",
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 9.081929\n",
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 8.504647\n",
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 8.300084\n",
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 8.755793\n",
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 8.864904\n",
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 8.954367\n",
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 9.366750\n",
      "Epoch: 837/1000... Step: 26784... Loss: 2.566607... Val Loss: 9.395281\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 13.418034\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 13.469821\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 12.539622\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 12.109816\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 13.829596\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 13.461649\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 12.789898\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 12.661442\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 12.304991\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 11.818882\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 11.943484\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 12.026647\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 11.699585\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 11.995795\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 12.639581\n",
      "Epoch: 838/1000... Step: 26816... Loss: 11.165541... Val Loss: 12.312518\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 7.707255\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 9.813888\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 8.874353\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 8.321897\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 9.579095\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 8.596709\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 7.854797\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 8.095039\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 7.694839\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 7.187448\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 7.040478\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 7.268604\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 7.185814\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 7.271492\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 7.769947\n",
      "Epoch: 839/1000... Step: 26848... Loss: 3.376606... Val Loss: 7.664428\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 8.416206\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 10.295733\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 8.973716\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 8.496353\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 10.181487\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 9.314811\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 8.554243\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 8.465127\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 8.035917\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 7.587960\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 7.578109\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 7.826208\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 7.655441\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 7.943520\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 8.623548\n",
      "Epoch: 840/1000... Step: 26880... Loss: 6.172367... Val Loss: 8.378123\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 8.697905\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 9.934109\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 8.962463\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 8.257941\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 10.168610\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 9.321027\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 8.458307\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 8.086034\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 7.683000\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 7.357530\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 7.321831\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 7.668391\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 7.532740\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 7.930166\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 8.582079\n",
      "Epoch: 841/1000... Step: 26912... Loss: 7.838181... Val Loss: 8.370275\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 8.167894\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 10.039539\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 8.870290\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 8.029841\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 9.565819\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 8.496034\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 7.611222\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 7.599088\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 7.225595\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 6.807099\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 6.523609\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 6.966968\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 6.922348\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 7.130466\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 7.699773\n",
      "Epoch: 842/1000... Step: 26944... Loss: 4.833497... Val Loss: 7.514147\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 7.357378\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 11.348337\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 10.209795\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 9.362244\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 11.083404\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 9.879661\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 8.791472\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 8.593959\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 8.091614\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 7.531577\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 7.311967\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 7.994182\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 8.009115\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 8.300833\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 8.801779\n",
      "Epoch: 843/1000... Step: 26976... Loss: 4.100815... Val Loss: 8.746384\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 7.058636\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 9.362906\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 9.040116\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 8.186668\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 9.965008\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 8.954286\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 7.993986\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 7.750068\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 7.512895\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 7.077899\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 6.906683\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 7.316692\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 7.420490\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 7.873949\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 8.400502\n",
      "Epoch: 844/1000... Step: 27008... Loss: 10.336701... Val Loss: 8.276301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 7.021215\n",
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 8.255752\n",
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 7.612657\n",
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 7.034522\n",
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 8.541763\n",
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 7.914112\n",
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 7.117141\n",
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 6.883236\n",
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 6.661890\n",
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 6.297612\n",
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 6.269818\n",
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 6.439503\n",
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 6.488999\n",
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 6.919392\n",
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 7.527298\n",
      "Epoch: 845/1000... Step: 27040... Loss: 2.893263... Val Loss: 7.365732\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 5.632438\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 7.828825\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 7.573292\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 7.038623\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 8.633639\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 7.776391\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 7.069874\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 7.123007\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 6.809082\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 6.387053\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 6.295255\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 6.413844\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 6.461251\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 6.642910\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 7.149936\n",
      "Epoch: 846/1000... Step: 27072... Loss: 2.476250... Val Loss: 7.194956\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 8.893199\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 10.567859\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 9.614340\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 9.041395\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 10.165073\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 9.325294\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 8.681518\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 8.911491\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 8.522997\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 8.042395\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 8.095642\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 8.105568\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 7.979073\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 8.112260\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 8.738856\n",
      "Epoch: 847/1000... Step: 27104... Loss: 5.909039... Val Loss: 8.604028\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 6.131989\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 9.837081\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 9.130265\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 8.468941\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 9.977267\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 8.890523\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 8.025012\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 8.094337\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 7.676508\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 7.127521\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 7.002347\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 7.255400\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 7.286990\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 7.413669\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 8.020776\n",
      "Epoch: 848/1000... Step: 27136... Loss: 2.951643... Val Loss: 7.946792\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 7.118474\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 9.577408\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 8.615231\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 7.814212\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 9.219359\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 8.140419\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 7.291058\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 7.264432\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 6.828427\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 6.391976\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 6.187808\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 6.167425\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 6.106293\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 6.335353\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 6.892879\n",
      "Epoch: 849/1000... Step: 27168... Loss: 4.915145... Val Loss: 6.698856\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 6.308377\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 8.651725\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 7.939865\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 7.197536\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 8.826255\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 7.815595\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 6.966163\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 6.779589\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 6.423729\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 5.982047\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 5.857664\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 6.186574\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 6.193259\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 6.455969\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 6.971029\n",
      "Epoch: 850/1000... Step: 27200... Loss: 4.201803... Val Loss: 6.830732\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 5.992296\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 9.034331\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 8.430240\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 7.741109\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 9.134171\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 8.058424\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 7.272116\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 7.223736\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 6.817800\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 6.410346\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 6.329563\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 6.375678\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 6.450419\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 6.634871\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 7.079871\n",
      "Epoch: 851/1000... Step: 27232... Loss: 3.615870... Val Loss: 7.059436\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 7.380646\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 9.041642\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 8.148330\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 7.655406\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 9.391551\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 8.603749\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 7.789789\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 7.664552\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 7.332128\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 6.887487\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 6.808929\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 7.218449\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 7.099849\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 7.286480\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 7.847782\n",
      "Epoch: 852/1000... Step: 27264... Loss: 4.599743... Val Loss: 7.717186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 13.211244\n",
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 12.996033\n",
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 12.111262\n",
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 11.312247\n",
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 13.384098\n",
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 12.603441\n",
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 11.606506\n",
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 11.173805\n",
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 10.726198\n",
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 10.370101\n",
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 10.348019\n",
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 10.493374\n",
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 10.300823\n",
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 10.752764\n",
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 11.515602\n",
      "Epoch: 853/1000... Step: 27296... Loss: 7.648436... Val Loss: 11.314508\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 7.798767\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 10.087054\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 9.752433\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 9.008957\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 10.285824\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 9.278558\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 8.590069\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 8.621652\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 8.322174\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 7.954633\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 7.861234\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 7.954431\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 8.185397\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 8.336496\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 8.688247\n",
      "Epoch: 854/1000... Step: 27328... Loss: 2.234803... Val Loss: 8.726450\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 6.350096\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 9.419890\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 9.146381\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 8.475400\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 9.914393\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 8.846401\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 8.089322\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 8.159474\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 7.818227\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 7.377675\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 7.255363\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 7.414611\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 7.555756\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 7.710555\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 8.130239\n",
      "Epoch: 855/1000... Step: 27360... Loss: 1.952462... Val Loss: 8.161626\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 9.035975\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 11.130805\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 9.769104\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 9.336204\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 10.681530\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 9.986780\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 9.398884\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 9.697230\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 9.338494\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 8.820097\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 8.934316\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 9.082009\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 8.917785\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 8.986803\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 9.694599\n",
      "Epoch: 856/1000... Step: 27392... Loss: 3.281020... Val Loss: 9.520362\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 9.509099\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 10.225468\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 8.850781\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 8.292816\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 9.975235\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 9.189357\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 8.349974\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 8.010288\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 7.720971\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 7.362513\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 7.341775\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 7.794719\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 7.705230\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 7.975316\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 8.628016\n",
      "Epoch: 857/1000... Step: 27424... Loss: 4.252077... Val Loss: 8.411862\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 10.278741\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 11.608835\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 10.088429\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 9.611217\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 11.783948\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 10.988350\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 10.224796\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 9.853818\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 9.410498\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 8.901604\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 8.920072\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 9.070743\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 8.928885\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 9.288054\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 9.961312\n",
      "Epoch: 858/1000... Step: 27456... Loss: 6.837003... Val Loss: 9.719251\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 6.739604\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 9.633652\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 8.668682\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 7.921140\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 9.406845\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 8.360313\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 7.464735\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 7.332011\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 6.957020\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 6.495800\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 6.336163\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 6.567893\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 6.620820\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 6.856304\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 7.434686\n",
      "Epoch: 859/1000... Step: 27488... Loss: 3.479662... Val Loss: 7.301543\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 5.966860\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 8.082844\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 7.947557\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 7.506772\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 8.691839\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 7.774098\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 6.942342\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 6.786226\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 6.419925\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 6.093091\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 6.133309\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 6.214663\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 6.212283\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 6.549881\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 7.104896\n",
      "Epoch: 860/1000... Step: 27520... Loss: 2.302958... Val Loss: 7.008939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 12.515054\n",
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 13.397798\n",
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 11.993897\n",
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 11.932289\n",
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 13.507772\n",
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 13.154824\n",
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 12.498905\n",
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 12.297855\n",
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 11.998162\n",
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 11.498469\n",
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 11.756817\n",
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 11.806131\n",
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 11.523524\n",
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 11.948082\n",
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 12.690305\n",
      "Epoch: 861/1000... Step: 27552... Loss: 9.362366... Val Loss: 12.384113\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 5.971394\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 8.936052\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 8.442989\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 7.867936\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 9.226986\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 8.194907\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 7.326558\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 7.219282\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 6.809349\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 6.369446\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 6.315889\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 6.487218\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 6.507626\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 6.785246\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 7.320716\n",
      "Epoch: 862/1000... Step: 27584... Loss: 2.016889... Val Loss: 7.271419\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 12.128914\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 12.523920\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 11.491273\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 11.262296\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 13.044963\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 12.628311\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 11.928679\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 11.647530\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 11.308597\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 10.831653\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 11.004136\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 11.126799\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 10.852906\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 11.306141\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 12.032146\n",
      "Epoch: 863/1000... Step: 27616... Loss: 9.537061... Val Loss: 11.710488\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 8.034556\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 11.605601\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 11.492728\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 11.045468\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 11.916999\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 10.693592\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 9.908777\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 10.181605\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 9.738275\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 9.225704\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 9.078833\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 9.285530\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 9.568203\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 9.635525\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 10.138289\n",
      "Epoch: 864/1000... Step: 27648... Loss: 2.730125... Val Loss: 10.366767\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 6.480030\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 9.984302\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 9.245459\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 8.774736\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 9.823132\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 8.772883\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 8.035182\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 8.221906\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 7.815426\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 7.418724\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 7.335675\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 7.664907\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 7.914967\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 8.084674\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 8.563684\n",
      "Epoch: 865/1000... Step: 27680... Loss: 1.282347... Val Loss: 8.690355\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 7.735146\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 9.067488\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 8.100084\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 7.491301\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 9.378900\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 8.449616\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 7.631944\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 7.203971\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 6.855951\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 6.497531\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 6.473931\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 6.944290\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 6.932870\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 7.394340\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 8.027130\n",
      "Epoch: 866/1000... Step: 27712... Loss: 4.284711... Val Loss: 7.882884\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 6.019407\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 8.436492\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 7.847416\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 7.020432\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 8.538110\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 7.537552\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 6.647555\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 6.360731\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 5.979892\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 5.596901\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 5.471718\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 5.694815\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 5.688989\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 6.078690\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 6.551646\n",
      "Epoch: 867/1000... Step: 27744... Loss: 3.969559... Val Loss: 6.393842\n",
      "Validation loss decreased (6.630958 --> 6.393842).  Saving model ...\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 6.597641\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 7.911086\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 7.318702\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 6.867952\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 8.692837\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 7.950376\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 7.185006\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 6.924861\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 6.631222\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 6.246461\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 6.288558\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 6.508833\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 6.429379\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 6.729893\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 7.331325\n",
      "Epoch: 868/1000... Step: 27776... Loss: 3.741536... Val Loss: 7.152671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 6.668330\n",
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 8.982760\n",
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 8.519552\n",
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 7.689767\n",
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 9.435890\n",
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 8.466922\n",
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 7.674590\n",
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 7.496236\n",
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 7.269934\n",
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 6.851286\n",
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 6.689589\n",
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 7.099293\n",
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 7.218071\n",
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 7.510775\n",
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 8.034295\n",
      "Epoch: 869/1000... Step: 27808... Loss: 5.351039... Val Loss: 7.952678\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 9.659062\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 10.959764\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 9.497454\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 8.888048\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 11.030222\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 10.181637\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 9.402243\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 9.175518\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 8.741856\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 8.267649\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 8.261187\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 8.839505\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 8.705841\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 8.976176\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 9.739352\n",
      "Epoch: 870/1000... Step: 27840... Loss: 6.641113... Val Loss: 9.543874\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 9.151746\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 9.751695\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 8.543490\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 8.137933\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 9.957968\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 9.123699\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 8.366055\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 8.199698\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 7.829008\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 7.429780\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 7.393834\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 7.810624\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 7.688428\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 7.945776\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 8.609244\n",
      "Epoch: 871/1000... Step: 27872... Loss: 3.877517... Val Loss: 8.384699\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 9.225503\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 10.038247\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 8.588764\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 8.054430\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 9.859507\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 9.019431\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 8.321592\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 8.183068\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 7.806807\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 7.391370\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 7.353042\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 7.814523\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 7.687567\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 7.941229\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 8.608910\n",
      "Epoch: 872/1000... Step: 27904... Loss: 4.770395... Val Loss: 8.375565\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 11.777592\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 12.064784\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 10.685705\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 10.247739\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 12.326978\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 11.686721\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 11.061221\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 10.910486\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 10.513668\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 10.015705\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 10.084505\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 10.399175\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 10.206568\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 10.493005\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 11.276050\n",
      "Epoch: 873/1000... Step: 27936... Loss: 6.382950... Val Loss: 11.008125\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 8.146429\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 9.127270\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 8.493599\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 8.333298\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 9.906890\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 9.343808\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 8.772951\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 8.791694\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 8.490749\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 8.081386\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 8.305806\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 8.237996\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 8.142725\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 8.346289\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 8.981134\n",
      "Epoch: 874/1000... Step: 27968... Loss: 3.583957... Val Loss: 8.872836\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 9.344826\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 9.945776\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 8.853661\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 8.356532\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 10.157228\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 9.423262\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 8.653390\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 8.368188\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 8.002922\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 7.638848\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 7.672233\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 7.875957\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 7.739309\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 8.102646\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 8.789738\n",
      "Epoch: 875/1000... Step: 28000... Loss: 7.448911... Val Loss: 8.564982\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 9.236177\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 9.883746\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 8.531973\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 8.157477\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 9.809305\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 8.945022\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 8.144816\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 8.097447\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 7.719906\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 7.328192\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 7.214874\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 7.575371\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 7.426843\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 7.653816\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 8.320754\n",
      "Epoch: 876/1000... Step: 28032... Loss: 4.185013... Val Loss: 8.075091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 7.655632\n",
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 8.529396\n",
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 7.789977\n",
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 7.426627\n",
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 8.717514\n",
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 7.821358\n",
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 7.079546\n",
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 6.965796\n",
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 6.622855\n",
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 6.218230\n",
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 6.176781\n",
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 6.395399\n",
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 6.352580\n",
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 6.562073\n",
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 7.117663\n",
      "Epoch: 877/1000... Step: 28064... Loss: 4.183733... Val Loss: 6.943490\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 12.228488\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 13.060544\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 11.492613\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 11.263747\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 13.529077\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 12.891440\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 12.219466\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 11.958895\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 11.520842\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 10.964157\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 11.121395\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 11.583326\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 11.329001\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 11.618840\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 12.439690\n",
      "Epoch: 878/1000... Step: 28096... Loss: 7.246174... Val Loss: 12.184783\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 9.092785\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 9.632683\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 8.947710\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 8.708555\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 10.200973\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 9.703840\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 9.099027\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 9.129063\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 8.869333\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 8.446363\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 8.638645\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 8.532602\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 8.416736\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 8.622494\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 9.258389\n",
      "Epoch: 879/1000... Step: 28128... Loss: 3.928595... Val Loss: 9.112143\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 6.956417\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 8.400525\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 7.618104\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 6.989456\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 8.239135\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 7.367665\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 6.646294\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 6.548224\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 6.233381\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 5.853905\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 5.784927\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 5.903626\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 5.950326\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 6.188763\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 6.727659\n",
      "Epoch: 880/1000... Step: 28160... Loss: 2.501787... Val Loss: 6.592039\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 6.176186\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 9.436966\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 8.526688\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 8.059506\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 9.414217\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 8.337959\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 7.491652\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 7.644538\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 7.205312\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 6.698715\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 6.555710\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 6.799790\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 6.870532\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 7.043484\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 7.659581\n",
      "Epoch: 881/1000... Step: 28192... Loss: 3.310738... Val Loss: 7.649713\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 9.070573\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 9.462292\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 8.685551\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 8.361044\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 9.567867\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 8.776597\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 8.047377\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 8.039197\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 7.690476\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 7.301298\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 7.278480\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 7.495140\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 7.327239\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 7.543747\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 8.167139\n",
      "Epoch: 882/1000... Step: 28224... Loss: 6.023703... Val Loss: 7.911389\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 11.420895\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 10.842537\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 9.788902\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 9.464004\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 11.580021\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 10.986026\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 10.212039\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 10.034427\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 9.740324\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 9.256542\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 9.305402\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 9.572876\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 9.408164\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 9.732778\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 10.635055\n",
      "Epoch: 883/1000... Step: 28256... Loss: 4.310548... Val Loss: 10.345246\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 8.732886\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 9.169684\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 8.462556\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 8.257208\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 9.577007\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 8.938790\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 8.242651\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 8.137415\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 7.847350\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 7.493752\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 7.590643\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 7.650306\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 7.548242\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 7.791879\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 8.359662\n",
      "Epoch: 884/1000... Step: 28288... Loss: 4.083170... Val Loss: 8.203697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 6.621939\n",
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 9.201761\n",
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 8.057852\n",
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 7.695338\n",
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 9.121646\n",
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 8.101523\n",
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 7.198962\n",
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 7.194310\n",
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 6.752982\n",
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 6.251173\n",
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 6.167704\n",
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 6.759377\n",
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 6.719284\n",
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 6.967493\n",
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 7.657245\n",
      "Epoch: 885/1000... Step: 28320... Loss: 4.170168... Val Loss: 7.568157\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 5.725104\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 8.243710\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 7.762664\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 7.272950\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 8.692223\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 7.722735\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 6.947302\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 6.888850\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 6.527642\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 6.107958\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 6.015076\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 6.293967\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 6.381739\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 6.655822\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 7.139001\n",
      "Epoch: 886/1000... Step: 28352... Loss: 2.059131... Val Loss: 7.199838\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 6.761155\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 10.011242\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 9.302828\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 8.751634\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 10.174968\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 9.150736\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 8.321517\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 8.146981\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 7.788126\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 7.266943\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 7.122036\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 7.559621\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 7.769347\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 7.994475\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 8.461028\n",
      "Epoch: 887/1000... Step: 28384... Loss: 1.468491... Val Loss: 8.521395\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 7.714406\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 10.173676\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 9.454794\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 9.245648\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 10.068859\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 8.983459\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 8.273779\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 8.678420\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 8.263132\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 7.704359\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 7.481132\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 7.675548\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 7.865240\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 7.973093\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 8.598005\n",
      "Epoch: 888/1000... Step: 28416... Loss: 1.875589... Val Loss: 8.620584\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 8.815605\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 10.478487\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 9.219734\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 8.787157\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 10.530039\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 9.957028\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 9.206999\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 9.012322\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 8.482664\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 8.045334\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 8.041204\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 8.434820\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 8.163207\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 8.441695\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 9.153274\n",
      "Epoch: 889/1000... Step: 28448... Loss: 7.937341... Val Loss: 8.862902\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 7.101773\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 8.224815\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 7.731279\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 7.038549\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 8.612163\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 7.625973\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 6.845883\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 6.724488\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 6.378137\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 5.954261\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 5.786576\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 5.857912\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 5.819830\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 6.053648\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 6.629032\n",
      "Epoch: 890/1000... Step: 28480... Loss: 2.603497... Val Loss: 6.456074\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 5.149454\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 7.439970\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 7.061664\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 6.725889\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 8.420388\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 7.430266\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 6.664286\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 6.531475\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 6.160321\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 5.760313\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 5.702790\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 5.957178\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 5.982785\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 6.214460\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 6.787206\n",
      "Epoch: 891/1000... Step: 28512... Loss: 2.594378... Val Loss: 6.784214\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 5.901998\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 7.949094\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 7.227519\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 6.697195\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 8.272212\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 7.281131\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 6.510274\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 6.392323\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 6.023671\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 5.615914\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 5.514321\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 5.758778\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 5.741476\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 5.999149\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 6.550786\n",
      "Epoch: 892/1000... Step: 28544... Loss: 3.671123... Val Loss: 6.427313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 7.627068\n",
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 8.572544\n",
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 7.670690\n",
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 7.246539\n",
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 8.783501\n",
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 7.938739\n",
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 7.231327\n",
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 7.163028\n",
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 6.806322\n",
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 6.430084\n",
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 6.375644\n",
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 6.484156\n",
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 6.354722\n",
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 6.644651\n",
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 7.276674\n",
      "Epoch: 893/1000... Step: 28576... Loss: 3.209108... Val Loss: 7.029311\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 5.904262\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 8.668797\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 8.008522\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 7.593636\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 9.057793\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 8.045882\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 7.308815\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 7.488405\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 7.077404\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 6.628148\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 6.517162\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 6.666413\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 6.693427\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 6.858173\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 7.404271\n",
      "Epoch: 894/1000... Step: 28608... Loss: 2.960703... Val Loss: 7.399097\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 8.001204\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 9.021108\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 8.444178\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 8.052596\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 9.366187\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 8.699174\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 8.026846\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 7.920337\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 7.587939\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 7.224037\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 7.312830\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 7.356512\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 7.217672\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 7.479084\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 8.095530\n",
      "Epoch: 895/1000... Step: 28640... Loss: 7.655993... Val Loss: 7.882722\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 6.847411\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 7.802775\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 7.233938\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 6.524696\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 8.268037\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 7.489211\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 6.718399\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 6.530827\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 6.242589\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 5.826311\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 5.726585\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 5.882360\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 5.825272\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 6.125225\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 6.702680\n",
      "Epoch: 896/1000... Step: 28672... Loss: 2.640975... Val Loss: 6.537349\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 12.432365\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 12.482979\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 11.223377\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 10.733563\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 12.412847\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 11.824281\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 11.177599\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 11.093276\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 10.761859\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 10.294579\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 10.385665\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 10.632893\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 10.352557\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 10.611396\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 11.246153\n",
      "Epoch: 897/1000... Step: 28704... Loss: 9.471467... Val Loss: 10.927798\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 10.366060\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 10.767835\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 9.468596\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 9.382473\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 10.957502\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 10.158739\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 9.456605\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 9.461025\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 9.110248\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 8.668107\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 8.681745\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 8.951861\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 8.682659\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 8.855930\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 9.525913\n",
      "Epoch: 898/1000... Step: 28736... Loss: 8.052440... Val Loss: 9.260362\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 6.708119\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 7.089481\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 7.023809\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 6.690230\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 8.065226\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 7.377777\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 6.704398\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 6.514866\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 6.271282\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 5.948330\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 6.059032\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 6.066168\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 6.077946\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 6.356729\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 6.873019\n",
      "Epoch: 899/1000... Step: 28768... Loss: 3.674609... Val Loss: 6.764762\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 8.567026\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 11.667454\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 11.792755\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 11.261779\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 12.332001\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 11.105525\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 10.308318\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 10.473867\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 10.052799\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 9.652703\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 9.562737\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 9.779318\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 10.169509\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 10.263131\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 10.765995\n",
      "Epoch: 900/1000... Step: 28800... Loss: 2.090644... Val Loss: 11.041159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 6.353184\n",
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 7.911110\n",
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 7.618090\n",
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 6.897406\n",
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 8.232566\n",
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 7.389129\n",
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 6.707138\n",
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 6.509521\n",
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 6.228473\n",
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 5.886007\n",
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 5.849685\n",
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 6.131787\n",
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 6.270777\n",
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 6.524975\n",
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 6.947550\n",
      "Epoch: 901/1000... Step: 28832... Loss: 3.348532... Val Loss: 6.847415\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 10.856493\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 10.691547\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 9.412543\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 9.143362\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 11.005942\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 10.546674\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 9.944398\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 9.855905\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 9.718740\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 9.224702\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 9.392853\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 9.627619\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 9.482463\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 9.723183\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 10.433142\n",
      "Epoch: 902/1000... Step: 28864... Loss: 5.691849... Val Loss: 10.131124\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 5.961984\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 8.470800\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 8.266671\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 7.876476\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 9.308939\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 8.301665\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 7.564510\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 7.609106\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 7.235720\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 6.871780\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 6.824005\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 7.041691\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 7.213085\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 7.372590\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 7.905150\n",
      "Epoch: 903/1000... Step: 28896... Loss: 1.781913... Val Loss: 8.080670\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 6.448452\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 8.540638\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 8.261298\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 7.547580\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 8.930174\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 7.947591\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 7.121694\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 6.976205\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 6.622009\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 6.182592\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 6.052603\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 6.288793\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 6.342562\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 6.613869\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 7.180952\n",
      "Epoch: 904/1000... Step: 28928... Loss: 1.225061... Val Loss: 7.122319\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 9.866274\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 10.126879\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 8.921219\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 8.837908\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 11.208036\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 10.640814\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 10.017579\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 9.839259\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 9.520765\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 9.024831\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 9.175311\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 9.690160\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 9.515342\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 9.730475\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 10.550879\n",
      "Epoch: 905/1000... Step: 28960... Loss: 4.979424... Val Loss: 10.276971\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 7.298161\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 8.959027\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 8.141865\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 7.627618\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 8.868870\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 7.941260\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 7.227840\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 7.201373\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 6.891876\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 6.482510\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 6.356023\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 6.576593\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 6.721338\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 6.937524\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 7.473586\n",
      "Epoch: 906/1000... Step: 28992... Loss: 2.267285... Val Loss: 7.437806\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 13.918069\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 12.916729\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 11.502558\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 11.419124\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 13.204556\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 13.080670\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 12.421351\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 12.176105\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 12.084440\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 11.634638\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 11.860370\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 11.880870\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 11.748153\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 12.126757\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 12.906946\n",
      "Epoch: 907/1000... Step: 29024... Loss: 5.972005... Val Loss: 12.596171\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 5.249031\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 6.389151\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 6.329218\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 6.134123\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 7.684081\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 7.081015\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 6.363000\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 6.154532\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 5.906450\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 5.617977\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 5.730844\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 5.957275\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 6.046102\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 6.436502\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 7.068412\n",
      "Epoch: 908/1000... Step: 29056... Loss: 1.979538... Val Loss: 7.101924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 6.535900\n",
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 8.276070\n",
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 7.698060\n",
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 6.951124\n",
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 8.624388\n",
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 7.719377\n",
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 6.953391\n",
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 6.615236\n",
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 6.177956\n",
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 5.795448\n",
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 5.787300\n",
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 5.858159\n",
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 5.771144\n",
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 6.130421\n",
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 6.678649\n",
      "Epoch: 909/1000... Step: 29088... Loss: 4.187616... Val Loss: 6.537448\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 9.277582\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 9.878382\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 8.650818\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 8.344970\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 10.028873\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 9.434801\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 8.710193\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 8.602239\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 8.289840\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 7.854090\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 7.897375\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 8.197643\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 8.069180\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 8.327584\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 9.190811\n",
      "Epoch: 910/1000... Step: 29120... Loss: 4.001363... Val Loss: 8.934044\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 6.471237\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 7.373315\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 7.137099\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 6.530554\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 8.172070\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 7.293010\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 6.591137\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 6.409890\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 6.224613\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 5.905656\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 5.808880\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 6.005767\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 6.109798\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 6.464280\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 7.002774\n",
      "Epoch: 911/1000... Step: 29152... Loss: 5.031102... Val Loss: 6.903785\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 6.379311\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 8.087213\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 7.084754\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 6.637267\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 8.210953\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 7.289948\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 6.489374\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 6.299302\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 5.927109\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 5.546380\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 5.471874\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 5.965530\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 5.965165\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 6.272007\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 6.871337\n",
      "Epoch: 912/1000... Step: 29184... Loss: 3.730810... Val Loss: 6.714702\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 6.441283\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 8.690281\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 8.380482\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 7.849002\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 9.212679\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 8.231671\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 7.486601\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 7.443324\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 7.131549\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 6.737140\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 6.654546\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 6.843163\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 7.026641\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 7.235954\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 7.690513\n",
      "Epoch: 913/1000... Step: 29216... Loss: 1.907152... Val Loss: 7.724029\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 7.389563\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 8.644791\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 7.474640\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 6.953150\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 8.676519\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 7.938738\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 7.250892\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 7.115114\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 6.861937\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 6.421465\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 6.433680\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 6.777430\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 6.740360\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 6.974401\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 7.638536\n",
      "Epoch: 914/1000... Step: 29248... Loss: 2.102766... Val Loss: 7.474313\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 7.390347\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 9.969335\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 9.555455\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 8.862452\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 10.049749\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 9.026622\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 8.343878\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 8.433362\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 8.110392\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 7.798782\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 7.807838\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 8.046080\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 8.307550\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 8.418567\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 8.926416\n",
      "Epoch: 915/1000... Step: 29280... Loss: 1.382158... Val Loss: 9.112374\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 10.693601\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 10.825939\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 9.555452\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 9.428837\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 11.033760\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 10.820046\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 10.203136\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 9.962844\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 9.840252\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 9.402784\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 9.696374\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 9.723563\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 9.648884\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 9.962568\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 10.715106\n",
      "Epoch: 916/1000... Step: 29312... Loss: 6.414571... Val Loss: 10.457535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 11.053649\n",
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 12.007822\n",
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 10.504711\n",
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 10.481088\n",
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 12.222576\n",
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 11.678569\n",
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 11.024218\n",
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 10.966422\n",
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 10.606673\n",
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 10.090579\n",
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 10.256934\n",
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 10.458756\n",
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 10.222422\n",
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 10.472284\n",
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 11.340203\n",
      "Epoch: 917/1000... Step: 29344... Loss: 4.625359... Val Loss: 11.062329\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 7.020401\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 7.965884\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 7.396759\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 6.924417\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 8.450620\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 7.603106\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 6.912945\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 6.872420\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 6.551933\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 6.126614\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 6.110099\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 6.160006\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 6.090122\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 6.313550\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 6.910429\n",
      "Epoch: 918/1000... Step: 29376... Loss: 2.294904... Val Loss: 6.755362\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 9.596978\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 9.849138\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 8.536772\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 8.109217\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 9.960446\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 9.268851\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 8.509048\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 8.325400\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 8.131860\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 7.684686\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 7.704476\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 7.990341\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 7.878825\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 8.162266\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 8.874763\n",
      "Epoch: 919/1000... Step: 29408... Loss: 3.142146... Val Loss: 8.655189\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 7.220832\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 7.946472\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 7.229751\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 6.742343\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 8.390580\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 7.624325\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 6.889157\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 6.676545\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 6.374114\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 6.016811\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 6.006910\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 6.090168\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 6.028913\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 6.384092\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 7.017457\n",
      "Epoch: 920/1000... Step: 29440... Loss: 3.034841... Val Loss: 6.809100\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 9.455411\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 9.773759\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 8.765487\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 8.142768\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 10.317405\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 9.474972\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 8.668383\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 8.206629\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 7.849910\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 7.546854\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 7.508000\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 7.860266\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 7.818442\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 8.295369\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 8.984495\n",
      "Epoch: 921/1000... Step: 29472... Loss: 6.640792... Val Loss: 8.810452\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 7.871734\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 7.968850\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 7.336080\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 6.975892\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 8.837825\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 8.245547\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 7.519899\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 7.096824\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 6.871645\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 6.538939\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 6.651784\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 6.830003\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 6.783460\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 7.267724\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 7.924794\n",
      "Epoch: 922/1000... Step: 29504... Loss: 4.169991... Val Loss: 7.724086\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 7.445533\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 8.295051\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 7.341112\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 6.701421\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 8.240624\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 7.340562\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 6.553626\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 6.377966\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 6.021458\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 5.662418\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 5.572523\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 5.806024\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 5.779716\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 6.079868\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 6.703372\n",
      "Epoch: 923/1000... Step: 29536... Loss: 3.032046... Val Loss: 6.525073\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 5.625455\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 7.699646\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 7.376359\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 6.901767\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 8.448536\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 7.570731\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 6.845380\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 6.858119\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 6.548073\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 6.162397\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 6.116068\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 6.241598\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 6.296369\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 6.529979\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 7.078893\n",
      "Epoch: 924/1000... Step: 29568... Loss: 3.067578... Val Loss: 7.078384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 5.507032\n",
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 8.088054\n",
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 7.553734\n",
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 7.280056\n",
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 8.746627\n",
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 7.720112\n",
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 6.988952\n",
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 6.963129\n",
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 6.552999\n",
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 6.215589\n",
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 6.217488\n",
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 6.486365\n",
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 6.581607\n",
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 6.782214\n",
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 7.310344\n",
      "Epoch: 925/1000... Step: 29600... Loss: 2.949847... Val Loss: 7.369366\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 6.109963\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 8.303962\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 7.507963\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 7.063771\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 8.698090\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 7.698800\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 6.936814\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 6.822905\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 6.472481\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 6.020207\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 5.912090\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 6.157388\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 6.196737\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 6.404443\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 6.980487\n",
      "Epoch: 926/1000... Step: 29632... Loss: 3.103067... Val Loss: 6.921343\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 6.998229\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 8.262423\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 7.253654\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 6.869883\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 8.698065\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 8.060665\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 7.311900\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 6.964054\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 6.637250\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 6.272061\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 6.361013\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 6.564618\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 6.502746\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 6.839802\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 7.448213\n",
      "Epoch: 927/1000... Step: 29664... Loss: 5.852390... Val Loss: 7.299922\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 13.573529\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 13.353026\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 11.490851\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 11.379450\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 13.034901\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 12.557476\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 11.877384\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 11.751130\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 11.553881\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 11.112737\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 11.307937\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 11.494974\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 11.242792\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 11.482928\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 12.189999\n",
      "Epoch: 928/1000... Step: 29696... Loss: 8.949070... Val Loss: 11.852991\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 5.786301\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 7.181210\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 7.165593\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 6.682066\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 8.255918\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 7.329759\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 6.691443\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 6.694429\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 6.381609\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 6.039884\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 6.074846\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 6.229180\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 6.308121\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 6.488055\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 6.962949\n",
      "Epoch: 929/1000... Step: 29728... Loss: 1.512106... Val Loss: 7.034021\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 7.020874\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 8.958412\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 8.404738\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 7.418525\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 9.281156\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 8.124260\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 7.172041\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 6.838140\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 6.360742\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 5.997932\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 5.837716\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 6.296130\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 6.208254\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 6.537534\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 7.107500\n",
      "Epoch: 930/1000... Step: 29760... Loss: 5.267129... Val Loss: 6.989888\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 6.472553\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 7.061779\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 7.053036\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 6.410609\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 7.979047\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 7.175786\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 6.483779\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 6.195159\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 5.952814\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 5.574237\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 5.598919\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 5.781726\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 5.789100\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 6.040784\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 6.571054\n",
      "Epoch: 931/1000... Step: 29792... Loss: 2.561996... Val Loss: 6.467103\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 8.797418\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 11.234691\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 10.557422\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 10.000704\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 11.689706\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 10.922698\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 9.948966\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 9.664902\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 9.232005\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 8.745707\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 8.698092\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 8.825699\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 8.916470\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 9.150403\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 9.670848\n",
      "Epoch: 932/1000... Step: 29824... Loss: 5.786450... Val Loss: 9.729738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 8.106138\n",
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 9.224821\n",
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 8.154683\n",
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 7.922658\n",
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 9.275470\n",
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 8.545122\n",
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 7.795079\n",
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 7.715104\n",
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 7.396967\n",
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 7.014099\n",
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 7.034335\n",
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 7.415844\n",
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 7.266684\n",
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 7.602917\n",
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 8.317107\n",
      "Epoch: 933/1000... Step: 29856... Loss: 7.039532... Val Loss: 8.117417\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 13.832564\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 13.369245\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 11.993738\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 12.016196\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 13.258411\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 12.783126\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 12.040737\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 11.959226\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 11.691707\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 11.268395\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 11.429655\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 11.505305\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 11.210855\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 11.468476\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 12.164774\n",
      "Epoch: 934/1000... Step: 29888... Loss: 8.593144... Val Loss: 11.831498\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 7.610430\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 8.214453\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 7.399148\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 7.185395\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 8.540458\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 7.716476\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 6.999451\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 6.967824\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 6.631336\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 6.226821\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 6.229623\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 6.414345\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 6.389698\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 6.634282\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 7.255858\n",
      "Epoch: 935/1000... Step: 29920... Loss: 4.477490... Val Loss: 7.080475\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 7.215415\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 7.593440\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 6.995255\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 6.560452\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 8.312074\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 7.441602\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 6.667507\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 6.492270\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 6.146805\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 5.757422\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 5.691309\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 6.081426\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 5.995912\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 6.323242\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 6.928833\n",
      "Epoch: 936/1000... Step: 29952... Loss: 4.975268... Val Loss: 6.733921\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 7.030326\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 7.853432\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 7.424574\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 6.981030\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 8.715908\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 7.703248\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 6.866713\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 6.671789\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 6.367610\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 5.954317\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 5.806436\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 6.155264\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 6.185684\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 6.497389\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 7.046543\n",
      "Epoch: 937/1000... Step: 29984... Loss: 2.882149... Val Loss: 6.914556\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 6.481524\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 8.148823\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 7.133216\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 6.847263\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 8.454992\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 7.582520\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 6.850222\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 6.780028\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 6.388518\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 5.984215\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 6.006046\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 6.308745\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 6.264747\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 6.611574\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 7.241825\n",
      "Epoch: 938/1000... Step: 30016... Loss: 3.931216... Val Loss: 7.120483\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 5.726211\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 7.375396\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 6.786585\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 6.391196\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 8.096881\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 7.117946\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 6.332604\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 6.151276\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 5.784371\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 5.390230\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 5.330999\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 5.654974\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 5.603422\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 5.904715\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 6.492775\n",
      "Epoch: 939/1000... Step: 30048... Loss: 3.289817... Val Loss: 6.376150\n",
      "Validation loss decreased (6.393842 --> 6.376150).  Saving model ...\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 9.166991\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 9.901037\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 8.474257\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 8.526251\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 10.042090\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 9.268125\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 8.712194\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 8.899724\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 8.521558\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 8.070347\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 8.210711\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 8.602413\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 8.395464\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 8.532896\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 9.217221\n",
      "Epoch: 940/1000... Step: 30080... Loss: 5.664434... Val Loss: 8.948019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 8.770325\n",
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 9.212474\n",
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 8.216670\n",
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 7.875390\n",
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 9.746892\n",
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 9.072095\n",
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 8.382676\n",
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 8.037880\n",
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 7.695483\n",
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 7.339289\n",
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 7.433408\n",
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 7.786794\n",
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 7.655089\n",
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 8.105899\n",
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 8.767973\n",
      "Epoch: 941/1000... Step: 30112... Loss: 7.087331... Val Loss: 8.535969\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 5.867084\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 7.389307\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 7.211153\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 6.964300\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 8.101967\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 7.175901\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 6.550686\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 6.610765\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 6.294944\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 6.015104\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 6.035573\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 6.129031\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 6.282430\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 6.535632\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 7.079927\n",
      "Epoch: 942/1000... Step: 30144... Loss: 2.357580... Val Loss: 7.085548\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 7.024123\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 7.870404\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 6.954334\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 6.536204\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 8.211373\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 7.276357\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 6.523468\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 6.381984\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 6.066893\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 5.646312\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 5.550375\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 5.951018\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 5.918233\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 6.168479\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 6.746967\n",
      "Epoch: 943/1000... Step: 30176... Loss: 3.939329... Val Loss: 6.609045\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 6.389742\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 8.762651\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 8.324964\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 7.896979\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 9.246459\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 8.202965\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 7.443388\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 7.487995\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 7.110113\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 6.702150\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 6.613083\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 6.742906\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 6.902149\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 7.088136\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 7.582711\n",
      "Epoch: 944/1000... Step: 30208... Loss: 2.184665... Val Loss: 7.677664\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 7.135151\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 7.659956\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 7.163449\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 6.744583\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 8.578488\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 7.806413\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 7.038977\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 6.803794\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 6.574516\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 6.163134\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 6.227517\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 6.333700\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 6.301402\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 6.615273\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 7.292742\n",
      "Epoch: 945/1000... Step: 30240... Loss: 5.855838... Val Loss: 7.231706\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 8.946229\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 9.452883\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 8.362622\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 7.889026\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 9.596883\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 9.005058\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 8.257648\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 7.948842\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 7.663086\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 7.298811\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 7.375141\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 7.620317\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 7.527970\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 7.918771\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 8.636469\n",
      "Epoch: 946/1000... Step: 30272... Loss: 6.373742... Val Loss: 8.383079\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 8.499691\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 9.244133\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 8.213527\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 7.582299\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 9.212289\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 8.132280\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 7.254134\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 7.012317\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 6.647794\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 6.301946\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 6.068092\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 6.328248\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 6.351852\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 6.642272\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 7.271312\n",
      "Epoch: 947/1000... Step: 30304... Loss: 4.305853... Val Loss: 7.120173\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 7.446343\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 8.434470\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 7.415830\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 6.901621\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 8.717978\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 7.704482\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 6.813864\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 6.571452\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 6.176183\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 5.772845\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 5.636604\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 6.136964\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 6.082278\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 6.394376\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 7.034440\n",
      "Epoch: 948/1000... Step: 30336... Loss: 4.644011... Val Loss: 6.872320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 8.642568\n",
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 9.326633\n",
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 8.112878\n",
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 8.187599\n",
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 9.788284\n",
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 9.055578\n",
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 8.521452\n",
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 8.549899\n",
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 8.199527\n",
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 7.801468\n",
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 7.956473\n",
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 8.458196\n",
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 8.286966\n",
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 8.576710\n",
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 9.208296\n",
      "Epoch: 949/1000... Step: 30368... Loss: 6.323939... Val Loss: 8.921535\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 5.873756\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 7.250093\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 6.572086\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 6.045681\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 7.707253\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 6.771515\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 6.017179\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 5.741366\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 5.407903\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 5.053428\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 5.011547\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 5.367763\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 5.335614\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 5.682695\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 6.215362\n",
      "Epoch: 950/1000... Step: 30400... Loss: 3.980709... Val Loss: 6.083259\n",
      "Validation loss decreased (6.376150 --> 6.083259).  Saving model ...\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 7.810962\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 7.847317\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 7.236174\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 6.956810\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 8.407440\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 7.642839\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 6.931957\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 6.766102\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 6.562241\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 6.241755\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 6.228605\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 6.412506\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 6.344336\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 6.670606\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 7.271044\n",
      "Epoch: 951/1000... Step: 30432... Loss: 3.016150... Val Loss: 7.082138\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 6.115811\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 7.024261\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 6.582987\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 5.948919\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 7.438088\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 6.601907\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 5.871879\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 5.554604\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 5.331391\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 5.058377\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 5.037784\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 5.170792\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 5.261278\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 5.705157\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 6.186873\n",
      "Epoch: 952/1000... Step: 30464... Loss: 3.952832... Val Loss: 6.093239\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 6.276422\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 8.781670\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 8.070164\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 7.869847\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 9.189569\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 8.167048\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 7.418001\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 7.696231\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 7.274290\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 6.825470\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 6.761089\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 6.953683\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 7.098350\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 7.216826\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 7.852279\n",
      "Epoch: 953/1000... Step: 30496... Loss: 2.505836... Val Loss: 7.814719\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 6.024285\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 8.419535\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 7.766849\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 7.384621\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 8.965565\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 7.962295\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 7.218092\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 7.187001\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 6.841726\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 6.485632\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 6.461982\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 6.921949\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 7.091677\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 7.270929\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 7.775651\n",
      "Epoch: 954/1000... Step: 30528... Loss: 2.399948... Val Loss: 7.811726\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 7.123412\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 8.250220\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 7.679059\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 6.855906\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 8.743342\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 7.783310\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 6.939711\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 6.658396\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 6.419077\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 6.072383\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 5.934478\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 6.263367\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 6.345291\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 6.744747\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 7.278489\n",
      "Epoch: 955/1000... Step: 30560... Loss: 10.108332... Val Loss: 7.137807\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 8.234025\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 8.261642\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 7.715408\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 7.059471\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 8.823374\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 7.807214\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 6.950858\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 6.629604\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 6.341095\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 6.079188\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 5.877452\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 6.299005\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 6.373250\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 6.728099\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 7.350037\n",
      "Epoch: 956/1000... Step: 30592... Loss: 3.746274... Val Loss: 7.261890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 7.771050\n",
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 8.294744\n",
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 7.364854\n",
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 6.882861\n",
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 8.873251\n",
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 7.923638\n",
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 7.027559\n",
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 6.718406\n",
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 6.339138\n",
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 5.988742\n",
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 5.901774\n",
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 6.296760\n",
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 6.265059\n",
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 6.648096\n",
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 7.306762\n",
      "Epoch: 957/1000... Step: 30624... Loss: 5.012537... Val Loss: 7.147859\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 10.877748\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 10.390882\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 9.280834\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 8.588207\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 10.490927\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 9.904327\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 9.132426\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 8.758104\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 8.520108\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 8.214264\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 8.226515\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 8.476632\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 8.403824\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 8.893460\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 9.550497\n",
      "Epoch: 958/1000... Step: 30656... Loss: 6.106255... Val Loss: 9.343048\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 7.299501\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 8.014865\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 7.259986\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 6.612725\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 7.814458\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 6.919249\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 6.238157\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 6.159896\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 5.873729\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 5.543865\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 5.445975\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 5.493129\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 5.551279\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 5.833405\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 6.343709\n",
      "Epoch: 959/1000... Step: 30688... Loss: 2.165601... Val Loss: 6.188354\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 6.009479\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 7.130126\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 6.598942\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 6.267901\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 7.915207\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 7.016353\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 6.299835\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 6.200233\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 5.932519\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 5.555963\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 5.554924\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 5.706081\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 5.820388\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 6.076763\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 6.613157\n",
      "Epoch: 960/1000... Step: 30720... Loss: 2.507554... Val Loss: 6.588410\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 7.966718\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 9.011528\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 7.847566\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 7.617648\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 9.335595\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 8.735741\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 8.009951\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 7.765303\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 7.441259\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 7.085462\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 7.237370\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 7.397555\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 7.308062\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 7.597647\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 8.227654\n",
      "Epoch: 961/1000... Step: 30752... Loss: 6.363838... Val Loss: 8.059576\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 7.078947\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 7.366797\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 6.565725\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 6.190144\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 7.759095\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 7.068763\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 6.421481\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 6.271953\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 6.055812\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 5.746761\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 5.811131\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 5.987324\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 5.991459\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 6.288069\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 6.869930\n",
      "Epoch: 962/1000... Step: 30784... Loss: 2.386711... Val Loss: 6.709276\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 8.617323\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 8.834646\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 7.785010\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 7.316602\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 9.218007\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 8.475207\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 7.716974\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 7.393524\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 7.095380\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 6.768375\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 6.793794\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 7.062947\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 6.965658\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 7.410794\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 8.050290\n",
      "Epoch: 963/1000... Step: 30816... Loss: 4.297583... Val Loss: 7.844701\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 6.744882\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 7.545209\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 6.743366\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 6.230944\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 7.940639\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 7.064666\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 6.308749\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 6.119339\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 5.790911\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 5.439271\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 5.396398\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 5.448365\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 5.427819\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 5.754995\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 6.366912\n",
      "Epoch: 964/1000... Step: 30848... Loss: 3.737939... Val Loss: 6.191128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 8.890914\n",
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 10.923120\n",
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 10.472953\n",
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 9.750846\n",
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 11.745125\n",
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 10.716219\n",
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 10.045659\n",
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 9.883905\n",
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 9.791565\n",
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 9.355102\n",
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 9.260910\n",
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 9.900598\n",
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 10.205140\n",
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 10.469462\n",
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 10.931179\n",
      "Epoch: 965/1000... Step: 30880... Loss: 9.393729... Val Loss: 10.971354\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 9.097759\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 8.896227\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 7.860027\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 7.418140\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 9.409059\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 8.870399\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 8.071492\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 7.687599\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 7.453870\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 7.130960\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 7.152689\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 7.414012\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 7.371125\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 7.802329\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 8.543592\n",
      "Epoch: 966/1000... Step: 30912... Loss: 6.335786... Val Loss: 8.337430\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 9.483234\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 9.753022\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 8.694131\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 8.560942\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 10.380136\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 9.769459\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 9.109313\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 8.986088\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 8.655886\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 8.269137\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 8.420381\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 8.446775\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 8.271573\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 8.554862\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 9.241192\n",
      "Epoch: 967/1000... Step: 30944... Loss: 7.228661... Val Loss: 9.001896\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 9.012058\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 8.965754\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 7.979589\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 7.941964\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 9.620077\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 9.116155\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 8.541406\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 8.467184\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 8.236031\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 7.897181\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 8.103741\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 8.215967\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 8.113934\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 8.397522\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 9.056770\n",
      "Epoch: 968/1000... Step: 30976... Loss: 4.468401... Val Loss: 8.838067\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 8.190718\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 9.647310\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 9.490754\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 8.843792\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 9.887806\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 8.900318\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 8.376925\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 8.534214\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 8.291797\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 7.956196\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 7.907537\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 7.949179\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 8.276267\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 8.459195\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 8.864803\n",
      "Epoch: 969/1000... Step: 31008... Loss: 1.656947... Val Loss: 8.910163\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 5.685713\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 7.269802\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 6.640274\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 6.361076\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 7.898408\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 6.978266\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 6.281926\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 6.273638\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 5.916118\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 5.547771\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 5.550691\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 5.683512\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 5.711802\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 5.956827\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 6.535784\n",
      "Epoch: 970/1000... Step: 31040... Loss: 2.646600... Val Loss: 6.433553\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 10.054750\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 10.039603\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 8.857472\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 8.701473\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 10.559358\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 9.904891\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 9.230708\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 9.013093\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 8.699097\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 8.326947\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 8.447235\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 8.731988\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 8.539047\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 8.869388\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 9.550546\n",
      "Epoch: 971/1000... Step: 31072... Loss: 7.770696... Val Loss: 9.290789\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 8.319483\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 8.727700\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 7.467676\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 7.310154\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 9.018667\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 8.313331\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 7.699855\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 7.632118\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 7.315585\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 6.945207\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 7.057690\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 7.314995\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 7.199481\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 7.429500\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 8.065494\n",
      "Epoch: 972/1000... Step: 31104... Loss: 3.968223... Val Loss: 7.843023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 8.819245\n",
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 8.884419\n",
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 7.670321\n",
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 7.525265\n",
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 9.215100\n",
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 8.659319\n",
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 8.037270\n",
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 7.868071\n",
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 7.645363\n",
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 7.322930\n",
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 7.461411\n",
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 7.621363\n",
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 7.535743\n",
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 7.841396\n",
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 8.490556\n",
      "Epoch: 973/1000... Step: 31136... Loss: 3.945244... Val Loss: 8.268777\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 7.590520\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 8.188013\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 7.629425\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 7.330627\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 8.791992\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 7.762032\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 6.937437\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 6.771070\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 6.382515\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 5.993904\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 5.842110\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 6.040653\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 5.978694\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 6.188373\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 6.801015\n",
      "Epoch: 974/1000... Step: 31168... Loss: 2.741493... Val Loss: 6.619512\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 7.120682\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 7.249084\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 6.443252\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 6.106588\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 7.877814\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 7.078726\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 6.344307\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 6.136277\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 5.874041\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 5.535104\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 5.509589\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 5.866771\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 5.834057\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 6.149870\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 6.809329\n",
      "Epoch: 975/1000... Step: 31200... Loss: 4.569989... Val Loss: 6.667505\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 6.646978\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 7.670894\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 6.834601\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 6.726735\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 8.349572\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 7.603937\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 6.954296\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 6.868051\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 6.513048\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 6.159266\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 6.277511\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 6.327837\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 6.264009\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 6.555971\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 7.229447\n",
      "Epoch: 976/1000... Step: 31232... Loss: 3.806074... Val Loss: 7.043899\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 5.552768\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 6.469920\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 6.411355\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 6.401205\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 7.926894\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 7.196359\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 6.488591\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 6.295908\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 6.010879\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 5.732164\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 5.949246\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 6.085441\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 6.133646\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 6.376265\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 6.995787\n",
      "Epoch: 977/1000... Step: 31264... Loss: 2.016969... Val Loss: 6.940410\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 12.761757\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 12.741138\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 11.077127\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 10.864265\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 12.554073\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 12.028991\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 11.511596\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 11.526207\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 11.211542\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 10.695687\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 10.806132\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 11.224908\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 10.896902\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 11.126189\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 11.856479\n",
      "Epoch: 978/1000... Step: 31296... Loss: 11.166328... Val Loss: 11.499372\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 6.878751\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 7.692370\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 6.857289\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 6.461422\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 8.444524\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 7.480635\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 6.638436\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 6.357997\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 6.014873\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 5.680024\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 5.617588\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 6.160872\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 6.067109\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 6.410088\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 7.089540\n",
      "Epoch: 979/1000... Step: 31328... Loss: 5.421330... Val Loss: 6.976861\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 6.185565\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 7.004531\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 7.101852\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 6.786208\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 8.478477\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 7.581345\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 6.913724\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 6.827884\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 6.576561\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 6.264780\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 6.291294\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 6.443515\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 6.583831\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 6.809755\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 7.368516\n",
      "Epoch: 980/1000... Step: 31360... Loss: 1.727342... Val Loss: 7.404638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 5.852058\n",
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 6.827140\n",
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 6.760587\n",
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 6.410213\n",
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 8.109640\n",
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 7.285860\n",
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 6.520638\n",
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 6.258812\n",
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 6.010223\n",
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 5.641603\n",
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 5.650468\n",
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 5.847311\n",
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 6.018459\n",
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 6.369063\n",
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 7.001683\n",
      "Epoch: 981/1000... Step: 31392... Loss: 3.249233... Val Loss: 7.019411\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 6.559064\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 8.480896\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 7.564060\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 7.036617\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 8.486309\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 7.475285\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 6.726587\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 6.648679\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 6.253886\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 5.866465\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 5.821756\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 5.857230\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 5.910613\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 6.147750\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 6.746509\n",
      "Epoch: 982/1000... Step: 31424... Loss: 2.832229... Val Loss: 6.649868\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 7.101644\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 6.604927\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 6.179161\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 5.987118\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 7.494197\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 6.978972\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 6.390469\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 6.212016\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 6.059045\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 5.799705\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 5.887904\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 6.132947\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 6.129080\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 6.469465\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 7.145948\n",
      "Epoch: 983/1000... Step: 31456... Loss: 3.885486... Val Loss: 6.958563\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 13.537799\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 12.058351\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 10.722119\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 10.950120\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 12.783373\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 12.688919\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 12.185366\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 12.132974\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 12.090779\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 11.534494\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 11.895241\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 11.935985\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 11.757671\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 11.988517\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 12.738542\n",
      "Epoch: 984/1000... Step: 31488... Loss: 7.658913... Val Loss: 12.366244\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 7.118922\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 8.419378\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 7.784743\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 7.481522\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 8.825603\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 7.814249\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 7.122601\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 7.236249\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 6.858449\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 6.406132\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 6.341181\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 6.516923\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 6.601047\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 6.765274\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 7.389389\n",
      "Epoch: 985/1000... Step: 31520... Loss: 1.940950... Val Loss: 7.307530\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 5.949854\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 7.492623\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 6.910895\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 6.671880\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 8.209972\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 7.259418\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 6.562953\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 6.637001\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 6.279530\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 5.902697\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 5.865881\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 6.122056\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 6.194326\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 6.415007\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 7.001714\n",
      "Epoch: 986/1000... Step: 31552... Loss: 2.410374... Val Loss: 6.933118\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 7.619499\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 8.883800\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 8.419452\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 7.817213\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 8.834395\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 7.815483\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 7.157011\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 7.137221\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 6.782973\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 6.397807\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 6.322007\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 6.387148\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 6.443769\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 6.636785\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 7.118346\n",
      "Epoch: 987/1000... Step: 31584... Loss: 1.341460... Val Loss: 7.004165\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 5.742911\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 6.858353\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 6.426846\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 6.190361\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 7.594589\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 6.719788\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 6.001032\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 5.804104\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 5.443690\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 5.128155\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 5.178384\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 5.412536\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 5.414131\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 5.694231\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 6.238835\n",
      "Epoch: 988/1000... Step: 31616... Loss: 2.432374... Val Loss: 6.096722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 8.106758\n",
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 9.332964\n",
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 9.380496\n",
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 8.995260\n",
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 10.067282\n",
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 9.076587\n",
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 8.454274\n",
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 8.684935\n",
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 8.363936\n",
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 7.961754\n",
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 7.890734\n",
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 7.952857\n",
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 8.265560\n",
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 8.437430\n",
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 8.985356\n",
      "Epoch: 989/1000... Step: 31648... Loss: 3.685811... Val Loss: 9.027958\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 7.033053\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 8.401017\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 7.894713\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 7.399698\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 8.898334\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 7.919271\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 7.261038\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 7.199858\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 6.917538\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 6.489497\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 6.404414\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 6.705234\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 6.892142\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 7.094157\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 7.645784\n",
      "Epoch: 990/1000... Step: 31680... Loss: 2.512836... Val Loss: 7.645226\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 5.859372\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 7.828470\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 7.533139\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 7.388269\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 8.672117\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 7.684943\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 6.934639\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 6.892593\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 6.484489\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 6.228311\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 6.323557\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 6.670877\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 6.859632\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 7.066654\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 7.577303\n",
      "Epoch: 991/1000... Step: 31712... Loss: 1.476045... Val Loss: 7.649706\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 6.772110\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 9.399887\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 8.821981\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 8.286764\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 9.730434\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 8.649224\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 7.862186\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 7.819183\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 7.320013\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 6.959321\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 6.973050\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 7.021439\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 7.187131\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 7.354805\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 7.859447\n",
      "Epoch: 992/1000... Step: 31744... Loss: 1.736143... Val Loss: 7.928875\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 6.762408\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 6.931485\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 7.013608\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 6.896436\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 8.355015\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 7.631061\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 6.997894\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 7.065189\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 6.753955\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 6.329567\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 6.377274\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 6.743847\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 6.752054\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 6.883875\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 7.584423\n",
      "Epoch: 993/1000... Step: 31776... Loss: 3.154724... Val Loss: 7.399912\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 11.102028\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 10.769095\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 9.441467\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 9.420752\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 11.226364\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 10.753823\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 10.223427\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 10.125766\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 9.828557\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 9.360800\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 9.566698\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 9.624778\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 9.474681\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 9.762060\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 10.443158\n",
      "Epoch: 994/1000... Step: 31808... Loss: 6.022675... Val Loss: 10.132723\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 7.782820\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 8.884280\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 8.196447\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 7.495634\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 8.758824\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 7.825283\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 7.156740\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 7.051741\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 6.779114\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 6.398997\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 6.262148\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 6.439954\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 6.616353\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 6.922304\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 7.445609\n",
      "Epoch: 995/1000... Step: 31840... Loss: 3.415732... Val Loss: 7.358289\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 6.666871\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 7.184076\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 6.657304\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 6.386821\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 7.869614\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 7.083625\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 6.419318\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 6.365533\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 6.098215\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 5.744057\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 5.815139\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 5.937438\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 5.972708\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 6.204947\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 6.833306\n",
      "Epoch: 996/1000... Step: 31872... Loss: 3.321836... Val Loss: 6.732366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 6.239572\n",
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 7.761332\n",
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 6.997638\n",
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 6.552501\n",
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 8.364344\n",
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 7.449526\n",
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 6.619338\n",
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 6.328149\n",
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 6.020957\n",
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 5.623298\n",
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 5.525546\n",
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 5.918766\n",
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 5.967020\n",
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 6.312934\n",
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 6.849662\n",
      "Epoch: 997/1000... Step: 31904... Loss: 3.395784... Val Loss: 6.750327\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 6.841115\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 7.888204\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 7.075421\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 6.743364\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 8.122459\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 7.240501\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 6.462704\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 6.422417\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 6.060576\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 5.676647\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 5.641238\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 5.848469\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 5.846793\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 6.149891\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 6.824949\n",
      "Epoch: 998/1000... Step: 31936... Loss: 4.612766... Val Loss: 6.690796\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 7.290042\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 9.534466\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 8.915572\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 8.519157\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 10.139258\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 9.144911\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 8.439240\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 8.494733\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 8.174039\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 7.697578\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 7.577614\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 7.915917\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 8.087436\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 8.234309\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 8.682082\n",
      "Epoch: 999/1000... Step: 31968... Loss: 1.907362... Val Loss: 8.723349\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 6.326009\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 6.567075\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 6.053940\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 5.771853\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 7.597268\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 6.938495\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 6.293027\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 6.073885\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 5.823081\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 5.507867\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 5.555374\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 5.750039\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 5.708146\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 6.071248\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 6.683120\n",
      "Epoch: 1000/1000... Step: 32000... Loss: 3.492358... Val Loss: 6.486288\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "counter = 0\n",
    "print_every = 32\n",
    "\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    \n",
    "    for batches, labels in train_data_loader:\n",
    "        counter += 1\n",
    "        \n",
    "        batch_size_calc = len(labels)\n",
    "        hidden = model.init_hidden(batch_size=batch_size_calc)\n",
    "    \n",
    "        \n",
    "\n",
    "        hidden = tuple([e.data for e in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        output, hidden = model(batches, hidden)\n",
    "        \n",
    "        \n",
    "        loss = abs(criterion(output.squeeze(), labels.float()))\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "        if counter%print_every == 0:\n",
    "            \n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for inp, lab in val_data_loader:\n",
    "                \n",
    "                batch_size_calc = len(lab)\n",
    "\n",
    "                val_h = model.init_hidden(batch_size = batch_size_calc)\n",
    "\n",
    "                \n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = abs(criterion(out.squeeze(), lab.float()))\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "                \n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), '/home/sven/LCBench/state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Msqrt: 0.180\n",
      "Test loss: 5.635\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/home/sven/LCBench/state_dict.pt'))\n",
    "\n",
    "test_losses = []\n",
    "msqrt = 0\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "for inputs, labels in test_data_loader:\n",
    "    \n",
    "    batch_size_calc = len(labels)\n",
    "    h = model.init_hidden(batch_size_calc)\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    output, h = model(inputs, h)\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = (output.squeeze())\n",
    "    \n",
    "    msqrt = msqrt + mean_squared_error(labels.detach().numpy(), pred.detach().numpy())\n",
    "    \n",
    "test_acc = msqrt/len(test_data_loader.dataset)\n",
    "print(\"Msqrt: {:.3f}\".format(test_acc))\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLearningCurvePredictor():\n",
    "    \"\"\"A learning curve predictor that predicts the last observed epoch of the validation accuracy as final performance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for datapoint in X:\n",
    "            predictions.append(datapoint[\"Train/val_accuracy\"][-1])\n",
    "        return predictions\n",
    "    \n",
    "def score(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on validation set: 31.921338670622784\n"
     ]
    }
   ],
   "source": [
    "# Training & tuning\n",
    "predictor = SimpleLearningCurvePredictor()\n",
    "for data in train_data:\n",
    "    data['Train/val_accuracy']=data['Train/val_accuracy'][0:10]\n",
    "\n",
    "predictor.fit(train_data, train_targets)\n",
    "preds = predictor.predict(val_data)\n",
    "mse = score(val_targets, preds)\n",
    "print(\"Score on validation set:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test score: 24.199496266785523\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation (after tuning)\n",
    "final_preds = predictor.predict(test_data)\n",
    "final_score = score(test_targets, final_preds)\n",
    "print(\"Final test score:\", final_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
