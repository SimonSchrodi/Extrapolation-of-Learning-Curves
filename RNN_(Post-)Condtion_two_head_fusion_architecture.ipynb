{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task A: Creating a Performance Predictor\n",
    "\n",
    "In this task, you will use training data from 2000 configurations on a single OpenML dataset to train a performance predictor. The data will be splitted into train, test and validation set and we will only use the first 10 epochs of the learning curves for predicitons. You are provided with the full benchmark logs for Fashion-MNIST, that is learning curves, config parameters and gradient statistics, and you can use them freely.\n",
    "\n",
    "For questions, you can contact zimmerl@informatik.uni-freiburg.\n",
    "\n",
    "__Note: Please use the dataloading and splits you are provided with in this notebook.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifications:\n",
    "\n",
    "* Data: fashion_mnist.json\n",
    "* Number of datasets: 1\n",
    "* Number of configurations: 2000\n",
    "* Number of epochs seed during prediction: 10\n",
    "* Available data: Learning curves, architecture parameters and hyperparameters, gradient statistics \n",
    "* Target: Final validation accuracy\n",
    "* Evaluation metric: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and splitting data\n",
    "\n",
    "__Note__: There are 51 steps logged, 50 epochs plus the 0th epoch, prior to any weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd ..\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils_prep\n",
    "from api import Benchmark\n",
    "import utils_prep2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading data...\n",
      "==> No cached data found or cache set to False.\n",
      "==> Reading json data...\n",
      "==> Done.\n"
     ]
    }
   ],
   "source": [
    "bench_dir = \"/home/sven/LCBench/data/11604705/fashion_mnist.json\"\n",
    "bench = Benchmark(bench_dir, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1000\n",
      "Validation: 500\n",
      "Test: 500\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "def cut_data(data, cut_position):\n",
    "    targets = []\n",
    "    for dp in data:\n",
    "        targets.append(dp[\"Train/val_accuracy\"][50])\n",
    "        for tag in dp:\n",
    "            if tag.startswith(\"Train/\"):\n",
    "                dp[tag] = dp[tag][0:cut_position]\n",
    "    return data, targets\n",
    "\n",
    "def read_data():\n",
    "    dataset_name = 'Fashion-MNIST'\n",
    "    n_configs = bench.get_number_of_configs(dataset_name)\n",
    "    \n",
    "    # Query API\n",
    "    data = []\n",
    "    for config_id in range(n_configs):\n",
    "        data_point = dict()\n",
    "        data_point[\"config\"] = bench.query(dataset_name=dataset_name, tag=\"config\", config_id=config_id)\n",
    "        for tag in bench.get_queriable_tags(dataset_name=dataset_name, config_id=config_id):\n",
    "            if tag.startswith(\"Train/\"):\n",
    "                data_point[tag] = bench.query(dataset_name=dataset_name, tag=tag, config_id=config_id)    \n",
    "        data.append(data_point)\n",
    "        \n",
    "    # Split: 50% train, 25% validation, 25% test (the data is already shuffled)\n",
    "    indices = np.arange(n_configs)\n",
    "    ind_train = indices[0:int(np.floor(0.5*n_configs))]\n",
    "    ind_val = indices[int(np.floor(0.5*n_configs)):int(np.floor(0.75*n_configs))]\n",
    "    ind_test = indices[int(np.floor(0.75*n_configs)):]\n",
    "\n",
    "    array_data = np.array(data)\n",
    "    train_data = array_data[ind_train]\n",
    "    val_data = array_data[ind_val]\n",
    "    test_data = array_data[ind_test]\n",
    "    \n",
    "    # Cut curves for validation and test\n",
    "    cut_position = 11\n",
    "    val_data, val_targets = cut_data(val_data, cut_position)\n",
    "    test_data, test_targets = cut_data(test_data, cut_position)\n",
    "    train_data, train_targets = cut_data(train_data, 51)   # Cut last value as it is repeated\n",
    "    \n",
    "    return train_data, val_data, test_data, train_targets, val_targets, test_targets\n",
    "    \n",
    "train_data, val_data, test_data, train_targets, val_targets, test_targets = read_data()\n",
    "\n",
    "print(\"Train:\", len(train_data))\n",
    "print(\"Validation:\", len(val_data))\n",
    "print(\"Test:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains the configuration of the trained model and learning curves as well as global and layer-wise gradient statistics.\n",
    "\n",
    "__Note__: Not all parameters vary across different configurations. The varying parameters are batch_size, max_dropout, max_units, num_layers, learning_rate, momentum, weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config example: {'batch_size': 93, 'imputation_strategy': 'mean', 'learning_rate_scheduler': 'cosine_annealing', 'loss': 'cross_entropy_weighted', 'network': 'shapedmlpnet', 'max_dropout': 0.06145480624960298, 'normalization_strategy': 'standardize', 'optimizer': 'sgd', 'cosine_annealing_T_max': 50, 'cosine_annealing_eta_min': 1e-08, 'activation': 'relu', 'max_units': 402, 'mlp_shape': 'funnel', 'num_layers': True, 'learning_rate': 0.07306153347321286, 'momentum': 0.5844418984083981, 'weight_decay': 0.05967268273584057}\n",
      "\n",
      "\n",
      "DATA Keys: dict_keys(['config', 'Train/loss', 'Train/train_accuracy', 'Train/val_accuracy', 'Train/train_cross_entropy', 'Train/val_cross_entropy', 'Train/train_balanced_accuracy', 'Train/val_balanced_accuracy', 'Train/test_result', 'Train/test_cross_entropy', 'Train/test_balanced_accuracy', 'Train/gradient_max', 'Train/gradient_mean', 'Train/gradient_median', 'Train/gradient_std', 'Train/gradient_q10', 'Train/gradient_q25', 'Train/gradient_q75', 'Train/gradient_q90', 'Train/layer_wise_gradient_max_layer_0', 'Train/layer_wise_gradient_mean_layer_0', 'Train/layer_wise_gradient_median_layer_0', 'Train/layer_wise_gradient_std_layer_0', 'Train/layer_wise_gradient_q10_layer_0', 'Train/layer_wise_gradient_q25_layer_0', 'Train/layer_wise_gradient_q75_layer_0', 'Train/layer_wise_gradient_q90_layer_0', 'Train/gradient_norm', 'Train/lr'])\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "print(\"Config example:\", test_data[0][\"config\"])\n",
    "print(\"\\n\")\n",
    "print(\"DATA Keys:\", test_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f44963ac390>]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Learning curve\n",
    "plt.plot(train_data[10][\"Train/val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f448639a080>]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAD4CAYAAAA6j0u4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3de3xV5Z3v8c9v751swjWJhBgCiCjg4B2jYGuttYpoLzjT1tNOp1BHRMeec6adnrZ22jnO0XbGmTMzbZ3OOEM9KGgvYrWFtlpFKs6MFiXgBUQF5B4CBBIuIZDr7/yxV2Ab906EfVm5fN+v137ttZ79rOeiIb88l72WuTsiIiK5FAm7ASIi0v8p2IiISM4p2IiISM4p2IiISM4p2IiISM7Fwm5AbzVy5EgfP3582M0QEelTVq9evc/dy7qmK9ikMX78eKqrq8NuhohIn2Jm21KlaxpNRERyTsFGRERyTsFGRERyTsFGRERyTsFGRERyLivBxsxmmtnbZrbJzO5M8XnczB4NPn/JzMYnffbNIP1tM7uupzLN7MygjE1BmYWnWoeIiORHxsHGzKLAvwDXA1OAz5nZlC7ZbgEa3P1s4HvA3wXXTgE+C5wLzAT+1cyiPZT5d8D3grIagrJPuo5M+y0iIu9fNr5ncxmwyd03A5jZz4BZwPqkPLOAvw6Ofw780MwsSP+ZuzcDW8xsU1Aeqco0szeBq4E/DvIsDMq9/xTq+H0W+v4eD72whfojLSk/u3JSGVXjS3NRrYhIr5aNYFMJ7Eg63wlMS5fH3dvM7CBwWpC+ssu1lcFxqjJPAw64e1uK/KdSx7uY2TxgHsC4cePSdrg7P3l5Oxv3Nr4n3R1Wbqln8W2Xn1K5IiJ9me4gkMTd5wPzAaqqqk7pqXLPfOXDKdO/+ODLaUc8IiL9XTY2CNQAY5POxwRpKfOYWQwYAezv5tp06fuB4qCMrnWdbB15FY9FaG7tyHe1IiK9QjaCzSpgYrBLrJDEYvzSLnmWAnOC408Dv/PE86iXAp8NdpKdCUwEXk5XZnDNc0EZBGUuOcU68ioei9LSrmAjIgNTxtNowfrIfweeBqLAAnd/w8zuBqrdfSnw/4CHg8X5ehLBgyDfYhKbCdqAL7l7O0CqMoMqvwH8zMy+A7wSlM2p1JFPiZFN3qsVEekVLPHHv3RVVVXl2bzr87d/uZan1u5m9V9dm7UyRUR6GzNb7e5VXdN1B4E8iceiNLdpGk1EBiYFmzyJxyI0t2kaTUQGJgWbPInHorS2O+0dmrYUkYFHwSZP4gWJ/9QtmkoTkQFIwSZPCqOJ/9SaShORgUjBJk86RzbaJCAiA5GCTZ7EY4kbTWsaTUQGIgWbPInHNI0mIgOXgk2edAabY7o/mogMQAo2eRIvSEyjac1GRAYiBZs80TSaiAxkCjZ5ciLYaGQjIgOPgk2eFHYGG63ZiMgApGCTJ51bnzWNJiIDkR4LnSed02j6no1I3+DuNLd1cKy1nWOtHRxtbedoSztHW9s51tpOS1sHzW0dtLR30NKWeLW2d76cts7jjsRxS5C3uS3xeUtbooy2jsQ9E9s6nI7Od0+kdXiiHe7Q4R68oLW9g7b2RN62jg7a2xOfRczAIGJGxMDMAGgP6mjvcNr9xD0aoxEjFrGk9wixiLHia1cxKNjUlC0KNnmiOwiI5E97h3PoaCsHj7Zy4GgrB5paOHi0lUNHWznc3EbjsTYag/fDzW0c6Xy1tL/rOBs3zi2IGrFIhMJY8Iq++z0WTfyij5gRL4hQZCd++UMiaETMiETAOPFZLGrEoongEItEMON4UIITwQkgFokQMSMagWgkQnD3LNo7oL3j3QGvvd2DurNLwSZPTkyjKdiIvF8dHU5DUwv1R1poaGqloamFA00njg82JQJK11djcxvdPRcyFjGGDYoxdFCMIYUxhsZjFA8uZExJjMGFUYbEYwyJRxlcGKOoIEpRYZSigiiDCiIMKogyqCBKPAge8ViEwmj0eDApiBoFQRCIRuz46GKgU7DJE219FjnB3Tl4tJXag8eoPXiUXQcS73sPNVPX2Ezd4Wb2NTazr7El7eiiMBphxOACiosKGF5UQPnwQUwqH8aI4Ly4qIDiwYnXiKJCigcXMHxQAcMGxYjHIgoCeZZRsDGzUuBRYDywFbjJ3RtS5JsDfDs4/Y67LwzSLwEeAoqAJ4E/d3dPV64lfjp+ANwANAFfdPc1PdTxXWA2UOLuQzPpbybi2o0mA8ix1nZ2NjRRc+AYtQeOsutg4r324DF2HTzK7oPHaGp59x9e0YhRNjRO2bA45cMHce7o4ZQNi1M2NE7p0DglgwsoGZwIGiWDCxlcGFXA6EMyHdncCSx393vN7M7g/BvJGYLAcRdQBTiw2syWBkHpfuBW4CUSwWYm8FQ35V4PTAxe04Lrp/VQx6+AHwIbM+xrRsyMwlhE02jSbxw61srmuiNs2dfItv1NbK9vYkd94n3PoeZ35TWDUcPiVIwo4pzTh/GRyaOoGDGIihFFVBQPYvSIIsqGxYnmYK1AeodMg80s4KrgeCGwgi7BBrgOWObu9QBmtgyYaWYrgOHuvjJIXwTcSCLYpCt3FrDI3R1YaWbFZlYR5H1PHcBPk8rPsKuZ06Ohpa9xd3YfOsaGPY1s3HOYd+qO8E5dI5vrjrCv8URAMYOK4YMYWzqYKyeWMa50MGNLB1NZUkTFiEGUDx9EQVTftBjIMg025e5eGxzvBspT5KkEdiSd7wzSKoPjrundldtdWanST4qZzQPmAYwbN+5kL+9RXCMb6cUOH2tl/a5DvFl7iLf3NLJhz2E27DnM4WNtx/OUDC5gQtlQrj6njAllQ5kwcggTyoYytrTo+CYYkVR6DDZm9ixweoqPvpV8Eqy1ZL5PsItclZumrvnAfICqqqqs1xmPRbVmI71Cw5EWXtt5gDd2HWL9rkOs23WQbfubjn9ePLiASeXDmHXRaCaXD2Ni+TAmlQ+jdEhhiK2WvqzHYOPu16T7zMz2mFmFu9cG01l7U2Sr4cSUGMAYEtNiNcFxcnpNcJyu3BpgbIpr0tXRq8RjEVraFWwkv9raO3h7z2HWbD/AK9sbeHX7ATbvO3L883Glgzl39HA+c8kYzh09gimjhzNqWLxXTD1L/5HpNNpSYA5wb/C+JEWep4G/MbOS4HwG8E13rzezQ2Y2ncQGgdnAP/dQ7lLgv5vZz0hsEDgYBKSUdWTYt6wrjEVobtWajeTWoWOtrNnWwOptDVRvbeC1nQeO7/waObSQi8eV8OmqMVw0tphzR49gRFFByC2WgSDTYHMvsNjMbgG2ATcBmFkVcLu7zw2Cyj3AquCauzsX8oE7OLH1+anglbZcEjvWbgA2kdj6fDNAd3WY2d8DfwwMNrOdwAPu/tcZ9vuUxAuiWrORrNvf2MyL7+znpS37qd7awNt7DuOe2Er8BxXDuKlqLBePK2bquBLGlBRpxCKhMO/ua7YDWFVVlVdXV2e1zJv+/fdEDH427/KslisDS2NzGy9v2c8Lm/bzwqZ9vLX7MABDCqNMPaOEqjNKqRpfwkVjixkS1/e2Jb/MbLW7V3VN109iHsVjERqb23rOKNLFzoYmnnljD8+s30311gbaOpzCWISqM0r42nWT+cBZp3F+5Qhi2l4svZSCTR7FY1H2N7aE3QzpA9ydt/cc5ul1iQDzxq5DAEwqH8qtV07girNHcskZJVm/M69IrijY5FG8QF/qlPTa2jtYva2BZ9bvYdn6PWyvb8IMpo4r4ZvXn8OMc0/nzJFDwm6myClRsMmjeFRf6pR3a2pp4z827GPZ+j387q09NDS1UhiN8IGzT+O2D0/g2inljBo2KOxmimRMwSaPEiMbBZuBrqPDWbl5P4+vqeGpdbU0tbQzfFCMq88ZxYxzT+fKSWUM1cK+9DP6ic6jeCyqJ3UOYO/UNfLEmp38Yk0Nuw4eY2g8xicuGM0nLxrNZWeW6t5h0q8p2OSRbsQ58Bxrbec3r9fy45e2sWb7ASIGH5pYxp03/AHX/kE5RYVa4JeBQcEmjzpvxOnu+mJdP7e5rpEfv7Sdn6/eycGjrUwoG8Jf3nAON15UyajhWoORgUfBJo/iBVHcobXdKYwp2PQ3be0dLFu/h4dXbuPFd/YTixjXnXc6fzLtDKZPKNUfGDKgKdjkUfKjoQtjmp/vLxqb23h01Q4efGELOxuOUllcxNeum8xnqsZoJ5lIQMEmj04Emw6GhdwWyVztwaM89MJWfvLydg4fa+PS8SV8+2NTuHZKuZ44KdKFgk0edT5cStuf+7ZNew/zL8+9w69e20WHO9efX8GtH5rARWOLw26aSK+lYJNHnVNnesxA37S5rpH7lm9kyWu7KCqIMvvy8dz8wfGMLR0cdtNEej0FmzxKnkaTvmPLviP88/KN/PLVGuKxKPOunMC8D03gtKHxsJsm0mco2ORRvCARbPTFzr6h5sBRvrdsA794pYaCqHHLFWdy24fPYqSCjMhJU7DJI63Z9A2NzW3cv2ITD/znFhz44gfGc9uHJ2hnmUgGFGzyKHnrs/Q+be0dLK7eyT8te5t9jS3ceNFovjbzHCqLi8Jumkifp2CTR8dHNq0a2fQ2z2+o47u/Wc+GPY1cOr6EB+Zcqt1lIlmU0TcLzazUzJaZ2cbgvSRNvjlBno1mNicp/RIzW2tmm8zsPgu+Yp2uXEu4L8j/uplN7a4OMxtsZr8xs7fM7A0zuzeT/maqc81G02i9x+6Dx7jt4WrmLHiZ5rYO7v/8VBbfdrkCjUiWZfo19juB5e4+EVgenL+LmZUCdwHTgMuAu5KC0v3ArcDE4DWzh3KvT8o7L7i+pzr+wd3PAS4GPmhm12fY51OmabTeo73DeeiFLVzzT8+z4u06vnbdZJ75ypVcf36FbisjkgOZBptZwMLgeCFwY4o81wHL3L3e3RuAZcBMM6sAhrv7Snd3YFHS9enKnQUs8oSVQHFQTso63L3J3Z8DcPcWYA0wJsM+n7JCbX3uFdbvOsQf3f8if/2r9Vw8rphnvnIlX/rI2cenOUUk+zJdsyl399rgeDdQniJPJbAj6XxnkFYZHHdN767c7spKlX6cmRUDnwB+kK4zZjaPxIiJcePGpct2yk6s2WhkE4ajLe18/9kNPPBfWygZXMAPPnsRn7xwtEYyInnQY7Axs2eB01N89K3kE3d3M/NsNSyb5ZpZDPgpcJ+7b+6mrvnAfICqqqqs96VzGq2lXSObfHth0z7ufOJ1dtQf5bOXjuXO68+heHBh2M0SGTB6DDbufk26z8xsj5lVuHttMJ21N0W2GuCqpPMxwIogfUyX9JrgOF25NcDYFNekq6PTfGCju38/XV/y4fiajXaj5c3Bpla+++R6Flfv5MyRQ/jZvOlMn3Ba2M0SGXAyXbNZCnTuLpsDLEmR52lghpmVBIv2M4Cng2myQ2Y2PdiFNjvp+nTlLgVmB7vSpgMHg3JS1gFgZt8BRgBfzrCvGYtFI0QjpjWbPPntulqu+d7zPL6mhts/fBZP/fmHFGhEQpLpms29wGIzuwXYBtwEYGZVwO3uPtfd683sHmBVcM3d7l4fHN8BPAQUAU8Fr7TlAk8CNwCbgCbgZoB0dZjZGBLTfW8Ba4K5+R+6+wMZ9vuU6dHQube/sZlv/3IdT63bzZSK4Tz4xUs5r3JE2M0SGdAyCjbuvh/4aIr0amBu0vkCYEGafOedRLkOfClNW95Th7vvBHrV6m/no6ElN17bcYDbH1nN/iMtfH3mZG790AQKonpQnUjYdAeBPIvHolqzyZHFq3bw7SXrKBsa54k/+4BGMyK9iIJNnsULNI2WbS1tHdz96zd4ZOV2rjh7JPd97mJKh2inmUhvomCTZ4VRTaNl095Dx/izH69h9bYGbvvwBL42YzIxTZuJ9DoKNnmWGNko2GTDmu0N3P7wag4fa+OHf3wxH79gdNhNEpE0FGzyLB6L6uFpWfCr13bx1cde4/Thg1h0y2Wcc/rwsJskIt1QsMkzbX3OjLvzw99t4h+XbeDS8SX8+xeqtD4j0gco2ORZPBahsbkt7Gb0Sc1t7XzzibU8saaGP7y4kns/db5uninSRyjY5Jm2Pp+ahiMt3PbIal7eUs9fXDuJ/3H12bqBpkgfomCTZ9r6fPK27DvCzQ++zK4Dx/jBZy9i1kWVPV8kIr2Kgk2e6Q4CJ2ddzUHmLHgZB35y6zSqxpeG3SQROQUKNnkWj0UVbN6nVVvr+dMHVzFsUIxH5k5jQtnQsJskIqdIwSbPCmMRPTztfVjx9l5uf2Q1o0cU8fDcaVQWF4XdJBHJgIJNnmkarWe/eb2WLz/6ChNHDWPRLZcxcmg87CaJSIZ0X488i8eitHU47R1ZfxBov/Doqu38j5+u4cIxxfx03nQFGpF+QsEmz+IFwaOhNbp5jwX/tYVvPL6WKyaW8fAt0xhRVBB2k0QkSxRs8uz4o6G1/fldfvzSNu7+9Xpmnns6D8yuoqhQX9YU6U8UbPKs8xvvWrc54Rev7OTbv1zHRyaXcd/nLqYwph9Lkf5G/6rz7PjIRncRAOC363bzvx57nelnnsb9f3KJAo1IP5XRv2wzKzWzZWa2MXgvSZNvTpBno5nNSUq/xMzWmtkmM7vPgvuPpCvXEu4L8r9uZlPfRx2/NbPXzOwNM/s3Mwt1fqZzzUbTaPD8hjr+509f4YIxI/jRnCoGFWjqTKS/yvTPyDuB5e4+EVgenL+LmZUCdwHTgMuAu5KC0v3ArcDE4DWzh3KvT8o7L7i+pzpucvcLgfOAMuAzGfY5I5pGS3hp835ue7ias0cN5aEvXsbQuHbhi/RnmQabWcDC4HghcGOKPNcBy9y93t0bgGXATDOrAIa7+0p3d2BR0vXpyp0FLPKElUBxUE7KOgDc/VBwbQwoBELdc1yoDQK8tuMAtyysprK4iEW3XMaIwdp1JtLfZRpsyt29NjjeDZSnyFMJ7Eg63xmkVQbHXdO7K7e7slKlA2BmTwN7gcPAz9N1xszmmVm1mVXX1dWly5aRE7vRBubIZvv+Jm5+aBUlQwr48Vx9j0ZkoOgx2JjZs2a2LsVrVnK+YHSS9VFDNsp19+uACiAOXN1NvvnuXuXuVWVlZZlUmdZADjYHm1r54kMv0+HOoj+dxukjBoXdJBHJkx4nyt39mnSfmdkeM6tw99pgOmtvimw1wFVJ52OAFUH6mC7pNcFxunJrgLEprklXR3I/jpnZEhJTccvS9SnXjq/ZDLDdaC1tHdz2SDU764/yyNxpnDlySNhNEpE8ynQabSnQufNrDrAkRZ6ngRlmVhIs2s8Ang6myQ6Z2fRgF9rspOvTlbsUmB3sSpsOHAzKSVmHmQ0NghVmFgM+BryVYZ8zMhB3o7k733xiLSs31/P3n76Ay87UYwJEBppMtwDdCyw2s1uAbcBNAGZWBdzu7nPdvd7M7gFWBdfc7e71wfEdwENAEfBU8EpbLvAkcAOwCWgCbgZIV4eZlQNLzSxOIrA+B/xbhn3OyECcRvvh7zbx+JqdfOWaSdx4sR58JjIQZRRs3H0/8NEU6dXA3KTzBcCCNPnOO4lyHfhSmra8pw533wNc2lM/8mmgbX1e8moN/7hsA390cSX/86Nnh90cEQmJvq6dZ8en0QbAM21Wba3na4+9zrQzS/nbT51P8J1dERmAFGzyrDA6MKbRag8e5baHVzOmpIh//8Ilx0d0IjIwKdjk2UBYs2lp6+COH6+hubWdH82ponhwYdhNEpGQ6R4heWZmFMYi/fp5Nn/z5Ju8sv0A//r5qZxVNjTs5ohIL6CRTQgSj4bun2s2S16t4aEXtzL3ijO54fyKsJsjIr2Egk0I4rFov5xG27DnMHc+vpZLx5fwjevPCbs5ItKLKNiEIB6L9Ls7CDQ2t3H7I6sZEo/xwz+eSkFUP1oicoLWbEIQL+hf02juzjd+/jrb9jfx47nTKB+ue56JyLvpz88Q9LdptAUvbOU3a2v5+nWTmT7htLCbIyK9kIJNCBIbBPpHsFm78yB/++SbzJhSzrwrJ4TdHBHppRRsQlAYi/SLOwgcbWnny4++wsihcf7+0xfoDgEikpaCTQj6y8jm3qfe5J26I/zDZy7UFzdFpFsKNiGIx6J9/kudz2+oY+Hvt3HzB8dzxcSRYTdHRHo5BZsQ9PXdaA1HWvjaY68xcdRQvjFT36cRkZ5p63MI+vI0mrvzl79YS0NTCw/efCmDCnSDTRHpmUY2IejLW58fX1PDU+t28xfXTubc0SPCbo6I9BEKNiGI99HdaDvqm/jrpW9w2fhSbXMWkZOiYBOCxJpN3xrZdHQ4X138GgD/eNOFRCPa5iwi719GwcbMSs1smZltDN5L0uSbE+TZaGZzktIvMbO1ZrbJzO6z4Isa6cq1hPuC/K+b2dSe6kj6fKmZrcukv9nSOY2WeMp13/DIS9t4eWs9d31iCmNLB4fdHBHpYzId2dwJLHf3icDy4PxdzKwUuAuYBlwG3JUUlO4HbgUmBq+ZPZR7fVLeecH1PdWBmf0R0JhhX7Om8wFqLe19Y3RTe/Aof//bt/nQxJF8+pIxYTdHRPqgTIPNLGBhcLwQuDFFnuuAZe5e7+4NwDJgpplVAMPdfaUn/sRflHR9unJnAYs8YSVQHJSTsg4AMxsK/AXwnQz7mjV97Wmddy15g7aODr574/m6S4CInJJMg025u9cGx7uB8hR5KoEdSec7g7TK4LhrenfldldWqnSAe4B/BJp66oyZzTOzajOrrqur6yn7KTs+sukDwea363bzzPo9fPmaSYw7TdNnInJqevyejZk9C5ye4qNvJZ+4u5tZ1hchMinXzC4CznL3r5jZ+PdR13xgPkBVVVXOFlTiscR3U3r7yObQsVbuWrqOP6gYzi1XnBl2c0SkD+sx2Lj7Nek+M7M9Zlbh7rXBdNbeFNlqgKuSzscAK4L0MV3Sa4LjdOXWAGNTXJOujsuBKjPbSqKvo8xshbsn5827eEEwjdbLtz//39++Td3hZuZ/oUoPQxORjGT6G2Qp0Lnzaw6wJEWep4EZZlYSLNrPAJ4OpskOmdn0YBfa7KTr05W7FJgd7EqbDhwMyklXx/3uPtrdxwNXABvCDjTQN9ZsVm+r55GXtvHFD5zJhWOLw26OiPRxmd6u5l5gsZndAmwDbgIwsyrgdnef6+71ZnYPsCq45m53rw+O7wAeAoqAp4JX2nKBJ4EbgE0k1mBuBuihjl6nt0+jtbR18M0n1jJ6RBFfnTEp7OaISD+QUbBx9/3AR1OkVwNzk84XAAvS5DvvJMp14Etp2pKyjqTPt6aqKwzHRza9dBrt359/hw17GlnwxSqGxHX7PBHJnCbiQ3B8zaYXjmy27T/CPz+3iY9dUMHV56TaXCgicvIUbEJQGO2902h/++RbxCLG//74lLCbIiL9iIJNCDpHNr3teza/f2c/v31jN3dcdRblwweF3RwR6UcUbEJwYjda71mzae9wvvOb9VQWFzH3Q7qjs4hkl4JNCHrjbrTHV+/kjV2H+Mb15+iBaCKSdQo2Iehtu9Eam9v4v8+8zdRxxXzigoqwmyMi/ZCCTQh62260+1dsou5wM3/18Sm60aaI5ISCTQgKo70n2OxsaOJH/7mFP7y4kovHpXwckYhIxhRsQhCLRohFrFdsELj3qbeIGHx95uSwmyIi/ZiCTUgKYxGaW8Md2azeVs+vX6/ltivPomJEUahtEZH+TcEmJPFYJNRptI4O5+5frad8eJzbPqytziKSWwo2IYnHoqF+qfM3a2t5bedBvn7dOQwu1P3PRCS3FGxCEi+IhLZm09bewfee3cDk8mH84cWVPV8gIpIhBZuQhDmNtuTVXWyuO8JXrp1EJKKtziKSewo2IYnHoqEEm9b2Dn6wfCPnVQ7nunN1V2cRyQ8Fm5AkRjb5n0b7+eqdbK9v4qvXTtYXOEUkbxRsQhIvyP/W52Ot7dy3fCNTxxVz1eSyvNYtIgObgk1IwphG+9nL26k9eIyvztCoRkTyK6NgY2alZrbMzDYG7ynvd2Jmc4I8G81sTlL6JWa21sw2mdl9FvwGTFeuJdwX5H/dzKa+jzpWmNnbZvZq8BqVSZ+zpTCa32m0oy3t/MuKd5g+oZQPnHVa3uoVEYHMRzZ3AsvdfSKwPDh/FzMrBe4CpgGXAXclBaX7gVuBicFrZg/lXp+Ud15wfU91AHze3S8KXnsz7HNWJLY+529k8/DKrdQdbtaoRkRCkWmwmQUsDI4XAjemyHMdsMzd6929AVgGzDSzCmC4u690dwcWJV2frtxZwCJPWAkUB+WkrCPDvuVUPBbJ25c6G5vbuH/FO1w5qYxLx5fmpU4RkWSZBptyd68NjncDqfbSVgI7ks53BmmVwXHX9O7K7a6sVOmdHgym0P7Kuvmz3szmmVm1mVXX1dWly5YV+VyzeeiFLTQ0tfLVayflpT4Rka56vE+JmT0LnJ7io28ln7i7m5lnq2FZLPfz7l5jZsOAx4EvkBhFpaprPjAfoKqqKut9SRaPRfLy8LSDR1uZ/x+buXZKOReOLc55fSIiqfQYbNz9mnSfmdkeM6tw99pgOivVekgNcFXS+RhgRZA+pkt6TXCcrtwaYGyKa9LVgbvXBO+HzewnJNZ0UgabfMrXms2iF7dy6FgbX7lGoxoRCU+m02hLgc6dX3OAJSnyPA3MMLOSYNF+BvB0ME12yMymB1Nbs5OuT1fuUmB2sCttOnAwKCdlHWYWM7ORAGZWAHwcWJdhn7MiHovS1uG0tecu4BxtaefBF7fy0XNGMWX08JzVIyLSk0xv93svsNjMbgG2ATcBmFkVcLu7z3X3ejO7B1gVXHO3u9cHx3cADwFFwFPBK225wJPADcAmoAm4GSBdHWY2hETQKQCiwLPAjzLsc1bEY4k439LeQSyam687La7eQf2RFv7sqrNyUr6IyPuVUbBx9/3AR1OkVwNzk84XAAvS5DvvJMp14Etp2vKeOtz9CHBJT/0IQ2ewaW7tYHBh9stvbe9g/n9s5tLxJVRpB5qIhEx3EAhJYSwKkLN1m1+9touaA0c1qhGRXkHBJiTHRzY5uItAR4fzb8+/w+TyYXxkcq+4YYKIDHAKNiGJFwRrNjkY2W5gOMwAAA1HSURBVPzurb1s2NPIn111lu4WICK9goJNSOI5mkZzd/51xSbGlBTx8Qsqslq2iMipUrAJSa6m0VZtbWDN9gPMu3JCzna5iYicLP02CknybrRsun/FJk4bUshnLhnbc2YRkTxRsAlJvCD702hv1h7iubfruPmD4ykqjGatXBGRTCnYhCQX02j/9vw7DCmM8oXp47NWpohINijYhKTweLDJzshmR30Tv3ptF5+ffgYjBhdkpUwRkWxRsAlJttdsHnxhKxEz/vSDZ2alPBGRbFKwCcnxrc9ZuBHnkeY2Hlu9gxvOr+D0EYMyLk9EJNsUbELS+aXObDzT5pev1nD4WBuzLz8j47JERHJBwSYk8Syt2bg7i17cxpSK4VxyRkk2miYiknUKNiEpjGYn2Ly0pZ639xxmzgfO0K1pRKTXUrAJiZklHg2d4dbnRb/fyoiiAj55YWV2GiYikgMKNiGKxyIZ7UarPXiUp9/Yw3+7dKy+xCkivZqCTYjiBdGMptF++tJ2Otz5k2naGCAivZuCTYgKo6c+jdbc1s5PXt7O1ZNHMe60wVlumYhIdmUUbMys1MyWmdnG4D3ldigzmxPk2Whmc5LSLzGztWa2yczus2CFO125lnBfkP91M5v6PuooNLP5ZrbBzN4ys09l0udsihdETnlk89t1u9nX2MLsD4zPbqNERHIg05HNncByd58ILA/O38XMSoG7gGnAZcBdSUHpfuBWYGLwmtlDudcn5Z0XXN9THd8C9rr7JGAK8HyGfc6aeCx6yg9PW/jiVs4cOYQPnT0yy60SEcm+TIPNLGBhcLwQuDFFnuuAZe5e7+4NwDJgpplVAMPdfaW7O7Ao6fp05c4CFnnCSqA4KCdlHcE1fwr8LYC7d7j7vgz7nDWJ3WgnH2zW7jzImu0H+ML0M4hEtN1ZRHq/TINNubvXBse7gfIUeSqBHUnnO4O0yuC4a3p35XZX1nvSzaw4OL/HzNaY2WNmlqqNAJjZPDOrNrPqurq6dNmyJrEb7eTXbBb9fiuDC6N86pIx2W+UiEgO9BhszOxZM1uX4jUrOV8wOvFsNzDDcmPAGOBFd58K/B74h27qmu/uVe5eVVZWdopVvn+nshut4UgLS17bxR9eXMmIIt3dWUT6hlhPGdz9mnSfmdkeM6tw99pgOmtvimw1wFVJ52OAFUH6mC7pNcFxunJrgLEprklXx36gCXgiSH8MuCVdf/LtVKbRFlfvoKWtg9mXj89No0REciDTabSlQOfOrznAkhR5ngZmmFlJsGg/A3g6mCY7ZGbTg11os5OuT1fuUmB2sCttOnAwKCddHQ78ihOB6KPA+gz7nDUnewcBd+fR6h1UnVHC5NOH5bBlIiLZ1ePIpgf3AovN7BZgG3ATgJlVAbe7+1x3rzeze4BVwTV3u3t9cHwH8BBQBDwVvNKWCzwJ3ABsIjFiuRmghzq+ATxsZt8H6jqv6Q3isehJ3UFgzfYGNtcd4fZPnZXDVomIZF9Gwcbd95MYLXRNrwbmJp0vABakyXfeSZTrwJfStCVdHduAK7vrR1gKT3IabfGqnQwujHLDBRU5bJWISPbpDgIhOplptCPNbfz69V18/IIKhsYzHZCKiOSXgk2I4gWR9/2lzt+sreVISzs3VY3tObOISC+jYBOieCyx9TkxO9i9x6p3MGHkED0gTUT6JAWbEHU+rbOlvfvRzea6RlZtbeAzVWP1gDQR6ZMUbEL0fh8N/djqnUQjxqem6gFpItI3KdiEKF6QeOBZd9uf29o7eHz1Tj4yuYxRwwflq2kiIlmlYBOiEyOb9DvSnt9Qx97DzXxGGwNEpA9TsAnR+5lGW1y9g5FDC7n6nFH5apaISNYp2IToeLBJM422r7GZ5W/u5Y+mjqEgqv9VItJ36TdYiOKxxJpNut1ov1hTQ1uHc1OVHiUgIn2bgk2IToxs3rtm4+4srt7B1HHFnD1KN90Ukb5NwSZE8YL0azav7jjAxr2NumOAiPQLCjYh6pxGSxVsHlu9k6KCKB/TTTdFpB9QsAlRuq3PLW0dPLm2lhnnljNskJ7GKSJ9n4JNiI6PbLrsRvuvTXUcaGpl1kWjw2iWiEjWKdiEKN2azZJXd1E8uIArzi4Lo1kiIlmnYBOiwuh7p9GaWtp45o093HB+BYUx/e8Rkf5Bv81ClGpks2z9Ho62tjPrQk2hiUj/kVGwMbNSM1tmZhuD95QPWzGzOUGejWY2Jyn9EjNba2abzOw+C+6fn65cS7gvyP+6mU3trg4zG2Zmrya99pnZ9zPpczZ1jmySH6C29NVdVIwYxKXjS8NqlohI1mU6srkTWO7uE4Hlwfm7mFkpcBcwDbgMuCspKN0P3ApMDF4zeyj3+qS884Lr09bh7ofd/aLOF7ANeCLDPmdNLBohFrHj02gNR1p4fkMdn7xwNJGInlsjIv1HpsFmFrAwOF4I3Jgiz3XAMnevd/cGYBkw08wqgOHuvtITj6pclHR9unJnAYs8YSVQHJSTso7kRpjZJGAU8J8Z9jmr4rHI8d1oT63bTVuH8wlNoYlIP5NpsCl399rgeDdQniJPJbAj6XxnkFYZHHdN767c7spKlZ7ss8Cj3s0zmM1snplVm1l1XV1dumxZFS+IHl+zWfJqDWeVDeHc0cPzUreISL7EespgZs8Cp6f46FvJJ+7uZpb2F/mpymK5nwW+0ENd84H5AFVVVVnvSyrxWITmtnZ2HTjKy1vr+co1k/ToZxHpd3oMNu5+TbrPzGyPmVW4e20wnbU3RbYa4Kqk8zHAiiB9TJf0muA4Xbk1wNgU16Sro7OdFwIxd1+dri9hSQSbDn79+i7c4ZOaQhORfijTabSlQOfusjnAkhR5ngZmmFlJsDFgBvB0ME12yMymB7vQZiddn67cpcDsYFfadOBgUE7KOpLa8Dngpxn2NSfisSjNrR0seXUXF44tZvzIIWE3SUQk6zINNvcC15rZRuCa4BwzqzKzBwDcvR64B1gVvO4O0gDuAB4ANgHvAE91Vy7wJLA5yP+j4Pqe6gC4iV4abApjEd7cfYg3dh3Sd2tEpN+ybtbLB7Sqqiqvrq7OeT2fvv9Fqrc1EDFY+c2PMmr4oJzXKSKSK2a22t2ruqbrDgIh67yLwOVnnaZAIyL9loJNyDrv/Dzrwq47tUVE+g8Fm5DFYxEKoxGuOy/V7nIRkf6hx63Pklufn3YGH55UxogiPSRNRPovBZuQXTFxZNhNEBHJOU2jiYhIzinYiIhIzinYiIhIzinYiIhIzinYiIhIzinYiIhIzinYiIhIzinYiIhIzumuz2mYWR2w7RQvHwnsy2Jz+gL1eWAYaH0eaP2FzPt8hruXdU1UsMkBM6tOdYvt/kx9HhgGWp8HWn8hd33WNJqIiOScgo2IiOScgk1uzA+7ASFQnweGgdbngdZfyFGftWYjIiI5p5GNiIjknIKNiIjknIJNFpnZTDN728w2mdmdYbcnV8xsgZntNbN1SWmlZrbMzDYG7yVhtjGbzGysmT1nZuvN7A0z+/MgvT/3eZCZvWxmrwV9/j9B+plm9lLwM/6omRWG3dZsM7Oomb1iZr8Ozvt1n81sq5mtNbNXzaw6SMv6z7aCTZaYWRT4F+B6YArwOTObEm6rcuYhYGaXtDuB5e4+EVgenPcXbcBX3X0KMB34UvD/tj/3uRm42t0vBC4CZprZdODvgO+5+9lAA3BLiG3MlT8H3kw6Hwh9/oi7X5T0/Zqs/2wr2GTPZcAmd9/s7i3Az4BZIbcpJ9z9P4D6LsmzgIXB8ULgxrw2Kofcvdbd1wTHh0n8Iqqkf/fZ3b0xOC0IXg5cDfw8SO9XfQYwszHAx4AHgnOjn/c5jaz/bCvYZE8lsCPpfGeQNlCUu3ttcLwbKA+zMbliZuOBi4GX6Od9DqaTXgX2AsuAd4AD7t4WZOmPP+PfB74OdATnp9H/++zAM2a22szmBWlZ/9mOZVqASFfu7mbW7/bUm9lQ4HHgy+5+KPFHb0J/7LO7twMXmVkx8AvgnJCblFNm9nFgr7uvNrOrwm5PHl3h7jVmNgpYZmZvJX+YrZ9tjWyypwYYm3Q+JkgbKPaYWQVA8L435PZklZkVkAg0P3b3J4Lkft3nTu5+AHgOuBwoNrPOP1L728/4B4FPmtlWEtPgVwM/oH/3GXevCd73kvij4jJy8LOtYJM9q4CJwc6VQuCzwNKQ25RPS4E5wfEcYEmIbcmqYN7+/wFvuvs/JX3Un/tcFoxoMLMi4FoSa1XPAZ8OsvWrPrv7N919jLuPJ/Hv93fu/nn6cZ/NbIiZDes8BmYA68jBz7buIJBFZnYDiTnfKLDA3b8bcpNywsx+ClxF4lbke4C7gF8Ci4FxJB7NcJO7d91E0CeZ2RXAfwJrOTGX/5ck1m36a58vILEwHCXxR+lid7/bzCaQ+Ku/FHgF+BN3bw6vpbkRTKP9L3f/eH/uc9C3XwSnMeAn7v5dMzuNLP9sK9iIiEjOaRpNRERyTsFGRERyTsFGRERyTsFGRERyTsFGRERyTsFGRERyTsFGRERy7v8DbRIJDShuWn8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gradient statistics\n",
    "plt.plot(train_data[10][\"Train/layer_wise_gradient_mean_layer_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = utils_prep2.check_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_classes, hidden_size, seq_length, \n",
    "                 num_layers, config_size, bidirectional = False, drop_prob=0.5, relative_size = 0.2):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_classes = num_classes\n",
    "        self.config_size = config_size\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        if bidirectional == True:\n",
    "            self.factor = 2\n",
    "        else:\n",
    "            self.factor = 1\n",
    "        \n",
    "        \n",
    "        # self.forecaster = torch.nn.Embedding(100, embedding_dim)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(drop_prob)\n",
    "        #self.lstm = torch.nn.LSTM(input_size = self.input_size, hidden_size = self.hidden_dim, \n",
    "        #                    num_layers = self.num_layers, dropout =drop_prob, bidirectional = bidirectional)\n",
    "        \n",
    " \n",
    "        self.rnn1 = torch.nn.RNN(input_size = self.input_size, hidden_size = self.hidden_dim, \n",
    "                            num_layers = self.num_layers, bidirectional = bidirectional)\n",
    "\n",
    "\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(self.config_size, int(self.hidden_dim*relative_size))\n",
    "                                       \n",
    "                                       \n",
    "        self.linear2 = torch.nn.Linear(int(self.hidden_dim*relative_size),\n",
    "                                       int(self.hidden_dim*relative_size))\n",
    "                                       \n",
    "                                       \n",
    "        self.linear3 = torch.nn.Linear(self.hidden_dim + int(self.hidden_dim*relative_size),\n",
    "                                       self.hidden_dim + int(self.hidden_dim*relative_size))\n",
    "\n",
    "        \n",
    "        self.linear4 = torch.nn.Linear(self.hidden_dim + int(self.hidden_dim*relative_size), self.hidden_dim//2)\n",
    "        \n",
    "        self.linear5 = torch.nn.Linear(self.hidden_dim//2, num_classes)\n",
    "        \n",
    "        \n",
    "        self.dropout2 = torch.nn.Dropout(drop_prob)\n",
    "        \n",
    "        \n",
    "        self.linear_reduce1 = torch.nn.Linear(self.hidden_dim*10*self.factor, self.hidden_dim*5*self.factor)\n",
    "\n",
    "        self.linear_reduce2 = torch.nn.Linear(self.hidden_dim*5*self.factor, self.hidden_dim*self.factor)\n",
    "\n",
    "        self.linear_reduce3 = torch.nn.Linear(self.hidden_dim*self.factor, self.hidden_dim)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x, configs, hidden):\n",
    "        \n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = torch.t(x)\n",
    "\n",
    "        x=x.unsqueeze(-1)\n",
    "        \n",
    "        x = x.float()\n",
    "        \n",
    "        #forecast = self.forecaster(x)\n",
    "\n",
    "        lstm_x, hidden = self.rnn1(x, hidden)\n",
    "        \n",
    "        lstm_x = lstm_x.permute(1,0,2)\n",
    "\n",
    "        lstm_x = lstm_x.contiguous().view(batch_size, -1)\n",
    "        #print(lstm_x.size())\n",
    "\n",
    "        # lstm_x = lstm_x.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        x = self.dropout(lstm_x)\n",
    "        # try \n",
    "        x = lstm_x\n",
    "        \n",
    "        x = torch.nn.functional.relu(self.linear_reduce1(x))\n",
    "        x = torch.nn.functional.relu(self.linear_reduce2(x))\n",
    "        x = torch.nn.functional.relu(self.linear_reduce3(x))\n",
    "\n",
    "\n",
    "        x_config = torch.nn.functional.relu(self.linear1(configs))\n",
    "        x_config = torch.nn.functional.relu(self.linear2(x_config))\n",
    "\n",
    "        x_cat =  torch.cat([x, x_config], dim =1)\n",
    "\n",
    "        \n",
    "        x_cat = torch.nn.functional.relu(self.linear3(x_cat))\n",
    "        \n",
    "        #x_cat = self.dropout2(x_cat)\n",
    "        \n",
    "        x_cat = torch.nn.functional.relu(self.linear4(x_cat))\n",
    "        x_cat = self.linear5(x_cat)\n",
    "\n",
    "        \n",
    "        x_cat = x_cat.view(batch_size, -1)\n",
    "        x_cat = x_cat[:,-1]\n",
    "        \n",
    "        return x_cat, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size = 32):\n",
    "        \n",
    "        if self.bidirectional == False:\n",
    "            return torch.randn(self.num_layers, batch_size, self.hidden_dim)\n",
    "        else:\n",
    "            return torch.randn(self.num_layers*2, batch_size, self.hidden_dim)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "### alternative weighted MSE Loss\n",
    "\n",
    "\n",
    "def weighted_mse_loss(prediction, target, weight):\n",
    "    ### construct weight vector\n",
    "        \n",
    "    return torch.sum((prediction-target)**2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### calculate weights\n",
    "\n",
    "weights = [1/(len([x for x in train_targets if x < 80.0])), 1/(len([x for x in train_targets if x >= 80.0]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hidden_dim, num_layers, lr= 0.001, relative_size = 0.75, weight_decay = 0.001, bidirectional = False):\n",
    "    \n",
    "    \n",
    "    def weights_init(m):\n",
    "        for name, param in m.named_parameters():\n",
    "            torch.nn.init.uniform_(param) \n",
    "    \n",
    "    input_size = 1\n",
    "    outcome_dim = 1\n",
    "    embedding_dim = 400\n",
    "    seq_length = 10\n",
    "    config_size = 7\n",
    "\n",
    "    model = LSTM_Net(input_size, outcome_dim, hidden_dim, seq_length, num_layers, config_size,\n",
    "                     relative_size = relative_size, bidirectional = bidirectional)\n",
    "    model.to(device)\n",
    "    \n",
    "    model.apply(weights_init)\n",
    "\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    criterion_val = torch.nn.MSELoss(reduction = \"sum\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    return model, optimizer, criterion, criterion_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_model(model, optimizer, criterion, criterion_val):\n",
    "    \n",
    "    epochs = 3000\n",
    "    counter = 0\n",
    "    print_every = 100\n",
    "\n",
    "    clip = 5\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_data_loader))\n",
    "\n",
    "    \n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "\n",
    "        for batches, configs , labels in train_data_loader:\n",
    "            counter += 1\n",
    "\n",
    "            batch_size_calc = len(labels)\n",
    "            hidden = model.init_hidden(batch_size=batch_size_calc)\n",
    "\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            output, hidden = model(batches, configs, hidden)\n",
    "\n",
    "\n",
    "            loss = abs(criterion(output.squeeze(), labels.float()))\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            if counter%print_every == 0:\n",
    "\n",
    "                val_losses = []\n",
    "                val_losses_sum = []\n",
    "                model.eval()\n",
    "\n",
    "                for inp,configs, lab in val_data_loader:\n",
    "\n",
    "                    batch_size_calc = len(lab)\n",
    "\n",
    "                    val_h = model.init_hidden(batch_size = batch_size_calc)\n",
    "\n",
    "\n",
    "                    inp, lab = inp.to(device), lab.to(device)\n",
    "                    out, val_h = model(inp, configs,val_h)\n",
    "                    val_loss = criterion(out.squeeze(), lab.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    \n",
    "                    val_loss_sum = criterion_val(out.squeeze(), lab.float())/(batch_size_calc)\n",
    "                    val_losses_sum.append(val_loss_sum.item())\n",
    "\n",
    "                    print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses_sum)))\n",
    "\n",
    "                if np.mean(val_losses) <= valid_loss_min:\n",
    "                    torch.save(model.state_dict(), '/home/sven/LCBench/state_dict.pt')\n",
    "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses_sum)))\n",
    "                    valid_loss_min = np.mean(val_losses_sum)\n",
    "                \n",
    "                model.train()\n",
    "                \n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion, criterion_val):\n",
    "    \n",
    "    model.load_state_dict(torch.load('/home/sven/LCBench/state_dict.pt'))\n",
    "\n",
    "    test_losses = []\n",
    "    val_losses_sum = []\n",
    "    msqrt = 0\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    for inputs,configs, labels in test_data_loader:\n",
    "        counter = counter +1\n",
    "        batch_size_calc = len(labels)\n",
    "        h = model.init_hidden(batch_size_calc)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output, h = model(inputs,configs, h)\n",
    "        test_loss = criterion(output.squeeze(), labels.float())\n",
    "        test_losses.append(test_loss.item())\n",
    "        \n",
    "                                    \n",
    "        test_loss_sum = (criterion_val(output.squeeze(), labels.float()))/batch_size_calc\n",
    "        val_losses_sum.append(test_loss_sum.item())\n",
    "        \n",
    "        pred = (output.squeeze())\n",
    "\n",
    "        msqrt = msqrt + mean_squared_error(labels.detach().numpy(), pred.detach().numpy())\n",
    "        \n",
    "    \n",
    "        print((labels.detach()))\n",
    "        print((pred.detach()))\n",
    "    print(counter)\n",
    "    total_test_acc = msqrt\n",
    "    print(\"TOTAL Loss: {:.3f}\".format(total_test_acc))\n",
    "    test_acc2 = msqrt/counter\n",
    "    \n",
    "    print(\"Msqrt: {:.3f}\".format(test_acc2))\n",
    "\n",
    "    print(\"Test loss: {:.3f}\".format(np.mean(val_losses_sum)))\n",
    "    \n",
    "    return test_acc2, np.mean(val_losses_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364586523607695360.000000\n",
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364516154863517696.000000\n",
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364586523607695360.000000\n",
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364621707979784192.000000\n",
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364811703589011456.000000\n",
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364867998584406016.000000\n",
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364847893228814336.000000\n",
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364762445468139520.000000\n",
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364789811090817024.000000\n",
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364769482342662144.000000\n",
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364740055413293056.000000\n",
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364680348599844864.000000\n",
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364673131292917760.000000\n",
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364636786996281344.000000\n",
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364670966100656128.000000\n",
      "Epoch: 4/3000... Step: 100... Loss: 1743243773613363429376.000000... Val Loss: 1742364709668910006272.000000\n",
      "Validation loss decreased (inf --> 1742364709668910006272.000000).  Saving model ...\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502860283063042048.000000\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502807506504908800.000000\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502813370566901760.000000\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502825098690953216.000000\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502881393686282240.000000\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502907195559182336.000000\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502900493774028800.000000\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502873477202575360.000000\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502879829936439296.000000\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502870838374694912.000000\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502860283063042048.000000\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502842690876997632.000000\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502841337631932416.000000\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502835151368683520.000000\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502846209314193408.000000\n",
      "Epoch: 7/3000... Step: 200... Loss: 482517739954428182528.000000... Val Loss: 472502858084039786496.000000\n",
      "Validation loss decreased (1742364709668910006272.000000 --> 472502858084039786496.000000).  Saving model ...\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768315844364337152.000000\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768315844364337152.000000\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768321708426362880.000000\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768324640457359360.000000\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768347510299230208.000000\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768353960767422464.000000\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768351028736425984.000000\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768340033620148224.000000\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768341255299727360.000000\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768338714206208000.000000\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768336635129659392.000000\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768329038503870464.000000\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768329376815153152.000000\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768325897042067456.000000\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768329918113185792.000000\n",
      "Epoch: 10/3000... Step: 300... Loss: 176857887887025242112.000000... Val Loss: 175768334536062009344.000000\n",
      "Validation loss decreased (472502858084039786496.000000 --> 175768334536062009344.000000).  Saving model ...\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780805566159650816.000000\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780803367136395264.000000\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780805566159650816.000000\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780806665671278592.000000\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780811723424768000.000000\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780814362252673024.000000\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780813733960310784.000000\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780811063717789696.000000\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780811918893498368.000000\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780810843815460864.000000\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780809964206161920.000000\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780808131686785024.000000\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780807934338539520.000000\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780807451036729344.000000\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780808498190655488.000000\n",
      "Epoch: 13/3000... Step: 400... Loss: 43341657651395166208.000000... Val Loss: 42780809689328254976.000000\n",
      "Validation loss decreased (175768334536062009344.000000 --> 42780809689328254976.000000).  Saving model ...\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447447105343062016.000000\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447446005831434240.000000\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447446372335310848.000000\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447446555587248128.000000\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447448644659341312.000000\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447449487618254848.000000\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447449147293227008.000000\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447448067415736320.000000\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447448449190606848.000000\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447448094903527424.000000\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447447605121075200.000000\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447446830465155072.000000\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447446851609610240.000000\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447446555587248128.000000\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447447105343062016.000000\n",
      "Epoch: 16/3000... Step: 500... Loss: 16702884547196354560.000000... Val Loss: 16447447655098875904.000000\n",
      "Validation loss decreased (42780809689328254976.000000 --> 16447447655098875904.000000).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327069388111872.000000\n",
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327067240628224.000000\n",
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327067956456106.000000\n",
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327068582805504.000000\n",
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327074327324262.000000\n",
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327076725347670.000000\n",
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327076444129572.000000\n",
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327073951514624.000000\n",
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327074637516344.000000\n",
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327074005201716.000000\n",
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327072999788916.000000\n",
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327071267160064.000000\n",
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327071205213420.000000\n",
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327070538549540.000000\n",
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327071965092250.000000\n",
      "Epoch: 19/3000... Step: 600... Loss: 12960498681118720.000000... Val Loss: 12327073146208256.000000\n",
      "Validation loss decreased (16447447655098875904.000000 --> 12327073146208256.000000).  Saving model ...\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525523104.000000\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525415392.000000\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525376213.333333\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525384896.000000\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525498060.800000\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525563610.666667\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525534656.000000\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525491976.000000\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525499306.666667\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525487523.200000\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525478754.909091\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525432578.666667\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525423392.000000\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525418297.142857\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525452797.866667\n",
      "Epoch: 22/3000... Step: 700... Loss: 1358080901120.000000... Val Loss: 525476494.000000\n",
      "Validation loss decreased (12327073146208256.000000 --> 525476494.000000).  Saving model ...\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2774442752.000000\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2773954816.000000\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2773900800.000000\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2773906048.000000\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2774041292.800000\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2774193152.000000\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2774207597.714286\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2774131488.000000\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2774126620.444445\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2774086784.000000\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2774044811.636364\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2773986346.666667\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2773986796.307693\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2773969682.285714\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2774028800.000000\n",
      "Epoch: 25/3000... Step: 800... Loss: 3015107328.000000... Val Loss: 2774083520.000000\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446034804736.000000\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446036197376.000000\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446035613013.333313\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446035263488.000000\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446033362944.000000\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446032145066.666687\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446032436077.714294\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446032896000.000000\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446032794965.333313\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446032435609.599976\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446032719499.636353\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446033177258.666687\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446033367985.230774\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446033568914.285706\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446032877977.599976\n",
      "Epoch: 29/3000... Step: 900... Loss: 631302016.000000... Val Loss: 446032287744.000000\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265595200.000000\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265565424.000000\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265553834.666667\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265556976.000000\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265583443.200000\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265610013.333333\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265602189.714286\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265595680.000000\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265589077.333333\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265594272.000000\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265589152.000000\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265576749.333333\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265572621.538462\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265566581.714286\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265573356.800000\n",
      "Epoch: 32/3000... Step: 1000... Loss: 2905932032.000000... Val Loss: 265584291.000000\n",
      "Validation loss decreased (525476494.000000 --> 265584291.000000).  Saving model ...\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908569600.000000\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908989440.000000\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908934485.333334\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908957696.000000\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908764364.799999\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908642816.000000\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908703305.142857\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908793472.000000\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908769280.000000\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908728217.600000\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908759040.000000\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908874069.333334\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908883101.538462\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908908909.714285\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908796518.400000\n",
      "Epoch: 35/3000... Step: 1100... Loss: 383202912.000000... Val Loss: 16908726208.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157434768.000000\n",
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157414968.000000\n",
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157414138.666667\n",
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157414920.000000\n",
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157432272.000000\n",
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157444709.333333\n",
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157438676.571429\n",
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157433068.000000\n",
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157433180.444444\n",
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157436326.400000\n",
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157436029.090909\n",
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157429906.666667\n",
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157427394.461538\n",
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157427634.285714\n",
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157434106.666667\n",
      "Epoch: 38/3000... Step: 1200... Loss: 604606400.000000... Val Loss: 157438172.000000\n",
      "Validation loss decreased (265584291.000000 --> 157438172.000000).  Saving model ...\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586375680.000000\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586539520.000000\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586534741.333334\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586542080.000000\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586455347.200001\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586386773.333334\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586430390.857143\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586458880.000000\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586482631.111111\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586472960.000000\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586467467.636364\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586497365.333334\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586519748.923077\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586519844.571428\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586483541.333334\n",
      "Epoch: 41/3000... Step: 1300... Loss: 90161256.000000... Val Loss: 9586438784.000000\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658311680.000000\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658218048.000000\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658220885.333333\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658225536.000000\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658255360.000000\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658286293.333333\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658268562.285714\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658260672.000000\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658248533.333333\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658257241.600000\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658263028.363636\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658255584.000000\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658247670.153846\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658247204.571429\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658261947.733333\n",
      "Epoch: 44/3000... Step: 1400... Loss: 789790.562500... Val Loss: 1658270896.000000\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102307896.000000\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102291452.000000\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102291725.333333\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102293030.000000\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102301790.400000\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102309993.333333\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102305587.428571\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102305915.000000\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102302718.222222\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102305384.000000\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102307435.636364\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102305716.666667\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102304139.076923\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102305405.714286\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102308811.733333\n",
      "Epoch: 47/3000... Step: 1500... Loss: 2570.342529... Val Loss: 102312074.500000\n",
      "Validation loss decreased (157438172.000000 --> 102312074.500000).  Saving model ...\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 411017152.000000\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 410973920.000000\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 410967872.000000\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 410970984.000000\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 410982240.000000\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 410996320.000000\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 410987629.714286\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 410986688.000000\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 410980096.000000\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 410984428.800000\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 410989009.454545\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 410987013.333333\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 410983662.769231\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 410984795.428571\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 410991441.066667\n",
      "Epoch: 50/3000... Step: 1600... Loss: 3911619584.000000... Val Loss: 410996816.000000\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6429195.000000\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6433669.500000\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6433680.166667\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6433398.000000\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6431897.700000\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6430262.416667\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6431298.571429\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6431481.125000\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6432253.500000\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6431588.950000\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6431109.818182\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6431480.375000\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6431817.653846\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6431555.535714\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6430661.766667\n",
      "Epoch: 54/3000... Step: 1700... Loss: 830006.500000... Val Loss: 6429762.875000\n",
      "Validation loss decreased (102312074.500000 --> 6429762.875000).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228122624.000000\n",
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228054656.000000\n",
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228049962.666667\n",
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228057312.000000\n",
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228075289.600000\n",
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228097642.666667\n",
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228082194.285714\n",
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228080400.000000\n",
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228068337.777778\n",
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228076505.600000\n",
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228084154.181818\n",
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228081877.333333\n",
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228076189.538461\n",
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228079058.285714\n",
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228089924.266667\n",
      "Epoch: 57/3000... Step: 1800... Loss: 748244160.000000... Val Loss: 1228100128.000000\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116230952.000000\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116209800.000000\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116207578.666667\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116209248.000000\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116214675.200000\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116222005.333333\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116217166.857143\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116216941.000000\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116213357.333333\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116215964.000000\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116218207.272727\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116217136.666667\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116215410.461538\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116216566.857143\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116219835.733333\n",
      "Epoch: 60/3000... Step: 1900... Loss: 281558016.000000... Val Loss: 116222804.000000\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 355698.406250\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 354500.515625\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 354397.677083\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 354479.640625\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 354752.175000\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 355163.447917\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 354897.517857\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 354885.125000\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 354686.149306\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 354826.456250\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 354962.605114\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 354913.981771\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 354814.834135\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 354878.962054\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 355055.685417\n",
      "Epoch: 63/3000... Step: 2000... Loss: 1767854.000000... Val Loss: 355215.037109\n",
      "Validation loss decreased (6429762.875000 --> 355215.037109).  Saving model ...\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45583760.000000\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45596848.000000\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45598236.000000\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45597125.000000\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45593828.000000\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45589494.000000\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45592661.714286\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45592603.000000\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45594937.777778\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45593208.000000\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45591696.000000\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45592354.000000\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45593431.384615\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45592784.857143\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45590750.933333\n",
      "Epoch: 66/3000... Step: 2100... Loss: 20772008.000000... Val Loss: 45589020.500000\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8589052.000000\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8594843.000000\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8595581.333333\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8595273.750000\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8594037.600000\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8592143.500000\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8593494.857143\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8593580.500000\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8594613.333333\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8593892.700000\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8593231.818182\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8593425.083333\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8593921.461538\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8593579.928571\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8592714.866667\n",
      "Epoch: 69/3000... Step: 2200... Loss: 23030344.000000... Val Loss: 8591960.437500\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18184302.000000\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18175991.000000\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18175248.666667\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18175863.500000\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18177615.600000\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18180818.000000\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18178852.857143\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18178503.250000\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18177305.555556\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18178467.800000\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18179580.545455\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18179284.166667\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18178510.461538\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18179034.285714\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18180180.000000\n",
      "Epoch: 72/3000... Step: 2300... Loss: 150777.750000... Val Loss: 18181501.000000\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5528.683594\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5370.550293\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5364.085775\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5369.777588\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5409.446582\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5473.974121\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5437.333566\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5434.619019\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5406.714627\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5428.267920\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5447.998535\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5443.314209\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5429.432542\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5437.206613\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5459.917741\n",
      "Epoch: 75/3000... Step: 2400... Loss: 8880.015625... Val Loss: 5479.487030\n",
      "Validation loss decreased (355215.037109 --> 5479.487030).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5131989.000000\n",
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5036625.250000\n",
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5033959.166667\n",
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5102680.625000\n",
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5093506.100000\n",
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5093372.083333\n",
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5111122.285714\n",
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5090779.312500\n",
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5112549.611111\n",
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5110231.000000\n",
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5139731.454545\n",
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5132945.208333\n",
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5133477.115385\n",
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5139792.928571\n",
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5160058.566667\n",
      "Epoch: 79/3000... Step: 2500... Loss: 7392659.500000... Val Loss: 5166024.062500\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 60.433769\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 49.499165\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 62.205068\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 56.584501\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 64.681041\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 83.630843\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 78.967793\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 76.769109\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 74.044155\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 77.634597\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 82.495397\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 85.488559\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 83.509011\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 82.305487\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 83.921566\n",
      "Epoch: 82/3000... Step: 2600... Loss: 55.643539... Val Loss: 85.027800\n",
      "Validation loss decreased (5479.487030 --> 85.027800).  Saving model ...\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 39.788414\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 25.118550\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 32.604289\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 30.193870\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 35.428084\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 48.709302\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 46.083395\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 43.636705\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 41.329340\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 42.912446\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 45.120677\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 46.553890\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 45.589659\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 45.384417\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 45.762099\n",
      "Epoch: 85/3000... Step: 2700... Loss: 49.254799... Val Loss: 46.480115\n",
      "Validation loss decreased (85.027800 --> 46.480115).  Saving model ...\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 40.055397\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 29.376696\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 37.042183\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 33.226315\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 38.284451\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 50.742400\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 47.450280\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 45.348447\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 42.986397\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 45.534497\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 47.341135\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 48.915666\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 47.975537\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 48.228389\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 48.818265\n",
      "Epoch: 88/3000... Step: 2800... Loss: 51.328880... Val Loss: 49.923088\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 42.671555\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 30.196405\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 34.313446\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 35.472525\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 39.720981\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 50.444606\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 49.469426\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 47.163927\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 47.115832\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 46.596586\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 46.836616\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 47.340868\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 47.653889\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 47.238550\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 47.149142\n",
      "Epoch: 91/3000... Step: 2900... Loss: 52.140427... Val Loss: 47.211218\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 42.299507\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 29.938469\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 34.014219\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 35.497158\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 39.539835\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 50.477270\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 49.455684\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 46.968795\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 46.796617\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 46.141677\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 46.265907\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 46.661561\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 46.953780\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 46.688996\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 46.661146\n",
      "Epoch: 94/3000... Step: 3000... Loss: 58.197056... Val Loss: 46.910492\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 33.394142\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 22.982934\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 28.146983\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 28.202466\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 32.293181\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 43.088184\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 41.600490\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 39.279492\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 38.599906\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 38.785205\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 39.238139\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 39.743098\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 39.592077\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 39.468303\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 39.526919\n",
      "Epoch: 97/3000... Step: 3100... Loss: 46.686508... Val Loss: 39.758805\n",
      "Validation loss decreased (46.480115 --> 39.758805).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 42.889034\n",
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 41.181177\n",
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 48.855546\n",
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 43.248723\n",
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 46.029842\n",
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 56.738133\n",
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 53.641831\n",
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 51.640543\n",
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 49.007501\n",
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 52.028452\n",
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 53.692101\n",
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 54.906202\n",
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 54.048067\n",
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 54.535813\n",
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 54.977639\n",
      "Epoch: 100/3000... Step: 3200... Loss: 25.167654... Val Loss: 55.822835\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 31.186184\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 21.990918\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 26.331066\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 25.869126\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 29.015681\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 40.697775\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 38.911110\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 36.636618\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 35.468279\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 36.337295\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 37.324577\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 37.719857\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 37.350663\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 37.322567\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 37.454294\n",
      "Epoch: 104/3000... Step: 3300... Loss: 34.580750... Val Loss: 37.886614\n",
      "Validation loss decreased (39.758805 --> 37.886614).  Saving model ...\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 33.429264\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 23.924859\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 28.502935\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 28.778035\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 32.271285\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 43.241077\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 41.695192\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 39.333519\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 38.644755\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 38.855857\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 39.361345\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 39.575350\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 39.196808\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 39.130540\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 39.125233\n",
      "Epoch: 107/3000... Step: 3400... Loss: 22.927219... Val Loss: 39.241371\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 32.987595\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 28.766197\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 35.856712\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 31.905615\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 35.021226\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 46.974129\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 44.216940\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 41.901077\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 39.620479\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 42.016451\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 43.514014\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 44.334756\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 43.295701\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 43.610529\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 44.041790\n",
      "Epoch: 110/3000... Step: 3500... Loss: 39.990063... Val Loss: 44.777602\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 30.913612\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 23.001356\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 28.365706\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 27.840498\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 31.496611\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 42.024625\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 40.234469\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 37.859094\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 36.904774\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 37.414393\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 37.928007\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 38.209024\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 37.712803\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 37.726038\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 37.771290\n",
      "Epoch: 113/3000... Step: 3600... Loss: 30.459244... Val Loss: 37.868313\n",
      "Validation loss decreased (37.886614 --> 37.868313).  Saving model ...\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 34.787819\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 24.882604\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 28.981103\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 29.953883\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 33.398039\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 45.080803\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 43.752808\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 41.417573\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 40.914207\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 40.941832\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 41.455675\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 41.534013\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 41.192076\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 41.031235\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 40.952136\n",
      "Epoch: 116/3000... Step: 3700... Loss: 45.725014... Val Loss: 41.010638\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 30.001223\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 21.659405\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 27.032082\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 26.115161\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 29.393535\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 41.136862\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 39.253973\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 36.877938\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 35.693053\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 36.630077\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 37.517502\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 37.897022\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 37.260065\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 37.298422\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 37.371463\n",
      "Epoch: 119/3000... Step: 3800... Loss: 45.571270... Val Loss: 37.673270\n",
      "Validation loss decreased (37.868313 --> 37.673270).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 28.467133\n",
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 23.588441\n",
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 29.250021\n",
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 27.411892\n",
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 30.434677\n",
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 41.288611\n",
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 39.228500\n",
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 36.929516\n",
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 35.477462\n",
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 36.602158\n",
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 37.347266\n",
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 37.757780\n",
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 37.275732\n",
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 37.426337\n",
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 37.604768\n",
      "Epoch: 122/3000... Step: 3900... Loss: 43.895729... Val Loss: 37.912069\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 30.888798\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 24.100626\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 28.279354\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 28.426317\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 31.539511\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 42.117040\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 40.588348\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 38.358980\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 37.667380\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 37.863223\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 38.192831\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 38.194190\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 38.025367\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 37.998200\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 38.004840\n",
      "Epoch: 125/3000... Step: 4000... Loss: 16.688929... Val Loss: 37.995553\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 29.864109\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 22.590514\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 27.611539\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 26.443415\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 29.189751\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 40.426703\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 38.516104\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 36.202036\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 34.941772\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 36.063298\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 36.987613\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 37.248893\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 36.708940\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 36.812210\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 36.866222\n",
      "Epoch: 129/3000... Step: 4100... Loss: 35.546658... Val Loss: 37.142990\n",
      "Validation loss decreased (37.673270 --> 37.142990).  Saving model ...\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 30.002090\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 22.686890\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 28.468825\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 27.315090\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 30.708804\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 41.223977\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 39.212099\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 36.759461\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 35.614955\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 36.388271\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 36.932835\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 37.280406\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 36.784625\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 36.927528\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 36.982878\n",
      "Epoch: 132/3000... Step: 4200... Loss: 19.713985... Val Loss: 37.150797\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 30.346445\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 25.579941\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 31.842325\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 28.673932\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 31.548702\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 42.415949\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 40.015198\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 37.694556\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 35.810627\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 37.814021\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 38.986266\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 39.559563\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 38.855788\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 39.088280\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 39.283205\n",
      "Epoch: 135/3000... Step: 4300... Loss: 38.191589... Val Loss: 39.817612\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 29.480423\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 25.222717\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 30.858747\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 28.332088\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 30.893570\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 41.282482\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 39.114994\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 36.842245\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 35.186693\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 36.772138\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 37.672943\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 38.072681\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 37.641600\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 37.867908\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 38.001250\n",
      "Epoch: 138/3000... Step: 4400... Loss: 32.214378... Val Loss: 38.372443\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 27.946630\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 21.867438\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 24.282653\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 24.239957\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 26.867465\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 31.905857\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 31.467692\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 30.894095\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 30.552900\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 29.448670\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 29.100659\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 29.584573\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 29.762941\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 29.669426\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 29.901412\n",
      "Epoch: 141/3000... Step: 4500... Loss: 29.493319... Val Loss: 30.966983\n",
      "Validation loss decreased (37.142990 --> 30.966983).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 10.090656\n",
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 8.548311\n",
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 10.050796\n",
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 9.802365\n",
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 10.787862\n",
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 16.877904\n",
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 16.092497\n",
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 15.512576\n",
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 15.414512\n",
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 15.070917\n",
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 14.399591\n",
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 15.209592\n",
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 15.419868\n",
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 16.423813\n",
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 16.641453\n",
      "Epoch: 144/3000... Step: 4600... Loss: 10.835105... Val Loss: 17.567357\n",
      "Validation loss decreased (30.966983 --> 17.567357).  Saving model ...\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 6.441270\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 5.649236\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 7.593111\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 7.136199\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 7.880760\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 10.817959\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 10.250142\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 9.719454\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 9.405587\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 9.419296\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 9.005224\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 10.004438\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 9.844614\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 10.609804\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 11.171163\n",
      "Epoch: 147/3000... Step: 4700... Loss: 17.970688... Val Loss: 11.340945\n",
      "Validation loss decreased (17.567357 --> 11.340945).  Saving model ...\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 6.108924\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 6.122548\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 7.877936\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 6.853799\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 7.220055\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 8.691211\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 8.688004\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 8.578397\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 8.065689\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 8.613448\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 8.780536\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 9.562601\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 9.329891\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 10.168509\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 10.581750\n",
      "Epoch: 150/3000... Step: 4800... Loss: 1.353475... Val Loss: 10.586668\n",
      "Validation loss decreased (11.340945 --> 10.586668).  Saving model ...\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 5.725854\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 4.669952\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 6.025974\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 5.263148\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 5.887380\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 7.430818\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 7.105117\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 6.967432\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 6.739153\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 6.673044\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 6.484535\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 7.432275\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 7.348584\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 8.240011\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 8.724719\n",
      "Epoch: 154/3000... Step: 4900... Loss: 4.645539... Val Loss: 8.949158\n",
      "Validation loss decreased (10.586668 --> 8.949158).  Saving model ...\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 11.757053\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 9.609179\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 11.496432\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 11.667759\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 12.859543\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 16.776659\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 16.383068\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 15.664228\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 15.534987\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 15.012561\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 15.430573\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 16.277046\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 15.991090\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 16.379538\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 16.608630\n",
      "Epoch: 157/3000... Step: 5000... Loss: 10.729793... Val Loss: 16.751035\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 5.515753\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 4.246443\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 5.705644\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 5.044535\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 5.963843\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 6.945975\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 6.626317\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 6.446922\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 6.323088\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 6.056954\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 5.954272\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 6.685923\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 6.651588\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 7.285853\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 7.626970\n",
      "Epoch: 160/3000... Step: 5100... Loss: 5.170835... Val Loss: 7.958750\n",
      "Validation loss decreased (8.949158 --> 7.958750).  Saving model ...\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 6.440682\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 4.774252\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 5.933437\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 5.269051\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 6.118499\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 6.762496\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 6.490929\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 6.330051\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 6.264544\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 6.172927\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 5.946126\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 6.639702\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 6.586000\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 7.321362\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 7.636465\n",
      "Epoch: 163/3000... Step: 5200... Loss: 3.770580... Val Loss: 8.087767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 6.072730\n",
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 4.774613\n",
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 5.879378\n",
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 5.579702\n",
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 6.547786\n",
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 7.768533\n",
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 7.379387\n",
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 7.213377\n",
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 7.005755\n",
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 6.748039\n",
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 6.525376\n",
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 7.155626\n",
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 7.100515\n",
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 7.775154\n",
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 8.144019\n",
      "Epoch: 166/3000... Step: 5300... Loss: 8.254356... Val Loss: 8.626515\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 5.475525\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 5.085796\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 6.034560\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 5.272744\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 6.197297\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 6.684239\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 6.206235\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 5.882144\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 5.590334\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 5.510067\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 5.283736\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 6.308874\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 6.165518\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 6.876850\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 7.312939\n",
      "Epoch: 169/3000... Step: 5400... Loss: 4.420650... Val Loss: 7.522444\n",
      "Validation loss decreased (7.958750 --> 7.522444).  Saving model ...\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 5.596216\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 5.860858\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 6.973958\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 5.962615\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 7.267398\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 7.562947\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 7.027727\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 6.683488\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 6.309663\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 6.208972\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 6.213073\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 7.058782\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 6.907524\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 7.771041\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 8.307085\n",
      "Epoch: 172/3000... Step: 5500... Loss: 12.619011... Val Loss: 8.328678\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 5.192047\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 4.446924\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 5.249766\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 4.535734\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 5.381846\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 5.948545\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 5.594576\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 5.451713\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 5.189308\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 5.102320\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 4.844051\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 5.411829\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 5.270617\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 5.984296\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 6.364025\n",
      "Epoch: 175/3000... Step: 5600... Loss: 1.120792... Val Loss: 6.457616\n",
      "Validation loss decreased (7.522444 --> 6.457616).  Saving model ...\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 6.089058\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 6.553267\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 6.856871\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 6.210674\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 7.308747\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 7.597340\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 7.194085\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 6.851307\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 6.564361\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 6.410344\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 6.280173\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 6.928563\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 6.765460\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 7.531977\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 8.037251\n",
      "Epoch: 179/3000... Step: 5700... Loss: 5.813752... Val Loss: 8.197165\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 4.498604\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 3.826384\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 4.814456\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 4.292350\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 5.262015\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 5.569345\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 5.259370\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 5.050674\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 4.816742\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 4.743462\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 4.795113\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 5.556408\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 5.433697\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 6.133501\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 6.521805\n",
      "Epoch: 182/3000... Step: 5800... Loss: 3.096838... Val Loss: 6.772691\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 4.980501\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 5.365697\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 6.941667\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 6.189953\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 7.221020\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 7.597034\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 7.308198\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 6.814069\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 6.475653\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 6.368403\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 6.562442\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 7.178122\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 6.966750\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 7.687517\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 8.177342\n",
      "Epoch: 185/3000... Step: 5900... Loss: 5.591100... Val Loss: 8.254998\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 6.679627\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 7.781171\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 8.223297\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 7.478013\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 8.878325\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 9.134235\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 8.538739\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 8.151117\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 7.751594\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 7.839312\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 7.622982\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 8.620424\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 8.435879\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 9.421741\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 10.019446\n",
      "Epoch: 188/3000... Step: 6000... Loss: 4.365531... Val Loss: 9.918632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 5.246402\n",
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 4.698535\n",
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 5.292212\n",
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 5.098905\n",
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 6.114858\n",
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 6.642263\n",
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 6.265608\n",
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 6.083495\n",
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 5.906320\n",
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 5.815569\n",
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 5.689804\n",
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 6.265236\n",
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 6.161953\n",
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 6.828522\n",
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 7.257255\n",
      "Epoch: 191/3000... Step: 6100... Loss: 4.952679... Val Loss: 7.596612\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 6.034368\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 7.038298\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 7.528519\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 6.556083\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 7.899726\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 8.439091\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 7.914459\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 7.485103\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 7.190896\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 7.021758\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 6.843240\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 7.605340\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 7.397709\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 8.220707\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 8.704039\n",
      "Epoch: 194/3000... Step: 6200... Loss: 11.353410... Val Loss: 8.740143\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 4.139367\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 3.808265\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 4.576637\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 4.166905\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 5.096692\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 21.910566\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 19.293202\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 17.370177\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 15.810016\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 14.583741\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 13.552113\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 13.406810\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 12.689434\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 12.882268\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 12.762738\n",
      "Epoch: 197/3000... Step: 6300... Loss: 11.378546... Val Loss: 12.554953\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 4.362607\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 4.057035\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 5.420972\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 4.728240\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 6.286867\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 7.442188\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 6.999683\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 6.456932\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 6.103006\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 6.272208\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 6.441883\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 7.258676\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 7.002369\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 7.695335\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 8.071006\n",
      "Epoch: 200/3000... Step: 6400... Loss: 5.556968... Val Loss: 8.058220\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 3.096364\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 3.463854\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 4.475058\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 3.904948\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 5.129115\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 5.807265\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 5.313261\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 5.025184\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 4.743964\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 4.616904\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 4.419633\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 5.066860\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 4.937834\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 5.707906\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 6.165310\n",
      "Epoch: 204/3000... Step: 6500... Loss: 3.391612... Val Loss: 6.241586\n",
      "Validation loss decreased (6.457616 --> 6.241586).  Saving model ...\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 6.556060\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 7.349696\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 7.467752\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 6.661373\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 8.070885\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 8.566656\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 7.982846\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 7.597543\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 7.282592\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 7.174985\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 6.980190\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 7.919165\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 7.710492\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 8.624940\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 9.088442\n",
      "Epoch: 207/3000... Step: 6600... Loss: 5.422117... Val Loss: 9.083397\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 8.289218\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 8.167429\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 9.050892\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 8.567075\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 9.512066\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 10.028360\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 9.502241\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 9.334424\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 9.161391\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 8.859519\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 8.716938\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 9.161710\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 9.054103\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 9.803134\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 10.003887\n",
      "Epoch: 210/3000... Step: 6700... Loss: 8.052683... Val Loss: 10.480473\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 6.230643\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 5.792776\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 5.733243\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 5.481011\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 6.564871\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 6.937416\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 6.478846\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 6.435660\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 6.275601\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 6.106455\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 6.057253\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 6.425740\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 6.359895\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 7.058207\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 7.606052\n",
      "Epoch: 213/3000... Step: 6800... Loss: 4.522230... Val Loss: 8.423656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 4.364709\n",
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 4.505805\n",
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 4.957756\n",
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 4.281142\n",
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 5.384651\n",
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 5.977288\n",
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 5.627822\n",
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 5.392524\n",
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 5.133700\n",
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 5.136246\n",
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 5.034818\n",
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 5.594924\n",
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 5.426648\n",
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 6.135006\n",
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 6.473765\n",
      "Epoch: 216/3000... Step: 6900... Loss: 3.473738... Val Loss: 6.843467\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 4.362580\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 4.269858\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 4.856570\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 4.130999\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 5.433774\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 5.800686\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 5.311422\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 4.944163\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 4.698642\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 4.463941\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 4.314222\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 5.040390\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 4.864752\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 5.634722\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 5.980935\n",
      "Epoch: 219/3000... Step: 7000... Loss: 2.618921... Val Loss: 6.256591\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 4.297894\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 4.560096\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 5.506944\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 4.913190\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 6.134656\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 6.616151\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 6.093061\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 5.802484\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 5.570242\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 5.297382\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 5.092522\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 5.760122\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 5.669051\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 6.381005\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 6.730246\n",
      "Epoch: 222/3000... Step: 7100... Loss: 12.309222... Val Loss: 7.120296\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 4.336121\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 4.385365\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 4.948236\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 4.235714\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 5.485307\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 5.900639\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 5.340625\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 5.085702\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 4.810990\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 4.629373\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 4.426118\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 5.181064\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 5.029048\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 5.919567\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 6.283793\n",
      "Epoch: 225/3000... Step: 7200... Loss: 3.386926... Val Loss: 6.387046\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 4.734696\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 4.869547\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 5.338674\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 4.714889\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 5.806417\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 6.322461\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 6.301523\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 6.044074\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 5.758060\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 5.698223\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 5.696157\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 6.225490\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 6.057871\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 6.658634\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 6.998782\n",
      "Epoch: 229/3000... Step: 7300... Loss: 7.414773... Val Loss: 7.376246\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 5.323526\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 5.158654\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 5.947212\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 5.321062\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 6.471169\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 6.893244\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 6.399042\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 6.159683\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 5.913830\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 5.673033\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 5.434715\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 6.069558\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 5.932012\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 6.624159\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 6.932736\n",
      "Epoch: 232/3000... Step: 7400... Loss: 2.516442... Val Loss: 7.084878\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 6.336435\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 5.352607\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 5.868848\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 5.502209\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 6.449928\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 6.877865\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 6.462628\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 6.357995\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 6.258619\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 6.126674\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 6.015372\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 6.706114\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 6.626558\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 7.267256\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 7.572446\n",
      "Epoch: 235/3000... Step: 7500... Loss: 2.944381... Val Loss: 7.822721\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 5.765332\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 6.467917\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 6.888131\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 6.172805\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 7.329528\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 7.779015\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 7.379707\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 7.209863\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 6.952216\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 6.606443\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 6.507159\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 7.125098\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 6.903125\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 7.589263\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 8.000233\n",
      "Epoch: 238/3000... Step: 7600... Loss: 3.446954... Val Loss: 8.123889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 4.032876\n",
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 4.381591\n",
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 4.931999\n",
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 4.341275\n",
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 5.230054\n",
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 5.662537\n",
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 5.211997\n",
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 5.020111\n",
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 4.788953\n",
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 4.601600\n",
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 4.416536\n",
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 5.043526\n",
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 4.913091\n",
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 5.687981\n",
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 6.089179\n",
      "Epoch: 241/3000... Step: 7700... Loss: 2.766658... Val Loss: 6.245662\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 5.108153\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 5.560261\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 6.778889\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 6.003919\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 7.802500\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 8.157976\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 7.551044\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 7.021835\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 6.604942\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 6.688574\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 6.491767\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 7.462024\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 7.183130\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 7.981318\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 8.516720\n",
      "Epoch: 244/3000... Step: 7800... Loss: 10.956480... Val Loss: 8.452099\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 3.666472\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 3.849683\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 4.312630\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 3.885312\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 4.889916\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 5.323969\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 4.918596\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 4.731848\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 4.518237\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 4.352391\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 4.172099\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 4.830940\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 4.701261\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 5.457994\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 5.774698\n",
      "Epoch: 247/3000... Step: 7900... Loss: 12.481468... Val Loss: 5.938928\n",
      "Validation loss decreased (6.241586 --> 5.938928).  Saving model ...\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 9.434887\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 9.044396\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 10.117559\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 9.775955\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 10.654947\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 11.111108\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 10.735943\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 10.620042\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 10.334179\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 10.132685\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 10.121689\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 10.519798\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 10.488328\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 11.074641\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 11.321451\n",
      "Epoch: 250/3000... Step: 8000... Loss: 2.047417... Val Loss: 11.926203\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 3.801010\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 4.083446\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 4.465408\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 3.934983\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 5.083583\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 5.497022\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 5.061171\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 4.809701\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 4.561643\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 4.366584\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 4.389623\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 4.936571\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 4.797368\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 5.530367\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 5.838072\n",
      "Epoch: 254/3000... Step: 8100... Loss: 2.154874... Val Loss: 6.134364\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 3.770305\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 4.021645\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 4.822915\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 4.291758\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 5.206271\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 5.704757\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 5.251515\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 5.050454\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 4.858401\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 4.664151\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 4.613088\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 5.147289\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 5.036552\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 5.815228\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 6.099045\n",
      "Epoch: 257/3000... Step: 8200... Loss: 2.020609... Val Loss: 6.385547\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 4.266427\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 4.324732\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 4.814347\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 4.219874\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 5.185376\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 5.602324\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 5.152242\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 4.936364\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 4.693901\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 4.486205\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 4.351930\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 4.845295\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 4.705727\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 5.402875\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 5.699587\n",
      "Epoch: 260/3000... Step: 8300... Loss: 2.658072... Val Loss: 5.980301\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 3.021055\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 3.772053\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 4.576326\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 4.007739\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 5.262044\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 5.568072\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 5.087832\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 4.749881\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 4.505042\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 4.266420\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 4.132142\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 4.823611\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 4.659959\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 5.480543\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 5.870322\n",
      "Epoch: 263/3000... Step: 8400... Loss: 1.369032... Val Loss: 5.989159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 9.134720\n",
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 9.279838\n",
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 10.537592\n",
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 10.234869\n",
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 11.028493\n",
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 11.151380\n",
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 10.678354\n",
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 10.545715\n",
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 10.297077\n",
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 9.882720\n",
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 9.583048\n",
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 10.194306\n",
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 10.099363\n",
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 10.639174\n",
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 10.904382\n",
      "Epoch: 266/3000... Step: 8500... Loss: 4.278063... Val Loss: 10.988467\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 4.017421\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 4.184220\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 4.684859\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 4.095341\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 5.085191\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 5.330009\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 4.862540\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 4.635361\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 4.414767\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 4.211621\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 4.012095\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 4.663076\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 4.544124\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 5.296385\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 5.564554\n",
      "Epoch: 269/3000... Step: 8600... Loss: 4.336658... Val Loss: 5.788673\n",
      "Validation loss decreased (5.938928 --> 5.788673).  Saving model ...\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 4.590261\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 4.966815\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 5.425064\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 4.787572\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 6.370623\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 6.523351\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 6.074785\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 5.731103\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 5.475682\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 5.244806\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 5.105348\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 5.857717\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 5.635125\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 6.414040\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 6.745017\n",
      "Epoch: 272/3000... Step: 8700... Loss: 12.606119... Val Loss: 6.768060\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 3.416673\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 3.612792\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 4.421646\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 3.936276\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 5.012148\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 5.232302\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 4.768997\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 4.510039\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 4.281839\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 4.105803\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 4.065769\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 4.871850\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 4.696241\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 5.433842\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 5.736014\n",
      "Epoch: 275/3000... Step: 8800... Loss: 2.348745... Val Loss: 5.875516\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 4.500916\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 4.346116\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 4.945006\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 4.269603\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 5.065711\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 5.416966\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 4.968832\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 4.737823\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 4.516792\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 4.354893\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 4.218633\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 4.849347\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 4.708453\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 5.498568\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 5.789759\n",
      "Epoch: 279/3000... Step: 8900... Loss: 5.876082... Val Loss: 6.026040\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 3.763268\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 4.189771\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 4.969299\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 4.362103\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 5.549004\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 5.927685\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 5.466753\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 5.148795\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 4.868899\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 4.603698\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 4.484452\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 5.213984\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 5.008591\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 5.731095\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 6.054785\n",
      "Epoch: 282/3000... Step: 9000... Loss: 1.933199... Val Loss: 6.245285\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 8.550880\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 7.319445\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 7.961988\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 7.392913\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 8.028667\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 8.758087\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 8.273304\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 8.152249\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 7.988028\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 7.770476\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 7.668906\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 8.191867\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 8.084924\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 8.820761\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 8.822634\n",
      "Epoch: 285/3000... Step: 9100... Loss: 3.969522... Val Loss: 8.993813\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 4.221121\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 4.146234\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 4.700201\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 4.034206\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 5.132356\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 5.589013\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 5.127553\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 4.874638\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 4.630274\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 4.396235\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 4.198911\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 4.848023\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 4.704752\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 5.441936\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 5.810906\n",
      "Epoch: 288/3000... Step: 9200... Loss: 2.064073... Val Loss: 5.866220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 4.407927\n",
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 4.539577\n",
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 4.790385\n",
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 4.274293\n",
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 5.254899\n",
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 5.670111\n",
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 5.263965\n",
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 5.136493\n",
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 4.934562\n",
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 4.764356\n",
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 4.638604\n",
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 5.196489\n",
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 5.028337\n",
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 5.728666\n",
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 6.070358\n",
      "Epoch: 291/3000... Step: 9300... Loss: 3.439679... Val Loss: 6.307249\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 6.164877\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 7.181079\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 7.772818\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 6.886975\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 8.104142\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 9.838002\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 9.112957\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 8.638500\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 8.237848\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 7.854678\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 7.629883\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 8.316664\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 8.042755\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 8.774259\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 9.171588\n",
      "Epoch: 294/3000... Step: 9400... Loss: 10.514795... Val Loss: 9.090363\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 3.492733\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 3.730328\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 4.464836\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 4.001347\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 5.072432\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 5.505561\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 5.076874\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 4.829405\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 4.593648\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 4.365602\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 4.170459\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 4.783908\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 4.645483\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 5.372541\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 5.697713\n",
      "Epoch: 297/3000... Step: 9500... Loss: 11.640491... Val Loss: 5.876370\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 3.960536\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 3.994479\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 5.095833\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 4.599773\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 5.856784\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 6.319253\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 5.797603\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 5.516972\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 5.244910\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 4.919987\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 4.751578\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 5.394288\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 5.305775\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 6.118958\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 6.530443\n",
      "Epoch: 300/3000... Step: 9600... Loss: 2.930408... Val Loss: 6.925897\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 4.072659\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 4.279604\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 5.116444\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 4.533446\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 5.506702\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 5.889228\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 5.514401\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 5.325594\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 5.043313\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 4.703929\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 4.486801\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 5.139093\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 5.001391\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 5.632733\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 5.985559\n",
      "Epoch: 304/3000... Step: 9700... Loss: 1.835201... Val Loss: 6.174905\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 3.631130\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 4.004234\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 5.016244\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 4.413913\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 5.793587\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 6.096929\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 5.530980\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 5.156240\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 4.894063\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 4.672770\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 4.618430\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 5.319402\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 5.142835\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 5.896489\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 6.194076\n",
      "Epoch: 307/3000... Step: 9800... Loss: 2.761223... Val Loss: 6.323864\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 3.335334\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 3.628264\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 4.412363\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 3.925410\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 5.006723\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 5.202299\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 4.745989\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 4.523861\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 4.303369\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 4.048667\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 3.881126\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 4.584420\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 4.479686\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 5.201201\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 5.601562\n",
      "Epoch: 310/3000... Step: 9900... Loss: 2.485238... Val Loss: 5.767150\n",
      "Validation loss decreased (5.788673 --> 5.767150).  Saving model ...\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 5.435853\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 5.312788\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 5.499287\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 5.147391\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 6.087614\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 6.264618\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 5.852715\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 5.798068\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 5.585370\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 5.341237\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 5.288130\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 5.784214\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 5.700256\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 6.392681\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 6.899802\n",
      "Epoch: 313/3000... Step: 10000... Loss: 2.984704... Val Loss: 7.177911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 3.606755\n",
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 4.128850\n",
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 4.899631\n",
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 4.331209\n",
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 5.448874\n",
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 5.788395\n",
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 5.247081\n",
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 4.946509\n",
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 4.720513\n",
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 4.473075\n",
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 4.301770\n",
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 4.977086\n",
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 4.821433\n",
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 5.600000\n",
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 5.948939\n",
      "Epoch: 316/3000... Step: 10100... Loss: 2.664490... Val Loss: 6.153880\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 3.872128\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 4.194937\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 5.198264\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 4.475227\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 5.625579\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 5.751413\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 5.301081\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 4.994723\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 4.730912\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 4.455405\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 4.271201\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 4.977473\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 4.813612\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 5.549020\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 5.981116\n",
      "Epoch: 319/3000... Step: 10200... Loss: 3.517026... Val Loss: 6.018853\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 4.062074\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 4.329006\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 4.925524\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 4.239449\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 5.322948\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 5.636793\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 5.203266\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 4.948249\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 4.704557\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 4.430354\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 4.269666\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 4.922431\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 4.745865\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 5.577070\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 5.951232\n",
      "Epoch: 322/3000... Step: 10300... Loss: 12.437120... Val Loss: 6.020753\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 4.060339\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 4.051866\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 4.662038\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 4.028113\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 5.163601\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 5.467666\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 4.988562\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 4.648317\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 4.412467\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 4.197723\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 4.121572\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 4.776046\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 4.595059\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 5.395233\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 5.716083\n",
      "Epoch: 325/3000... Step: 10400... Loss: 3.862882... Val Loss: 5.874058\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 4.762826\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 4.798700\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 5.536393\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 5.061195\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 6.041352\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 6.388628\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 5.964549\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 5.758573\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 5.497257\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 5.278810\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 5.210375\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 5.823710\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 5.687621\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 6.301222\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 6.634184\n",
      "Epoch: 329/3000... Step: 10500... Loss: 4.532786... Val Loss: 6.918693\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 4.058280\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 3.940933\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 4.530635\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 3.954511\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 4.976920\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 5.204481\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 4.762257\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 4.560097\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 4.325763\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 4.113170\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 3.927642\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 4.576512\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 4.433714\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 5.188765\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 5.517597\n",
      "Epoch: 332/3000... Step: 10600... Loss: 1.866362... Val Loss: 5.708564\n",
      "Validation loss decreased (5.767150 --> 5.708564).  Saving model ...\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 4.313009\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 4.190077\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 4.922893\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 4.372716\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 5.267232\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 5.538321\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 5.106272\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 4.877940\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 4.651116\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 4.412466\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 4.303002\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 4.968451\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 4.838604\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 5.520244\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 5.814773\n",
      "Epoch: 335/3000... Step: 10700... Loss: 3.168852... Val Loss: 5.931307\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 5.364992\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 6.146693\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 6.627267\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 5.946503\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 6.933163\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 7.244610\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 6.855624\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 6.615247\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 6.360895\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 6.121363\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 5.979859\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 6.646416\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 6.424032\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 7.224072\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 7.643453\n",
      "Epoch: 338/3000... Step: 10800... Loss: 3.375689... Val Loss: 7.549283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 4.268791\n",
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 4.425137\n",
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 5.568712\n",
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 4.895900\n",
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 5.333657\n",
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 5.559380\n",
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 5.118123\n",
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 4.996151\n",
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 4.759466\n",
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 4.471786\n",
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 4.278265\n",
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 4.864974\n",
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 4.736526\n",
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 5.427513\n",
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 5.781596\n",
      "Epoch: 341/3000... Step: 10900... Loss: 3.045393... Val Loss: 5.667262\n",
      "Validation loss decreased (5.708564 --> 5.667262).  Saving model ...\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 4.070120\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 4.600166\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 5.082696\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 4.398809\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 5.288704\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 5.614433\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 5.239740\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 4.970807\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 4.730077\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 4.713677\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 4.707527\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 5.515104\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 5.329214\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 6.117318\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 6.490234\n",
      "Epoch: 344/3000... Step: 11000... Loss: 6.294597... Val Loss: 6.383812\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 3.391418\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 3.774473\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 4.482640\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 3.880484\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 4.871911\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 5.202067\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 4.860985\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 4.562945\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 4.327821\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 4.092977\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 4.016563\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 4.856219\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 4.688990\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 5.426901\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 5.797716\n",
      "Epoch: 347/3000... Step: 11100... Loss: 11.320442... Val Loss: 5.795593\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 8.122238\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 7.434132\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 7.820581\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 7.604555\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 8.373062\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 8.725559\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 8.582277\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 8.578735\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 8.298646\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 8.130048\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 8.154645\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 8.632799\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 8.630972\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 9.193047\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 9.533811\n",
      "Epoch: 350/3000... Step: 11200... Loss: 1.755033... Val Loss: 10.114772\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 4.256422\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 4.432600\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 4.977515\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 4.431480\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 4.859526\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 5.257284\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 4.845329\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 4.760123\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 4.545038\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 4.394755\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 4.253675\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 4.917042\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 4.780860\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 5.422859\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 5.672121\n",
      "Epoch: 354/3000... Step: 11300... Loss: 2.888557... Val Loss: 5.674260\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 6.632469\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 6.594962\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 7.148680\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 6.691869\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 7.401704\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 7.720101\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 7.275034\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 7.084550\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 6.925835\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 6.687453\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 6.542584\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 7.239531\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 7.178646\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 7.864028\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 8.083960\n",
      "Epoch: 357/3000... Step: 11400... Loss: 3.174402... Val Loss: 8.445371\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 4.318655\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 4.351749\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 4.818457\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 4.342347\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 4.692439\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 5.177933\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 4.755394\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 4.666555\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 4.495383\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 4.249886\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 4.146004\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 4.815066\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 4.718478\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 5.385907\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 5.655160\n",
      "Epoch: 360/3000... Step: 11500... Loss: 2.769006... Val Loss: 5.619546\n",
      "Validation loss decreased (5.667262 --> 5.619546).  Saving model ...\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 4.382776\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 4.629464\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 4.631623\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 4.206660\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 4.550979\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 5.677039\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 5.267532\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 5.199111\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 4.932065\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 4.627514\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 4.532818\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 5.114361\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 4.988498\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 5.657124\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 6.024844\n",
      "Epoch: 363/3000... Step: 11600... Loss: 1.913295... Val Loss: 5.955192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 4.466980\n",
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 4.753734\n",
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 5.101998\n",
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 4.736047\n",
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 5.015342\n",
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 5.645900\n",
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 5.228594\n",
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 5.122985\n",
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 4.870672\n",
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 4.645327\n",
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 4.480649\n",
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 5.251991\n",
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 5.132897\n",
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 5.772028\n",
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 6.105402\n",
      "Epoch: 366/3000... Step: 11700... Loss: 2.341727... Val Loss: 6.045239\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 4.422522\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 4.607704\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 4.991724\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 4.458577\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 4.951939\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 5.294663\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 4.913095\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 4.741047\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 4.531282\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 4.380745\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 4.236540\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 5.061221\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 4.922966\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 5.680471\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 5.995150\n",
      "Epoch: 369/3000... Step: 11800... Loss: 3.694038... Val Loss: 5.897933\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 4.720204\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 4.903138\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 5.238330\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 4.770558\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 5.516343\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 6.202612\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 5.666678\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 5.375502\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 5.135563\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 4.943581\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 4.808002\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 5.675308\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 5.506304\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 6.282978\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 6.592286\n",
      "Epoch: 372/3000... Step: 11900... Loss: 12.399602... Val Loss: 6.726788\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 3.785096\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 4.205564\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 5.806013\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 4.883210\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 4.719634\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 5.469754\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 4.975171\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 4.732942\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 4.486063\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 4.183860\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 4.073112\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 4.900665\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 4.726256\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 5.446141\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 5.565687\n",
      "Epoch: 375/3000... Step: 12000... Loss: 2.416260... Val Loss: 5.504585\n",
      "Validation loss decreased (5.619546 --> 5.504585).  Saving model ...\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 4.662085\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 4.975680\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 4.665990\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 4.318538\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 5.932070\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 7.217589\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 6.717089\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 6.579527\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 6.301767\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 5.985126\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 5.839863\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 6.567795\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 6.300594\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 7.094078\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 7.371774\n",
      "Epoch: 379/3000... Step: 12100... Loss: 3.824082... Val Loss: 8.651561\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 4.344839\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 4.242232\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 4.390118\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 4.029747\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 5.253600\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 5.894605\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 5.394014\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 5.226005\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 4.972244\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 4.657633\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 4.505702\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 5.114848\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 4.982950\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 5.664112\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 6.021859\n",
      "Epoch: 382/3000... Step: 12200... Loss: 1.622742... Val Loss: 6.475563\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 5.257196\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 6.065225\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 6.193288\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 5.700982\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 6.892541\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 7.301179\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 6.907354\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 6.683051\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 6.422052\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 6.161354\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 6.155496\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 6.741827\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 6.505354\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 7.299707\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 7.682601\n",
      "Epoch: 385/3000... Step: 12300... Loss: 2.579496... Val Loss: 7.715530\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 5.954320\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 6.603343\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 6.257995\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 5.655416\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 7.148422\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 7.663114\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 7.233357\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 6.974242\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 6.725517\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 6.390151\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 6.270246\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 6.955243\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 6.726996\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 7.488688\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 7.917877\n",
      "Epoch: 388/3000... Step: 12400... Loss: 3.568971... Val Loss: 8.236945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 3.577260\n",
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 3.966257\n",
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 5.603447\n",
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 4.834263\n",
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 5.313467\n",
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 5.593288\n",
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 5.129284\n",
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 4.841415\n",
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 4.608580\n",
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 4.345583\n",
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 4.259982\n",
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 4.994963\n",
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 4.870810\n",
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 5.594117\n",
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 5.872475\n",
      "Epoch: 391/3000... Step: 12500... Loss: 3.062421... Val Loss: 5.761503\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 4.205327\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 4.786060\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 7.187585\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 6.012876\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 6.574361\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 6.679614\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 6.116013\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 5.697413\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 5.394031\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 5.176903\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 5.091070\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 5.821707\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 5.596916\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 6.365336\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 6.618311\n",
      "Epoch: 394/3000... Step: 12600... Loss: 4.894054... Val Loss: 6.592229\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 4.566359\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 4.469208\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 5.297170\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 4.721921\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 4.855129\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 5.099333\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 4.720510\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 4.625246\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 4.466612\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 4.251330\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 4.204047\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 4.969119\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 4.882485\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 5.568770\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 5.759433\n",
      "Epoch: 397/3000... Step: 12700... Loss: 9.728430... Val Loss: 5.672473\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 8.331053\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 9.099415\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 8.162003\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 7.327688\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 7.608985\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 8.208337\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 7.869612\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 7.549232\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 7.335537\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 7.209005\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 7.224770\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 8.199886\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 7.991819\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 8.873451\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 9.213869\n",
      "Epoch: 400/3000... Step: 12800... Loss: 6.420896... Val Loss: 9.041192\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 3.578969\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 4.053501\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 4.690918\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 4.163339\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 4.216342\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 4.772742\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 4.429729\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 4.268663\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 4.117177\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 3.966336\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 3.907170\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 4.759629\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 4.624944\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 5.385997\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 5.715280\n",
      "Epoch: 404/3000... Step: 12900... Loss: 3.409858... Val Loss: 5.665061\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 4.072238\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 4.440358\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 4.251771\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 3.841082\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 3.950558\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 4.523054\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 4.209956\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 4.054911\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 3.921889\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 3.778077\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 3.833908\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 4.704991\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 4.550089\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 5.322409\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 5.502762\n",
      "Epoch: 407/3000... Step: 13000... Loss: 1.462129... Val Loss: 5.443168\n",
      "Validation loss decreased (5.504585 --> 5.443168).  Saving model ...\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 4.784779\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 4.696750\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 4.345447\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 4.054743\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 3.821816\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 4.433447\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 4.152254\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 4.209215\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 4.088785\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 3.860499\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 3.885100\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 4.704929\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 4.582664\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 5.270378\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 5.380675\n",
      "Epoch: 410/3000... Step: 13100... Loss: 2.597049... Val Loss: 5.273413\n",
      "Validation loss decreased (5.443168 --> 5.273413).  Saving model ...\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 4.388991\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 4.401077\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 3.959935\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 3.606382\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 4.616619\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 4.991819\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 4.590220\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 4.486177\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 4.304097\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 4.235508\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 4.182850\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 4.827525\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 4.727170\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 5.499701\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 5.712706\n",
      "Epoch: 413/3000... Step: 13200... Loss: 5.987869... Val Loss: 6.302997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 5.210062\n",
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 5.021597\n",
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 4.642198\n",
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 4.289729\n",
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 4.324305\n",
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 4.973412\n",
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 4.633858\n",
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 4.505515\n",
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 4.384649\n",
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 4.210449\n",
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 4.138266\n",
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 4.899846\n",
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 4.786257\n",
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 5.602810\n",
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 5.802304\n",
      "Epoch: 416/3000... Step: 13300... Loss: 3.877839... Val Loss: 5.707905\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 4.346926\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 4.408794\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 4.036958\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 3.709393\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 3.448268\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 3.960954\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 3.714169\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 3.677975\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 3.590120\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 3.456831\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 3.392820\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 4.249468\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 4.145728\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 4.869798\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 5.157559\n",
      "Epoch: 419/3000... Step: 13400... Loss: 3.639736... Val Loss: 5.083951\n",
      "Validation loss decreased (5.273413 --> 5.083951).  Saving model ...\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 5.910792\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 5.964635\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 5.861710\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 5.622954\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 5.232258\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 5.874429\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 5.605013\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 5.668338\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 5.556712\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 5.345293\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 5.385302\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 6.271532\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 6.207557\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 6.884401\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 6.863563\n",
      "Epoch: 422/3000... Step: 13500... Loss: 8.800498... Val Loss: 6.792831\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 3.464272\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 3.751973\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 3.899259\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 3.527825\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 3.398060\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 3.942380\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 3.753521\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 3.656084\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 3.536595\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 3.450142\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 3.687486\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 4.544750\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 4.431802\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 5.108914\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 5.364973\n",
      "Epoch: 425/3000... Step: 13600... Loss: 2.466257... Val Loss: 5.609778\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 5.625493\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 5.687351\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 4.400832\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 3.940992\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 5.655134\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 6.612860\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 6.374869\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 6.039894\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 5.705596\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 5.626227\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 5.659364\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 6.603246\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 6.313202\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 7.170581\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 7.586633\n",
      "Epoch: 429/3000... Step: 13700... Loss: 5.968534... Val Loss: 9.402406\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 4.276599\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 4.343966\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 5.100813\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 4.519967\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 4.171870\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 4.708724\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 4.389243\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 4.328817\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 4.163597\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 3.956577\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 3.911615\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 4.670627\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 4.578529\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 5.257676\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 5.541826\n",
      "Epoch: 432/3000... Step: 13800... Loss: 2.023747... Val Loss: 5.895790\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 7.759868\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 7.685009\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 7.115421\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 6.913646\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 7.137552\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 7.767198\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 7.441322\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 7.407653\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 7.246284\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 7.091366\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 7.227655\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 7.990742\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 7.885155\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 8.470943\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 8.500936\n",
      "Epoch: 435/3000... Step: 13900... Loss: 3.514187... Val Loss: 8.667429\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 4.903684\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 5.660872\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 4.720643\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 4.362504\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 4.222126\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 4.898622\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 4.747664\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 4.814216\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 4.743141\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 4.584036\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 4.812206\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 5.711432\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 5.530648\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 6.267433\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 6.342140\n",
      "Epoch: 438/3000... Step: 14000... Loss: 3.366899... Val Loss: 6.207266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 4.324614\n",
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 4.503133\n",
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 5.581440\n",
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 4.750571\n",
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 4.493259\n",
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 5.130446\n",
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 4.709666\n",
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 4.486263\n",
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 4.278019\n",
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 4.120813\n",
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 4.060132\n",
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 4.997936\n",
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 4.851489\n",
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 5.609260\n",
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 5.887832\n",
      "Epoch: 441/3000... Step: 14100... Loss: 3.677078... Val Loss: 6.243970\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 5.855878\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 6.131397\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 5.226731\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 4.647374\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 4.383982\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 6.382421\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 6.010720\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 5.800614\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 5.637071\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 5.540303\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 5.400980\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 6.402427\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 6.213111\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 6.985320\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 7.025645\n",
      "Epoch: 444/3000... Step: 14200... Loss: 4.100904... Val Loss: 6.982584\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 4.150146\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 4.925374\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 4.131236\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 3.933606\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 3.686450\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 4.233642\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 4.123759\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 4.223881\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 4.084862\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 3.895514\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 4.340390\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 5.073938\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 4.915659\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 5.549186\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 5.615436\n",
      "Epoch: 447/3000... Step: 14300... Loss: 5.699636... Val Loss: 5.483052\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 6.442104\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 7.009068\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 5.634819\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 5.236158\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 5.270699\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 6.110564\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 5.909903\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 5.689683\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 5.582654\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 5.429773\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 5.455372\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 6.405516\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 6.259013\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 7.107580\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 7.281800\n",
      "Epoch: 450/3000... Step: 14400... Loss: 7.680636... Val Loss: 7.336841\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 4.546346\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 4.928487\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 4.805368\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 4.209889\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 3.851696\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 4.254902\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 3.920064\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 3.806859\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 3.697029\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 3.534413\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 3.457906\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 4.239492\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 4.181299\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 4.924744\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 5.187102\n",
      "Epoch: 454/3000... Step: 14500... Loss: 1.814166... Val Loss: 5.299482\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 13.754702\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 14.829049\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 13.145872\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 12.731692\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 13.020801\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 13.779476\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 13.619432\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 13.314280\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 13.262970\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 13.058825\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 13.089727\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 14.151546\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 13.928987\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 14.807905\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 15.101127\n",
      "Epoch: 457/3000... Step: 14600... Loss: 11.582738... Val Loss: 14.924494\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 4.736005\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 5.071538\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 4.753126\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 4.541004\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 4.176568\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 4.962772\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 4.642448\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 4.704108\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 4.558831\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 4.347877\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 4.476392\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 5.206411\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 5.109485\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 5.771833\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 5.803035\n",
      "Epoch: 460/3000... Step: 14700... Loss: 1.580377... Val Loss: 5.767865\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 4.758046\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 4.508844\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 3.904482\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 3.693508\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 5.898054\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 6.254330\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 5.708126\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 5.510102\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 5.260878\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 4.984092\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 4.781081\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 5.454045\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 5.331717\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 5.959494\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 6.228451\n",
      "Epoch: 463/3000... Step: 14800... Loss: 1.980196... Val Loss: 8.360362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 4.099876\n",
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 4.419928\n",
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 3.779457\n",
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 3.477291\n",
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 3.327449\n",
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 4.070173\n",
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 3.843479\n",
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 3.856359\n",
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 3.762102\n",
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 3.575625\n",
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 3.502532\n",
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 4.237504\n",
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 4.125097\n",
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 4.837669\n",
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 5.060866\n",
      "Epoch: 466/3000... Step: 14900... Loss: 2.042654... Val Loss: 4.948668\n",
      "Validation loss decreased (5.083951 --> 4.948668).  Saving model ...\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 4.867601\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 5.132679\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 5.012650\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 4.477500\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 4.256342\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 5.020726\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 4.865351\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 4.806507\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 4.692028\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 4.578615\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 4.548838\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 5.323235\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 5.169952\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 5.907230\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 5.975087\n",
      "Epoch: 469/3000... Step: 15000... Loss: 4.700454... Val Loss: 5.903875\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 4.370196\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 4.464342\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 3.962006\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 3.597788\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 3.300580\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 3.926821\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 3.697113\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 3.699207\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 3.598515\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 3.471868\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 3.438594\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 4.269828\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 4.183363\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 4.871448\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 4.950536\n",
      "Epoch: 472/3000... Step: 15100... Loss: 7.832778... Val Loss: 4.895019\n",
      "Validation loss decreased (4.948668 --> 4.895019).  Saving model ...\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 3.977765\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 4.445138\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 3.427795\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 3.041513\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 3.079458\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 3.643780\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 3.361341\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 3.227020\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 3.157509\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 3.069127\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 2.979115\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 3.959665\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 3.885990\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 4.712936\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 4.951008\n",
      "Epoch: 475/3000... Step: 15200... Loss: 3.462738... Val Loss: 5.120694\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 3.698713\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 3.663306\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 2.982474\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 2.782770\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 2.692538\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 3.966527\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 3.688424\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 3.571120\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 3.467090\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 3.304231\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 3.184367\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 4.214679\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 4.124462\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 4.890284\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 5.052862\n",
      "Epoch: 479/3000... Step: 15300... Loss: 2.366539... Val Loss: 5.131246\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 3.906079\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 4.193327\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 3.676158\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 3.302149\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 3.064048\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 3.756726\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 3.537575\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 3.483727\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 3.405001\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 3.250851\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 3.246902\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 4.212374\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 4.115970\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 4.828183\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 4.931580\n",
      "Epoch: 482/3000... Step: 15400... Loss: 1.844297... Val Loss: 5.097234\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 4.070148\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 4.320918\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 3.524111\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 3.349547\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 3.187902\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 4.284018\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 4.003308\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 4.041000\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 3.899272\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 3.680031\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 3.769142\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 4.456813\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 4.358656\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 4.994111\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 5.035663\n",
      "Epoch: 485/3000... Step: 15500... Loss: 2.288012... Val Loss: 4.967653\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 5.757024\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 6.159338\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 5.189193\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 4.802589\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 4.512687\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 5.179401\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 4.982789\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 4.994485\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 4.899431\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 4.737473\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 4.857175\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 5.523587\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 5.396091\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 6.152415\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 6.237446\n",
      "Epoch: 488/3000... Step: 15600... Loss: 4.230854... Val Loss: 6.105320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 4.658627\n",
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 4.514957\n",
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 3.754114\n",
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 3.470082\n",
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 3.161910\n",
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 3.751253\n",
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 3.522049\n",
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 3.603641\n",
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 3.532023\n",
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 3.443312\n",
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 3.550873\n",
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 4.273484\n",
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 4.183474\n",
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 4.901217\n",
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 4.897062\n",
      "Epoch: 491/3000... Step: 15700... Loss: 2.675710... Val Loss: 4.985704\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 4.107776\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 4.141766\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 3.253103\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 2.851763\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 3.643650\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 4.185596\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 3.917823\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 3.792552\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 3.689600\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 3.553872\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 3.689968\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 4.582522\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 4.459720\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 5.143805\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 5.317501\n",
      "Epoch: 494/3000... Step: 15800... Loss: 4.991345... Val Loss: 5.925325\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 4.439540\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 4.278572\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 4.332209\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 3.914308\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 3.781098\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 4.312289\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 4.014508\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 4.001694\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 3.899358\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 3.717298\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 3.726676\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 4.393548\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 4.335929\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 5.009135\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 5.015374\n",
      "Epoch: 497/3000... Step: 15900... Loss: 5.804238... Val Loss: 5.373376\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 5.679889\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 5.794865\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 4.769280\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 4.309670\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 4.124159\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 5.049183\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 4.904776\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 4.843624\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 4.771642\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 4.802442\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 4.833416\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 5.731863\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 5.600974\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 6.444931\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 6.410596\n",
      "Epoch: 500/3000... Step: 16000... Loss: 5.925909... Val Loss: 6.279117\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 4.032056\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 4.069753\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 3.870722\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 3.461424\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 3.259014\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 4.242496\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 3.948250\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 3.858337\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 3.729317\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 3.559231\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 3.556969\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 4.301885\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 4.200772\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 4.942165\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 4.975244\n",
      "Epoch: 504/3000... Step: 16100... Loss: 1.984141... Val Loss: 5.074070\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 4.418720\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 4.273567\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 3.492675\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 3.194482\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 3.521413\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 4.196088\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 3.908506\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 3.765007\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 3.641044\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 3.635827\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 3.623570\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 4.604064\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 4.511751\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 5.270591\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 5.618377\n",
      "Epoch: 507/3000... Step: 16200... Loss: 1.644698... Val Loss: 5.868468\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 4.147260\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 4.326129\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 3.841118\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 3.516401\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 3.323318\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 4.225961\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 3.947607\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 4.003206\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 3.876171\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 3.638648\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 3.609946\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 4.306964\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 4.221570\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 4.906171\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 4.963034\n",
      "Epoch: 510/3000... Step: 16300... Loss: 1.861018... Val Loss: 5.080769\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 4.057261\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 3.906735\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 3.152692\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 2.943809\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 4.814557\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 5.130077\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 4.729200\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 4.607142\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 4.408105\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 4.225377\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 4.113907\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 4.767874\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 4.644987\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 5.333293\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 5.547458\n",
      "Epoch: 513/3000... Step: 16400... Loss: 2.705036... Val Loss: 7.287826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 4.815593\n",
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 4.612976\n",
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 4.097438\n",
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 3.760360\n",
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 3.498127\n",
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 4.094643\n",
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 3.985224\n",
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 4.049345\n",
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 3.948908\n",
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 3.767875\n",
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 3.753775\n",
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 4.660169\n",
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 4.567108\n",
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 5.245752\n",
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 5.375229\n",
      "Epoch: 516/3000... Step: 16500... Loss: 2.267400... Val Loss: 5.354210\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 4.814185\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 5.588682\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 5.425944\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 5.191646\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 5.601513\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 6.175306\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 5.848676\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 5.646559\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 5.533465\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 5.281831\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 5.148044\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 5.989699\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 5.790701\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 6.554347\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 6.944002\n",
      "Epoch: 519/3000... Step: 16600... Loss: 4.085668... Val Loss: 7.389711\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 3.939987\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 3.899703\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 3.519998\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 3.220389\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 2.997843\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 3.612210\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 3.463417\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 3.438167\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 3.352724\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 3.189619\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 3.130851\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 3.876016\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 3.839846\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 4.570378\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 4.769178\n",
      "Epoch: 522/3000... Step: 16700... Loss: 7.523228... Val Loss: 4.844517\n",
      "Validation loss decreased (4.895019 --> 4.844517).  Saving model ...\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 3.948236\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 4.145785\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 3.761391\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 3.378039\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 3.300650\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 3.889452\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 3.625609\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 3.549233\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 3.456822\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 3.338222\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 3.379863\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 4.193118\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 4.115472\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 4.880458\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 4.880362\n",
      "Epoch: 525/3000... Step: 16800... Loss: 1.403870... Val Loss: 5.148124\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 5.861346\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 5.248748\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 4.834340\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 4.757339\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 4.677848\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 6.764470\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 6.346353\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 6.162691\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 5.948197\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 5.695907\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 5.584760\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 6.271327\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 6.200484\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 6.834966\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 6.838729\n",
      "Epoch: 529/3000... Step: 16900... Loss: 3.275971... Val Loss: 7.085543\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 4.476029\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 4.434586\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 4.072319\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 3.847547\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 3.577799\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 4.184683\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 3.971940\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 4.018732\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 3.958317\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 3.810771\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 3.795707\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 4.571139\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 4.525843\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 5.247376\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 5.257239\n",
      "Epoch: 532/3000... Step: 17000... Loss: 1.912327... Val Loss: 5.231682\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 3.791783\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 3.962159\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 3.246028\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 2.927258\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 2.766549\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 3.327270\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 3.160707\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 3.188928\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 3.148594\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 3.085767\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 3.062465\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 3.831560\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 3.781282\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 4.543724\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 4.590617\n",
      "Epoch: 535/3000... Step: 17100... Loss: 1.089575... Val Loss: 4.512534\n",
      "Validation loss decreased (4.844517 --> 4.512534).  Saving model ...\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 4.547419\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 4.452173\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 3.548869\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 3.274939\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 3.101621\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 3.624054\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 3.432843\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 3.431516\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 3.342541\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 3.221218\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 3.152185\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 3.857880\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 3.820262\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 4.630586\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 4.751186\n",
      "Epoch: 538/3000... Step: 17200... Loss: 2.282639... Val Loss: 4.682142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 4.294926\n",
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 4.448474\n",
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 3.678372\n",
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 3.481365\n",
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 3.312137\n",
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 3.802383\n",
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 3.553515\n",
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 3.525286\n",
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 3.450750\n",
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 3.302813\n",
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 3.415982\n",
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 4.228451\n",
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 4.198801\n",
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 4.902189\n",
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 4.938126\n",
      "Epoch: 541/3000... Step: 17300... Loss: 2.113219... Val Loss: 4.933432\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 3.917334\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 3.808333\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 2.962967\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 2.640278\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 2.648733\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 3.277532\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 3.081399\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 3.006065\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 2.960285\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 2.897704\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 2.889027\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 3.731633\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 3.705081\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 4.503274\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 4.511811\n",
      "Epoch: 544/3000... Step: 17400... Loss: 2.666858... Val Loss: 4.604723\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 4.209516\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 4.298501\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 4.144440\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 3.556943\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 3.353372\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 3.763966\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 3.492787\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 3.416224\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 3.319820\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 3.211411\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 3.107911\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 3.809629\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 3.754901\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 4.556459\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 4.695257\n",
      "Epoch: 547/3000... Step: 17500... Loss: 9.234403... Val Loss: 4.740888\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 4.268126\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 3.940768\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 3.258659\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 2.968872\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 3.592510\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 4.033531\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 3.713613\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 3.623707\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 3.516360\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 3.461011\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 3.455283\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 4.221114\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 4.165799\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 4.976395\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 5.066386\n",
      "Epoch: 550/3000... Step: 17600... Loss: 4.750616... Val Loss: 5.406786\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 4.661022\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 4.497729\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 5.420290\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 4.571250\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 4.577585\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 4.706936\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 4.314705\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 4.162150\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 4.021372\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 3.901933\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 3.746499\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 4.493682\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 4.409551\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 5.100037\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 5.228317\n",
      "Epoch: 554/3000... Step: 17700... Loss: 2.532884... Val Loss: 5.882323\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 4.386609\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 4.195624\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 3.424771\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 3.226732\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 3.791730\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 4.335909\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 4.129516\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 4.141365\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 3.986087\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 3.809477\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 3.707349\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 4.483337\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 4.403437\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 5.055884\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 5.305258\n",
      "Epoch: 557/3000... Step: 17800... Loss: 2.896566... Val Loss: 5.902458\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 4.367543\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 4.524529\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 3.802480\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 3.519821\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 3.435280\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 3.899700\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 3.699309\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 3.726616\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 3.656492\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 3.463225\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 3.386238\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 4.059342\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 4.023225\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 4.704577\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 4.700132\n",
      "Epoch: 560/3000... Step: 17900... Loss: 1.208239... Val Loss: 4.617070\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 4.241003\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 4.511953\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 3.717966\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 3.458960\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 3.324362\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 4.006906\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 3.879787\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 4.019474\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 4.024755\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 3.798395\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 3.848102\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 4.477317\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 4.380931\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 5.091013\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 5.153217\n",
      "Epoch: 563/3000... Step: 18000... Loss: 1.825694... Val Loss: 5.074881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 5.275381\n",
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 5.597586\n",
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 4.577527\n",
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 4.194393\n",
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 4.059781\n",
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 4.641185\n",
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 4.540305\n",
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 4.573892\n",
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 4.533047\n",
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 4.418798\n",
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 4.493108\n",
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 5.277509\n",
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 5.176671\n",
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 5.968353\n",
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 6.013002\n",
      "Epoch: 566/3000... Step: 18100... Loss: 2.899297... Val Loss: 5.881233\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 4.668711\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 4.886776\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 5.395711\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 4.596595\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 4.984691\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 5.156519\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 4.774080\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 4.562482\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 4.399555\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 4.268966\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 4.139079\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 4.911230\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 4.822996\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 5.612218\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 5.635587\n",
      "Epoch: 569/3000... Step: 18200... Loss: 2.774809... Val Loss: 6.483841\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 5.840016\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 5.669650\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 4.418913\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 4.113637\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 5.354654\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 6.012438\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 5.740584\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 5.513276\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 5.290787\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 5.296561\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 5.315983\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 6.115400\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 5.965076\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 6.704303\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 6.997024\n",
      "Epoch: 572/3000... Step: 18300... Loss: 4.162334... Val Loss: 8.064775\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 3.806325\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 4.050269\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 4.812618\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 4.177650\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 5.049715\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 7.516492\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 7.081626\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 6.608134\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 6.166632\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 6.021081\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 5.991790\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 6.507342\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 6.228692\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 6.842004\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 6.784082\n",
      "Epoch: 575/3000... Step: 18400... Loss: 1.620447... Val Loss: 7.781105\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 5.603176\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 5.188301\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 4.210942\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 4.255011\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 4.518227\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 8.188013\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 7.496183\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 7.431128\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 7.073702\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 6.573846\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 6.342079\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 6.744841\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 6.525095\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 7.038042\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 7.033384\n",
      "Epoch: 579/3000... Step: 18500... Loss: 1.667099... Val Loss: 7.113378\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 4.076076\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 4.229461\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 3.520633\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 3.213162\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 2.993059\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 3.389223\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 3.211231\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 3.195805\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 3.121895\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 2.996518\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 2.986303\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 3.697301\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 3.616502\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 4.363399\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 4.377868\n",
      "Epoch: 582/3000... Step: 18600... Loss: 1.293837... Val Loss: 4.519881\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 5.405404\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 4.923093\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 4.346292\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 4.308702\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 4.582069\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 5.326706\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 5.049293\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 5.285310\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 5.166695\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 4.870426\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 4.971474\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 5.572269\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 5.490560\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 6.114879\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 6.236272\n",
      "Epoch: 585/3000... Step: 18700... Loss: 1.809183... Val Loss: 6.397382\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 5.450450\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 5.724338\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 4.936391\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 4.653393\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 4.577484\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 5.013257\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 4.943513\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 5.133698\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 5.100449\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 4.889702\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 5.024073\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 5.665126\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 5.571482\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 6.283775\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 6.333627\n",
      "Epoch: 588/3000... Step: 18800... Loss: 2.967387... Val Loss: 6.221799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 3.926036\n",
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 4.191904\n",
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 3.643014\n",
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 3.395488\n",
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 3.203211\n",
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 3.545660\n",
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 3.408254\n",
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 3.471853\n",
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 3.399294\n",
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 3.320940\n",
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 3.377593\n",
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 4.038332\n",
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 3.976842\n",
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 4.706130\n",
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 4.766280\n",
      "Epoch: 591/3000... Step: 18900... Loss: 2.210980... Val Loss: 4.788755\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 3.807776\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 3.891842\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 3.171992\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 2.905481\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 2.784731\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 3.100097\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 2.936419\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 2.952784\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 2.900269\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 2.806008\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 3.004199\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 3.688298\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 3.652658\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 4.400429\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 4.330518\n",
      "Epoch: 594/3000... Step: 19000... Loss: 2.998673... Val Loss: 4.322566\n",
      "Validation loss decreased (4.512534 --> 4.322566).  Saving model ...\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 4.619522\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 4.618050\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 3.758701\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 3.257877\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 3.131717\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 3.755827\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 3.495111\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 3.524223\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 3.469331\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 3.388072\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 3.624727\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 4.301694\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 4.234769\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 4.976906\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 4.927172\n",
      "Epoch: 597/3000... Step: 19100... Loss: 4.487646... Val Loss: 5.054551\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 5.560513\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 5.261988\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 4.001260\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 3.576069\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 3.740606\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 4.049536\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 3.835333\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 3.818960\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 3.749376\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 3.805620\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 3.778813\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 4.462527\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 4.382909\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 5.160247\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 5.189727\n",
      "Epoch: 600/3000... Step: 19200... Loss: 3.117644... Val Loss: 5.342004\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 4.315355\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 4.128586\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 3.177108\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 2.879459\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 2.671900\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 3.158286\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 3.012224\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 3.053430\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 3.000560\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 2.915657\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 2.969866\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 3.595782\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 3.554761\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 4.295594\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 4.279324\n",
      "Epoch: 604/3000... Step: 19300... Loss: 2.185172... Val Loss: 4.217162\n",
      "Validation loss decreased (4.322566 --> 4.217162).  Saving model ...\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 5.301888\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 4.901115\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 4.108402\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 3.942476\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 3.758764\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 4.608986\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 4.386889\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 4.545204\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 4.438231\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 4.335889\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 4.457717\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 5.276288\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 5.097443\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 5.681662\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 5.940044\n",
      "Epoch: 607/3000... Step: 19400... Loss: 1.658284... Val Loss: 5.982063\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 4.154780\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 4.024857\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 3.569164\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 3.408141\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 3.201696\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 3.811426\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 3.617066\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 3.675690\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 3.582022\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 3.366188\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 3.329153\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 3.948954\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 3.911334\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 4.600792\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 4.619763\n",
      "Epoch: 610/3000... Step: 19500... Loss: 1.449554... Val Loss: 4.639302\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 6.521498\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 5.798051\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 5.018292\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 5.108568\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 4.819656\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 5.366700\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 5.120517\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 5.561349\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 5.379549\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 5.115084\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 5.252958\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 5.690226\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 5.615484\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 6.229297\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 6.338328\n",
      "Epoch: 613/3000... Step: 19600... Loss: 1.947666... Val Loss: 6.335110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 4.498975\n",
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 4.286113\n",
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 3.331280\n",
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 3.069448\n",
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 3.180280\n",
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 3.873374\n",
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 3.655493\n",
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 3.656427\n",
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 3.608137\n",
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 3.454901\n",
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 3.340749\n",
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 3.881974\n",
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 3.859249\n",
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 4.673523\n",
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 4.622711\n",
      "Epoch: 616/3000... Step: 19700... Loss: 1.867877... Val Loss: 4.588600\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 4.645956\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 4.660441\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 3.627026\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 3.260836\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 3.591881\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 4.188770\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 3.969565\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 3.869668\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 3.807562\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 3.722533\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 3.607811\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 4.188947\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 4.134854\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 4.985143\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 4.996397\n",
      "Epoch: 619/3000... Step: 19800... Loss: 2.930804... Val Loss: 5.089104\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 5.155448\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 4.531721\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 3.972373\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 4.006169\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 4.265617\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 4.645240\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 4.413594\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 4.497967\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 4.400613\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 4.169359\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 4.066101\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 4.626140\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 4.587690\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 5.212521\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 5.406319\n",
      "Epoch: 622/3000... Step: 19900... Loss: 6.088266... Val Loss: 5.573810\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 4.020954\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 4.145694\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 3.567347\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 3.281311\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 3.328824\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 3.795836\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 3.511354\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 3.449440\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 3.357713\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 3.264340\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 3.456540\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 4.135175\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 4.058241\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 4.800144\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 4.784070\n",
      "Epoch: 625/3000... Step: 20000... Loss: 1.364821... Val Loss: 5.152965\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 4.894025\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 4.697971\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 3.857323\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 3.319073\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 3.144746\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 3.860446\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 3.573662\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 3.427758\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 3.328321\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 3.423185\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 3.279336\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 4.131840\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 4.061373\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 4.866283\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 4.822898\n",
      "Epoch: 629/3000... Step: 20100... Loss: 3.884666... Val Loss: 4.754796\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 4.592917\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 4.934362\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 5.256154\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 4.631943\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 4.506987\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 5.028127\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 4.686908\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 4.542210\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 4.388607\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 4.175492\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 4.042147\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 4.518666\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 4.423167\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 5.152397\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 5.279700\n",
      "Epoch: 632/3000... Step: 20200... Loss: 1.751936... Val Loss: 5.607297\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 5.469116\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 5.451661\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 4.291631\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 4.035951\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 5.044343\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 5.623467\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 5.358259\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 5.256408\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 5.114986\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 4.893175\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 4.928846\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 5.433823\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 5.313181\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 6.013550\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 6.180430\n",
      "Epoch: 635/3000... Step: 20300... Loss: 2.260491... Val Loss: 6.639340\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 4.569646\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 4.567843\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 3.765214\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 3.421928\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 3.251927\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 4.488200\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 4.268847\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 4.214759\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 4.103589\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 3.916960\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 4.032941\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 4.593459\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 4.476476\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 5.201990\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 5.176688\n",
      "Epoch: 638/3000... Step: 20400... Loss: 2.231649... Val Loss: 5.154305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 4.626427\n",
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 4.236348\n",
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 3.299576\n",
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 3.199243\n",
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 3.073950\n",
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 3.606215\n",
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 3.477373\n",
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 3.657220\n",
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 3.587192\n",
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 3.393099\n",
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 3.462639\n",
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 4.007966\n",
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 3.919594\n",
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 4.579232\n",
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 4.632960\n",
      "Epoch: 641/3000... Step: 20500... Loss: 1.182434... Val Loss: 4.628105\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 5.316901\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 4.738828\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 4.231626\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 4.079555\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 3.873733\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 4.465209\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 4.284801\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 4.331829\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 4.313095\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 4.158737\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 4.335525\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 4.839835\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 4.809342\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 5.448605\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 5.433769\n",
      "Epoch: 644/3000... Step: 20600... Loss: 4.140034... Val Loss: 5.449563\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 4.779617\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 4.169697\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 3.474699\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 3.437981\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 3.212948\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 3.647519\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 3.466480\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 3.598853\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 3.539206\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 3.453736\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 3.622066\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 4.113342\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 4.088256\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 4.727892\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 4.765889\n",
      "Epoch: 647/3000... Step: 20700... Loss: 5.147163... Val Loss: 4.715329\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 9.017329\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 9.447853\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 8.215007\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 7.860595\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 7.835720\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 8.451697\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 8.336491\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 8.152401\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 8.132962\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 8.113637\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 8.105551\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 8.805411\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 8.692063\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 9.532222\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 9.565517\n",
      "Epoch: 650/3000... Step: 20800... Loss: 9.242499... Val Loss: 9.479180\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 4.281672\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 3.988191\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 3.435563\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 3.220119\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 3.012888\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 3.683370\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 3.716672\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 3.744669\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 3.627770\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 3.474672\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 3.488029\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 3.861277\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 3.790284\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 4.475487\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 4.543720\n",
      "Epoch: 654/3000... Step: 20900... Loss: 2.688514... Val Loss: 4.474081\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 4.659468\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 4.158493\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 3.474926\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 3.584854\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 3.484254\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 4.711163\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 4.475577\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 4.501101\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 4.362975\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 4.143260\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 4.218608\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 4.575837\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 4.465595\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 5.077864\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 5.026805\n",
      "Epoch: 657/3000... Step: 21000... Loss: 1.835868... Val Loss: 5.026631\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 4.467299\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 4.403924\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 3.680337\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 3.359686\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 3.100099\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 5.520438\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 5.047990\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 4.925183\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 4.686615\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 4.476564\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 4.289538\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 4.727106\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 4.548027\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 5.240690\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 5.234831\n",
      "Epoch: 660/3000... Step: 21100... Loss: 1.863572... Val Loss: 5.228031\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 5.892867\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 5.023665\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 4.216515\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 4.181468\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 4.714602\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 5.361355\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 5.058645\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 5.326140\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 5.143238\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 4.868848\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 4.840558\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 5.239552\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 5.161012\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 5.756797\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 5.862769\n",
      "Epoch: 663/3000... Step: 21200... Loss: 4.229949... Val Loss: 6.033422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 5.136195\n",
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 4.488346\n",
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 3.911971\n",
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 3.702148\n",
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 3.492985\n",
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 4.295704\n",
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 4.069698\n",
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 4.129624\n",
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 4.018849\n",
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 3.858951\n",
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 3.771305\n",
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 4.114155\n",
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 4.059292\n",
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 4.718146\n",
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 4.776485\n",
      "Epoch: 666/3000... Step: 21300... Loss: 1.643233... Val Loss: 5.014047\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 4.735490\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 4.454220\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 3.413736\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 3.129775\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 3.435405\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 3.848781\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 3.685563\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 3.724296\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 3.641051\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 3.526484\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 3.599120\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 4.078446\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 3.974672\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 4.654025\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 4.723839\n",
      "Epoch: 669/3000... Step: 21400... Loss: 3.473645... Val Loss: 4.718584\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 4.069776\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 3.879117\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 3.024577\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 2.741189\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 2.585393\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 3.343147\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 3.158477\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 3.102803\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 3.017996\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 2.978796\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 2.924815\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 3.387265\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 3.320868\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 4.100398\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 4.088103\n",
      "Epoch: 672/3000... Step: 21500... Loss: 4.799981... Val Loss: 4.103148\n",
      "Validation loss decreased (4.217162 --> 4.103148).  Saving model ...\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 4.039834\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 3.591649\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 3.033223\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 2.836702\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 2.614032\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 3.455902\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 3.223479\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 3.168478\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 3.094558\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 3.020462\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 2.954280\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 3.388302\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 3.342855\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 4.090815\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 4.126575\n",
      "Epoch: 675/3000... Step: 21600... Loss: 1.153372... Val Loss: 4.135549\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 5.804227\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 5.773250\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 5.672111\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 5.055421\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 5.176526\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 5.769346\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 5.568296\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 5.435376\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 5.288010\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 5.237605\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 5.123558\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 5.655348\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 5.511170\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 6.311770\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 6.299541\n",
      "Epoch: 679/3000... Step: 21700... Loss: 7.175124... Val Loss: 6.428291\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 4.393104\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 4.022124\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 4.334339\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 3.933058\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 3.645308\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 4.057199\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 3.797877\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 3.741123\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 3.628683\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 3.559190\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 3.483496\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 3.975978\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 3.921307\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 4.625669\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 4.552778\n",
      "Epoch: 682/3000... Step: 21800... Loss: 1.351628... Val Loss: 4.998050\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 5.903005\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 5.296215\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 4.625018\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 4.768845\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 6.072989\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 6.162549\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 5.856449\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 6.139888\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 5.873150\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 5.583682\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 5.409258\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 5.556146\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 5.510865\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 6.088213\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 6.475220\n",
      "Epoch: 685/3000... Step: 21900... Loss: 2.783029... Val Loss: 7.115950\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 4.665275\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 4.572773\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 3.908357\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 3.525246\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 3.307924\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 4.001403\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 3.773186\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 3.730545\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 3.634294\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 3.596361\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 3.708184\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 4.237029\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 4.137521\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 4.917836\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 4.868560\n",
      "Epoch: 688/3000... Step: 22000... Loss: 1.860069... Val Loss: 4.858461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 7.343120\n",
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 7.940142\n",
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 7.145965\n",
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 6.839013\n",
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 6.748159\n",
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 7.306397\n",
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 7.135495\n",
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 7.002312\n",
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 6.991691\n",
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 6.859587\n",
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 7.356901\n",
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 7.698912\n",
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 7.578790\n",
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 8.369250\n",
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 8.373463\n",
      "Epoch: 691/3000... Step: 22100... Loss: 4.140352... Val Loss: 8.172775\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 3.982474\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 3.538497\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 2.937841\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 2.888023\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 3.417579\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 4.082096\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 3.824277\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 3.893379\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 3.751483\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 3.592545\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 3.524676\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 3.830070\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 3.770628\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 4.444903\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 4.760137\n",
      "Epoch: 694/3000... Step: 22200... Loss: 2.761019... Val Loss: 4.831620\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 4.870461\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 4.265576\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 3.773489\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 3.548591\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 3.535860\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 3.979213\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 3.794308\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 3.890488\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 3.744944\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 3.595733\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 3.475318\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 3.803565\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 3.709844\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 4.338135\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 4.589039\n",
      "Epoch: 697/3000... Step: 22300... Loss: 10.543425... Val Loss: 4.609031\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 4.887324\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 4.244845\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 3.967096\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 3.710792\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 3.619841\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 4.387345\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 4.100860\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 4.000838\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 3.918928\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 3.895651\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 3.874364\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 4.460676\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 4.370728\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 5.093637\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 4.982375\n",
      "Epoch: 700/3000... Step: 22400... Loss: 3.273964... Val Loss: 4.932605\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 4.236950\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 3.878969\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 3.468627\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 3.100142\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 2.886186\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 3.726709\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 3.451205\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 3.371767\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 3.265110\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 3.213757\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 3.103175\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 3.436002\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 3.355709\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 4.056058\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 4.009796\n",
      "Epoch: 704/3000... Step: 22500... Loss: 2.180197... Val Loss: 4.073434\n",
      "Validation loss decreased (4.103148 --> 4.073434).  Saving model ...\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 5.638738\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 5.194690\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 4.095986\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 3.754544\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 4.679426\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 5.626397\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 5.257980\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 5.567501\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 5.319248\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 5.021795\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 4.896845\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 5.211383\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 4.989782\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 5.590872\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 5.849032\n",
      "Epoch: 707/3000... Step: 22600... Loss: 1.665465... Val Loss: 6.018568\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 4.185933\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 3.822680\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 3.466536\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 3.382766\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 3.410618\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 4.456072\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 4.152158\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 4.200298\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 4.066120\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 3.953115\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 3.953817\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 4.408113\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 4.325687\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 4.990225\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 4.957320\n",
      "Epoch: 710/3000... Step: 22700... Loss: 1.096189... Val Loss: 4.871368\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 5.487551\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 5.035182\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 4.209777\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 4.308118\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 4.606508\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 5.232968\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 5.116365\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 5.223894\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 5.109645\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 4.886047\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 4.911244\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 5.344502\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 5.172572\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 5.822708\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 5.904249\n",
      "Epoch: 713/3000... Step: 22800... Loss: 3.082843... Val Loss: 5.716247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 4.251698\n",
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 4.192518\n",
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 3.561999\n",
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 3.307132\n",
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 3.937794\n",
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 4.827324\n",
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 4.580740\n",
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 4.644536\n",
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 4.485551\n",
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 4.229932\n",
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 4.060652\n",
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 4.469028\n",
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 4.355073\n",
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 4.991921\n",
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 5.054274\n",
      "Epoch: 716/3000... Step: 22900... Loss: 1.052248... Val Loss: 4.960371\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 4.120190\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 4.037953\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 3.271191\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 3.039669\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 2.987382\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 3.853489\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 3.616302\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 3.557795\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 3.451577\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 3.373085\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 3.282266\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 3.669648\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 3.586997\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 4.348531\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 4.305708\n",
      "Epoch: 719/3000... Step: 23000... Loss: 3.194694... Val Loss: 4.335122\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 7.034167\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 5.959949\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 5.614895\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 5.656584\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 7.083175\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 7.490853\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 7.180547\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 7.292237\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 7.106981\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 6.926852\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 6.862010\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 7.142216\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 7.065204\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 7.600394\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 7.840566\n",
      "Epoch: 722/3000... Step: 23100... Loss: 6.006367... Val Loss: 8.687943\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 4.276427\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 3.498212\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 2.728017\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 2.812208\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 4.288144\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 4.724978\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 4.394323\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 4.418285\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 4.208555\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 3.989654\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 3.808050\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 4.119538\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 4.024957\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 4.617492\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 4.688165\n",
      "Epoch: 725/3000... Step: 23200... Loss: 0.851866... Val Loss: 5.386279\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 5.404776\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 4.289695\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 3.863879\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 3.521590\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 4.075454\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 4.704766\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 4.419230\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 4.388598\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 4.267594\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 4.065744\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 3.849116\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 4.389800\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 4.292488\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 4.937485\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 4.972245\n",
      "Epoch: 729/3000... Step: 23300... Loss: 1.644398... Val Loss: 5.017419\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 3.491135\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 3.130207\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 2.775499\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 2.707895\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 2.760795\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 3.114974\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 2.994367\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 3.047263\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 2.978384\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 2.937908\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 3.095568\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 3.385199\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 3.333625\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 4.007765\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 4.011957\n",
      "Epoch: 732/3000... Step: 23400... Loss: 0.823274... Val Loss: 4.346950\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 5.520919\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 4.376535\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 3.733202\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 3.584313\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 3.391995\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 3.736420\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 3.701880\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 4.003219\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 3.940505\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 3.739689\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 3.655491\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 4.003811\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 3.953660\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 4.559761\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 4.566112\n",
      "Epoch: 735/3000... Step: 23500... Loss: 1.598150... Val Loss: 4.434196\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 5.179778\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 4.679781\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 3.823902\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 3.574218\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 3.611219\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 4.188695\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 4.091482\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 4.267845\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 4.145507\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 3.990400\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 4.007091\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 4.294595\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 4.218209\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 4.889404\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 5.013580\n",
      "Epoch: 738/3000... Step: 23600... Loss: 1.929890... Val Loss: 4.922495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 4.145133\n",
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 3.409476\n",
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 2.973602\n",
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 2.835518\n",
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 2.724011\n",
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 3.128350\n",
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 3.001866\n",
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 3.167807\n",
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 3.114699\n",
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 3.145288\n",
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 3.451045\n",
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 3.831999\n",
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 3.775325\n",
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 4.408182\n",
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 4.401927\n",
      "Epoch: 741/3000... Step: 23700... Loss: 0.897577... Val Loss: 4.391051\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 5.623857\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 4.339591\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 4.501223\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 4.102908\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 3.986996\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 4.158534\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 3.999539\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 4.213985\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 4.051924\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 3.922820\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 3.784340\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 4.149626\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 4.035392\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 4.620878\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 4.534581\n",
      "Epoch: 744/3000... Step: 23800... Loss: 3.599923... Val Loss: 4.804276\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 4.439632\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 3.701923\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 3.433233\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 3.471307\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 3.531809\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 3.992603\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 3.793256\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 3.933830\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 3.780177\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 3.601346\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 3.429363\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 3.694091\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 3.598971\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 4.233693\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 4.289693\n",
      "Epoch: 747/3000... Step: 23900... Loss: 5.790412... Val Loss: 4.393534\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 4.078343\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 3.618301\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 3.479890\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 3.229297\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 3.520863\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 4.650621\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 4.259111\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 4.217251\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 4.047719\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 3.941126\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 3.797419\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 4.118082\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 3.983879\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 4.684301\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 4.568831\n",
      "Epoch: 750/3000... Step: 24000... Loss: 2.053016... Val Loss: 4.678702\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 3.808115\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 3.397259\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 3.012842\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 2.794076\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 2.821300\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 3.256849\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 3.047431\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 3.067627\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 2.997384\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 2.972422\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 2.834761\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 3.152470\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 3.080268\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 3.810349\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 3.783039\n",
      "Epoch: 754/3000... Step: 24100... Loss: 1.270574... Val Loss: 4.122705\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 8.428400\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 8.618940\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 7.359356\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 6.709156\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 6.827938\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 7.338032\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 7.076195\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 6.928518\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 6.826788\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 7.070808\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 7.152617\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 7.606666\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 7.494097\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 8.529426\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 8.367686\n",
      "Epoch: 757/3000... Step: 24200... Loss: 8.366095... Val Loss: 8.307839\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 4.041582\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 3.525658\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 2.777026\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 2.603794\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 2.504176\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 3.035005\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 2.865130\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 2.951362\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 2.937217\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 2.903928\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 2.808748\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 3.107049\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 3.036099\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 3.752582\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 3.780250\n",
      "Epoch: 760/3000... Step: 24300... Loss: 0.631927... Val Loss: 3.729037\n",
      "Validation loss decreased (4.073434 --> 3.729037).  Saving model ...\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 3.698877\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 3.681012\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 3.006122\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 2.954684\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 2.884711\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 3.402261\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 3.247799\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 3.228659\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 3.157166\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 3.137714\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 3.338009\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 3.649032\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 3.543793\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 4.324325\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 4.285171\n",
      "Epoch: 763/3000... Step: 24400... Loss: 2.276761... Val Loss: 4.264195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 3.730249\n",
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 3.240796\n",
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 2.545358\n",
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 2.441784\n",
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 2.914444\n",
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 3.157913\n",
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 2.972727\n",
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 2.987531\n",
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 2.916150\n",
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 2.922485\n",
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 3.252682\n",
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 3.616591\n",
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 3.539480\n",
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 4.199951\n",
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 4.272450\n",
      "Epoch: 766/3000... Step: 24500... Loss: 0.912382... Val Loss: 4.539834\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 4.137331\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 3.375764\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 2.744069\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 2.697649\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 2.480986\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 2.923984\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 2.803521\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 2.958065\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 2.908072\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 2.840039\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 2.746717\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 3.054695\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 2.973753\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 3.707340\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 3.674719\n",
      "Epoch: 769/3000... Step: 24600... Loss: 2.745506... Val Loss: 3.704203\n",
      "Validation loss decreased (3.729037 --> 3.704203).  Saving model ...\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 4.778992\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 4.468913\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 3.571271\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 3.310139\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 3.275063\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 3.701297\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 3.555539\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 3.666318\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 3.583874\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 3.525680\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 3.504777\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 3.764007\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 3.653899\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 4.401903\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 4.481105\n",
      "Epoch: 772/3000... Step: 24700... Loss: 5.544403... Val Loss: 4.409816\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 3.853279\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 3.289161\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 2.867683\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 2.697138\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 2.723061\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 3.129556\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 2.963822\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 3.045603\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 2.964991\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 2.918358\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 2.912116\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 3.236881\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 3.190643\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 3.926577\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 3.851722\n",
      "Epoch: 775/3000... Step: 24800... Loss: 0.623916... Val Loss: 3.828959\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 5.260122\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 4.162047\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 3.396479\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 3.362015\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 3.229260\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 4.036720\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 4.213783\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 4.495926\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 4.390636\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 4.466208\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 4.846155\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 5.000932\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 4.864057\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 5.468552\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 5.509002\n",
      "Epoch: 779/3000... Step: 24900... Loss: 6.785086... Val Loss: 5.350508\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 3.953484\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 3.512328\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 3.105417\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 2.955897\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 2.802966\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 3.292283\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 3.069437\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 3.094350\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 2.988165\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 2.940943\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 2.812349\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 3.079498\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 2.977890\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 3.719765\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 3.791679\n",
      "Epoch: 782/3000... Step: 25000... Loss: 0.669714... Val Loss: 3.844315\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 4.928596\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 4.580784\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 3.603496\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 3.330625\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 3.396082\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 5.794642\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 5.342706\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 5.244493\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 4.987604\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 4.796426\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 4.656780\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 4.804154\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 4.611791\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 5.282127\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 5.289295\n",
      "Epoch: 785/3000... Step: 25100... Loss: 0.872090... Val Loss: 5.147670\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 4.621211\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 4.304666\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 3.446285\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 3.077792\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 2.906655\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 3.425075\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 3.282148\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 3.296511\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 3.201016\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 3.287541\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 3.417840\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 3.828081\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 3.705620\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 4.510282\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 4.443985\n",
      "Epoch: 788/3000... Step: 25200... Loss: 1.465223... Val Loss: 4.362059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 4.004699\n",
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 3.738046\n",
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 3.094370\n",
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 3.135555\n",
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 2.930807\n",
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 3.352935\n",
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 3.173898\n",
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 3.351259\n",
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 3.221924\n",
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 3.109911\n",
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 2.985695\n",
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 3.112008\n",
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 3.054384\n",
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 3.783210\n",
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 3.959524\n",
      "Epoch: 791/3000... Step: 25300... Loss: 0.945712... Val Loss: 4.187721\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 4.083666\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 3.451680\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 2.838617\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 2.644021\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 2.837973\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 3.268827\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 3.062760\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 3.037127\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 2.956137\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 2.999125\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 2.904835\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 3.284523\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 3.192606\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 3.957152\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 3.851550\n",
      "Epoch: 794/3000... Step: 25400... Loss: 2.830262... Val Loss: 3.823346\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 3.753719\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 3.211622\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 2.822249\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 2.693484\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 2.569590\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 3.197143\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 2.954137\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 2.975937\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 2.871543\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 2.843617\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 2.730068\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 3.053380\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 2.973979\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 3.778030\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 3.718993\n",
      "Epoch: 797/3000... Step: 25500... Loss: 4.570295... Val Loss: 3.711105\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 5.680770\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 4.793712\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 4.442652\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 4.522152\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 4.305775\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 4.771272\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 4.645604\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 4.956392\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 4.909712\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 4.810207\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 4.892347\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 5.197007\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 5.131980\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 5.758603\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 5.752297\n",
      "Epoch: 800/3000... Step: 25600... Loss: 1.564345... Val Loss: 5.744774\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 3.760476\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 3.375334\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 2.738737\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 2.564725\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 2.406490\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 2.981599\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 2.796072\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 2.816581\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 2.751563\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 2.771089\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 2.833761\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 3.114325\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 3.016939\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 3.766008\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 3.681188\n",
      "Epoch: 804/3000... Step: 25700... Loss: 1.384289... Val Loss: 3.599200\n",
      "Validation loss decreased (3.704203 --> 3.599200).  Saving model ...\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 4.039486\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 3.348489\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 2.708333\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 2.705989\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 2.457594\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 2.856131\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 2.698570\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 2.815698\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 2.773427\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 2.815007\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 2.815595\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 3.254458\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 3.163661\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 3.899532\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 3.807302\n",
      "Epoch: 807/3000... Step: 25800... Loss: 1.963080... Val Loss: 3.771808\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 4.071262\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 3.352462\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 2.642732\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 2.537981\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 2.412936\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 2.935716\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 2.793171\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 2.973600\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 2.885071\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 2.855595\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 3.064367\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 3.272337\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 3.224814\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 3.910971\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 3.858823\n",
      "Epoch: 810/3000... Step: 25900... Loss: 0.538553... Val Loss: 3.813123\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 4.817426\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 3.753872\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 3.114335\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 3.352027\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 3.376634\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 3.774892\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 3.696141\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 4.206021\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 4.087416\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 3.784270\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 3.605122\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 3.763858\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 3.728038\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 4.331938\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 4.435333\n",
      "Epoch: 813/3000... Step: 26000... Loss: 1.987820... Val Loss: 4.317474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 5.710237\n",
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 4.670889\n",
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 4.214476\n",
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 4.313137\n",
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 4.350143\n",
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 4.642514\n",
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 4.530198\n",
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 4.838873\n",
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 4.810096\n",
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 4.697273\n",
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 4.595512\n",
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 4.886378\n",
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 4.871297\n",
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 5.478656\n",
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 5.435596\n",
      "Epoch: 816/3000... Step: 26100... Loss: 1.368152... Val Loss: 5.424366\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 3.872125\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 3.434360\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 2.800686\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 2.706146\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 2.451901\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 2.985009\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 2.841148\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 3.044390\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 2.964099\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 3.014936\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 2.977646\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 3.328264\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 3.195888\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 3.927720\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 3.866379\n",
      "Epoch: 819/3000... Step: 26200... Loss: 2.286361... Val Loss: 3.819017\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 6.264731\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 6.252910\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 5.622823\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 5.384956\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 5.465416\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 5.849615\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 5.763589\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 5.854361\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 5.738323\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 5.714289\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 5.513756\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 5.787471\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 5.658118\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 6.393797\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 6.372266\n",
      "Epoch: 822/3000... Step: 26300... Loss: 3.822788... Val Loss: 6.278113\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 4.075849\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 3.911278\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 3.140077\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 2.942009\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 2.767029\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 3.436366\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 3.249874\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 3.315079\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 3.205278\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 3.221374\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 3.204555\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 3.521898\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 3.400646\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 4.169205\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 4.111909\n",
      "Epoch: 825/3000... Step: 26400... Loss: 1.612889... Val Loss: 4.002131\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 4.623889\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 3.907963\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 3.226916\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 3.138254\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 3.057021\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 3.536815\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 3.440920\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 3.776128\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 3.659217\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 3.573206\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 3.562647\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 3.771715\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 3.646808\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 4.289771\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 4.335484\n",
      "Epoch: 829/3000... Step: 26500... Loss: 2.798011... Val Loss: 4.388612\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 3.488976\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 3.153372\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 2.582088\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 2.589930\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 2.429635\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 2.848893\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 2.723296\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 2.790114\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 2.739043\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 2.774140\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 2.642557\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 2.864992\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 2.805600\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 3.575540\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 3.512700\n",
      "Epoch: 832/3000... Step: 26600... Loss: 0.751854... Val Loss: 3.462658\n",
      "Validation loss decreased (3.599200 --> 3.462658).  Saving model ...\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 6.133709\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 5.278379\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 4.701317\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 4.917343\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 4.789003\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 5.543851\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 5.560642\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 6.123158\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 5.991396\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 5.804283\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 5.905362\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 6.115425\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 6.025129\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 6.626930\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 6.607487\n",
      "Epoch: 835/3000... Step: 26700... Loss: 1.985226... Val Loss: 6.527882\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 4.673164\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 4.221771\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 3.249062\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 3.003714\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 3.228363\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 4.269608\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 3.947695\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 3.892855\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 3.721067\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 3.711477\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 3.964021\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 4.185066\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 4.024037\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 4.885388\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 4.849321\n",
      "Epoch: 838/3000... Step: 26800... Loss: 2.317384... Val Loss: 4.726289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 3.834526\n",
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 3.265783\n",
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 2.649586\n",
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 2.845370\n",
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 2.747836\n",
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 3.168408\n",
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 3.062522\n",
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 3.183609\n",
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 3.121993\n",
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 3.054035\n",
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 3.018131\n",
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 3.285016\n",
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 3.188172\n",
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 3.890548\n",
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 3.891894\n",
      "Epoch: 841/3000... Step: 26900... Loss: 0.972732... Val Loss: 4.174184\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 3.877779\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 3.340509\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 3.819192\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 3.507390\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 3.736922\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 3.910452\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 3.582190\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 3.553033\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 3.442458\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 3.481901\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 3.305748\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 3.571694\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 3.474311\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 4.264800\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 4.203573\n",
      "Epoch: 844/3000... Step: 27000... Loss: 3.226894... Val Loss: 4.644483\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 3.560344\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 3.335978\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 2.710135\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 2.668972\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 2.460947\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 2.822029\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 2.634983\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 2.613010\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 2.558469\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 2.669361\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 2.587255\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 2.805062\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 2.756592\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 3.610364\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 3.552859\n",
      "Epoch: 847/3000... Step: 27100... Loss: 7.181094... Val Loss: 3.483116\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 7.403397\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 5.678032\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 5.122027\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 5.250613\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 7.100917\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 7.098796\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 6.907409\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 7.253448\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 7.037350\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 6.765316\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 6.667480\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 6.837172\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 6.744994\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 7.262568\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 7.460527\n",
      "Epoch: 850/3000... Step: 27200... Loss: 1.484688... Val Loss: 8.944156\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 4.356436\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 4.167157\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 3.459061\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 3.343556\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 3.426509\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 3.812657\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 3.623680\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 3.690523\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 3.556250\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 3.589361\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 3.465278\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 3.642696\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 3.532510\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 4.339768\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 4.274416\n",
      "Epoch: 854/3000... Step: 27300... Loss: 2.730576... Val Loss: 4.195120\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 4.421432\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 4.127348\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 3.509625\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 3.343541\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 3.461237\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 3.969213\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 3.780707\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 3.860370\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 3.742951\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 3.746436\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 3.680894\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 3.825525\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 3.700516\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 4.477378\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 4.338292\n",
      "Epoch: 857/3000... Step: 27400... Loss: 5.787321... Val Loss: 4.363655\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 3.710840\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 3.182171\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 2.649257\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 2.700235\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 2.481552\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 2.818766\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 2.665374\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 2.801174\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 2.763403\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 2.692550\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 2.737414\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 2.937754\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 2.892575\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 3.678168\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 3.631460\n",
      "Epoch: 860/3000... Step: 27500... Loss: 0.768712... Val Loss: 3.623614\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 4.285225\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 3.925031\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 3.275571\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 3.025289\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 2.984556\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 3.312919\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 3.114816\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 3.210539\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 3.108644\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 3.174188\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 3.231762\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 3.392177\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 3.292010\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 4.108562\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 4.013044\n",
      "Epoch: 863/3000... Step: 27600... Loss: 2.270657... Val Loss: 4.044799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 3.632935\n",
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 3.201901\n",
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 2.770540\n",
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 2.820922\n",
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 2.865365\n",
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 3.121687\n",
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 2.977344\n",
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 3.059885\n",
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 2.979227\n",
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 2.880925\n",
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 2.959514\n",
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 3.189961\n",
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 3.119468\n",
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 3.892436\n",
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 3.893644\n",
      "Epoch: 866/3000... Step: 27700... Loss: 0.700839... Val Loss: 3.861062\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 4.467128\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 4.259250\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 3.572408\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 3.397361\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 3.239170\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 3.622104\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 3.463023\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 3.486610\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 3.424140\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 3.497556\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 3.480158\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 3.819168\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 3.704214\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 4.543880\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 4.426196\n",
      "Epoch: 869/3000... Step: 27800... Loss: 4.299722... Val Loss: 4.357613\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 3.960970\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 4.045031\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 3.717706\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 3.252228\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 3.548751\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 3.999645\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 3.632204\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 3.488917\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 3.323700\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 3.455353\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 3.324236\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 3.634576\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 3.499449\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 4.437947\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 4.439060\n",
      "Epoch: 872/3000... Step: 27900... Loss: 4.793468... Val Loss: 5.056794\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 3.822570\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 3.298101\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 3.277815\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 2.948392\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 3.231706\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 3.585659\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 3.333407\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 3.372888\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 3.256575\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 3.295651\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 3.109176\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 3.334498\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 3.222999\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 3.978846\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 3.933648\n",
      "Epoch: 875/3000... Step: 28000... Loss: 0.672361... Val Loss: 4.498980\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 4.734082\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 3.700441\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 2.869483\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 2.752779\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 4.226426\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 4.980698\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 4.695307\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 4.912512\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 4.634855\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 4.420498\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 4.398108\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 4.411850\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 4.293559\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 4.881810\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 5.097429\n",
      "Epoch: 879/3000... Step: 28100... Loss: 1.512722... Val Loss: 6.125000\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 4.302938\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 3.727907\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 3.126776\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 2.999592\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 2.896495\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 3.389657\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 3.284653\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 3.408579\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 3.340800\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 3.316962\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 3.206130\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 3.519597\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 3.390232\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 4.123974\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 4.082315\n",
      "Epoch: 882/3000... Step: 28200... Loss: 0.802625... Val Loss: 4.087406\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 5.930122\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 4.417640\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 3.857416\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 4.021405\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 3.723597\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 4.198870\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 4.130925\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 4.885348\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 4.721811\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 4.456936\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 4.471029\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 4.495167\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 4.526650\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 5.138775\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 5.173416\n",
      "Epoch: 885/3000... Step: 28300... Loss: 0.890471... Val Loss: 5.033675\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 3.947677\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 3.298965\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 2.760763\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 2.707015\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 2.498130\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 2.807048\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 2.804766\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 3.125417\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 3.048924\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 2.950244\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 3.052520\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 3.196267\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 3.151262\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 3.800472\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 3.705622\n",
      "Epoch: 888/3000... Step: 28400... Loss: 1.351884... Val Loss: 3.681448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 3.775769\n",
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 3.126345\n",
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 2.684990\n",
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 2.828153\n",
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 2.677278\n",
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 3.025918\n",
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 2.836511\n",
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 2.957256\n",
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 2.929970\n",
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 2.959563\n",
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 3.154452\n",
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 3.372954\n",
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 3.295068\n",
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 4.020891\n",
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 3.988588\n",
      "Epoch: 891/3000... Step: 28500... Loss: 0.762576... Val Loss: 4.002719\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 4.290795\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 3.843136\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 3.052035\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 2.743400\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 2.457700\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 2.756569\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 2.626823\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 2.684938\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 2.628193\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 2.787679\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 2.839050\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 3.012422\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 2.905174\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 3.661750\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 3.624135\n",
      "Epoch: 894/3000... Step: 28600... Loss: 2.428895... Val Loss: 3.624747\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 3.531860\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 3.196010\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 2.975168\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 2.698243\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 2.558104\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 3.090389\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 2.817086\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 2.717681\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 2.663203\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 2.724221\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 2.584407\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 2.738349\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 2.672729\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 3.518393\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 3.456628\n",
      "Epoch: 897/3000... Step: 28700... Loss: 7.215194... Val Loss: 3.611547\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 5.265970\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 3.909466\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 3.380072\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 3.382625\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 3.208728\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 3.848839\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 3.689932\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 4.003357\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 3.901170\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 3.893914\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 3.759120\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 3.857518\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 3.773138\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 4.350164\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 4.236835\n",
      "Epoch: 900/3000... Step: 28800... Loss: 0.506486... Val Loss: 4.293066\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 3.712939\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 3.259156\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 3.415619\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 3.104570\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 3.206613\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 3.433582\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 3.156391\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 3.171359\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 3.057102\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 3.104554\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 2.989484\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 3.066617\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 2.977866\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 3.736167\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 3.821957\n",
      "Epoch: 904/3000... Step: 28900... Loss: 1.846174... Val Loss: 4.289201\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 4.121242\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 3.408344\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 2.617084\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 2.384074\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 2.682776\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 2.949591\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 2.763768\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 2.937959\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 2.834762\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 2.909716\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 3.397393\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 3.520208\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 3.368069\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 4.074083\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 4.058887\n",
      "Epoch: 907/3000... Step: 29000... Loss: 1.674850... Val Loss: 4.236314\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 3.884870\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 3.185141\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 2.549893\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 2.544126\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 2.603863\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 3.056885\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 2.903577\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 3.029942\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 2.930297\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 2.903090\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 2.890202\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 3.018112\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 2.968276\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 3.728183\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 3.657518\n",
      "Epoch: 910/3000... Step: 29100... Loss: 0.468957... Val Loss: 3.654387\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 4.057494\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 3.602671\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 2.861045\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 2.753609\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 2.759070\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 3.431378\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 3.268511\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 3.322067\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 3.215336\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 3.159204\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 3.046873\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 3.190424\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 3.087741\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 3.902750\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 3.811040\n",
      "Epoch: 913/3000... Step: 29200... Loss: 2.266973... Val Loss: 3.750823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 3.860122\n",
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 3.200154\n",
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 2.671402\n",
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 2.639239\n",
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 2.772100\n",
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 3.126320\n",
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 2.966044\n",
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 3.210163\n",
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 3.139776\n",
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 3.056229\n",
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 2.970253\n",
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 3.079855\n",
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 2.996745\n",
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 3.709460\n",
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 3.629389\n",
      "Epoch: 916/3000... Step: 29300... Loss: 0.801148... Val Loss: 3.665099\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 3.710628\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 3.171005\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 2.595098\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 2.625458\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 2.441493\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 2.809392\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 2.683679\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 2.792421\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 2.742402\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 2.751368\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 2.859517\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 3.023477\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 2.946408\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 3.711914\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 3.628692\n",
      "Epoch: 919/3000... Step: 29400... Loss: 3.077794... Val Loss: 3.564839\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 4.220066\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 3.565387\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 2.883735\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 2.696416\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 2.485648\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 3.013056\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 2.868930\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 2.851917\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 2.780255\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 2.956720\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 2.942307\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 3.178310\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 3.057022\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 3.833150\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 3.705753\n",
      "Epoch: 922/3000... Step: 29500... Loss: 3.589374... Val Loss: 3.694814\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 3.829538\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 3.348538\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 2.835970\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 2.704645\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 2.453936\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 3.029288\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 2.866949\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 3.033917\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 2.957541\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 2.957221\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 2.965464\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 3.260111\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 3.140935\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 3.855835\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 3.732513\n",
      "Epoch: 925/3000... Step: 29600... Loss: 1.227910... Val Loss: 3.863301\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 6.261227\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 6.107071\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 4.990914\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 4.529057\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 4.516025\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 5.047576\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 4.891085\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 4.840749\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 4.728908\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 4.983289\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 4.783920\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 4.889028\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 4.756833\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 5.661121\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 5.596657\n",
      "Epoch: 929/3000... Step: 29700... Loss: 4.799774... Val Loss: 5.495588\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 3.998253\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 3.959953\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 3.340747\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 3.170711\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 2.923365\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 3.422673\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 3.265524\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 3.248449\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 3.189486\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 3.206160\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 3.279663\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 3.662647\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 3.525981\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 4.391738\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 4.248203\n",
      "Epoch: 932/3000... Step: 29800... Loss: 0.847535... Val Loss: 4.202759\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 4.814148\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 3.410121\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 2.989262\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 3.035697\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 2.780124\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 3.446163\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 3.300377\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 3.719327\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 3.582618\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 3.405526\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 3.540075\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 3.734575\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 3.753862\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 4.343820\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 4.792054\n",
      "Epoch: 935/3000... Step: 29900... Loss: 0.769867... Val Loss: 4.729843\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 3.655637\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 3.304687\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 2.834579\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 2.669841\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 2.436997\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 3.092135\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 2.829748\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 2.761246\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 2.715954\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 2.801477\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 2.840414\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 3.110624\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 3.000743\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 3.897042\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 3.807364\n",
      "Epoch: 938/3000... Step: 30000... Loss: 0.452034... Val Loss: 3.739924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 3.586973\n",
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 3.514019\n",
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 3.330453\n",
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 3.067541\n",
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 3.252151\n",
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 3.878094\n",
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 3.533562\n",
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 3.340083\n",
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 3.246453\n",
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 3.366965\n",
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 3.212655\n",
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 3.442226\n",
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 3.347204\n",
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 4.149710\n",
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 4.124541\n",
      "Epoch: 941/3000... Step: 30100... Loss: 3.568812... Val Loss: 4.309665\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 5.014101\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 3.797773\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 3.094956\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 3.147149\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 3.334733\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 4.005303\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 3.869448\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 4.247399\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 4.132937\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 3.940378\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 3.846902\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 4.080880\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 3.997237\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 4.575040\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 4.660892\n",
      "Epoch: 944/3000... Step: 30200... Loss: 3.827534... Val Loss: 4.944940\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 3.616266\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 3.159428\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 2.706013\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 2.586579\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 2.731890\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 3.270692\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 2.958073\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 2.940950\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 2.844746\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 2.857481\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 2.804682\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 3.158114\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 3.038797\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 3.925813\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 3.809016\n",
      "Epoch: 947/3000... Step: 30300... Loss: 5.516842... Val Loss: 3.813426\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 5.434260\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 4.788161\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 4.321008\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 4.256633\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 4.284601\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 5.129880\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 5.099524\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 5.092985\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 5.012450\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 5.113729\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 5.136773\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 5.285264\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 5.227640\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 6.138179\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 5.955757\n",
      "Epoch: 950/3000... Step: 30400... Loss: 1.748927... Val Loss: 5.898404\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 3.983224\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 3.389736\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 2.952827\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 3.000595\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 3.054136\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 3.471040\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 3.221551\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 3.421485\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 3.303652\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 3.284508\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 3.154181\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 3.346263\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 3.232859\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 3.899109\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 3.832373\n",
      "Epoch: 954/3000... Step: 30500... Loss: 0.819692... Val Loss: 3.792234\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 4.847248\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 3.658867\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 2.944804\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 2.605420\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 2.393658\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 3.280075\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 3.092857\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 3.448839\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 3.343205\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 3.359474\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 3.312480\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 3.326940\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 3.207020\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 3.977402\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 3.886725\n",
      "Epoch: 957/3000... Step: 30600... Loss: 1.635330... Val Loss: 3.785863\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 3.471634\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 3.015453\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 2.495248\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 2.404134\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 2.184955\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 2.878284\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 2.667532\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 2.666263\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 2.636978\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 2.755757\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 2.875543\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 3.146731\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 3.049530\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 3.821469\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 3.662928\n",
      "Epoch: 960/3000... Step: 30700... Loss: 0.646419... Val Loss: 3.623459\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 3.545493\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 3.294070\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 2.633866\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 2.555140\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 2.497341\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 2.821851\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 2.701439\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 2.834905\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 2.768116\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 2.902818\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 2.844963\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 2.939961\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 2.862034\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 3.633604\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 3.534102\n",
      "Epoch: 963/3000... Step: 30800... Loss: 1.171235... Val Loss: 3.512400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 3.867687\n",
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 3.076770\n",
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 2.542412\n",
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 2.555175\n",
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 2.544799\n",
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 2.943721\n",
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 2.782298\n",
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 3.035001\n",
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 2.977607\n",
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 2.962498\n",
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 3.002467\n",
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 3.078389\n",
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 3.027160\n",
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 3.702199\n",
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 3.610332\n",
      "Epoch: 966/3000... Step: 30900... Loss: 0.805358... Val Loss: 3.600315\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 3.938782\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 3.410697\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 2.703823\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 2.484191\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 2.422797\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 2.975974\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 2.790232\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 2.902515\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 2.820976\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 2.889395\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 2.783234\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 2.945236\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 2.864705\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 3.639489\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 3.511621\n",
      "Epoch: 969/3000... Step: 31000... Loss: 3.039259... Val Loss: 3.595830\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 4.153247\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 3.795922\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 3.082117\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 2.811946\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 2.814358\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 3.145323\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 3.055897\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 3.189289\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 3.103349\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 3.315340\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 3.109063\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 3.318290\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 3.190917\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 3.942172\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 3.955739\n",
      "Epoch: 972/3000... Step: 31100... Loss: 4.271770... Val Loss: 4.270083\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 2.988535\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 2.963259\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 2.579830\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 2.486612\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 2.198137\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 3.035426\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 2.818814\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 2.756115\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 2.705772\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 2.759132\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 2.836634\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 2.923374\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 2.810892\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 3.614966\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 3.482962\n",
      "Epoch: 975/3000... Step: 31200... Loss: 1.853316... Val Loss: 3.477516\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 6.239237\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 5.087754\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 4.478317\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 4.408197\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 4.185339\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 4.483444\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 4.401441\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 4.628585\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 4.566281\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 4.754168\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 4.607366\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 4.783440\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 4.724494\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 5.367711\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 5.171978\n",
      "Epoch: 979/3000... Step: 31300... Loss: 1.740150... Val Loss: 5.187695\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 3.487937\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 3.128492\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 2.562853\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 2.379213\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 2.138613\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 2.715090\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 2.597196\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 2.724800\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 2.676544\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 2.712914\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 2.821342\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 3.051997\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 2.941762\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 3.690779\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 3.554264\n",
      "Epoch: 982/3000... Step: 31400... Loss: 0.699346... Val Loss: 3.585740\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 4.233651\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 3.567286\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 2.872175\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 2.880027\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 3.759687\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 4.164797\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 4.006763\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 4.300364\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 4.130522\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 3.934487\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 3.731863\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 3.744929\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 3.625784\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 4.325464\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 4.175284\n",
      "Epoch: 985/3000... Step: 31500... Loss: 0.636597... Val Loss: 4.337627\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 5.320572\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 4.384466\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 3.789054\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 3.615482\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 3.315733\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 3.710859\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 3.759335\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 4.041670\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 3.967574\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 4.146417\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 4.154670\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 4.206418\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 4.071752\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 4.665797\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 4.517548\n",
      "Epoch: 988/3000... Step: 31600... Loss: 0.988361... Val Loss: 4.485852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 4.753398\n",
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 4.954159\n",
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 4.429144\n",
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 4.152540\n",
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 4.073381\n",
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 6.712530\n",
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 6.294304\n",
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 6.047244\n",
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 5.823542\n",
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 5.790548\n",
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 5.631110\n",
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 5.628273\n",
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 5.410955\n",
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 6.116163\n",
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 5.948750\n",
      "Epoch: 991/3000... Step: 31700... Loss: 3.567371... Val Loss: 5.866600\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 3.429490\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 3.527002\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 3.111486\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 2.904383\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 2.903843\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 3.289020\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 3.102548\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 2.987439\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 2.909867\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 3.101143\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 2.926041\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 3.052325\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 2.972781\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 3.866314\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 3.794435\n",
      "Epoch: 994/3000... Step: 31800... Loss: 2.614592... Val Loss: 3.996641\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 3.417279\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 3.028651\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 2.550535\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 2.485579\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 2.312273\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 2.666063\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 2.581223\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 2.673015\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 2.625508\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 2.683079\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 2.751366\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 2.885850\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 2.826396\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 3.542774\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 3.457262\n",
      "Epoch: 997/3000... Step: 31900... Loss: 4.173325... Val Loss: 3.558846\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 4.671359\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 3.753091\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 3.063782\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 2.782746\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 3.323150\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 3.730376\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 3.461259\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 3.621907\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 3.487502\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 3.650921\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 3.407570\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 3.509745\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 3.369288\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 4.003825\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 3.833373\n",
      "Epoch: 1000/3000... Step: 32000... Loss: 1.631498... Val Loss: 3.827495\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 4.684595\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 3.521085\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 2.813335\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 2.573609\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 2.470664\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 2.845984\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 2.730317\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 2.998492\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 2.958750\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 3.066294\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 2.908810\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 3.077543\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 2.989257\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 3.682274\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 3.549502\n",
      "Epoch: 1004/3000... Step: 32100... Loss: 1.430055... Val Loss: 3.489701\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 4.688840\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 3.886540\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 3.278624\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 3.090113\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 3.730036\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 3.976032\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 3.809349\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 4.103694\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 3.970760\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 3.961773\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 3.725228\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 3.769605\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 3.618582\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 4.234528\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 4.089224\n",
      "Epoch: 1007/3000... Step: 32200... Loss: 2.137999... Val Loss: 4.109148\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 3.867323\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 3.478034\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 2.922901\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 2.793114\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 2.646838\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 3.510954\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 3.333368\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 3.438584\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 3.359100\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 3.320804\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 3.148805\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 3.187913\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 3.094162\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 3.784002\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 3.714112\n",
      "Epoch: 1010/3000... Step: 32300... Loss: 1.830830... Val Loss: 3.700781\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 4.141025\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 3.428687\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 2.844338\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 2.766240\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 2.504026\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 3.128801\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 2.979059\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 3.048078\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 2.961883\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 3.089307\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 3.048842\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 3.302774\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 3.225655\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 3.979593\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 3.854087\n",
      "Epoch: 1013/3000... Step: 32400... Loss: 0.627477... Val Loss: 3.848059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 4.377299\n",
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 4.240279\n",
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 3.572462\n",
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 3.288881\n",
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 3.025936\n",
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 3.592355\n",
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 3.449320\n",
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 3.507147\n",
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 3.451017\n",
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 3.572554\n",
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 3.560598\n",
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 3.617498\n",
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 3.519478\n",
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 4.278232\n",
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 4.154533\n",
      "Epoch: 1016/3000... Step: 32500... Loss: 2.085652... Val Loss: 4.147631\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 3.664356\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 3.073534\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 2.571289\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 2.605638\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 2.533898\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 2.930755\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 2.817287\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 2.886786\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 2.858978\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 3.074453\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 2.920126\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 3.053939\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 2.952518\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 3.702108\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 3.572548\n",
      "Epoch: 1019/3000... Step: 32600... Loss: 2.504344... Val Loss: 3.720809\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 4.424795\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 4.768366\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 3.785507\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 3.334495\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 3.456019\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 4.164192\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 3.821844\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 3.661154\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 3.578124\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 3.871351\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 3.878626\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 4.225977\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 4.120145\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 5.158270\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 5.139576\n",
      "Epoch: 1022/3000... Step: 32700... Loss: 4.736491... Val Loss: 5.198339\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 3.988084\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 3.216052\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 2.638686\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 2.552903\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 2.312467\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 3.091784\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 2.975427\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 3.138535\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 3.036733\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 3.047675\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 2.868030\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 2.923192\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 2.858100\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 3.525880\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 3.438602\n",
      "Epoch: 1025/3000... Step: 32800... Loss: 0.302768... Val Loss: 3.457197\n",
      "Validation loss decreased (3.462658 --> 3.457197).  Saving model ...\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 4.229916\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 3.509543\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 2.940461\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 2.816108\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 2.663295\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 3.270316\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 3.068731\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 3.359169\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 3.252658\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 3.214476\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 3.025778\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 3.131045\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 3.004686\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 3.697080\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 3.609185\n",
      "Epoch: 1029/3000... Step: 32900... Loss: 2.507924... Val Loss: 3.697630\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 3.698690\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 3.760258\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 3.103159\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 2.859994\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 2.713871\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 3.310203\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 3.253213\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 3.247304\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 3.218052\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 3.230630\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 3.340847\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 3.495616\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 3.399218\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 4.193408\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 4.069518\n",
      "Epoch: 1032/3000... Step: 33000... Loss: 0.805494... Val Loss: 4.064442\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 4.832016\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 3.825034\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 3.387107\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 3.322762\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 3.261817\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 4.024565\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 3.834744\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 4.045041\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 3.968905\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 3.960021\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 3.972540\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 4.185784\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 4.111881\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 4.774648\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 4.649321\n",
      "Epoch: 1035/3000... Step: 33100... Loss: 1.384311... Val Loss: 4.705732\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 4.428836\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 3.825665\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 3.135354\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 2.960953\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 2.776644\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 3.527944\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 3.497835\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 3.813085\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 3.714205\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 3.649187\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 3.605380\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 3.652183\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 3.557778\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 4.180980\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 4.058319\n",
      "Epoch: 1038/3000... Step: 33200... Loss: 2.115402... Val Loss: 4.053281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 3.665126\n",
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 3.048170\n",
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 2.526587\n",
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 2.440699\n",
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 2.274943\n",
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 2.591013\n",
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 2.569358\n",
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 2.801797\n",
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 2.763309\n",
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 2.770808\n",
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 2.922637\n",
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 2.950579\n",
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 2.897435\n",
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 3.558212\n",
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 3.472992\n",
      "Epoch: 1041/3000... Step: 33300... Loss: 1.281562... Val Loss: 3.563675\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 4.011884\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 3.463781\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 2.797189\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 2.609767\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 2.320292\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 2.965734\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 2.821999\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 2.990146\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 2.908252\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 2.996926\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 2.946846\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 2.970022\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 2.878873\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 3.575777\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 3.423853\n",
      "Epoch: 1044/3000... Step: 33400... Loss: 3.379820... Val Loss: 3.423118\n",
      "Validation loss decreased (3.457197 --> 3.423118).  Saving model ...\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 3.387013\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 3.123132\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 2.658440\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 2.565804\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 2.480239\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 2.985896\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 2.860920\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 2.985090\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 2.926454\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 2.978301\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 2.853874\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 2.828047\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 2.764818\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 3.538017\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 3.535596\n",
      "Epoch: 1047/3000... Step: 33500... Loss: 5.025726... Val Loss: 3.516707\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 8.970241\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 7.434077\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 6.875473\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 7.135324\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 6.991667\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 7.316160\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 7.274812\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 7.556724\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 7.529917\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 7.523133\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 7.380281\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 7.490084\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 7.477284\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 8.039583\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 7.821669\n",
      "Epoch: 1050/3000... Step: 33600... Loss: 0.742687... Val Loss: 7.791926\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 3.960257\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 3.178045\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 2.538405\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 2.421952\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 2.496078\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 2.823256\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 2.689276\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 2.839244\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 2.769217\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 2.789395\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 2.783679\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 2.936087\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 2.824686\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 3.588545\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 3.483292\n",
      "Epoch: 1054/3000... Step: 33700... Loss: 0.936324... Val Loss: 3.535285\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 6.878939\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 5.585919\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 4.858861\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 4.675295\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 5.932296\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 6.831320\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 6.357769\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 6.272721\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 6.081765\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 6.115843\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 6.100828\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 6.091569\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 5.944604\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 6.564299\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 6.346571\n",
      "Epoch: 1057/3000... Step: 33800... Loss: 4.230524... Val Loss: 6.881361\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 3.575390\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 3.073061\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 2.523906\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 2.431707\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 2.327127\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 2.795635\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 2.710056\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 2.879249\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 2.795519\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 2.834584\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 3.117402\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 3.132123\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 3.046954\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 3.692609\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 3.641442\n",
      "Epoch: 1060/3000... Step: 33900... Loss: 0.812950... Val Loss: 3.588153\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 5.396479\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 5.080620\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 4.418115\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 4.072814\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 3.889278\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 4.586146\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 4.482093\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 4.633711\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 4.556025\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 4.663209\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 4.517148\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 4.559207\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 4.428241\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 5.113226\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 5.014807\n",
      "Epoch: 1063/3000... Step: 34000... Loss: 2.744088... Val Loss: 4.967885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 4.497614\n",
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 3.515650\n",
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 2.881857\n",
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 2.903902\n",
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 2.653584\n",
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 3.013409\n",
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 2.990678\n",
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 3.223450\n",
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 3.181554\n",
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 3.173174\n",
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 3.000719\n",
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 3.055438\n",
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 3.015958\n",
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 3.689901\n",
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 3.563333\n",
      "Epoch: 1066/3000... Step: 34100... Loss: 0.594314... Val Loss: 3.562717\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 3.546542\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 3.067520\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 2.424127\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 2.424529\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 2.304345\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 2.752696\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 2.674288\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 2.700316\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 2.663724\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 2.769817\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 2.767505\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 2.810051\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 2.725209\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 3.527316\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 3.401898\n",
      "Epoch: 1069/3000... Step: 34200... Loss: 2.527633... Val Loss: 3.392821\n",
      "Validation loss decreased (3.423118 --> 3.392821).  Saving model ...\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 3.850022\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 3.118531\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 2.647052\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 2.508154\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 2.352535\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 2.857528\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 2.730323\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 2.854185\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 2.784909\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 2.829924\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 2.741244\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 2.755512\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 2.683661\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 3.371693\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 3.263079\n",
      "Epoch: 1072/3000... Step: 34300... Loss: 3.527932... Val Loss: 3.430757\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 4.553316\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 3.548342\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 3.064280\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 3.081889\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 2.776269\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 3.089433\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 3.027330\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 3.242573\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 3.231334\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 3.355554\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 3.250007\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 3.327415\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 3.253886\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 3.895034\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 3.742946\n",
      "Epoch: 1075/3000... Step: 34400... Loss: 0.445327... Val Loss: 3.722301\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 3.882617\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 3.268211\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 2.478982\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 2.317667\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 3.275500\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 3.840797\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 3.522645\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 3.726034\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 3.560776\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 3.563027\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 3.370900\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 3.386172\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 3.255815\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 3.903286\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 4.083152\n",
      "Epoch: 1079/3000... Step: 34500... Loss: 1.970230... Val Loss: 4.915853\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 3.944130\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 3.164584\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 2.604928\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 2.400858\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 2.194260\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 2.647076\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 2.661301\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 2.717883\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 2.665513\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 2.830395\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 2.753425\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 2.785249\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 2.722175\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 3.475132\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 3.357364\n",
      "Epoch: 1082/3000... Step: 34600... Loss: 0.492535... Val Loss: 3.325376\n",
      "Validation loss decreased (3.392821 --> 3.325376).  Saving model ...\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 3.407158\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 2.905835\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 2.482305\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 2.596600\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 2.499202\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 2.923341\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 2.802910\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 2.962061\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 2.902029\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 2.982878\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 3.202895\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 3.166942\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 3.162193\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 3.890860\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 3.858256\n",
      "Epoch: 1085/3000... Step: 34700... Loss: 1.401777... Val Loss: 3.904402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 4.177776\n",
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 3.300454\n",
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 2.581618\n",
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 2.403851\n",
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 2.427254\n",
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 2.902589\n",
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 2.768030\n",
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 3.041087\n",
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 2.959473\n",
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 3.030151\n",
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 3.242661\n",
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 3.184796\n",
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 3.102494\n",
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 3.762170\n",
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 3.710979\n",
      "Epoch: 1088/3000... Step: 34800... Loss: 1.411936... Val Loss: 3.666633\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 3.955580\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 3.408519\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 2.781381\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 2.544737\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 2.457532\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 3.054350\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 3.041696\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 3.246184\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 3.162995\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 3.225821\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 3.434373\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 3.371720\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 3.276378\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 3.952454\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 3.819738\n",
      "Epoch: 1091/3000... Step: 34900... Loss: 1.424988... Val Loss: 3.820131\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 3.155982\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 3.022211\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 2.576071\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 2.426022\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 2.357471\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 2.834061\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 2.679707\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 2.706818\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 2.653703\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 2.711804\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 2.725725\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 2.760263\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 2.661795\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 3.403961\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 3.305372\n",
      "Epoch: 1094/3000... Step: 35000... Loss: 2.343060... Val Loss: 3.664981\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 3.920624\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 3.094657\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 2.486963\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 2.392454\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 2.768157\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 3.056493\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 2.930158\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 3.160507\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 3.061218\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 3.056199\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 3.368981\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 3.328390\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 3.223329\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 3.841623\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 3.820976\n",
      "Epoch: 1097/3000... Step: 35100... Loss: 3.645962... Val Loss: 4.024458\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 2.551229\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 2.791760\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 2.661762\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 2.493813\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 2.675477\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 3.106093\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 2.786535\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 2.757303\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 2.683130\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 2.774437\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 2.644013\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 2.643685\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 2.565259\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 3.442073\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 3.540869\n",
      "Epoch: 1100/3000... Step: 35200... Loss: 1.436031... Val Loss: 4.378681\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 3.878890\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 3.171625\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 2.606966\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 2.322018\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 2.172229\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 2.584985\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 2.426283\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 2.593518\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 2.521564\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 2.690987\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 2.645586\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 2.654073\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 2.587474\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 3.285168\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 3.185424\n",
      "Epoch: 1104/3000... Step: 35300... Loss: 1.513643... Val Loss: 3.221223\n",
      "Validation loss decreased (3.325376 --> 3.221223).  Saving model ...\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 4.766154\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 3.917758\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 3.391702\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 3.004721\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 2.984915\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 4.181212\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 3.893723\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 3.972160\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 3.791237\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 3.842203\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 3.674198\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 3.722698\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 3.574228\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 4.183174\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 4.111689\n",
      "Epoch: 1107/3000... Step: 35400... Loss: 1.259409... Val Loss: 4.648324\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 3.838791\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 3.208281\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 2.949018\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 3.021489\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 2.671720\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 3.556861\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 3.352847\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 3.500757\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 3.410479\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 3.443469\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 3.446883\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 3.394346\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 3.341118\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 3.989638\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 3.892220\n",
      "Epoch: 1110/3000... Step: 35500... Loss: 0.683714... Val Loss: 3.999675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 5.796347\n",
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 4.144883\n",
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 3.402808\n",
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 3.374846\n",
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 3.326906\n",
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 3.956766\n",
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 3.901174\n",
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 4.405047\n",
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 4.276473\n",
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 4.193925\n",
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 4.363804\n",
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 4.278821\n",
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 4.199518\n",
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 4.734624\n",
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 4.601962\n",
      "Epoch: 1113/3000... Step: 35600... Loss: 1.875479... Val Loss: 4.513706\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 4.204556\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 3.234533\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 2.557476\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 2.398943\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 2.231414\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 2.631624\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 2.538231\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 2.920551\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 2.845749\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 2.934964\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 2.933394\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 2.864492\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 2.808083\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 3.487290\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 3.369705\n",
      "Epoch: 1116/3000... Step: 35700... Loss: 1.699562... Val Loss: 3.352576\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 3.896189\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 3.883085\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 3.228227\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 2.950033\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 2.706263\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 3.078453\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 2.958551\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 2.987521\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 2.963965\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 3.121182\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 2.995513\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 3.029977\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 2.953067\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 3.825918\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 3.714078\n",
      "Epoch: 1119/3000... Step: 35800... Loss: 2.170475... Val Loss: 3.792623\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 3.802161\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 3.089996\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 3.239996\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 2.958336\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 3.332664\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 3.557606\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 3.350721\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 3.440001\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 3.364462\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 3.572348\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 3.347564\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 3.229791\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 3.124391\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 3.805342\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 3.929033\n",
      "Epoch: 1122/3000... Step: 35900... Loss: 5.720006... Val Loss: 4.489414\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 3.391588\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 3.068264\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 2.436507\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 2.481925\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 2.251630\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 2.645835\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 2.514764\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 2.734597\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 2.670347\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 2.788765\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 2.801657\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 2.699956\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 2.627691\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 3.282230\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 3.166398\n",
      "Epoch: 1125/3000... Step: 36000... Loss: 0.751934... Val Loss: 3.126865\n",
      "Validation loss decreased (3.221223 --> 3.126865).  Saving model ...\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 5.566860\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 4.174467\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 3.395895\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 3.276460\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 3.102607\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 3.514070\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 3.404021\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 3.881995\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 3.835638\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 3.945114\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 3.842259\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 3.831164\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 3.753215\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 4.298424\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 4.140914\n",
      "Epoch: 1129/3000... Step: 36100... Loss: 3.564954... Val Loss: 4.086600\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 3.693699\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 3.025146\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 2.541046\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 2.553186\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 2.251474\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 2.889018\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 2.817008\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 3.133023\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 3.046690\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 3.235912\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 3.246154\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 3.221247\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 3.118653\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 3.773717\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 3.643822\n",
      "Epoch: 1132/3000... Step: 36200... Loss: 0.789050... Val Loss: 3.672108\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 4.752290\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 3.894629\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 3.488902\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 3.308050\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 3.527642\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 4.266450\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 4.054232\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 4.302805\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 4.169908\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 4.251929\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 4.140226\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 4.217145\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 4.172153\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 4.765029\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 4.739289\n",
      "Epoch: 1135/3000... Step: 36300... Loss: 1.169788... Val Loss: 5.223108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 3.581762\n",
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 3.388472\n",
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 2.891066\n",
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 2.745296\n",
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 2.526714\n",
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 3.063280\n",
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 2.989257\n",
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 3.170428\n",
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 3.136333\n",
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 3.158342\n",
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 3.193066\n",
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 3.139991\n",
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 3.064676\n",
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 3.757948\n",
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 3.672275\n",
      "Epoch: 1138/3000... Step: 36400... Loss: 1.514071... Val Loss: 3.731502\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 3.934433\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 3.137739\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 2.603180\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 2.494977\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 2.385661\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 2.845852\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 2.693832\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 2.794677\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 2.755005\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 2.881878\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 2.745264\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 2.704271\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 2.642824\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 3.290019\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 3.179550\n",
      "Epoch: 1141/3000... Step: 36500... Loss: 1.410005... Val Loss: 3.140370\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 3.120126\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 2.921408\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 2.403471\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 2.241301\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 2.031149\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 2.607104\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 2.441514\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 2.618479\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 2.563597\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 2.648626\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 2.892860\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 2.921377\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 2.824456\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 3.543833\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 3.543071\n",
      "Epoch: 1144/3000... Step: 36600... Loss: 2.383819... Val Loss: 3.556550\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 3.714396\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 2.776026\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 2.352154\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 2.199956\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 2.012038\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 2.550047\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 2.483361\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 2.762047\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 2.690729\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 2.775815\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 2.796467\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 2.820311\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 2.754217\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 3.396208\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 3.269452\n",
      "Epoch: 1147/3000... Step: 36700... Loss: 3.788520... Val Loss: 3.287254\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 4.637669\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 4.017250\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 3.673126\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 3.541428\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 3.403174\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 3.729059\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 3.574206\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 3.663480\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 3.665515\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 3.816612\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 3.778190\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 3.852862\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 3.796521\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 4.553039\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 4.418586\n",
      "Epoch: 1150/3000... Step: 36800... Loss: 3.996842... Val Loss: 4.598094\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 3.312724\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 3.015852\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 2.696800\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 2.706528\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 2.589470\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 3.055817\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 2.838130\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 2.920214\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 2.795956\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 2.911888\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 2.727045\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 2.655062\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 2.567048\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 3.294991\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 3.476133\n",
      "Epoch: 1154/3000... Step: 36900... Loss: 1.185104... Val Loss: 4.132413\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 3.502218\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 3.317587\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 2.710858\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 2.605937\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 2.436028\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 3.393300\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 3.218844\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 3.207097\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 3.155341\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 3.240486\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 3.162439\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 3.168650\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 3.065651\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 3.759879\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 3.662491\n",
      "Epoch: 1157/3000... Step: 37000... Loss: 0.772960... Val Loss: 3.658976\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 3.943150\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 3.080246\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 2.589163\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 2.580156\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 2.424830\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 3.019157\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 2.886825\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 3.027422\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 2.978561\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 3.074930\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 2.896679\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 2.946770\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 2.876294\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 3.524781\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 3.406674\n",
      "Epoch: 1160/3000... Step: 37100... Loss: 0.663401... Val Loss: 3.347960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 4.545219\n",
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 3.360423\n",
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 2.630046\n",
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 2.425435\n",
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 2.491265\n",
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 3.047857\n",
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 2.931736\n",
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 3.209608\n",
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 3.169359\n",
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 3.144356\n",
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 3.115617\n",
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 3.024235\n",
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 2.944843\n",
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 3.439503\n",
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 3.340384\n",
      "Epoch: 1163/3000... Step: 37200... Loss: 1.737293... Val Loss: 3.284426\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 4.495783\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 4.601560\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 3.835609\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 3.439964\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 3.245833\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 3.760882\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 3.593749\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 3.533584\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 3.478737\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 3.705706\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 3.667302\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 3.618862\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 3.548306\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 4.392706\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 4.323127\n",
      "Epoch: 1166/3000... Step: 37300... Loss: 2.973545... Val Loss: 4.578682\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 3.724632\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 3.398729\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 2.641915\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 2.417850\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 2.448392\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 2.647870\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 2.480147\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 2.565824\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 2.500946\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 2.734843\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 2.740264\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 2.664926\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 2.598907\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 3.312424\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 3.229596\n",
      "Epoch: 1169/3000... Step: 37400... Loss: 2.043651... Val Loss: 3.335260\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 6.422230\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 5.100656\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 4.377370\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 4.239499\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 3.935384\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 4.473650\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 4.337824\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 4.750569\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 4.661345\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 4.613954\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 4.662116\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 4.629930\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 4.577352\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 5.178662\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 4.991707\n",
      "Epoch: 1172/3000... Step: 37500... Loss: 3.639154... Val Loss: 4.981816\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 3.457544\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 2.967059\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 2.542281\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 2.396762\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 2.229177\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 2.756421\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 2.619441\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 2.693809\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 2.644667\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 2.649549\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 2.509498\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 2.564941\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 2.508216\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 3.200206\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 3.095594\n",
      "Epoch: 1175/3000... Step: 37600... Loss: 2.899952... Val Loss: 3.066249\n",
      "Validation loss decreased (3.126865 --> 3.066249).  Saving model ...\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 5.287239\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 3.826637\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 3.017923\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 2.917703\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 2.696689\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 3.282776\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 3.171211\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 3.690362\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 3.553112\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 3.560655\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 3.395777\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 3.270797\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 3.138628\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 3.714466\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 3.596710\n",
      "Epoch: 1179/3000... Step: 37700... Loss: 2.264638... Val Loss: 3.536035\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 3.513631\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 3.162133\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 2.645001\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 2.380344\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 2.125319\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 2.699841\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 2.544052\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 2.719638\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 2.674339\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 2.806218\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 2.701967\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 2.661739\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 2.557812\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 3.267584\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 3.147571\n",
      "Epoch: 1182/3000... Step: 37800... Loss: 0.504177... Val Loss: 3.158046\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 4.019757\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 3.677304\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 3.189150\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 2.888816\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 2.686443\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 3.477851\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 3.185561\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 3.227126\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 3.114036\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 3.347892\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 3.431684\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 3.289310\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 3.229119\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 3.934459\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 3.804889\n",
      "Epoch: 1185/3000... Step: 37900... Loss: 1.110006... Val Loss: 3.813867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 3.555100\n",
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 3.158701\n",
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 2.573678\n",
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 2.448334\n",
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 2.211658\n",
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 2.648800\n",
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 2.541471\n",
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 2.811577\n",
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 2.768866\n",
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 2.782448\n",
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 2.728925\n",
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 2.667515\n",
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 2.595525\n",
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 3.297141\n",
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 3.214546\n",
      "Epoch: 1188/3000... Step: 38000... Loss: 1.211417... Val Loss: 3.200284\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 3.365433\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 3.301594\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 2.666225\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 2.481830\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 2.376559\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 2.966052\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 2.959418\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 3.211788\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 3.149786\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 3.144392\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 2.950839\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 2.849443\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 2.798283\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 3.515497\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 3.400085\n",
      "Epoch: 1191/3000... Step: 38100... Loss: 0.750309... Val Loss: 3.371338\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 5.029038\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 4.406222\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 4.158315\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 4.085294\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 3.827234\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 4.415741\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 4.246248\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 4.311609\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 4.299568\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 4.366513\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 4.419148\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 4.397110\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 4.369006\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 5.105749\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 5.010980\n",
      "Epoch: 1194/3000... Step: 38200... Loss: 2.276513... Val Loss: 5.148447\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 3.588892\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 3.084410\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 2.567361\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 2.402117\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 2.173106\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 2.619707\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 2.488507\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 2.684312\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 2.659289\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 2.692061\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 2.712065\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 2.619531\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 2.550069\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 3.190997\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 3.128902\n",
      "Epoch: 1197/3000... Step: 38300... Loss: 2.913574... Val Loss: 3.218912\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 3.648758\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 3.485164\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 3.425204\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 3.343491\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 3.667327\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 4.113172\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 3.865703\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 3.746254\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 3.721185\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 3.963786\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 3.883090\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 4.049717\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 3.925165\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 4.799577\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 4.683715\n",
      "Epoch: 1200/3000... Step: 38400... Loss: 3.563285... Val Loss: 4.722146\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 3.790969\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 2.922823\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 2.719111\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 2.564262\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 2.335287\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 2.783544\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 2.626002\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 2.700054\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 2.674389\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 2.706430\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 2.616608\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 2.519537\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 2.468775\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 3.162001\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 3.032888\n",
      "Epoch: 1204/3000... Step: 38500... Loss: 0.808487... Val Loss: 3.112446\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 4.206927\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 4.640559\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 4.062908\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 3.829384\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 3.688289\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 4.683254\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 4.548074\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 4.561560\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 4.512650\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 4.543121\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 4.328003\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 4.232179\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 4.149794\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 4.856787\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 4.771612\n",
      "Epoch: 1207/3000... Step: 38600... Loss: 0.975059... Val Loss: 4.724834\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 3.371710\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 2.742434\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 2.378794\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 2.318117\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 2.063558\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 2.563482\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 2.454334\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 2.676938\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 2.624203\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 2.614841\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 2.691346\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 2.583428\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 2.520171\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 3.132789\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 3.026886\n",
      "Epoch: 1210/3000... Step: 38700... Loss: 0.477253... Val Loss: 3.059222\n",
      "Validation loss decreased (3.066249 --> 3.059222).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 3.640490\n",
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 4.197248\n",
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 3.587591\n",
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 3.376877\n",
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 3.184444\n",
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 3.816911\n",
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 3.675512\n",
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 3.748657\n",
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 3.689548\n",
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 3.788034\n",
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 3.643468\n",
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 3.558827\n",
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 3.488140\n",
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 4.257454\n",
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 4.361956\n",
      "Epoch: 1213/3000... Step: 38800... Loss: 3.044353... Val Loss: 4.676797\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 3.774222\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 3.011806\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 2.424841\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 2.390918\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 2.565211\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 2.969363\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 2.913682\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 3.303779\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 3.198068\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 3.135323\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 3.098865\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 3.004161\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 2.925762\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 3.528161\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 3.398233\n",
      "Epoch: 1216/3000... Step: 38900... Loss: 1.568017... Val Loss: 3.447510\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 3.372256\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 2.926693\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 2.386380\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 2.191530\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 2.024517\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 2.484708\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 2.429300\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 2.663565\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 2.625226\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 2.687577\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 2.526155\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 2.428796\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 2.379398\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 3.048419\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 2.957303\n",
      "Epoch: 1219/3000... Step: 39000... Loss: 2.785392... Val Loss: 2.985546\n",
      "Validation loss decreased (3.059222 --> 2.985546).  Saving model ...\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 3.317553\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 3.677132\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 3.289082\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 2.778593\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 3.002487\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 3.419676\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 3.118916\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 3.034772\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 2.943176\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 3.111952\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 2.970256\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 2.815046\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 2.783221\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 3.714047\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 3.748578\n",
      "Epoch: 1222/3000... Step: 39100... Loss: 5.784511... Val Loss: 4.297921\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 3.154810\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 2.708818\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 2.302844\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 2.281220\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 2.149223\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 2.576377\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 2.404296\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 2.629334\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 2.550660\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 2.576184\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 2.507256\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 2.396063\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 2.334822\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 3.064109\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 2.999196\n",
      "Epoch: 1225/3000... Step: 39200... Loss: 1.320982... Val Loss: 3.104061\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 5.899727\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 4.367224\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 3.613113\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 3.424543\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 3.428407\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 3.708558\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 3.679685\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 4.340169\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 4.269453\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 4.255907\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 4.200459\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 4.177520\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 4.113475\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 4.579883\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 4.393783\n",
      "Epoch: 1229/3000... Step: 39300... Loss: 3.782167... Val Loss: 4.368028\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 3.711769\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 2.925360\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 2.697957\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 2.582655\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 2.437393\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 2.919704\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 2.879743\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 3.240753\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 3.158137\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 3.174925\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 3.191758\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 3.149468\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 3.089737\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 3.672899\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 3.570526\n",
      "Epoch: 1232/3000... Step: 39400... Loss: 0.901420... Val Loss: 3.647445\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 5.188847\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 4.434323\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 3.758085\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 3.606184\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 3.586076\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 3.919531\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 3.850710\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 4.019778\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 3.957150\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 4.017780\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 3.930822\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 3.811494\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 3.705558\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 4.386460\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 4.278765\n",
      "Epoch: 1235/3000... Step: 39500... Loss: 1.373419... Val Loss: 4.235587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 3.190058\n",
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 3.151492\n",
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 2.596783\n",
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 2.310615\n",
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 2.162121\n",
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 2.679160\n",
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 2.592187\n",
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 2.777587\n",
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 2.745132\n",
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 2.842385\n",
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 2.874186\n",
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 2.807346\n",
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 2.737194\n",
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 3.473318\n",
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 3.394303\n",
      "Epoch: 1238/3000... Step: 39600... Loss: 0.828545... Val Loss: 3.404862\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 2.979500\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 2.613324\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 2.173848\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 2.022693\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 1.932000\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 2.612439\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 2.506705\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 2.667538\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 2.605872\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 2.590137\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 2.554134\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 2.502375\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 2.462866\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 3.176438\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 3.092379\n",
      "Epoch: 1241/3000... Step: 39700... Loss: 0.897026... Val Loss: 3.147336\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 3.225793\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 2.741485\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 2.319461\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 2.204470\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 1.988875\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 2.622389\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 2.502713\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 2.788515\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 2.716263\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 2.781814\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 2.864321\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 2.811163\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 2.763042\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 3.426389\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 3.371238\n",
      "Epoch: 1244/3000... Step: 39800... Loss: 2.413538... Val Loss: 3.412475\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 3.252299\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 2.627145\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 2.354149\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 2.098599\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 1.873194\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 2.537557\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 2.407538\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 2.611701\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 2.568609\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 2.600135\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 2.617149\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 2.572587\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 2.522899\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 3.220091\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 3.158283\n",
      "Epoch: 1247/3000... Step: 39900... Loss: 3.213080... Val Loss: 3.262210\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 4.172886\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 3.043542\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 2.413881\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 2.189168\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 2.503944\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 2.914257\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 2.855554\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 3.184777\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 3.100370\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 3.104160\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 2.973445\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 2.898679\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 2.820066\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 3.394326\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 3.360624\n",
      "Epoch: 1250/3000... Step: 40000... Loss: 2.146541... Val Loss: 3.618065\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.907179\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.874922\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.416752\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.308856\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.061738\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.567365\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.352860\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.344115\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.291119\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.302310\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.189093\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.174920\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.150562\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.965571\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.880757\n",
      "Epoch: 1254/3000... Step: 40100... Loss: 0.820912... Val Loss: 2.897493\n",
      "Validation loss decreased (2.985546 --> 2.897493).  Saving model ...\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 4.348961\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 3.262594\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 2.811498\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 2.887178\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 2.709438\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 3.298258\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 3.121339\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 3.402451\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 3.350503\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 3.371037\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 3.222380\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 3.142016\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 3.112791\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 3.729214\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 3.626733\n",
      "Epoch: 1257/3000... Step: 40200... Loss: 0.969491... Val Loss: 3.748156\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 7.024157\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 5.755926\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 5.439700\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 5.609170\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 5.396561\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 5.937733\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 5.757054\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 6.082699\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 6.005504\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 5.848680\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 5.770575\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 5.740721\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 5.750432\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 6.271722\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 6.207235\n",
      "Epoch: 1260/3000... Step: 40300... Loss: 3.108425... Val Loss: 6.221210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 4.751992\n",
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 4.705428\n",
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 4.073425\n",
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 3.729366\n",
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 3.663118\n",
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 4.542992\n",
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 4.360977\n",
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 4.340923\n",
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 4.270123\n",
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 4.347242\n",
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 4.297376\n",
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 4.192841\n",
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 4.106667\n",
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 4.835296\n",
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 4.761940\n",
      "Epoch: 1263/3000... Step: 40400... Loss: 3.178111... Val Loss: 4.838404\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 3.849511\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 3.096961\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 2.600940\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 2.440453\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 2.559329\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 3.081387\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 2.928653\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 3.222053\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 3.125073\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 3.157481\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 3.205595\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 3.210244\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 3.137527\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 3.740911\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 3.664512\n",
      "Epoch: 1266/3000... Step: 40500... Loss: 0.579543... Val Loss: 3.759655\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 3.077118\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 2.944352\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 2.958800\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 2.775714\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 2.751331\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 3.229151\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 3.053232\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 3.040921\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 2.944147\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 2.937648\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 2.924563\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 2.769912\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 2.691601\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 3.394019\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 3.285840\n",
      "Epoch: 1269/3000... Step: 40600... Loss: 3.151247... Val Loss: 3.349579\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 5.271494\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 4.186297\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 4.072138\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 4.063129\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 3.936729\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 4.453389\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 4.406443\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 4.635770\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 4.556718\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 4.484649\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 4.401452\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 4.368373\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 4.280268\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 4.927403\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 4.786972\n",
      "Epoch: 1272/3000... Step: 40700... Loss: 3.940576... Val Loss: 4.825870\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 4.523015\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 3.423158\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 2.716586\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 2.371169\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 2.429789\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 2.832075\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 2.817349\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 2.979006\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 2.920913\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 2.937223\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 2.796810\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 2.778936\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 2.684189\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 3.181915\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 3.066826\n",
      "Epoch: 1275/3000... Step: 40800... Loss: 1.659174... Val Loss: 3.075946\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 3.536710\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 2.730529\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 2.447469\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 2.248927\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 2.042895\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 2.668226\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 2.603487\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 2.932418\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 2.841658\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 2.918945\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 2.764734\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 2.682861\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 2.599468\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 3.191785\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 3.117281\n",
      "Epoch: 1279/3000... Step: 40900... Loss: 1.195699... Val Loss: 3.157943\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 3.129338\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 2.685982\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 2.161580\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 1.928467\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 2.435939\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 3.328287\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 3.102921\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 3.171603\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 3.050899\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 3.078920\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 2.945857\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 2.872840\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 2.782552\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 3.479038\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 3.481091\n",
      "Epoch: 1282/3000... Step: 41000... Loss: 0.420973... Val Loss: 3.692587\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 5.254087\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 3.726222\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 3.081859\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 3.176846\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 3.032726\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 3.480657\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 3.488199\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 4.377588\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 4.209480\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 4.015714\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 3.935306\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 3.870022\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 3.792975\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 4.199448\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 4.123893\n",
      "Epoch: 1285/3000... Step: 41100... Loss: 0.565585... Val Loss: 4.039286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 4.028260\n",
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 3.040408\n",
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 2.523344\n",
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 2.256845\n",
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 2.127659\n",
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 2.745334\n",
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 2.680758\n",
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 3.119662\n",
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 3.022755\n",
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 2.941059\n",
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 2.840869\n",
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 2.866703\n",
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 2.783925\n",
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 3.355739\n",
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 3.235454\n",
      "Epoch: 1288/3000... Step: 41200... Loss: 1.459634... Val Loss: 3.185693\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 3.567848\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 2.933051\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 2.662843\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 2.490029\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 2.188063\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 2.729032\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 2.630597\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 2.857387\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 2.776950\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 2.744655\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 2.771651\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 2.713729\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 2.626590\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 3.251667\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 3.183251\n",
      "Epoch: 1291/3000... Step: 41300... Loss: 0.676445... Val Loss: 3.393877\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 2.966851\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 2.919421\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 2.274600\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 1.992070\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 2.025035\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 2.664820\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 2.445576\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 2.427639\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 2.352780\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 2.495653\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 2.415369\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 2.574615\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 2.500818\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 3.349162\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 3.305924\n",
      "Epoch: 1294/3000... Step: 41400... Loss: 2.443604... Val Loss: 3.271980\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 3.693182\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 2.949703\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 2.283564\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 2.216208\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 2.393825\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 3.157104\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 3.060917\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 3.431439\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 3.304984\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 3.323347\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 3.135692\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 3.008648\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 2.897431\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 3.443149\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 3.334921\n",
      "Epoch: 1297/3000... Step: 41500... Loss: 2.493142... Val Loss: 3.282538\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 5.149643\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 5.482131\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 5.054856\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 4.461749\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 4.889568\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 5.451193\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 5.127362\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 5.092125\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 4.946948\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 5.166718\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 4.900858\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 4.734581\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 4.673548\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 5.443045\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 5.388863\n",
      "Epoch: 1300/3000... Step: 41600... Loss: 3.129159... Val Loss: 5.915033\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 3.105402\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 3.208948\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 2.609945\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 2.305573\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 2.004567\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 2.720738\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 2.476891\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 2.521322\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 2.438777\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 2.418672\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 2.398881\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 2.307468\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 2.228601\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 3.120465\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 3.032452\n",
      "Epoch: 1304/3000... Step: 41700... Loss: 1.238706... Val Loss: 3.049135\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 3.522963\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 2.717398\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 2.304588\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 2.133068\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 2.641783\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 3.196982\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 3.029197\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 3.085320\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 3.013511\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 3.066033\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 2.988508\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 2.982094\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 2.874910\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 3.477512\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 3.395133\n",
      "Epoch: 1307/3000... Step: 41800... Loss: 0.616769... Val Loss: 3.706229\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 3.389607\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 2.831939\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 2.430161\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 2.175345\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 1.984008\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 2.626867\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 2.459624\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 2.648470\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 2.595627\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 2.643555\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 2.526893\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 2.434027\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 2.374458\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 3.001000\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 2.936706\n",
      "Epoch: 1310/3000... Step: 41900... Loss: 0.440523... Val Loss: 2.915295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 4.984530\n",
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 4.374197\n",
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 3.869203\n",
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 3.632292\n",
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 3.565410\n",
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 4.407701\n",
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 4.305052\n",
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 4.525740\n",
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 4.457305\n",
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 4.362333\n",
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 4.252863\n",
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 4.195368\n",
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 4.074447\n",
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 4.655600\n",
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 4.633236\n",
      "Epoch: 1313/3000... Step: 42000... Loss: 3.011080... Val Loss: 4.695168\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 5.307216\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 4.530648\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 3.889940\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 3.701477\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 3.749198\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 4.186594\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 4.101096\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 4.269033\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 4.213892\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 4.495433\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 4.356714\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 4.477006\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 4.333710\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 4.899287\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 4.812171\n",
      "Epoch: 1316/3000... Step: 42100... Loss: 2.019943... Val Loss: 4.846252\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 4.014589\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 3.136292\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 2.800512\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 2.562829\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 2.407433\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 2.971650\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 2.833831\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 2.798687\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 2.719964\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 2.711536\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 2.531848\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 2.459601\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 2.385468\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 3.040956\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 2.927360\n",
      "Epoch: 1319/3000... Step: 42200... Loss: 2.384470... Val Loss: 3.026343\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 6.835340\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 6.907959\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 6.234075\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 5.777832\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 5.614678\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 6.233376\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 6.105461\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 6.235213\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 6.208047\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 6.312392\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 6.122246\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 5.969750\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 5.875211\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 6.487894\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 6.406484\n",
      "Epoch: 1322/3000... Step: 42300... Loss: 7.541356... Val Loss: 6.402304\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 3.067191\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 2.836310\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 2.251399\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 1.976601\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 1.913252\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 2.493667\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 2.343536\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 2.399512\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 2.340067\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 2.451464\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 2.328605\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 2.291175\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 2.247335\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 2.997381\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 2.939421\n",
      "Epoch: 1325/3000... Step: 42400... Loss: 2.300575... Val Loss: 2.877327\n",
      "Validation loss decreased (2.897493 --> 2.877327).  Saving model ...\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 4.070058\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 3.884179\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 3.196736\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 2.983818\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 2.785584\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 4.490160\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 4.217758\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 4.438897\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 4.259522\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 4.298345\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 4.078893\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 3.899854\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 3.742595\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 4.371857\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 4.258935\n",
      "Epoch: 1329/3000... Step: 42500... Loss: 1.358457... Val Loss: 4.128587\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 3.080755\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 2.669226\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 2.228417\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 2.099524\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 2.057826\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 2.662831\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 2.482229\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 2.653698\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 2.553639\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 2.540410\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 2.512289\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 2.442538\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 2.384732\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 3.037799\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 2.968402\n",
      "Epoch: 1332/3000... Step: 42600... Loss: 0.638807... Val Loss: 3.012404\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 3.738952\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 2.963979\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 2.372993\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 2.238053\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 2.170015\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 2.870801\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 2.693998\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 2.908057\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 2.815270\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 2.808120\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 3.003397\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 2.884734\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 2.793976\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 3.399309\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 3.303757\n",
      "Epoch: 1335/3000... Step: 42700... Loss: 0.994375... Val Loss: 3.217137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 4.020977\n",
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 3.809142\n",
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 3.303063\n",
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 3.053070\n",
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 2.878714\n",
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 3.551935\n",
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 3.373162\n",
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 3.476621\n",
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 3.401723\n",
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 3.459878\n",
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 3.700945\n",
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 3.550117\n",
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 3.442879\n",
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 4.221308\n",
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 4.197071\n",
      "Epoch: 1338/3000... Step: 42800... Loss: 1.148814... Val Loss: 4.138685\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 3.780154\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 3.231504\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 2.910885\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 2.809017\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 2.820092\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 3.372398\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 3.185840\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 3.271423\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 3.185814\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 3.164049\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 3.358264\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 3.341550\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 3.282040\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 4.026328\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 3.997456\n",
      "Epoch: 1341/3000... Step: 42900... Loss: 1.031758... Val Loss: 3.983644\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 4.215768\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 3.621215\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 3.100238\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 2.667046\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 2.460325\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 3.087847\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 2.971394\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 3.155328\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 3.043405\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 3.236065\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 3.118362\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 3.072905\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 2.956029\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 3.565633\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 3.436317\n",
      "Epoch: 1344/3000... Step: 43000... Loss: 3.337864... Val Loss: 3.537846\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 3.731019\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 3.098097\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 2.448896\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 2.187040\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 2.198366\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 2.787540\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 2.692736\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 2.901709\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 2.816952\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 2.841883\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 2.962947\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 2.912424\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 2.810858\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 3.434392\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 3.398816\n",
      "Epoch: 1347/3000... Step: 43100... Loss: 2.662735... Val Loss: 3.302260\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 6.335997\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 4.699246\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 4.368611\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 3.755146\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 3.750377\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 4.209856\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 4.098183\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 4.488173\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 4.273684\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 4.892130\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 4.717262\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 4.727894\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 4.624560\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 5.167127\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 5.036641\n",
      "Epoch: 1350/3000... Step: 43200... Loss: 1.445376... Val Loss: 5.347779\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 3.425998\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 2.772549\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 2.285660\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 2.088788\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 2.130635\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 2.719093\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 2.580681\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 2.874801\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 2.766914\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 2.811095\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 2.685268\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 2.604566\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 2.530587\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 3.186390\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 3.075265\n",
      "Epoch: 1354/3000... Step: 43300... Loss: 1.669016... Val Loss: 3.258474\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 3.444433\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 3.225473\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 2.774663\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 2.633834\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 2.433004\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 3.155967\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 3.083215\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 3.333934\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 3.279216\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 3.198835\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 3.137188\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 3.016723\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 2.945887\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 3.536411\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 3.457437\n",
      "Epoch: 1357/3000... Step: 43400... Loss: 1.226219... Val Loss: 3.435807\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 3.801558\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 3.030506\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 2.713584\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 2.422309\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 2.210949\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 2.866482\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 2.746991\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 2.944589\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 2.868788\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 2.961967\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 2.898075\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 2.866101\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 2.803448\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 3.367443\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 3.238458\n",
      "Epoch: 1360/3000... Step: 43500... Loss: 0.471986... Val Loss: 3.295971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 4.030256\n",
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 3.003797\n",
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 2.647426\n",
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 2.466852\n",
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 2.257193\n",
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 2.822121\n",
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 2.730299\n",
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 3.048901\n",
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 2.952557\n",
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 2.853333\n",
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 2.898794\n",
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 2.873742\n",
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 2.834501\n",
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 3.421011\n",
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 3.407051\n",
      "Epoch: 1363/3000... Step: 43600... Loss: 0.808094... Val Loss: 3.341137\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 3.734320\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 3.133272\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 2.476245\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 2.153007\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 2.015617\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 2.618151\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 2.452073\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 2.616846\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 2.540990\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 2.662933\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 2.604893\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 2.518268\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 2.450299\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 3.115788\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 3.040410\n",
      "Epoch: 1366/3000... Step: 43700... Loss: 0.678765... Val Loss: 3.131717\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 3.710164\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 2.900929\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 2.484421\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 2.160961\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 2.155365\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 2.701048\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 2.538676\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 2.652943\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 2.581950\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 2.668479\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 2.551150\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 2.496739\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 2.415211\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 3.054102\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 2.960065\n",
      "Epoch: 1369/3000... Step: 43800... Loss: 3.772254... Val Loss: 2.970749\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 3.912690\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 3.841001\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 3.285059\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 2.997780\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 2.911034\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 3.555235\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 3.354842\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 3.421922\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 3.348922\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 3.379889\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 3.208688\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 3.078307\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 3.000369\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 3.747342\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 3.672931\n",
      "Epoch: 1372/3000... Step: 43900... Loss: 3.481773... Val Loss: 3.683086\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 2.956901\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 2.502519\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 2.196054\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 2.016112\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 1.862917\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 2.411896\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 2.277205\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 2.403700\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 2.340282\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 2.344910\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 2.279770\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 2.329765\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 2.273089\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 2.907384\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 2.814110\n",
      "Epoch: 1375/3000... Step: 44000... Loss: 1.142739... Val Loss: 2.860903\n",
      "Validation loss decreased (2.877327 --> 2.860903).  Saving model ...\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 4.396083\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 3.145191\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 2.649633\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 2.526561\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 2.569294\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 3.306922\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 3.331705\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 3.936341\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 3.832869\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 3.691511\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 3.601305\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 3.484768\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 3.456594\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 3.932327\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 3.871819\n",
      "Epoch: 1379/3000... Step: 44100... Loss: 0.817222... Val Loss: 3.884166\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 3.712699\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 2.937388\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 2.408208\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 2.209878\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 1.979208\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 2.553952\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 2.402107\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 2.704457\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 2.617812\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 2.633508\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 2.609032\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 2.513933\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 2.459164\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 3.063336\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 2.942530\n",
      "Epoch: 1382/3000... Step: 44200... Loss: 0.783346... Val Loss: 2.930030\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 3.496274\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 2.625026\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 2.328749\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 2.122987\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 1.946257\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 2.476242\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 2.382767\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 2.676719\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 2.626780\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 2.632556\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 2.484147\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 2.429620\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 2.359101\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 2.987676\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 3.017731\n",
      "Epoch: 1385/3000... Step: 44300... Loss: 0.491838... Val Loss: 3.235176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 4.169976\n",
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 3.409495\n",
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 2.710673\n",
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 2.376035\n",
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 2.185842\n",
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 3.383786\n",
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 3.223141\n",
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 3.428903\n",
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 3.309623\n",
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 3.366508\n",
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 3.184593\n",
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 3.233466\n",
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 3.111096\n",
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 3.681882\n",
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 3.566947\n",
      "Epoch: 1388/3000... Step: 44400... Loss: 0.770133... Val Loss: 3.474254\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 3.658767\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 3.114957\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 2.695747\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 2.383383\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 2.152475\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 2.830260\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 2.689079\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 2.915938\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 2.825753\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 2.798113\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 2.759797\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 2.672609\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 2.604551\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 3.208648\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 3.160191\n",
      "Epoch: 1391/3000... Step: 44500... Loss: 0.780474... Val Loss: 3.134810\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 4.330387\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 3.394604\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 2.969757\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 2.803680\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 2.676517\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 3.183986\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 3.151902\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 3.441465\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 3.364418\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 3.387250\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 3.316982\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 3.295274\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 3.252745\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 3.874163\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 3.804252\n",
      "Epoch: 1394/3000... Step: 44600... Loss: 3.006788... Val Loss: 3.766446\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 3.288882\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 2.510490\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 2.113924\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 1.896552\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 1.742777\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 2.284508\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 2.198636\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 2.466612\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 2.422618\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 2.432445\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 2.463946\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 2.407225\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 2.340891\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 2.938825\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 2.844197\n",
      "Epoch: 1397/3000... Step: 44700... Loss: 2.302181... Val Loss: 2.830202\n",
      "Validation loss decreased (2.860903 --> 2.830202).  Saving model ...\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 5.534159\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 4.258342\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 3.994638\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 3.704815\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 3.556547\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 4.118198\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 4.014974\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 4.306234\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 4.193315\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 4.155933\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 4.104109\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 4.061881\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 4.005352\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 4.571992\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 4.483351\n",
      "Epoch: 1400/3000... Step: 44800... Loss: 0.595505... Val Loss: 4.449164\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 2.821342\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 2.539021\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 2.031975\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 1.807596\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 1.644307\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 2.248881\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 2.077379\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 2.186448\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 2.106856\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 2.205527\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 2.275361\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 2.278360\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 2.217578\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 2.928595\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 2.862631\n",
      "Epoch: 1404/3000... Step: 44900... Loss: 1.273359... Val Loss: 2.793524\n",
      "Validation loss decreased (2.830202 --> 2.793524).  Saving model ...\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 4.483275\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 3.236892\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 2.956587\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 2.978210\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 2.881091\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 3.391253\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 3.312974\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 3.624214\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 3.520569\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 3.496668\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 3.458973\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 3.360584\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 3.352962\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 3.841821\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 3.703560\n",
      "Epoch: 1407/3000... Step: 45000... Loss: 1.943089... Val Loss: 3.664309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 3.899395\n",
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 2.782428\n",
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 2.338360\n",
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 2.085656\n",
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 2.196722\n",
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 2.785799\n",
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 2.736740\n",
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 3.276459\n",
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 3.144807\n",
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 3.077641\n",
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 3.057035\n",
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 3.067275\n",
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 3.004572\n",
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 3.528280\n",
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 3.415642\n",
      "Epoch: 1410/3000... Step: 45100... Loss: 0.313443... Val Loss: 3.352195\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 2.804324\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 2.214326\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 1.881867\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 1.708142\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 1.547905\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 2.096866\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 1.979468\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 2.194827\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 2.135826\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 2.176111\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 2.184678\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 2.151080\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 2.110260\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 2.710750\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 2.729335\n",
      "Epoch: 1413/3000... Step: 45200... Loss: 0.789734... Val Loss: 2.737848\n",
      "Validation loss decreased (2.793524 --> 2.737848).  Saving model ...\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 3.590831\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 2.904066\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 2.679198\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 2.343677\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 2.155862\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 2.702348\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 2.588951\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 2.760267\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 2.670872\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 2.727288\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 2.607755\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 2.682488\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 2.589979\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 3.265188\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 3.216250\n",
      "Epoch: 1416/3000... Step: 45300... Loss: 0.994338... Val Loss: 3.190753\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 2.673603\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 2.756264\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 2.318855\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 2.084909\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 2.106888\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 2.689829\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 2.496920\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 2.499965\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 2.438998\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 2.511386\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 2.547612\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 2.487035\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 2.422417\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 3.192461\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 3.141113\n",
      "Epoch: 1419/3000... Step: 45400... Loss: 2.167909... Val Loss: 3.166011\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 4.011127\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 3.115032\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 2.498592\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 2.111668\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 2.045154\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 2.622974\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 2.443646\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 2.581770\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 2.460603\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 2.581783\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 2.526029\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 2.553076\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 2.474701\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 3.114011\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 2.993939\n",
      "Epoch: 1422/3000... Step: 45500... Loss: 2.396919... Val Loss: 3.040203\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 3.134887\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 2.553975\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 2.043683\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 1.835584\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 1.699193\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 2.336509\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 2.236436\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 2.493385\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 2.409625\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 2.421773\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 2.342135\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 2.358451\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 2.288352\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 2.889698\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 2.801501\n",
      "Epoch: 1425/3000... Step: 45600... Loss: 0.397033... Val Loss: 2.787155\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 4.723635\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 3.496529\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 2.948078\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 2.422385\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 2.172288\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 2.909759\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 2.772539\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 3.181903\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 3.031336\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 3.521180\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 3.343972\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 3.528302\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 3.451989\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 3.979324\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 3.837142\n",
      "Epoch: 1429/3000... Step: 45700... Loss: 2.319469... Val Loss: 3.723406\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 4.077791\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 3.895567\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 3.498661\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 3.188084\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 3.044193\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 3.827987\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 3.622031\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 3.726504\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 3.668967\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 3.730122\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 3.505101\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 3.621380\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 3.498723\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 4.269105\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 4.154469\n",
      "Epoch: 1432/3000... Step: 45800... Loss: 0.859535... Val Loss: 4.146030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 3.919299\n",
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 2.806098\n",
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 2.307003\n",
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 2.228545\n",
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 2.171127\n",
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 2.775018\n",
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 2.695746\n",
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 3.129657\n",
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 3.038260\n",
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 2.954897\n",
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 2.930900\n",
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 2.967091\n",
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 2.894111\n",
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 3.392882\n",
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 3.313190\n",
      "Epoch: 1435/3000... Step: 45900... Loss: 0.566647... Val Loss: 3.248877\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 4.043841\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 2.887119\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 2.412424\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 2.118193\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 2.088851\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 2.728702\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 2.671890\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 3.185573\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 3.069426\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 3.067613\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 2.968028\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 2.948966\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 2.865600\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 3.359840\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 3.293935\n",
      "Epoch: 1438/3000... Step: 46000... Loss: 0.794548... Val Loss: 3.225681\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 3.040711\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 2.631069\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 2.612098\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 2.197841\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 1.937008\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 2.586451\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 2.425974\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 2.505186\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 2.416316\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 2.517027\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 2.347913\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 2.287611\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 2.217231\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 2.833042\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 2.772763\n",
      "Epoch: 1441/3000... Step: 46100... Loss: 0.775020... Val Loss: 2.730225\n",
      "Validation loss decreased (2.737848 --> 2.730225).  Saving model ...\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 3.282456\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 2.648573\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 2.065675\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 1.775032\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 1.746649\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 2.446766\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 2.366340\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 2.587048\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 2.492843\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 2.483376\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 2.644376\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 2.655509\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 2.572742\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 3.177239\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 3.120145\n",
      "Epoch: 1444/3000... Step: 46200... Loss: 3.377063... Val Loss: 3.144719\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 3.324910\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 2.524775\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 2.194393\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 1.886607\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 1.692307\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 2.443878\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 2.364104\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 2.672777\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 2.568677\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 2.608560\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 2.521149\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 2.627247\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 2.538499\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 3.100460\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 3.020620\n",
      "Epoch: 1447/3000... Step: 46300... Loss: 2.759635... Val Loss: 3.028490\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 3.741881\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 2.895713\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 2.360730\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 2.039222\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 1.791996\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 2.449370\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 2.285572\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 2.496025\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 2.394267\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 2.649029\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 2.501894\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 2.452443\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 2.386927\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 3.078042\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 3.048387\n",
      "Epoch: 1450/3000... Step: 46400... Loss: 0.357960... Val Loss: 3.005665\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 2.827917\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 2.339953\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 1.978264\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 1.727138\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 1.571116\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 2.212074\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 2.085727\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 2.205141\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 2.138635\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 2.161398\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 2.158079\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 2.066308\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 2.013504\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 2.622509\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 2.565536\n",
      "Epoch: 1454/3000... Step: 46500... Loss: 0.958062... Val Loss: 2.576084\n",
      "Validation loss decreased (2.730225 --> 2.576084).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 4.319597\n",
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 3.400677\n",
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 3.013053\n",
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 2.952532\n",
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 2.744887\n",
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 6.822319\n",
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 6.274051\n",
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 6.385491\n",
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 5.939052\n",
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 5.603765\n",
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 5.344829\n",
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 5.163174\n",
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 4.979091\n",
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 5.429423\n",
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 5.361660\n",
      "Epoch: 1457/3000... Step: 46600... Loss: 1.345659... Val Loss: 5.254201\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 4.890531\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 4.982192\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 4.498329\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 4.231568\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 4.019189\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 4.702313\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 4.543410\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 4.468908\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 4.435896\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 4.470999\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 4.360091\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 4.353991\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 4.299771\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 5.100442\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 5.025387\n",
      "Epoch: 1460/3000... Step: 46700... Loss: 3.007108... Val Loss: 5.050014\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 2.966453\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 2.867425\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 2.595999\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 2.487460\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 2.433629\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 3.864440\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 3.655714\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 3.794788\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 3.690815\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 3.547858\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 3.418865\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 3.289214\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 3.220786\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 3.861222\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 3.889166\n",
      "Epoch: 1463/3000... Step: 46800... Loss: 3.518091... Val Loss: 3.858474\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 4.031307\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 2.927019\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 2.464118\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 2.211961\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 2.276051\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 2.867801\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 2.792762\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 3.289729\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 3.163977\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 3.085265\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 3.103309\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 3.094920\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 3.031701\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 3.585519\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 3.525079\n",
      "Epoch: 1466/3000... Step: 46900... Loss: 0.916562... Val Loss: 3.557508\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 3.817807\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 3.235791\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 2.815919\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 2.715904\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 2.540188\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 3.783165\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 3.568591\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 3.718004\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 3.627404\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 3.665964\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 3.520153\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 3.409046\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 3.289023\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 3.878337\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 3.766355\n",
      "Epoch: 1469/3000... Step: 47000... Loss: 3.437763... Val Loss: 3.688111\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 3.683299\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 2.635023\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 2.084379\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 1.776455\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 1.733536\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 2.353621\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 2.250448\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 2.537205\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 2.466849\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 2.533462\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 2.496602\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 2.494813\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 2.400303\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 2.980728\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 2.852887\n",
      "Epoch: 1472/3000... Step: 47100... Loss: 2.691285... Val Loss: 2.817308\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 3.939371\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 2.664869\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 2.114104\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 1.850649\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 2.264896\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 2.774198\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 2.660328\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 3.007706\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 2.898762\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 2.862323\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 2.797861\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 2.732725\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 2.629681\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 3.176326\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 3.114773\n",
      "Epoch: 1475/3000... Step: 47200... Loss: 0.408091... Val Loss: 3.213106\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 4.969360\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 3.478675\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 3.001212\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 2.787865\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 2.671038\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 3.736846\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 3.600607\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 3.945090\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 3.744230\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 3.638112\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 3.450766\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 3.353628\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 3.246443\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 3.743606\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 3.626553\n",
      "Epoch: 1479/3000... Step: 47300... Loss: 1.298887... Val Loss: 3.609935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 2.442262\n",
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 2.257806\n",
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 1.867732\n",
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 1.662470\n",
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 1.566115\n",
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 2.954719\n",
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 2.752376\n",
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 2.872986\n",
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 2.723134\n",
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 2.737013\n",
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 2.684612\n",
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 2.587305\n",
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 2.498964\n",
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 3.131946\n",
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 3.022249\n",
      "Epoch: 1482/3000... Step: 47400... Loss: 0.523608... Val Loss: 2.965323\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 5.798198\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 4.234042\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 3.658114\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 3.603452\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 3.576013\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 4.050449\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 4.183604\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 4.947180\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 4.819567\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 4.738090\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 4.599967\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 4.636342\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 4.591717\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 5.081535\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 4.966556\n",
      "Epoch: 1485/3000... Step: 47500... Loss: 1.017155... Val Loss: 4.984301\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 2.841362\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 2.419884\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 1.979565\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 1.807427\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 1.642053\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 2.146876\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 2.011000\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 2.113561\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 2.065840\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 2.171355\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 2.177910\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 2.118551\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 2.078344\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 2.762230\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 2.666998\n",
      "Epoch: 1488/3000... Step: 47600... Loss: 0.879109... Val Loss: 2.641436\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 2.819763\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 2.752102\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 2.420559\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 2.216957\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 2.146992\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 2.604059\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 2.456036\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 2.517676\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 2.466852\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 2.556548\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 2.583485\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 2.480304\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 2.435280\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 3.245273\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 3.205951\n",
      "Epoch: 1491/3000... Step: 47700... Loss: 1.994614... Val Loss: 3.205083\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 2.664679\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 2.287138\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 1.799255\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 1.590040\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 2.001491\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 2.515041\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 2.317419\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 2.366583\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 2.293348\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 2.294391\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 2.407032\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 2.396778\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 2.300872\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 2.936254\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 2.932132\n",
      "Epoch: 1494/3000... Step: 47800... Loss: 3.332127... Val Loss: 3.281602\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 3.221087\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 2.260007\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 2.301493\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 2.184191\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 2.138890\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 2.885871\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 2.874164\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 3.238105\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 3.144200\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 3.160614\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 3.014143\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 2.947286\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 2.862343\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 3.391848\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 3.298416\n",
      "Epoch: 1497/3000... Step: 47900... Loss: 4.052959... Val Loss: 3.252391\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 3.150358\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 2.337949\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 1.936249\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 1.823891\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 2.066340\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 2.980452\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 2.960284\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 3.306194\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 3.153679\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 3.355048\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 3.282634\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 3.321160\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 3.202974\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 3.735364\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 3.668539\n",
      "Epoch: 1500/3000... Step: 48000... Loss: 1.274496... Val Loss: 3.872716\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 2.597099\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 2.198089\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 1.858231\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 1.613159\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 1.478607\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 2.203268\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 2.073171\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 2.249836\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 2.184033\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 2.168400\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 2.175853\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 2.093363\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 2.046971\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 2.738340\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 2.672211\n",
      "Epoch: 1504/3000... Step: 48100... Loss: 1.058878... Val Loss: 2.632353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 3.408835\n",
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 3.432219\n",
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 3.537128\n",
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 3.163299\n",
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 3.232338\n",
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 3.749765\n",
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 3.542789\n",
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 3.626194\n",
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 3.540223\n",
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 3.758626\n",
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 3.601248\n",
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 3.441839\n",
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 3.361139\n",
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 4.036508\n",
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 4.011580\n",
      "Epoch: 1507/3000... Step: 48200... Loss: 2.812131... Val Loss: 4.206662\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 3.252002\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 2.303828\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 1.979671\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 1.902661\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 1.863958\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 2.404138\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 2.376678\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 3.011380\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 2.882459\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 2.739528\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 2.703429\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 2.636010\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 2.595988\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 3.109037\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 3.045976\n",
      "Epoch: 1510/3000... Step: 48300... Loss: 0.350376... Val Loss: 3.050645\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 4.650304\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 3.658263\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 2.963324\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 2.728342\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 2.703133\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 3.321195\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 3.250904\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 3.633122\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 3.574467\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 3.509522\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 3.359442\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 3.256980\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 3.187777\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 3.703707\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 3.663357\n",
      "Epoch: 1513/3000... Step: 48400... Loss: 2.497842... Val Loss: 3.615782\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 3.590261\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 2.560951\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 2.246412\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 2.355986\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 2.322519\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 2.820105\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 2.854292\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 3.332043\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 3.244536\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 3.143298\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 3.157145\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 3.116221\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 3.036638\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 3.559803\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 3.453870\n",
      "Epoch: 1516/3000... Step: 48500... Loss: 1.310404... Val Loss: 3.442000\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 2.773343\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 2.382290\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 1.848891\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 1.665294\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 1.509189\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 1.951965\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 1.857666\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 2.229576\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 2.147052\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 2.099915\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 2.079794\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 2.107405\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 2.031037\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 2.731199\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 2.654812\n",
      "Epoch: 1519/3000... Step: 48600... Loss: 2.295637... Val Loss: 2.592872\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 2.891982\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 2.385334\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 1.971158\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 1.737902\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 1.588001\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 2.114879\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 2.037280\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 2.281506\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 2.229319\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 2.302376\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 2.201829\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 2.151241\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 2.104732\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 2.730326\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 2.632204\n",
      "Epoch: 1522/3000... Step: 48700... Loss: 2.693191... Val Loss: 2.578373\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 3.168796\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 2.342747\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 1.991746\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 1.726809\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 1.606893\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 2.132788\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 2.065734\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 2.401356\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 2.320501\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 2.293880\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 2.282413\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 2.298116\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 2.231663\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 2.771142\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 2.666140\n",
      "Epoch: 1525/3000... Step: 48800... Loss: 0.490561... Val Loss: 2.705515\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 3.359773\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 2.400247\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 1.956474\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 1.687121\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 1.702019\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 2.189918\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 2.131531\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 2.625905\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 2.508582\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 2.457113\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 2.364662\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 2.281899\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 2.214064\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 2.711250\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 2.637495\n",
      "Epoch: 1529/3000... Step: 48900... Loss: 1.468709... Val Loss: 2.705768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 3.155638\n",
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 2.387274\n",
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 2.047975\n",
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 1.903560\n",
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 1.739033\n",
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 2.224276\n",
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 2.119122\n",
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 2.441920\n",
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 2.365757\n",
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 2.351808\n",
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 2.292151\n",
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 2.234112\n",
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 2.165333\n",
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 2.752936\n",
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 2.651087\n",
      "Epoch: 1532/3000... Step: 49000... Loss: 0.532959... Val Loss: 2.669670\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 3.190831\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 2.446298\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 2.342792\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 2.030217\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 2.098474\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 2.561987\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 2.552624\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 2.859387\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 2.781252\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 2.721800\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 2.741959\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 2.699499\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 2.598352\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 3.146674\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 3.115509\n",
      "Epoch: 1535/3000... Step: 49100... Loss: 0.743828... Val Loss: 3.162159\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 2.251133\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 2.111368\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 1.746815\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 1.610452\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 1.442556\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 1.957800\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 1.791826\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 1.909142\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 1.859435\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 1.915723\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 1.981449\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 1.986094\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 1.939955\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 2.688260\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 2.606558\n",
      "Epoch: 1538/3000... Step: 49200... Loss: 0.472960... Val Loss: 2.597945\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 2.118167\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 2.001203\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 1.703946\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 1.537888\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 1.374527\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 2.108117\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 1.950201\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 2.101090\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 2.019957\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 2.026602\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 2.122478\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 2.082865\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 2.024220\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 2.727950\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 2.635034\n",
      "Epoch: 1541/3000... Step: 49300... Loss: 1.267907... Val Loss: 2.591593\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 3.207398\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 2.299362\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 1.899453\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 1.723407\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 1.664457\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 2.282787\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 2.278427\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 2.810817\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 2.710088\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 2.639434\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 2.575390\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 2.539636\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 2.482567\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 3.018863\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 2.927121\n",
      "Epoch: 1544/3000... Step: 49400... Loss: 2.999140... Val Loss: 2.932809\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 3.141244\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 2.439035\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 1.978745\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 1.745448\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 1.746314\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 2.662520\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 2.535428\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 2.779269\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 2.659586\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 2.682833\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 2.590195\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 2.536981\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 2.454984\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 2.995178\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 2.892345\n",
      "Epoch: 1547/3000... Step: 49500... Loss: 2.050606... Val Loss: 2.867615\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 3.374091\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 2.590404\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 2.199740\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 1.830907\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 1.923597\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 3.020076\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 2.836213\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 3.050938\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 2.869623\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 3.075539\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 2.851237\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 2.744606\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 2.666015\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 3.248250\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 3.136194\n",
      "Epoch: 1550/3000... Step: 49600... Loss: 1.075552... Val Loss: 3.331534\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 2.876871\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 2.137519\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 1.903994\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 1.672835\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 1.712803\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 2.386283\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 2.265956\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 2.675702\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 2.555835\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 2.582317\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 2.459968\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 2.403929\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 2.337029\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 2.836734\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 2.802101\n",
      "Epoch: 1554/3000... Step: 49700... Loss: 0.956390... Val Loss: 2.835290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 3.797604\n",
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 3.332546\n",
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 3.009464\n",
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 2.771741\n",
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 2.822983\n",
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 3.807608\n",
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 3.728003\n",
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 4.193136\n",
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 4.064519\n",
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 4.009857\n",
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 3.813672\n",
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 3.714318\n",
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 3.623920\n",
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 4.083981\n",
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 3.999554\n",
      "Epoch: 1557/3000... Step: 49800... Loss: 1.006150... Val Loss: 4.033588\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 2.802515\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 2.118019\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 1.775540\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 1.640698\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 1.632934\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 2.212473\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 2.109297\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 2.539416\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 2.461305\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 2.434536\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 2.376040\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 2.344476\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 2.267570\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 2.804641\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 2.732741\n",
      "Epoch: 1560/3000... Step: 49900... Loss: 0.215452... Val Loss: 2.688299\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 3.874856\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 3.991582\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 3.584394\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 3.352562\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 3.122843\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 4.034046\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 3.853716\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 3.867517\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 3.792533\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 3.855809\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 3.724327\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 3.568150\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 3.519842\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 4.283277\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 4.211581\n",
      "Epoch: 1563/3000... Step: 50000... Loss: 2.401921... Val Loss: 4.216341\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 2.964340\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 2.215633\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 1.804174\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 1.619153\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 1.494220\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 2.177871\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 2.117764\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 2.638384\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 2.504868\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 2.469360\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 2.455851\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 2.342430\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 2.278869\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 2.768248\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 2.732934\n",
      "Epoch: 1566/3000... Step: 50100... Loss: 0.728103... Val Loss: 2.734205\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 2.990707\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 2.285437\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 2.055506\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 1.810390\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 1.718946\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 2.285243\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 2.225276\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 2.595419\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 2.492114\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 2.487320\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 2.377542\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 2.315443\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 2.250395\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 2.762290\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 2.681068\n",
      "Epoch: 1569/3000... Step: 50200... Loss: 2.944602... Val Loss: 2.683430\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 3.002251\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 2.675713\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 2.205105\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 2.060198\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 1.859751\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 2.882790\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 2.734107\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 2.832948\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 2.747094\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 2.796516\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 2.661125\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 2.630444\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 2.557445\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 3.204299\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 3.114848\n",
      "Epoch: 1572/3000... Step: 50300... Loss: 2.511433... Val Loss: 3.034743\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 3.162074\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 2.194935\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 1.798422\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 1.583816\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 1.577250\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 2.207474\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 2.158272\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 2.493803\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 2.381591\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 2.446052\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 2.344448\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 2.357027\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 2.275197\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 2.724158\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 2.630240\n",
      "Epoch: 1575/3000... Step: 50400... Loss: 0.470828... Val Loss: 2.672635\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 7.282189\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 5.168070\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 4.455140\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 4.073702\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 4.302211\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 4.714754\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 4.807863\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 6.003114\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 5.768419\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 5.608125\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 5.417189\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 5.347665\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 5.280848\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 5.681423\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 5.525361\n",
      "Epoch: 1579/3000... Step: 50500... Loss: 1.531101... Val Loss: 5.525253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 3.365208\n",
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 2.858252\n",
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 2.438632\n",
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 2.204564\n",
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 2.121650\n",
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 2.673078\n",
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 2.616786\n",
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 2.802374\n",
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 2.748950\n",
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 2.895641\n",
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 2.783921\n",
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 2.769641\n",
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 2.695936\n",
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 3.287412\n",
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 3.198481\n",
      "Epoch: 1582/3000... Step: 50600... Loss: 0.662876... Val Loss: 3.194749\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 2.719249\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 2.182994\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 1.837721\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 1.729812\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 1.653969\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 2.653644\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 2.561242\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 2.900131\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 2.776674\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 2.662566\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 2.554994\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 2.515298\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 2.451698\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 2.932745\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 2.938937\n",
      "Epoch: 1585/3000... Step: 50700... Loss: 0.341809... Val Loss: 2.920281\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 3.953291\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 2.604480\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 2.011570\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 1.745954\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 1.723252\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 2.244774\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 2.317549\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 3.045750\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 2.936986\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 2.861966\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 2.924033\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 2.876130\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 2.782711\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 3.187646\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 3.099499\n",
      "Epoch: 1588/3000... Step: 50800... Loss: 0.940179... Val Loss: 3.045800\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 2.851369\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 2.389939\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 1.987534\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 1.738614\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 1.665592\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 2.357320\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 2.305318\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 2.490516\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 2.411909\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 2.581206\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 2.583566\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 2.560958\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 2.499472\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 3.167456\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 3.082636\n",
      "Epoch: 1591/3000... Step: 50900... Loss: 0.948604... Val Loss: 3.011971\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 3.957809\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 3.693933\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 3.207665\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 3.029417\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 2.926820\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 3.639334\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 3.667324\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 3.883253\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 3.831748\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 3.900022\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 3.752065\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 3.695799\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 3.607719\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 4.189749\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 4.077909\n",
      "Epoch: 1594/3000... Step: 51000... Loss: 4.050088... Val Loss: 4.026480\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 3.059557\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 2.265707\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 1.790198\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 1.548496\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 1.546700\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 2.320097\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 2.216091\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 2.609452\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 2.476537\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 2.431941\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 2.317683\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 2.313681\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 2.237375\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 2.759038\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 2.655731\n",
      "Epoch: 1597/3000... Step: 51100... Loss: 1.673350... Val Loss: 2.598237\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 2.930364\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 2.750860\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 2.236674\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 2.092658\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 1.801113\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 2.567759\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 2.400642\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 2.579483\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 2.530271\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 2.715780\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 2.618246\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 2.615694\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 2.583537\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 3.297181\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 3.183895\n",
      "Epoch: 1600/3000... Step: 51200... Loss: 1.088260... Val Loss: 3.115164\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 2.581513\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 2.055910\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 1.683848\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 1.509108\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 1.481676\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 2.156500\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 2.070248\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 2.311899\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 2.225834\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 2.210454\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 2.079244\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 2.121021\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 2.064641\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 2.625815\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 2.534569\n",
      "Epoch: 1604/3000... Step: 51300... Loss: 0.519327... Val Loss: 2.558848\n",
      "Validation loss decreased (2.576084 --> 2.558848).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 4.234678\n",
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 2.965331\n",
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 2.311703\n",
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 2.045207\n",
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 3.095305\n",
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 4.072762\n",
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 3.966476\n",
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 4.568390\n",
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 4.380650\n",
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 4.132510\n",
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 3.986998\n",
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 3.961846\n",
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 3.897866\n",
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 4.258468\n",
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 4.375468\n",
      "Epoch: 1607/3000... Step: 51400... Loss: 1.021512... Val Loss: 4.989777\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 3.237141\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 2.373383\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 2.054217\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 1.817642\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 1.649284\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 2.297260\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 2.282801\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 2.746190\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 2.620075\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 2.543119\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 2.611777\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 2.568950\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 2.492055\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 2.992166\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 2.875493\n",
      "Epoch: 1610/3000... Step: 51500... Loss: 0.351244... Val Loss: 2.861183\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 3.528933\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 3.465214\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 3.016887\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 2.833257\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 2.677668\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 3.323775\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 3.205552\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 3.285274\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 3.242693\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 3.315911\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 3.189088\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 3.329800\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 3.271765\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 3.907872\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 3.815115\n",
      "Epoch: 1613/3000... Step: 51600... Loss: 1.616498... Val Loss: 3.763739\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 4.324229\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 3.450809\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 3.130799\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 2.944328\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 2.940307\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 3.553616\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 3.555156\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 4.126683\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 4.019459\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 3.863536\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 3.773799\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 3.891598\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 3.812823\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 4.307285\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 4.240934\n",
      "Epoch: 1616/3000... Step: 51700... Loss: 0.456230... Val Loss: 4.161590\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 2.786861\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 2.306592\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 1.787840\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 1.580065\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 1.722911\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 2.348717\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 2.189942\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 2.373264\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 2.294743\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 2.350450\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 2.251586\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 2.254806\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 2.200813\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 2.855853\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 2.767001\n",
      "Epoch: 1619/3000... Step: 51800... Loss: 2.376410... Val Loss: 2.766506\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 2.882696\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 2.764342\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 2.305769\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 2.116003\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 2.171509\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 2.779871\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 2.604822\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 2.691200\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 2.619171\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 2.617270\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 2.480520\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 2.468170\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 2.421523\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 3.107022\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 3.101213\n",
      "Epoch: 1622/3000... Step: 51900... Loss: 2.961465... Val Loss: 3.399823\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 2.889116\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 2.169063\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 1.943273\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 1.741837\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 1.552133\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 2.217485\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 2.164887\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 2.510001\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 2.398078\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 2.433867\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 2.306726\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 2.276366\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 2.202573\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 2.766876\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 2.690662\n",
      "Epoch: 1625/3000... Step: 52000... Loss: 0.280072... Val Loss: 2.688677\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 3.524090\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 2.680817\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 2.090352\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 1.707821\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 1.666717\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 2.229806\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 2.106278\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 2.270515\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 2.219151\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 2.301396\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 2.245696\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 2.274672\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 2.218340\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 2.861769\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 2.792356\n",
      "Epoch: 1629/3000... Step: 52100... Loss: 1.716231... Val Loss: 2.730740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 2.808102\n",
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 2.149993\n",
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 1.919002\n",
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 1.700072\n",
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 1.649562\n",
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 2.251118\n",
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 2.148415\n",
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 2.364879\n",
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 2.281103\n",
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 2.259695\n",
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 2.135196\n",
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 2.067572\n",
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 1.983266\n",
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 2.521975\n",
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 2.450485\n",
      "Epoch: 1632/3000... Step: 52200... Loss: 0.456367... Val Loss: 2.401995\n",
      "Validation loss decreased (2.558848 --> 2.401995).  Saving model ...\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 3.263153\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 2.364223\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 1.922703\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 1.667022\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 1.621375\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 2.402624\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 2.353818\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 2.829256\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 2.719825\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 2.695815\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 2.830921\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 2.888554\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 2.776438\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 3.198152\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 3.163117\n",
      "Epoch: 1635/3000... Step: 52300... Loss: 0.745780... Val Loss: 3.121621\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 3.023510\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 2.610162\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 2.189070\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 2.071891\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 1.958767\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 2.555510\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 2.521805\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 2.885970\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 2.805524\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 2.863489\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 2.762153\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 2.784769\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 2.702005\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 3.282404\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 3.181695\n",
      "Epoch: 1638/3000... Step: 52400... Loss: 0.543786... Val Loss: 3.150370\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 2.856888\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 2.377583\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 2.088043\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 1.931697\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 1.691477\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 2.146159\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 2.035704\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 2.443121\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 2.366266\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 2.409103\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 2.422495\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 2.363562\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 2.322331\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 2.822223\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 2.877095\n",
      "Epoch: 1641/3000... Step: 52500... Loss: 0.860946... Val Loss: 3.048976\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 4.458110\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 3.324501\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 3.032376\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 2.665876\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 2.518506\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 3.019841\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 3.130758\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 3.996886\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 3.880076\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 3.800962\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 3.632013\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 3.507417\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 3.412310\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 3.802475\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 3.753193\n",
      "Epoch: 1644/3000... Step: 52600... Loss: 3.848223... Val Loss: 3.884508\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 2.707253\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 2.106051\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 1.722018\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 1.482354\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 1.395208\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 2.233804\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 2.124067\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 2.374565\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 2.280950\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 2.280281\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 2.194366\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 2.115100\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 2.048180\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 2.552525\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 2.466274\n",
      "Epoch: 1647/3000... Step: 52700... Loss: 1.879060... Val Loss: 2.447624\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 3.068295\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 2.316649\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 2.072089\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 1.763244\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 2.206765\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 2.657293\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 2.503300\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 2.970265\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 2.796850\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 2.764083\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 2.625169\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 2.533939\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 2.448590\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 2.918677\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 2.912294\n",
      "Epoch: 1650/3000... Step: 52800... Loss: 1.912847... Val Loss: 3.529926\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 2.293208\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 1.907233\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 1.730476\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 1.595030\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 1.425341\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 2.137403\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 2.005471\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 2.191051\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 2.086025\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 2.086370\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 1.997889\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 1.943221\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 1.912308\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 2.556010\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 2.499873\n",
      "Epoch: 1654/3000... Step: 52900... Loss: 0.912165... Val Loss: 2.597296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 3.364325\n",
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 2.493613\n",
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 2.138478\n",
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 2.008959\n",
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 1.828568\n",
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 2.413749\n",
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 2.429412\n",
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 3.080142\n",
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 2.964623\n",
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 2.895096\n",
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 2.875026\n",
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 2.769162\n",
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 2.682218\n",
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 3.141239\n",
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 3.169122\n",
      "Epoch: 1657/3000... Step: 53000... Loss: 0.694111... Val Loss: 3.180436\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 2.287679\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 1.829845\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 1.535937\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 1.420068\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 1.308906\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 1.930808\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 1.839911\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 2.158292\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 2.087699\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 2.102914\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 2.115925\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 2.065902\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 2.011108\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 2.570979\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 2.494307\n",
      "Epoch: 1660/3000... Step: 53100... Loss: 0.402274... Val Loss: 2.479245\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 3.861516\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 2.661085\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 2.177677\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 1.950881\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 2.065643\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 2.592195\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 2.579853\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 2.997665\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 2.891207\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 2.793374\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 2.762122\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 2.709635\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 2.621901\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 3.081612\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 3.008154\n",
      "Epoch: 1663/3000... Step: 53200... Loss: 1.731273... Val Loss: 2.938991\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 3.324917\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 2.351606\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 1.874800\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 1.708552\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 1.695933\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 2.319158\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 2.380909\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 3.090295\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 2.944564\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 2.826180\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 2.680585\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 2.641089\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 2.562645\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 2.994045\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 2.916322\n",
      "Epoch: 1666/3000... Step: 53300... Loss: 0.649469... Val Loss: 2.845671\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 2.960866\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 2.260952\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 1.753164\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 1.515134\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 1.464263\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 2.189202\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 2.085665\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 2.448576\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 2.365973\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 2.371233\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 2.359840\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 2.346177\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 2.291599\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 2.832812\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 2.728158\n",
      "Epoch: 1669/3000... Step: 53400... Loss: 3.130633... Val Loss: 2.712309\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 2.772036\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 2.624045\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 2.230224\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 2.056307\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 1.980897\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 2.556283\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 2.406730\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 2.517938\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 2.458273\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 2.626712\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 2.494048\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 2.499484\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 2.458354\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 3.097269\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 2.986748\n",
      "Epoch: 1672/3000... Step: 53500... Loss: 2.118798... Val Loss: 2.950211\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 2.604767\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 1.974521\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 1.641674\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 1.435009\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 1.353517\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 2.173685\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 2.051066\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 2.410832\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 2.328315\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 2.314030\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 2.219715\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 2.320763\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 2.228689\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 2.739734\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 2.652498\n",
      "Epoch: 1675/3000... Step: 53600... Loss: 0.296664... Val Loss: 2.632323\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 2.667885\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 2.277257\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 1.827636\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 1.471028\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 1.439108\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 2.528277\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 2.291909\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 2.406546\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 2.297994\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 2.392770\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 2.233842\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 2.199219\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 2.124312\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 2.827511\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 2.734359\n",
      "Epoch: 1679/3000... Step: 53700... Loss: 1.699158... Val Loss: 2.708531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 2.360065\n",
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 2.168974\n",
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 1.731041\n",
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 1.573704\n",
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 1.538528\n",
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 2.123310\n",
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 1.985291\n",
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 2.130377\n",
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 2.057449\n",
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 2.135224\n",
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 2.020263\n",
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 2.017520\n",
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 1.963685\n",
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 2.671803\n",
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 2.612433\n",
      "Epoch: 1682/3000... Step: 53800... Loss: 0.515051... Val Loss: 2.623272\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 2.932052\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 2.133499\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 1.780889\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 1.654732\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 1.585094\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 2.147998\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 2.096072\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 2.508237\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 2.409482\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 2.379835\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 2.489517\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 2.455614\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 2.393474\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 2.875645\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 2.828530\n",
      "Epoch: 1685/3000... Step: 53900... Loss: 0.449076... Val Loss: 2.809145\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 2.661704\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 2.262796\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 1.826754\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 1.617649\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 1.509247\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 2.756076\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 2.559190\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 2.695960\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 2.600385\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 2.557771\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 2.562864\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 2.484328\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 2.402380\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 3.033363\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 2.949292\n",
      "Epoch: 1688/3000... Step: 54000... Loss: 1.042758... Val Loss: 2.942356\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 2.679458\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 1.994789\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 1.647385\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 1.551393\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 1.705149\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 2.324189\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 2.270978\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 2.651765\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 2.533604\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 2.486949\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 2.392054\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 2.384291\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 2.296924\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 2.850729\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 2.753100\n",
      "Epoch: 1691/3000... Step: 54100... Loss: 0.726930... Val Loss: 2.749924\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 2.792698\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 2.209831\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 1.738087\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 1.540848\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 1.483074\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 2.228690\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 2.195810\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 2.611640\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 2.526664\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 2.488619\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 2.371431\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 2.331757\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 2.253512\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 2.748138\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 2.680062\n",
      "Epoch: 1694/3000... Step: 54200... Loss: 2.967024... Val Loss: 2.686970\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 2.459385\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 1.939314\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 1.579601\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 1.374723\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 1.258580\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 1.975086\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 1.910283\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 2.197936\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 2.126588\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 2.193735\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 2.195191\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 2.249300\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 2.180688\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 2.743992\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 2.653778\n",
      "Epoch: 1697/3000... Step: 54300... Loss: 1.933109... Val Loss: 2.604551\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 2.748768\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 2.184245\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 1.942104\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 1.667693\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 2.094082\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 2.619362\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 2.423965\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 2.554879\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 2.419134\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 2.433515\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 2.312987\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 2.278956\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 2.197699\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 2.887806\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 2.889566\n",
      "Epoch: 1700/3000... Step: 54400... Loss: 0.540912... Val Loss: 3.571796\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 2.893247\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 2.231990\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 1.820481\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 1.611396\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 1.479303\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 2.099017\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 2.036946\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 2.349079\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 2.228833\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 2.282895\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 2.266747\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 2.294688\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 2.215128\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 2.695458\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 2.610459\n",
      "Epoch: 1704/3000... Step: 54500... Loss: 1.369730... Val Loss: 2.592207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 3.368445\n",
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 2.365194\n",
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 1.849269\n",
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 1.604499\n",
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 1.547745\n",
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 2.688057\n",
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 2.666907\n",
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 3.617076\n",
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 3.401618\n",
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 3.300722\n",
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 3.178232\n",
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 3.089993\n",
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 2.983483\n",
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 3.387267\n",
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 3.343503\n",
      "Epoch: 1707/3000... Step: 54600... Loss: 1.301624... Val Loss: 3.301294\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 2.660744\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 2.005192\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 1.731313\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 1.485823\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 1.334463\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 2.086763\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 1.967622\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 2.404768\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 2.292396\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 2.306962\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 2.296940\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 2.303904\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 2.233939\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 2.731831\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 2.646869\n",
      "Epoch: 1710/3000... Step: 54700... Loss: 0.283438... Val Loss: 2.607412\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 3.647935\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 3.547780\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 3.045540\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 2.835957\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 2.761406\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 3.734503\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 3.562186\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 3.761859\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 3.682130\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 3.676598\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 3.541618\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 3.509947\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 3.447992\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 4.038917\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 3.994119\n",
      "Epoch: 1713/3000... Step: 54800... Loss: 1.755084... Val Loss: 4.004106\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 2.994505\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 2.107358\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 1.659448\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 1.452115\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 1.416047\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 2.069629\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 2.011209\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 2.575619\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 2.465333\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 2.360397\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 2.280655\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 2.261925\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 2.194807\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 2.679756\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 2.636596\n",
      "Epoch: 1716/3000... Step: 54900... Loss: 0.673671... Val Loss: 2.622970\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 2.799962\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 2.568822\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 2.191034\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 1.998850\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 2.017307\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 2.585160\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 2.383932\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 2.366902\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 2.285448\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 2.452076\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 2.354789\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 2.296296\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 2.225916\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 2.910607\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 2.932967\n",
      "Epoch: 1719/3000... Step: 55000... Loss: 2.968171... Val Loss: 3.471630\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 3.604318\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 2.814919\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 2.243947\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 1.986163\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 1.984285\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 2.672534\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 2.654046\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 3.175346\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 3.077841\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 3.042548\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 2.918924\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 2.852785\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 2.768930\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 3.259909\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 3.232384\n",
      "Epoch: 1722/3000... Step: 55100... Loss: 1.777537... Val Loss: 3.586025\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 3.055021\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 2.013854\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 1.646234\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 1.541490\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 1.576926\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 2.343206\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 2.270938\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 2.733208\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 2.592645\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 2.543941\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 2.510820\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 2.439178\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 2.369363\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 2.783109\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 2.691363\n",
      "Epoch: 1725/3000... Step: 55200... Loss: 1.277147... Val Loss: 2.655389\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 3.332423\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 2.261672\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 2.056378\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 1.924188\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 1.804368\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 2.304026\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 2.304311\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 2.931772\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 2.770338\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 2.998743\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 2.949308\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 2.825289\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 2.755646\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 3.170113\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 3.054150\n",
      "Epoch: 1729/3000... Step: 55300... Loss: 1.442161... Val Loss: 2.975600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 2.481144\n",
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 1.984606\n",
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 1.805268\n",
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 1.543595\n",
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 1.568924\n",
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 2.260989\n",
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 2.154747\n",
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 2.476416\n",
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 2.377320\n",
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 2.417149\n",
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 2.396738\n",
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 2.351768\n",
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 2.273438\n",
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 2.737383\n",
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 2.669884\n",
      "Epoch: 1732/3000... Step: 55400... Loss: 0.466104... Val Loss: 2.992865\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 3.878522\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 2.546482\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 2.005499\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 1.678260\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 1.864344\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 2.352039\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 2.328991\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 2.822956\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 2.716178\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 2.597985\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 2.560452\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 2.616911\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 2.522334\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 3.053922\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 2.976424\n",
      "Epoch: 1735/3000... Step: 55500... Loss: 1.423162... Val Loss: 3.070361\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 3.098464\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 2.696107\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 2.318783\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 2.171031\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 2.039445\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 2.712039\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 2.612725\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 2.879167\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 2.819441\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 2.946133\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 2.856640\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 2.848889\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 2.770087\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 3.326903\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 3.248088\n",
      "Epoch: 1738/3000... Step: 55600... Loss: 0.709761... Val Loss: 3.250374\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 2.348493\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 1.872130\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 1.468474\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 1.585638\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 2.300892\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 2.904760\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 2.841726\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 3.164435\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 3.045711\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 3.036646\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 2.864979\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 2.770581\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 2.658266\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 3.133891\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 3.102057\n",
      "Epoch: 1741/3000... Step: 55700... Loss: 0.789711... Val Loss: 3.253109\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 3.547479\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 2.297967\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 1.826569\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 1.735111\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 1.744299\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 2.399288\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 2.341505\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 2.875836\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 2.772051\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 2.681123\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 2.553944\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 2.501032\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 2.445633\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 2.842724\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 2.833661\n",
      "Epoch: 1744/3000... Step: 55800... Loss: 3.610953... Val Loss: 2.809941\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 2.832608\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 2.037295\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 1.749253\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 1.578567\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 1.566814\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 2.384548\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 2.290867\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 2.762065\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 2.615805\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 2.515620\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 2.411305\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 2.387925\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 2.308481\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 2.744425\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 2.669210\n",
      "Epoch: 1747/3000... Step: 55900... Loss: 1.502458... Val Loss: 2.627716\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 2.772641\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 2.120365\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 1.972498\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 1.888683\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 1.889156\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 3.124111\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 2.897366\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 3.203225\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 3.056149\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 3.086415\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 2.936509\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 2.861508\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 2.752998\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 3.230130\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 3.195712\n",
      "Epoch: 1750/3000... Step: 56000... Loss: 0.867542... Val Loss: 3.245442\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 2.709200\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 2.037617\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 1.608017\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 1.442896\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 1.429711\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 2.261613\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 2.188527\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 2.395671\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 2.309301\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 2.306301\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 2.226856\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 2.212440\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 2.140588\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 2.725732\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 2.639146\n",
      "Epoch: 1754/3000... Step: 56100... Loss: 0.616700... Val Loss: 2.585051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 3.547237\n",
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 2.682830\n",
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 2.017531\n",
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 1.725518\n",
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 1.660437\n",
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 2.370151\n",
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 2.263493\n",
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 2.705471\n",
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 2.589783\n",
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 2.662450\n",
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 2.555167\n",
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 2.550067\n",
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 2.480889\n",
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 2.986373\n",
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 2.882954\n",
      "Epoch: 1757/3000... Step: 56200... Loss: 1.242678... Val Loss: 2.818758\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 2.602445\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 2.022084\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 1.634215\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 1.451128\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 1.700044\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 2.510998\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 2.416418\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 2.677952\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 2.541951\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 2.425411\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 2.321719\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 2.264685\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 2.190266\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 2.773657\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 2.725485\n",
      "Epoch: 1760/3000... Step: 56300... Loss: 0.227763... Val Loss: 2.800662\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 4.191133\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 4.015601\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 3.743798\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 3.567949\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 3.425201\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 4.041278\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 3.945282\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 4.093027\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 4.072600\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 4.007271\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 3.871186\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 3.812525\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 3.752221\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 4.377574\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 4.357879\n",
      "Epoch: 1763/3000... Step: 56400... Loss: 3.440905... Val Loss: 4.286658\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 2.466280\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 1.931272\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 1.498092\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 1.423505\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 1.323377\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 1.939480\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 1.921695\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 2.244033\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 2.158320\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 2.130423\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 2.112351\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 2.065833\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 1.997635\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 2.552339\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 2.468598\n",
      "Epoch: 1766/3000... Step: 56500... Loss: 0.659458... Val Loss: 2.433450\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 3.391716\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 2.845310\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 2.338324\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 2.204897\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 2.163599\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 2.735127\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 2.658711\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 2.910268\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 2.849343\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 2.924350\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 2.830159\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 2.750844\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 2.697880\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 3.232403\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 3.161301\n",
      "Epoch: 1769/3000... Step: 56600... Loss: 3.228406... Val Loss: 3.114264\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 5.184908\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 3.924354\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 3.631980\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 3.427889\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 3.780681\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 4.236259\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 4.226053\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 4.961475\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 4.749089\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 4.619752\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 4.474851\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 4.460417\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 4.412864\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 4.791305\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 4.764476\n",
      "Epoch: 1772/3000... Step: 56700... Loss: 3.105582... Val Loss: 4.960576\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 3.071333\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 2.089028\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 1.766869\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 1.507666\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 1.535619\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 2.119835\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 2.068252\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 2.551320\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 2.450307\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 2.422383\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 2.276456\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 2.229853\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 2.175715\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 2.654328\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 2.581134\n",
      "Epoch: 1775/3000... Step: 56800... Loss: 0.815198... Val Loss: 2.541548\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 3.127351\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 2.852446\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 2.447599\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 2.145582\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 1.870432\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 2.499303\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 2.360731\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 2.318370\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 2.267002\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 2.637526\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 2.496366\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 2.377720\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 2.337369\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 3.049815\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 3.029246\n",
      "Epoch: 1779/3000... Step: 56900... Loss: 1.423469... Val Loss: 3.047402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 2.965253\n",
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 2.209333\n",
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 1.728539\n",
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 1.631092\n",
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 1.531412\n",
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 2.165552\n",
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 2.168113\n",
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 2.418999\n",
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 2.363938\n",
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 2.415542\n",
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 2.333829\n",
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 2.341349\n",
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 2.263966\n",
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 2.772756\n",
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 2.667325\n",
      "Epoch: 1782/3000... Step: 57000... Loss: 0.393934... Val Loss: 2.701231\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 8.036890\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 6.392936\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 5.849770\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 5.691985\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 6.523487\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 7.690648\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 7.584831\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 8.162721\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 7.977898\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 7.766291\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 7.617513\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 7.605298\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 7.472672\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 7.847510\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 7.774355\n",
      "Epoch: 1785/3000... Step: 57100... Loss: 3.444088... Val Loss: 8.035520\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 4.155012\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 2.896388\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 2.598271\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 2.375615\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 2.384502\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 3.153476\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 3.134258\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 3.898077\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 3.735350\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 3.553872\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 3.484088\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 3.487118\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 3.460104\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 3.844309\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 3.770299\n",
      "Epoch: 1788/3000... Step: 57200... Loss: 1.500495... Val Loss: 3.753791\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 3.372526\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 2.134080\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 1.689338\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 1.510704\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 1.482132\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 2.038254\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 2.031868\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 2.787186\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 2.646268\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 2.574692\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 2.548824\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 2.521945\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 2.453032\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 2.867870\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 2.843134\n",
      "Epoch: 1791/3000... Step: 57300... Loss: 1.123306... Val Loss: 2.802427\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 3.177119\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 2.512097\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 2.045269\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 1.843886\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 1.702440\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 2.368836\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 2.265585\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 2.567172\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 2.462791\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 2.486130\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 2.460035\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 2.417975\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 2.351819\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 2.952527\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 2.883634\n",
      "Epoch: 1794/3000... Step: 57400... Loss: 3.778720... Val Loss: 2.904485\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 3.305245\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 2.250896\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 1.913824\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 1.674006\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 1.673177\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 2.272162\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 2.279527\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 2.730528\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 2.625185\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 2.643572\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 2.521997\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 2.480450\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 2.411863\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 2.887948\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 2.783367\n",
      "Epoch: 1797/3000... Step: 57500... Loss: 1.562587... Val Loss: 2.755676\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 4.053546\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 3.117550\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 2.830535\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 2.452284\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 2.390950\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 2.843761\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 2.647010\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 2.742817\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 2.642630\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 2.820896\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 2.686039\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 2.676493\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 2.655960\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 3.318173\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 3.344239\n",
      "Epoch: 1800/3000... Step: 57600... Loss: 1.450789... Val Loss: 3.419810\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 2.778304\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 2.033385\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 1.741893\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 1.485431\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 1.506114\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 2.025163\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 1.914281\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 2.244546\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 2.152553\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 2.184294\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 2.041653\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 2.007809\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 1.930875\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 2.459253\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 2.518908\n",
      "Epoch: 1804/3000... Step: 57700... Loss: 1.071983... Val Loss: 2.606112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 3.116274\n",
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 2.469431\n",
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 1.966204\n",
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 1.758283\n",
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 1.848243\n",
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 3.190112\n",
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 3.061166\n",
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 3.448019\n",
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 3.294793\n",
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 3.215368\n",
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 3.045309\n",
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 2.921491\n",
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 2.810578\n",
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 3.269476\n",
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 3.142718\n",
      "Epoch: 1807/3000... Step: 57800... Loss: 0.796821... Val Loss: 3.094050\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 2.855467\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 1.921524\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 1.626024\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 1.501258\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 1.461616\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 2.111159\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 2.030316\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 2.429222\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 2.332944\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 2.260435\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 2.228779\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 2.172150\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 2.125255\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 2.639088\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 2.610542\n",
      "Epoch: 1810/3000... Step: 57900... Loss: 0.142963... Val Loss: 2.641710\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 3.564247\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 2.412614\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 1.850538\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 1.636642\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 1.506033\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 2.650659\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 2.486287\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 2.907867\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 2.748217\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 2.645203\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 2.612887\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 2.689355\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 2.585257\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 3.115389\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 3.074788\n",
      "Epoch: 1813/3000... Step: 58000... Loss: 1.583736... Val Loss: 2.981372\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 3.621061\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 2.684917\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 2.237316\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 1.981844\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 1.891198\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 2.674661\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 2.653052\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 3.069831\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 2.931455\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 2.942389\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 2.893635\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 3.008075\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 2.922170\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 3.508871\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 3.422354\n",
      "Epoch: 1816/3000... Step: 58100... Loss: 1.239334... Val Loss: 3.416003\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 2.445826\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 2.272100\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 1.939411\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 1.706246\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 1.743330\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 2.387575\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 2.213830\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 2.246806\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 2.153301\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 2.265041\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 2.132482\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 2.010352\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 1.951532\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 2.600453\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 2.530310\n",
      "Epoch: 1819/3000... Step: 58200... Loss: 2.151774... Val Loss: 2.591981\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 3.986852\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 3.385577\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 2.585753\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 2.291045\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 2.395042\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 3.081867\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 2.842193\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 2.870872\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 2.759397\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 2.880017\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 2.884930\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 2.784352\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 2.756987\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 3.357912\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 3.270152\n",
      "Epoch: 1822/3000... Step: 58300... Loss: 2.606236... Val Loss: 3.345443\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 3.320201\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 2.295831\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 1.844735\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 1.694710\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 1.649326\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 2.414638\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 2.370861\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 2.966684\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 2.835224\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 2.795682\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 2.727446\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 2.645635\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 2.548469\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 2.986730\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 2.906974\n",
      "Epoch: 1825/3000... Step: 58400... Loss: 0.854306... Val Loss: 2.853734\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 7.552858\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 5.910518\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 5.472510\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 5.146996\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 5.364739\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 6.115072\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 6.132122\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 6.872000\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 6.710232\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 6.592285\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 6.493854\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 6.540352\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 6.471662\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 6.898049\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 6.814433\n",
      "Epoch: 1829/3000... Step: 58500... Loss: 1.874968... Val Loss: 6.852970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 2.555536\n",
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 2.364134\n",
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 1.952122\n",
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 1.915376\n",
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 1.780387\n",
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 2.440621\n",
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 2.310562\n",
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 2.490060\n",
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 2.427722\n",
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 2.494281\n",
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 2.418430\n",
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 2.367351\n",
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 2.311133\n",
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 2.913703\n",
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 2.863774\n",
      "Epoch: 1832/3000... Step: 58600... Loss: 0.555275... Val Loss: 2.843058\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 3.470099\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 2.289570\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 1.937046\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 1.740878\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 1.687513\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 2.479265\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 2.522233\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 3.393047\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 3.205230\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 3.021882\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 3.002246\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 2.932534\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 2.862849\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 3.214223\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 3.203643\n",
      "Epoch: 1835/3000... Step: 58700... Loss: 0.477002... Val Loss: 3.113704\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 4.065732\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 2.672394\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 2.199699\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 1.923520\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 1.968487\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 2.450756\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 2.564492\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 3.396893\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 3.238807\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 3.178032\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 3.096764\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 3.101066\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 3.034181\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 3.428448\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 3.369177\n",
      "Epoch: 1838/3000... Step: 58800... Loss: 0.657851... Val Loss: 3.295052\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 2.996672\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 2.575257\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 2.166710\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 1.957449\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 2.291393\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 3.138328\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 2.832060\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 2.819406\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 2.698625\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 2.652874\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 2.749316\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 2.708858\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 2.619683\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 3.366375\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 3.341100\n",
      "Epoch: 1841/3000... Step: 58900... Loss: 1.486138... Val Loss: 3.482890\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 2.757231\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 2.203217\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 1.762651\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 1.632111\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 1.801173\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 2.376851\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 2.263492\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 2.684214\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 2.590440\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 2.698029\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 2.574502\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 2.560880\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 2.463647\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 2.970266\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 2.898138\n",
      "Epoch: 1844/3000... Step: 59000... Loss: 3.138173... Val Loss: 2.856834\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 2.648971\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 1.948151\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 1.595688\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 1.407345\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 1.363453\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 1.891470\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 1.858168\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 2.278024\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 2.182579\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 2.199816\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 2.070622\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 2.004898\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 1.944056\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 2.452367\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 2.420743\n",
      "Epoch: 1847/3000... Step: 59100... Loss: 1.842818... Val Loss: 2.411411\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 4.443202\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 3.203978\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 2.457866\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 2.008299\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 2.158807\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 2.583114\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 2.385224\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 2.694195\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 2.526921\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 2.818243\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 2.665427\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 2.630632\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 2.600365\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 3.159828\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 3.081547\n",
      "Epoch: 1850/3000... Step: 59200... Loss: 0.953554... Val Loss: 3.071478\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 2.388308\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 2.071893\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 1.637571\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 1.616728\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 1.789732\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 2.406402\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 2.296703\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 2.641398\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 2.511886\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 2.478445\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 2.524389\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 2.450073\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 2.347508\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 2.859497\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 2.846483\n",
      "Epoch: 1854/3000... Step: 59300... Loss: 0.654859... Val Loss: 2.868347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 3.303116\n",
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 2.568180\n",
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 1.932927\n",
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 1.668045\n",
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 1.870019\n",
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 2.938385\n",
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 2.729418\n",
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 2.975206\n",
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 2.838496\n",
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 2.880191\n",
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 2.733994\n",
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 2.626212\n",
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 2.550432\n",
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 3.068306\n",
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 2.964439\n",
      "Epoch: 1857/3000... Step: 59400... Loss: 1.239732... Val Loss: 3.050993\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 2.480089\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 1.987368\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 1.527947\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 1.411924\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 2.039658\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 2.500075\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 2.344356\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 2.683347\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 2.527270\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 2.458449\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 2.434519\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 2.343042\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 2.257580\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 2.787550\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 2.814509\n",
      "Epoch: 1860/3000... Step: 59500... Loss: 0.539394... Val Loss: 2.994431\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 3.312108\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 2.335804\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 1.776393\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 1.511056\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 1.481136\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 2.492733\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 2.338983\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 2.722045\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 2.573394\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 2.659663\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 2.792260\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 2.723006\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 2.639110\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 3.105870\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 3.001805\n",
      "Epoch: 1863/3000... Step: 59600... Loss: 0.681934... Val Loss: 2.908911\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 3.541376\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 2.566463\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 1.950776\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 1.720291\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 1.845611\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 2.831862\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 2.748281\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 3.413762\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 3.218506\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 3.179047\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 3.141341\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 3.148687\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 3.031491\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 3.473624\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 3.516964\n",
      "Epoch: 1866/3000... Step: 59700... Loss: 1.036274... Val Loss: 3.480890\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 3.102707\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 2.247050\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 1.727435\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 1.724935\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 2.180443\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 2.684160\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 2.626609\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 2.994435\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 2.865667\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 2.814943\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 2.707767\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 2.749294\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 2.651485\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 3.117527\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 3.020567\n",
      "Epoch: 1869/3000... Step: 59800... Loss: 3.146955... Val Loss: 3.074777\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 2.410257\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 1.862372\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 1.630844\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 1.462550\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 1.586746\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 2.060543\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 1.967686\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 2.237619\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 2.147968\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 2.147862\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 2.116530\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 2.060954\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 1.989555\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 2.539908\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 2.467930\n",
      "Epoch: 1872/3000... Step: 59900... Loss: 2.248425... Val Loss: 2.584753\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 2.600389\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 1.992635\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 1.688056\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 1.521704\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 1.375901\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 2.012506\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 2.012992\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 2.236940\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 2.156545\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 2.171750\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 2.098424\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 2.165047\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 2.088011\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 2.563390\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 2.485913\n",
      "Epoch: 1875/3000... Step: 60000... Loss: 1.472291... Val Loss: 2.530960\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 3.798717\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 2.723918\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 2.451776\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 2.231655\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 2.447325\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 2.892070\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 2.911066\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 3.567323\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 3.382295\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 3.397210\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 3.226162\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 3.163199\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 3.072581\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 3.477215\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 3.401022\n",
      "Epoch: 1879/3000... Step: 60100... Loss: 1.068556... Val Loss: 3.504962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 2.380132\n",
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 1.823680\n",
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 1.494944\n",
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 1.464990\n",
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 1.416173\n",
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 1.945516\n",
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 1.923873\n",
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 2.375791\n",
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 2.282478\n",
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 2.369987\n",
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 2.310595\n",
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 2.211135\n",
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 2.134126\n",
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 2.543114\n",
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 2.498841\n",
      "Epoch: 1882/3000... Step: 60200... Loss: 0.418288... Val Loss: 2.733235\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 2.784426\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 1.910652\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 1.618935\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 1.552820\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 1.502750\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 2.293238\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 2.192304\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 2.654659\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 2.563463\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 2.455721\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 2.498673\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 2.422289\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 2.377232\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 2.808980\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 2.879026\n",
      "Epoch: 1885/3000... Step: 60300... Loss: 0.500055... Val Loss: 2.861366\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 3.597075\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 3.188542\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 2.574228\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 2.476447\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 2.374675\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 3.212077\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 3.146850\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 3.404332\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 3.323814\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 3.368065\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 3.239926\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 3.178433\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 3.105339\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 3.624060\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 3.522456\n",
      "Epoch: 1888/3000... Step: 60400... Loss: 0.781078... Val Loss: 3.513896\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 3.286307\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 2.269517\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 1.691044\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 1.478446\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 1.458215\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 2.074202\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 2.043019\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 2.547747\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 2.432712\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 2.524924\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 2.405728\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 2.450390\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 2.370044\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 2.803374\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 2.710110\n",
      "Epoch: 1891/3000... Step: 60500... Loss: 0.410293... Val Loss: 2.705155\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 4.812984\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 3.510319\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 3.171455\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 2.867538\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 2.870847\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 3.353059\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 3.426429\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 4.096951\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 3.979552\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 3.847548\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 3.764143\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 3.809848\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 3.755811\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 4.143497\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 4.098082\n",
      "Epoch: 1894/3000... Step: 60600... Loss: 4.498932... Val Loss: 4.029760\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 2.880399\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 2.109492\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 1.606427\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 1.391346\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 1.320404\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 1.879283\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 1.895342\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 2.348719\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 2.264375\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 2.231911\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 2.217884\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 2.270551\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 2.194685\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 2.683587\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 2.704638\n",
      "Epoch: 1897/3000... Step: 60700... Loss: 2.220697... Val Loss: 2.716798\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 2.860727\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 1.975517\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 1.537727\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 1.367799\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 1.515634\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 2.079732\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 1.978869\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 2.400246\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 2.309525\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 2.277514\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 2.190726\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 2.136635\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 2.075751\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 2.539938\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 2.515988\n",
      "Epoch: 1900/3000... Step: 60800... Loss: 1.310861... Val Loss: 2.637998\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 3.255116\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 2.217624\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 1.788852\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 1.547405\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 1.761025\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 2.653347\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 2.544301\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 2.884614\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 2.749596\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 2.743563\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 2.541799\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 2.454220\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 2.378139\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 2.802109\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 2.811762\n",
      "Epoch: 1904/3000... Step: 60900... Loss: 0.879373... Val Loss: 2.907143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 4.064701\n",
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 2.748612\n",
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 2.171011\n",
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 1.943164\n",
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 1.928389\n",
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 2.481272\n",
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 2.574944\n",
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 3.046185\n",
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 2.974972\n",
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 2.959676\n",
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 2.777428\n",
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 2.733855\n",
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 2.639599\n",
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 3.047672\n",
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 2.943364\n",
      "Epoch: 1907/3000... Step: 61000... Loss: 0.565346... Val Loss: 2.959418\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 3.637776\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 2.352351\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 1.850478\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 1.633186\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 1.563421\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 2.230695\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 2.330705\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 2.992307\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 2.844652\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 2.856591\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 2.862799\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 2.795024\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 2.721186\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 3.094982\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 3.020530\n",
      "Epoch: 1910/3000... Step: 61100... Loss: 0.258605... Val Loss: 2.960032\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 3.429793\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 2.479975\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 1.839823\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 1.751662\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 1.773482\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 3.061558\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 3.048395\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 3.609698\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 3.405817\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 3.397432\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 3.419022\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 3.291460\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 3.152002\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 3.608491\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 3.486254\n",
      "Epoch: 1913/3000... Step: 61200... Loss: 0.748597... Val Loss: 3.401043\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 2.841485\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 1.967400\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 1.619597\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 1.459910\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 1.512533\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 2.227561\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 2.224992\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 2.747606\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 2.610477\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 2.604776\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 2.462295\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 2.512780\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 2.416488\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 2.854337\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 2.780150\n",
      "Epoch: 1916/3000... Step: 61300... Loss: 0.506758... Val Loss: 2.734778\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 2.736483\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 2.182931\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 1.735932\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 1.613513\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 1.559858\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 2.168094\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 2.034041\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 2.301970\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 2.223395\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 2.246986\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 2.143042\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 2.056757\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 2.015473\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 2.573340\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 2.524994\n",
      "Epoch: 1919/3000... Step: 61400... Loss: 2.106097... Val Loss: 2.553237\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 2.776377\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 2.069566\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 1.674367\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 1.660154\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 1.806708\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 2.372894\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 2.341733\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 2.724145\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 2.617296\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 2.806694\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 2.656401\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 2.609551\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 2.504627\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 2.940245\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 2.851929\n",
      "Epoch: 1922/3000... Step: 61500... Loss: 2.122725... Val Loss: 2.841588\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 2.420878\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 1.825307\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 1.499957\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 1.342370\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 1.277828\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 1.888994\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 1.766569\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 2.035776\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 1.956932\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 1.987209\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 1.940875\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 1.879339\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 1.834673\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 2.380263\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 2.339114\n",
      "Epoch: 1925/3000... Step: 61600... Loss: 1.810811... Val Loss: 2.343796\n",
      "Validation loss decreased (2.401995 --> 2.343796).  Saving model ...\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 2.735354\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 2.325204\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 2.041099\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 1.714592\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 1.599959\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 2.106199\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 1.983202\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 2.114124\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 2.037223\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 2.220713\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 2.115122\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 2.067039\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 2.033841\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 2.694264\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 2.617351\n",
      "Epoch: 1929/3000... Step: 61700... Loss: 1.630261... Val Loss: 2.577308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 2.641547\n",
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 1.975571\n",
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 1.605078\n",
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 1.513310\n",
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 1.451295\n",
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 2.173801\n",
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 2.154635\n",
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 2.499067\n",
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 2.399607\n",
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 2.338730\n",
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 2.286910\n",
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 2.221969\n",
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 2.142696\n",
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 2.649422\n",
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 2.625604\n",
      "Epoch: 1932/3000... Step: 61800... Loss: 0.381709... Val Loss: 2.621725\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 2.954401\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 1.964792\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 1.566350\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 1.508506\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 1.886482\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 2.692675\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 2.594775\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 3.267860\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 3.087064\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 2.961841\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 2.759384\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 2.657768\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 2.557376\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 2.941476\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 2.924281\n",
      "Epoch: 1935/3000... Step: 61900... Loss: 0.149914... Val Loss: 2.918101\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 4.927416\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 3.408653\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 3.056321\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 2.772500\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 2.803170\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 3.255972\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 3.305225\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 4.075964\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 3.954611\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 3.858065\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 3.754326\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 3.773099\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 3.716447\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 4.172283\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 4.133331\n",
      "Epoch: 1938/3000... Step: 62000... Loss: 1.720546... Val Loss: 4.090279\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 2.361785\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 1.915264\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 1.571152\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 1.490112\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 1.714141\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 2.373537\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 2.193751\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 2.339260\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 2.260301\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 2.263901\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 2.201520\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 2.127554\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 2.067822\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 2.638212\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 2.594958\n",
      "Epoch: 1941/3000... Step: 62100... Loss: 0.832779... Val Loss: 2.667383\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 2.347062\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 1.991706\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 1.541173\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 1.471258\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 1.510205\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 2.203694\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 2.110193\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 2.467178\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 2.368699\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 2.333413\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 2.341870\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 2.432609\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 2.353364\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 2.906790\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 2.880177\n",
      "Epoch: 1944/3000... Step: 62200... Loss: 2.882146... Val Loss: 2.902500\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 3.045460\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 2.241852\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 1.813571\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 1.860106\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 1.874396\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 2.446339\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 2.380933\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 2.900256\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 2.759544\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 2.733483\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 2.685808\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 2.648666\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 2.539176\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 3.034489\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 3.052985\n",
      "Epoch: 1947/3000... Step: 62300... Loss: 2.365954... Val Loss: 3.018085\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 2.944909\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 2.506737\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 2.588816\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 2.285007\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 2.577415\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 3.133300\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 2.897663\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 3.022991\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 2.841153\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 3.071051\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 2.948014\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 2.880189\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 2.803040\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 3.420908\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 3.401774\n",
      "Epoch: 1950/3000... Step: 62400... Loss: 1.761624... Val Loss: 3.524857\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 2.645602\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 1.935060\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 1.505111\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 1.402399\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 1.310770\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 2.166388\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 2.044349\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 2.386635\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 2.254963\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 2.294857\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 2.235520\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 2.198293\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 2.126230\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 2.640394\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 2.623486\n",
      "Epoch: 1954/3000... Step: 62500... Loss: 0.804204... Val Loss: 2.588241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 3.356712\n",
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 2.213013\n",
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 1.787429\n",
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 1.642236\n",
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 1.713489\n",
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 2.394696\n",
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 2.371546\n",
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 3.208696\n",
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 3.035663\n",
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 2.971475\n",
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 2.801882\n",
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 2.736501\n",
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 2.631096\n",
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 2.969084\n",
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 2.923529\n",
      "Epoch: 1957/3000... Step: 62600... Loss: 1.193338... Val Loss: 2.884679\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 2.936714\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 2.075096\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 1.634515\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 1.496480\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 1.435771\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 2.088287\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 1.982713\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 2.320528\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 2.221497\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 2.319455\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 2.240348\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 2.274680\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 2.214924\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 2.747216\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 2.675458\n",
      "Epoch: 1960/3000... Step: 62700... Loss: 0.211693... Val Loss: 2.631109\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 2.980409\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 2.507446\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 1.933777\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 1.712663\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 1.610274\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 2.262392\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 2.055824\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 2.128219\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 2.072751\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 2.167662\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 2.266338\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 2.304055\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 2.253015\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 2.891461\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 2.873957\n",
      "Epoch: 1963/3000... Step: 62800... Loss: 0.566898... Val Loss: 2.952752\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 3.753535\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 2.712654\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 2.411945\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 2.284424\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 2.880301\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 3.433706\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 3.420962\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 4.010939\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 3.819204\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 3.688636\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 3.629731\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 3.640801\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 3.563578\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 3.985250\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 3.953867\n",
      "Epoch: 1966/3000... Step: 62900... Loss: 1.335274... Val Loss: 4.148148\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 3.323267\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 2.840298\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 2.294834\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 2.104107\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 2.108900\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 2.796398\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 2.698215\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 3.131456\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 3.042465\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 3.052833\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 2.908201\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 2.843314\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 2.757827\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 3.292728\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 3.233607\n",
      "Epoch: 1969/3000... Step: 63000... Loss: 2.699892... Val Loss: 3.173475\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 4.398437\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 2.866945\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 2.156405\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 2.024208\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 2.326505\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 2.747656\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 2.855800\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 3.779570\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 3.587572\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 3.411801\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 3.394328\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 3.330757\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 3.310799\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 3.643853\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 3.550873\n",
      "Epoch: 1972/3000... Step: 63100... Loss: 3.230190... Val Loss: 3.540041\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 3.149009\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 2.242911\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 1.747487\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 1.599710\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 1.444245\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 2.073447\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 1.931944\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 2.475700\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 2.348236\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 2.343580\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 2.347122\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 2.269170\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 2.192935\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 2.669717\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 2.595838\n",
      "Epoch: 1975/3000... Step: 63200... Loss: 0.799125... Val Loss: 2.519509\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 2.634999\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 1.908163\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 1.495993\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 1.312149\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 1.637823\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 2.347794\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 2.211426\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 2.637916\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 2.476334\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 2.454656\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 2.387325\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 2.362170\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 2.267966\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 2.692784\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 2.617212\n",
      "Epoch: 1979/3000... Step: 63300... Loss: 1.216144... Val Loss: 2.749293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 3.183383\n",
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 2.341707\n",
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 1.897251\n",
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 1.644760\n",
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 1.605906\n",
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 2.186155\n",
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 2.059209\n",
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 2.331100\n",
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 2.267292\n",
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 2.325043\n",
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 2.180179\n",
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 2.098238\n",
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 2.042846\n",
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 2.567024\n",
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 2.542331\n",
      "Epoch: 1982/3000... Step: 63400... Loss: 0.454069... Val Loss: 2.499020\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 3.668266\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 3.311234\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 2.776176\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 2.718495\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 2.634446\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 3.293566\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 3.247305\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 3.636876\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 3.608027\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 3.620975\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 3.656225\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 3.609813\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 3.513013\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 3.999505\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 3.906917\n",
      "Epoch: 1985/3000... Step: 63500... Loss: 0.903756... Val Loss: 3.919226\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 2.468996\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 2.096843\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 1.730792\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 1.700966\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 1.704403\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 2.410407\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 2.239620\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 2.563629\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 2.483525\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 2.518712\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 2.391737\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 2.321493\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 2.261462\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 2.853849\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 2.832175\n",
      "Epoch: 1988/3000... Step: 63600... Loss: 0.160600... Val Loss: 2.862745\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 3.067054\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 2.248368\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 1.814669\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 1.587572\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 1.556454\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 2.182896\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 2.053815\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 2.338734\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 2.242802\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 2.373208\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 2.268689\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 2.265710\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 2.235367\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 2.831914\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 2.786636\n",
      "Epoch: 1991/3000... Step: 63700... Loss: 0.929601... Val Loss: 2.736005\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 2.411288\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 1.887723\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 1.477601\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 1.409861\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 1.558296\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 2.179740\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 2.091630\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 2.443114\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 2.341404\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 2.430218\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 2.315871\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 2.290548\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 2.220158\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 2.708187\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 2.648354\n",
      "Epoch: 1994/3000... Step: 63800... Loss: 3.144421... Val Loss: 2.850284\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 2.939104\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 2.297445\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 1.803717\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 1.606786\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 1.530726\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 2.124944\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 2.083564\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 2.199206\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 2.113788\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 2.089250\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 2.071991\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 2.079867\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 2.005929\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 2.544791\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 2.496026\n",
      "Epoch: 1997/3000... Step: 63900... Loss: 2.182997... Val Loss: 2.454186\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 3.202751\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 2.680455\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 2.258343\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 2.151826\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 2.158342\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 2.771462\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 2.674008\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 2.952347\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 2.862637\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 2.905398\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 2.792095\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 2.666284\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 2.596367\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 3.100980\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 3.052584\n",
      "Epoch: 2000/3000... Step: 64000... Loss: 0.708855... Val Loss: 3.306863\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 2.558449\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 1.820269\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 1.379995\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 1.210183\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 1.304013\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 1.922488\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 1.808190\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 2.352394\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 2.228810\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 2.247997\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 2.230215\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 2.184925\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 2.112472\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 2.569724\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 2.519844\n",
      "Epoch: 2004/3000... Step: 64100... Loss: 0.759902... Val Loss: 2.470944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 3.575286\n",
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 2.327954\n",
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 1.779253\n",
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 1.574746\n",
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 1.611884\n",
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 2.223718\n",
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 2.339369\n",
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 3.188012\n",
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 3.055505\n",
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 2.972225\n",
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 2.845316\n",
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 2.833940\n",
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 2.746649\n",
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 3.104199\n",
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 3.175108\n",
      "Epoch: 2007/3000... Step: 64200... Loss: 1.323073... Val Loss: 3.129406\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 3.169408\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 1.975262\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 1.613981\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 1.485128\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 1.448826\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 1.998760\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 1.979641\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 2.526524\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 2.411690\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 2.398085\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 2.317168\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 2.271927\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 2.222168\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 2.609211\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 2.608725\n",
      "Epoch: 2010/3000... Step: 64300... Loss: 0.143413... Val Loss: 2.565384\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 4.131239\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 2.710334\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 2.275831\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 2.111737\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 2.455273\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 2.970159\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 3.169618\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 3.899948\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 3.789512\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 3.695318\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 3.669432\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 3.641586\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 3.602806\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 3.979974\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 3.871253\n",
      "Epoch: 2013/3000... Step: 64400... Loss: 0.624008... Val Loss: 3.787531\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 2.837265\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 2.187706\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 1.894440\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 1.800282\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 1.706734\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 2.330752\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 2.259940\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 2.521085\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 2.412046\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 2.381167\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 2.396114\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 2.440740\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 2.382799\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 2.941514\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 2.908004\n",
      "Epoch: 2016/3000... Step: 64500... Loss: 1.137471... Val Loss: 2.865988\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 2.777558\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 2.253223\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 1.708637\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 1.565782\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 1.428365\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 1.970204\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 1.853319\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 2.067470\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 2.019520\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 2.133527\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 2.056678\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 2.051453\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 1.999108\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 2.583860\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 2.552212\n",
      "Epoch: 2019/3000... Step: 64600... Loss: 2.017185... Val Loss: 2.567593\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 3.533271\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 2.803061\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 2.305449\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 2.141512\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 2.099211\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 2.709538\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 2.757610\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 3.128587\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 3.057669\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 3.085181\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 2.959208\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 2.985397\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 2.894341\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 3.271746\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 3.205610\n",
      "Epoch: 2022/3000... Step: 64700... Loss: 1.098658... Val Loss: 3.160415\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 2.943464\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 2.044672\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 1.676470\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 1.512957\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 1.481964\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 2.050335\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 1.993443\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 2.270522\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 2.185849\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 2.169304\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 2.131856\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 2.104810\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 2.042151\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 2.549910\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 2.528317\n",
      "Epoch: 2025/3000... Step: 64800... Loss: 0.280397... Val Loss: 2.561923\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 3.010278\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 2.059896\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 1.597291\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 1.572714\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 1.498500\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 2.076998\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 1.970461\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 2.405226\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 2.279891\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 2.228782\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 2.168753\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 2.086542\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 2.011489\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 2.410208\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 2.399420\n",
      "Epoch: 2029/3000... Step: 64900... Loss: 0.985668... Val Loss: 2.476281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 2.818808\n",
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 2.044572\n",
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 1.572235\n",
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 1.452834\n",
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 1.608070\n",
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 2.221781\n",
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 2.094259\n",
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 2.417959\n",
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 2.293455\n",
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 2.290434\n",
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 2.192003\n",
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 2.148095\n",
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 2.079554\n",
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 2.605469\n",
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 2.596019\n",
      "Epoch: 2032/3000... Step: 65000... Loss: 0.253983... Val Loss: 2.660946\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 3.726025\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 2.429833\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 1.922392\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 1.714372\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 1.713569\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 2.316533\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 2.402431\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 3.441060\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 3.221950\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 3.128069\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 2.975891\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 2.896628\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 2.792042\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 3.145377\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 3.155471\n",
      "Epoch: 2035/3000... Step: 65100... Loss: 0.501134... Val Loss: 3.157372\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 3.377753\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 2.275076\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 1.697454\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 1.497339\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 1.407712\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 1.979428\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 1.898269\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 2.355962\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 2.254627\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 2.241979\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 2.189906\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 2.188445\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 2.127137\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 2.582186\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 2.553459\n",
      "Epoch: 2038/3000... Step: 65200... Loss: 0.550578... Val Loss: 2.526403\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 3.200174\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 2.198376\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 1.756278\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 1.539908\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 1.467241\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 1.985986\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 1.876980\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 2.286808\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 2.175481\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 2.219522\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 2.143292\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 2.201199\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 2.134692\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 2.641530\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 2.576466\n",
      "Epoch: 2041/3000... Step: 65300... Loss: 0.730671... Val Loss: 2.503533\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 4.120649\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 2.764858\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 2.136978\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 1.915963\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 1.884818\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 2.441898\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 2.398451\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 3.177175\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 3.039638\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 2.985009\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 2.893527\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 2.871142\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 2.804349\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 3.115573\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 3.071229\n",
      "Epoch: 2044/3000... Step: 65400... Loss: 3.620563... Val Loss: 3.027358\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 2.979252\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 2.092607\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 1.767532\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 1.674193\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 1.513604\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 2.037226\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 1.883522\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 2.168997\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 2.063145\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 2.068450\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 2.073954\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 2.130427\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 2.070106\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 2.635573\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 2.598839\n",
      "Epoch: 2047/3000... Step: 65500... Loss: 2.198692... Val Loss: 2.560814\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 2.689424\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 2.175519\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 1.768922\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 1.545837\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 1.706039\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 2.234392\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 2.069008\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 2.289797\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 2.174489\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 2.266305\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 2.117303\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 2.026379\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 1.958188\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 2.525957\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 2.510156\n",
      "Epoch: 2050/3000... Step: 65600... Loss: 1.046178... Val Loss: 2.772492\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 2.192492\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 1.774254\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 1.499064\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 1.315307\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 1.581563\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 2.094428\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 1.984866\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 2.237577\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 2.135081\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 2.141584\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 2.026340\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 1.947728\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 1.877028\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 2.461358\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 2.418756\n",
      "Epoch: 2054/3000... Step: 65700... Loss: 0.784191... Val Loss: 2.546860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 2.808490\n",
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 1.995794\n",
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 1.541784\n",
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 1.335659\n",
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 1.346201\n",
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 1.913709\n",
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 1.775010\n",
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 2.052234\n",
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 1.974057\n",
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 2.045526\n",
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 1.919690\n",
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 1.893580\n",
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 1.830340\n",
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 2.405050\n",
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 2.341489\n",
      "Epoch: 2057/3000... Step: 65800... Loss: 1.662492... Val Loss: 2.277855\n",
      "Validation loss decreased (2.343796 --> 2.277855).  Saving model ...\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 2.779359\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 1.881524\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 1.465872\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 1.284793\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 1.249374\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 1.879477\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 1.824709\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 2.144128\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 2.046727\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 2.035743\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 2.019686\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 1.979850\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 1.923876\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 2.369482\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 2.335112\n",
      "Epoch: 2060/3000... Step: 65900... Loss: 0.241807... Val Loss: 2.311087\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 3.301289\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 2.898624\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 2.363837\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 2.238379\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 2.194028\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 2.817871\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 2.818283\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 3.020683\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 2.963186\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 2.935952\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 2.942864\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 2.976333\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 2.910182\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 3.423576\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 3.354505\n",
      "Epoch: 2063/3000... Step: 66000... Loss: 1.353560... Val Loss: 3.330313\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 3.425097\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 2.190423\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 1.728944\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 1.575630\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 1.688642\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 2.204368\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 2.229577\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 3.006142\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 2.834305\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 2.707100\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 2.752196\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 2.727148\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 2.641040\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 3.000898\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 2.978477\n",
      "Epoch: 2066/3000... Step: 66100... Loss: 0.684139... Val Loss: 3.073666\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 2.880161\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 2.561531\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 2.078407\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 1.946659\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 2.136489\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 2.698318\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 2.536344\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 2.512195\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 2.452627\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 2.405488\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 2.318188\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 2.283112\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 2.231226\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 3.043839\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 3.042976\n",
      "Epoch: 2069/3000... Step: 66200... Loss: 1.637794... Val Loss: 3.251700\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 2.749940\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 2.197789\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 1.817011\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 1.717942\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 1.840715\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 2.381488\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 2.306044\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 2.781162\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 2.606338\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 2.505532\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 2.421918\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 2.456959\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 2.369931\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 2.895044\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 2.843504\n",
      "Epoch: 2072/3000... Step: 66300... Loss: 1.885693... Val Loss: 2.816724\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 3.319501\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 2.019555\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 1.641704\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 1.472716\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 1.422089\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 2.030614\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 1.973614\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 2.491298\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 2.376701\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 2.297479\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 2.162892\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 2.088131\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 2.010913\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 2.482828\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 2.524710\n",
      "Epoch: 2075/3000... Step: 66400... Loss: 1.698180... Val Loss: 2.416352\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 3.034965\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 2.411563\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 2.083499\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 1.808587\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 1.854567\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 2.422919\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 2.211833\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 2.314106\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 2.235511\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 2.268100\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 2.167933\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 2.134892\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 2.103687\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 2.793736\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 2.809253\n",
      "Epoch: 2079/3000... Step: 66500... Loss: 1.073335... Val Loss: 2.732932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 2.829182\n",
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 1.902470\n",
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 1.461071\n",
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 1.290345\n",
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 1.305775\n",
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 1.911254\n",
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 1.905055\n",
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 2.245444\n",
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 2.145361\n",
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 2.126109\n",
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 2.013783\n",
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 2.043054\n",
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 1.972276\n",
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 2.437725\n",
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 2.356269\n",
      "Epoch: 2082/3000... Step: 66600... Loss: 0.476785... Val Loss: 2.328398\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 6.424125\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 4.628715\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 4.092622\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 3.903992\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 4.836566\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 5.177027\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 5.181551\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 6.146366\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 5.959749\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 5.722545\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 5.632604\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 5.699158\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 5.682945\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 5.995700\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 5.919974\n",
      "Epoch: 2085/3000... Step: 66700... Loss: 3.823366... Val Loss: 6.365422\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 4.239539\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 2.815791\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 2.408713\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 2.310871\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 2.245688\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 2.787425\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 2.885636\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 3.682300\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 3.497675\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 3.367185\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 3.361215\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 3.375480\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 3.307176\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 3.723917\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 3.721491\n",
      "Epoch: 2088/3000... Step: 66800... Loss: 1.376324... Val Loss: 3.670227\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 2.768175\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 1.982463\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 1.588119\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 1.483844\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 1.354856\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 1.952279\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 1.797407\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 1.907960\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 1.842139\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 1.848198\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 1.836865\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 1.825266\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 1.781197\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 2.549341\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 2.511201\n",
      "Epoch: 2091/3000... Step: 66900... Loss: 0.812760... Val Loss: 2.477886\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 2.987600\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 2.229932\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 1.833934\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 1.726660\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 1.693835\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 2.290804\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 2.338772\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 2.699073\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 2.621432\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 2.577204\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 2.553617\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 2.548465\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 2.471314\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 2.826173\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 2.757880\n",
      "Epoch: 2094/3000... Step: 67000... Loss: 3.152878... Val Loss: 2.686526\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 3.295663\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 2.262684\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 1.765296\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 1.541912\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 1.381083\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 2.006317\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 1.839484\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 2.080051\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 1.997294\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 2.020204\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 1.966406\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 1.983565\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 1.930096\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 2.415503\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 2.361248\n",
      "Epoch: 2097/3000... Step: 67100... Loss: 1.559752... Val Loss: 2.351817\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 3.027560\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 2.089503\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 1.649148\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 1.441415\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 1.442142\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 2.281969\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 2.228201\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 2.554510\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 2.396115\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 2.496636\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 2.372876\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 2.422785\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 2.339634\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 2.761083\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 2.691877\n",
      "Epoch: 2100/3000... Step: 67200... Loss: 0.758541... Val Loss: 2.632284\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 3.004442\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 1.957798\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 1.476455\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 1.307318\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 1.363613\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 2.035776\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 2.008255\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 2.609142\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 2.451990\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 2.395521\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 2.291565\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 2.303175\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 2.232236\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 2.634254\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 2.569222\n",
      "Epoch: 2104/3000... Step: 67300... Loss: 0.470454... Val Loss: 2.592047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 3.659836\n",
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 2.365502\n",
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 1.816739\n",
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 1.570901\n",
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 1.602495\n",
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 2.161114\n",
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 2.262645\n",
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 3.442602\n",
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 3.248668\n",
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 3.214181\n",
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 3.183507\n",
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 3.138411\n",
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 3.069901\n",
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 3.415865\n",
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 3.349776\n",
      "Epoch: 2107/3000... Step: 67400... Loss: 1.291013... Val Loss: 3.318015\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 2.819438\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 1.817693\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 1.429730\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 1.326251\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 1.335802\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 1.867131\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 1.793385\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 2.331021\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 2.199989\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 2.109620\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 2.083797\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 2.038523\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 1.986310\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 2.404117\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 2.392372\n",
      "Epoch: 2110/3000... Step: 67500... Loss: 0.163768... Val Loss: 2.384009\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 3.682920\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 3.268503\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 2.866210\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 2.767299\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 2.659278\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 3.214819\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 3.113906\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 3.294427\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 3.261925\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 3.298181\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 3.155239\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 3.124568\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 3.085734\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 3.695042\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 3.705230\n",
      "Epoch: 2113/3000... Step: 67600... Loss: 1.697864... Val Loss: 3.658389\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 3.690714\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 2.860959\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 2.373831\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 2.182872\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 2.063325\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 2.763656\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 2.711746\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 3.109392\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 3.052854\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 3.067059\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 3.027755\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 2.964465\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 2.906162\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 3.332358\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 3.277559\n",
      "Epoch: 2116/3000... Step: 67700... Loss: 1.019282... Val Loss: 3.217217\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 3.371608\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 2.392543\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 1.850564\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 1.848815\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 1.672349\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 2.267040\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 2.206746\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 2.521777\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 2.410226\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 2.564361\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 2.436932\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 2.368109\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 2.312656\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 2.815902\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 2.797170\n",
      "Epoch: 2119/3000... Step: 67800... Loss: 2.527162... Val Loss: 2.776366\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 3.272232\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 2.084337\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 1.641128\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 1.468491\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 1.395817\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 1.910161\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 1.872515\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 2.381598\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 2.250383\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 2.255436\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 2.138163\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 2.146760\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 2.073155\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 2.443467\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 2.437193\n",
      "Epoch: 2122/3000... Step: 67900... Loss: 1.164745... Val Loss: 2.381556\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 3.270564\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 2.071355\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 1.624025\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 1.512160\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 1.407862\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 1.879043\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 1.837429\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 2.246605\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 2.148630\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 2.132695\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 2.063507\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 2.023343\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 1.973737\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 2.404269\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 2.443556\n",
      "Epoch: 2125/3000... Step: 68000... Loss: 0.822969... Val Loss: 2.439229\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 4.261625\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 2.585412\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 1.980742\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 1.723586\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 1.866773\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 2.329446\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 2.416283\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 3.259898\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 3.106168\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 3.100940\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 2.963647\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 2.932204\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 2.868264\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 3.221852\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 3.176282\n",
      "Epoch: 2129/3000... Step: 68100... Loss: 0.894449... Val Loss: 3.145144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 2.891905\n",
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 1.986995\n",
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 1.562994\n",
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 1.397591\n",
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 1.250529\n",
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 1.806402\n",
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 1.707153\n",
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 2.066041\n",
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 1.942624\n",
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 1.968348\n",
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 1.872173\n",
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 1.925764\n",
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 1.867673\n",
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 2.341856\n",
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 2.311419\n",
      "Epoch: 2132/3000... Step: 68200... Loss: 0.468351... Val Loss: 2.274098\n",
      "Validation loss decreased (2.277855 --> 2.274098).  Saving model ...\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 3.974791\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 2.497353\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 2.135625\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 1.890877\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 1.782501\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 2.439181\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 2.507213\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 3.387404\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 3.252681\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 3.031802\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 2.964801\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 3.015647\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 2.964611\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 3.268199\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 3.496790\n",
      "Epoch: 2135/3000... Step: 68300... Loss: 0.777032... Val Loss: 3.369705\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 3.800759\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 2.361351\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 1.849851\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 1.666817\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 1.794234\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 2.287920\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 2.444374\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 3.357660\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 3.205155\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 3.081643\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 3.041992\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 3.018708\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 2.938155\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 3.185246\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 3.120936\n",
      "Epoch: 2138/3000... Step: 68400... Loss: 0.669466... Val Loss: 3.092111\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 3.196055\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 2.191348\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 1.696464\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 1.445126\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 1.710811\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 2.228144\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 2.042951\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 2.428651\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 2.274660\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 2.369313\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 2.231867\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 2.228655\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 2.167802\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 2.679340\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 2.611747\n",
      "Epoch: 2141/3000... Step: 68500... Loss: 0.705444... Val Loss: 2.568650\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 3.035125\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 1.977767\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 1.536674\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 1.347414\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 1.392678\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 1.935920\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 1.818560\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 2.326350\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 2.223591\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 2.177752\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 2.050699\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 2.054576\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 1.995452\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 2.436814\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 2.393228\n",
      "Epoch: 2144/3000... Step: 68600... Loss: 2.270724... Val Loss: 2.392396\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 3.429883\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 2.355286\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 1.874098\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 1.708945\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 1.735817\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 2.267249\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 2.185364\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 2.552918\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 2.463450\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 2.534132\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 2.398427\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 2.348152\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 2.296833\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 2.686989\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 2.644247\n",
      "Epoch: 2147/3000... Step: 68700... Loss: 1.461258... Val Loss: 2.680787\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 3.644584\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 2.611851\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 2.197955\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 2.102425\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 2.113293\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 2.595509\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 2.436623\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 2.862540\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 2.697606\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 2.860202\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 2.710668\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 2.637220\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 2.557506\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 3.049681\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 3.004369\n",
      "Epoch: 2150/3000... Step: 68800... Loss: 0.734481... Val Loss: 3.029473\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 3.000779\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 1.963644\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 1.559579\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 1.383978\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 1.501665\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 2.069324\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 2.038723\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 2.723557\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 2.555373\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 2.469448\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 2.403807\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 2.395478\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 2.298676\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 2.650975\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 2.668947\n",
      "Epoch: 2154/3000... Step: 68900... Loss: 0.657653... Val Loss: 2.663698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 3.363146\n",
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 2.305505\n",
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 1.723265\n",
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 1.551159\n",
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 1.789947\n",
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 2.440154\n",
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 2.283971\n",
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 2.777886\n",
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 2.639299\n",
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 2.670286\n",
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 2.586145\n",
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 2.554187\n",
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 2.449018\n",
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 2.872365\n",
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 2.907942\n",
      "Epoch: 2157/3000... Step: 69000... Loss: 0.864027... Val Loss: 3.070238\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 3.092694\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 1.930531\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 1.512323\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 1.368103\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 1.522462\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 1.999469\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 1.931164\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 2.462232\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 2.329542\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 2.235455\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 2.207142\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 2.250513\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 2.196536\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 2.524707\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 2.508869\n",
      "Epoch: 2160/3000... Step: 69100... Loss: 0.152429... Val Loss: 2.471885\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 2.674714\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 1.891965\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 1.706656\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 1.560607\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 1.806074\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 2.357565\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 2.239872\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 2.458849\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 2.332142\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 2.472039\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 2.482902\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 2.438179\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 2.351608\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 2.850766\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 2.913577\n",
      "Epoch: 2163/3000... Step: 69200... Loss: 0.501457... Val Loss: 3.042015\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 3.091995\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 2.260715\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 1.697377\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 1.524593\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 1.665266\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 2.227551\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 2.269361\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 2.883008\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 2.787902\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 2.799554\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 2.773877\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 2.741619\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 2.632580\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 3.027101\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 2.991499\n",
      "Epoch: 2166/3000... Step: 69300... Loss: 0.430649... Val Loss: 2.969423\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 2.997958\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 1.968864\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 1.582852\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 1.380615\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 1.779043\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 2.961024\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 2.862655\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 3.290460\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 3.054918\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 2.940358\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 2.833851\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 2.759769\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 2.649815\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 3.057668\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 3.039665\n",
      "Epoch: 2169/3000... Step: 69400... Loss: 2.470807... Val Loss: 3.003511\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 5.650262\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 3.635178\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 2.928132\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 2.713212\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 3.475866\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 3.909934\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 3.985661\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 5.160044\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 4.844706\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 4.572199\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 4.425384\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 4.416748\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 4.324652\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 4.614909\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 4.647141\n",
      "Epoch: 2172/3000... Step: 69500... Loss: 2.987164... Val Loss: 4.796651\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 2.834086\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 2.112566\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 1.549299\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 1.367702\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 1.435264\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 2.616687\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 2.435067\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 2.869424\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 2.698952\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 2.660513\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 2.565363\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 2.488879\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 2.381234\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 2.821430\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 2.747236\n",
      "Epoch: 2175/3000... Step: 69600... Loss: 0.959949... Val Loss: 2.692323\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 4.148515\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 2.990113\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 2.394186\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 2.079857\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 2.129203\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 2.597913\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 2.483390\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 2.736861\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 2.668898\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 2.741064\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 2.706429\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 2.649145\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 2.581830\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 3.133063\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 3.145396\n",
      "Epoch: 2179/3000... Step: 69700... Loss: 1.683201... Val Loss: 3.593117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 3.114241\n",
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 2.113787\n",
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 1.582029\n",
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 1.405791\n",
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 1.435736\n",
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 1.985252\n",
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 1.940710\n",
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 2.433515\n",
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 2.312711\n",
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 2.286154\n",
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 2.195352\n",
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 2.228217\n",
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 2.159936\n",
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 2.576205\n",
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 2.572199\n",
      "Epoch: 2182/3000... Step: 69800... Loss: 0.343651... Val Loss: 2.566329\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 4.715506\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 3.270860\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 3.053447\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 2.878734\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 3.542692\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 5.108276\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 4.821846\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 5.275009\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 5.003379\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 4.772762\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 4.625694\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 4.573850\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 4.461510\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 4.817743\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 4.880496\n",
      "Epoch: 2185/3000... Step: 69900... Loss: 2.451116... Val Loss: 4.930323\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 3.035785\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 2.080281\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 1.618882\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 1.406186\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 1.567295\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 2.011218\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 1.908489\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 2.282414\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 2.198478\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 2.151653\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 2.111721\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 2.086834\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 2.039537\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 2.509456\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 2.544705\n",
      "Epoch: 2188/3000... Step: 70000... Loss: 1.242608... Val Loss: 2.544549\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 2.981901\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 1.952475\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 1.519424\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 1.360248\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 1.458985\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 2.237793\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 2.315542\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 2.563916\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 2.452381\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 2.415364\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 2.366698\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 2.360873\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 2.278559\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 2.690787\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 2.640337\n",
      "Epoch: 2191/3000... Step: 70100... Loss: 0.806456... Val Loss: 2.651896\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 3.686748\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 2.339619\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 1.853078\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 1.732436\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 1.971333\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 2.630271\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 2.739626\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 3.465389\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 3.301896\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 3.143241\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 3.024792\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 3.065428\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 2.999840\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 3.288395\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 3.302086\n",
      "Epoch: 2194/3000... Step: 70200... Loss: 3.580884... Val Loss: 3.244574\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 2.628483\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 1.856740\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 1.478654\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 1.328019\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 1.524252\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 2.065159\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 1.991094\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 2.288044\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 2.177753\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 2.178644\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 2.058086\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 2.069245\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 2.004697\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 2.452119\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 2.394825\n",
      "Epoch: 2197/3000... Step: 70300... Loss: 1.279973... Val Loss: 2.439311\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 4.159410\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 3.280740\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 2.978413\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 2.781870\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 3.176529\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 3.652726\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 3.532049\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 3.827621\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 3.660922\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 3.652449\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 3.583130\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 3.704703\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 3.613662\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 4.090443\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 4.039947\n",
      "Epoch: 2200/3000... Step: 70400... Loss: 2.410112... Val Loss: 3.977737\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 2.911085\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 1.991309\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 1.445438\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 1.299884\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 1.538159\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 2.125749\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 2.022505\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 2.476279\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 2.330859\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 2.290635\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 2.184643\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 2.187203\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 2.111398\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 2.587013\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 2.553860\n",
      "Epoch: 2204/3000... Step: 70500... Loss: 0.712284... Val Loss: 2.509283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 3.386652\n",
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 2.380386\n",
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 2.010379\n",
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 1.800449\n",
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 3.372818\n",
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 3.916416\n",
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 3.697520\n",
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 4.023243\n",
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 3.736280\n",
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 3.636002\n",
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 3.467261\n",
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 3.497646\n",
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 3.377494\n",
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 3.752707\n",
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 3.755262\n",
      "Epoch: 2207/3000... Step: 70600... Loss: 0.426722... Val Loss: 4.735440\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 2.892082\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 1.896233\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 1.524880\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 1.398573\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 1.423065\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 1.920307\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 1.914148\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 2.244791\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 2.134696\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 2.146139\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 2.069435\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 2.039750\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 1.984252\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 2.415580\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 2.405101\n",
      "Epoch: 2210/3000... Step: 70700... Loss: 0.182451... Val Loss: 2.374330\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 3.514729\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 2.110387\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 1.817771\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 1.608543\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 1.847192\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 2.260785\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 2.197730\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 2.687019\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 2.526102\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 2.452188\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 2.351259\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 2.357919\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 2.291215\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 2.617533\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 2.600533\n",
      "Epoch: 2213/3000... Step: 70800... Loss: 1.283972... Val Loss: 2.562486\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 3.867781\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 2.582579\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 2.154786\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 1.984563\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 2.560194\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 3.042487\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 2.991785\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 3.812355\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 3.601092\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 3.467056\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 3.471062\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 3.508376\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 3.419994\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 3.756144\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 3.728170\n",
      "Epoch: 2216/3000... Step: 70900... Loss: 0.950509... Val Loss: 3.760724\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 3.059866\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 1.990838\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 1.548396\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 1.362623\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 1.482620\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 2.142945\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 1.974448\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 2.443279\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 2.294377\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 2.267396\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 2.235056\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 2.170325\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 2.095573\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 2.559671\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 2.527205\n",
      "Epoch: 2219/3000... Step: 71000... Loss: 1.679290... Val Loss: 2.493889\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 2.994175\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 2.059070\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 1.606119\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 1.446099\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 1.796867\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 2.685645\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 2.536038\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 2.871060\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 2.698273\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 2.688744\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 2.502421\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 2.474768\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 2.380283\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 2.800213\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 2.756178\n",
      "Epoch: 2222/3000... Step: 71100... Loss: 1.333914... Val Loss: 2.774588\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 2.792566\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 1.912581\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 1.478306\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 1.298808\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 1.422108\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 1.939212\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 1.837947\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 2.179107\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 2.076240\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 2.104521\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 2.007855\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 1.984928\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 1.928126\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 2.405332\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 2.379909\n",
      "Epoch: 2225/3000... Step: 71200... Loss: 0.703196... Val Loss: 2.378094\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 4.771671\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 3.403847\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 3.051057\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 2.929294\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 3.105799\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 3.576484\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 3.522928\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 4.094750\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 3.889566\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 3.846937\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 3.721725\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 3.706473\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 3.627260\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 4.033478\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 3.960914\n",
      "Epoch: 2229/3000... Step: 71300... Loss: 1.420627... Val Loss: 3.865980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 2.948911\n",
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 2.115384\n",
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 1.580993\n",
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 1.389031\n",
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 1.548299\n",
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 2.072886\n",
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 1.965659\n",
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 2.292538\n",
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 2.187214\n",
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 2.162384\n",
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 2.045187\n",
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 2.038980\n",
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 1.963908\n",
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 2.401028\n",
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 2.379661\n",
      "Epoch: 2232/3000... Step: 71400... Loss: 0.289894... Val Loss: 2.337023\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 3.141489\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 2.114839\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 1.687367\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 1.582164\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 1.607842\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 2.169036\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 2.142321\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 2.552802\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 2.446505\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 2.368916\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 2.305829\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 2.316420\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 2.248236\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 2.657120\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 2.707212\n",
      "Epoch: 2235/3000... Step: 71500... Loss: 0.428180... Val Loss: 2.722071\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 2.688803\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 2.028598\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 1.522989\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 1.379542\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 1.402956\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 1.944368\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 1.783071\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 1.858069\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 1.806888\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 1.865590\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 1.797851\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 1.822397\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 1.777825\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 2.362116\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 2.310946\n",
      "Epoch: 2238/3000... Step: 71600... Loss: 0.350024... Val Loss: 2.303394\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 2.448085\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 1.689292\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 1.392552\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 1.263116\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 1.427439\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 1.909087\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 1.839054\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 2.274185\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 2.138380\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 2.078780\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 1.975878\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 1.982154\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 1.896074\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 2.391180\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 2.372843\n",
      "Epoch: 2241/3000... Step: 71700... Loss: 0.580623... Val Loss: 2.401754\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 2.673680\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 1.941197\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 1.455831\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 1.262603\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 1.435522\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 1.958613\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 1.943698\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 2.456854\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 2.341236\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 2.404859\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 2.273622\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 2.267981\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 2.179641\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 2.585026\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 2.526352\n",
      "Epoch: 2244/3000... Step: 71800... Loss: 3.140692... Val Loss: 2.560334\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 2.853840\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 1.954986\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 1.499421\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 1.275911\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 1.324484\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 1.961739\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 1.937791\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 2.336950\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 2.200136\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 2.208494\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 2.148058\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 2.130262\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 2.059777\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 2.504633\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 2.477707\n",
      "Epoch: 2247/3000... Step: 71900... Loss: 1.206383... Val Loss: 2.454255\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 4.254934\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 3.849511\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 3.052511\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 2.866107\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 2.851756\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 3.428078\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 3.320207\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 3.447417\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 3.402143\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 3.593917\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 3.515553\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 3.600332\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 3.544194\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 4.070712\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 3.981511\n",
      "Epoch: 2250/3000... Step: 72000... Loss: 4.189086... Val Loss: 3.947697\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 3.007597\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 2.077350\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 1.673993\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 1.453167\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 1.421619\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 1.927616\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 1.833306\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 2.130851\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 2.039611\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 2.070219\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 2.015948\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 2.015957\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 1.947978\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 2.380397\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 2.415129\n",
      "Epoch: 2254/3000... Step: 72100... Loss: 0.577212... Val Loss: 2.399729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 3.525839\n",
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 2.438718\n",
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 1.876757\n",
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 1.636485\n",
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 1.639313\n",
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 2.186462\n",
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 2.175867\n",
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 2.749006\n",
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 2.633473\n",
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 2.659888\n",
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 2.504494\n",
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 2.530280\n",
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 2.461579\n",
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 2.723045\n",
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 2.722564\n",
      "Epoch: 2257/3000... Step: 72200... Loss: 0.439875... Val Loss: 2.903331\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 3.132472\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 1.967285\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 1.634633\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 1.461955\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 1.599402\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 2.102241\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 2.130263\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 2.826921\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 2.646315\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 2.521087\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 2.647536\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 2.602516\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 2.500932\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 2.820064\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 2.789353\n",
      "Epoch: 2260/3000... Step: 72300... Loss: 0.231651... Val Loss: 2.758505\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 2.502895\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 1.924298\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 1.549623\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 1.413365\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 1.335742\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 1.971541\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 1.908587\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 2.147043\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 2.086486\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 2.030902\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 1.988053\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 1.997972\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 1.942721\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 2.399019\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 2.360107\n",
      "Epoch: 2263/3000... Step: 72400... Loss: 0.938250... Val Loss: 2.337981\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 2.561691\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 1.821414\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 1.491938\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 1.374210\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 1.390603\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 1.977262\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 1.885405\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 2.066621\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 1.983456\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 2.017529\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 1.924249\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 1.953079\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 1.887489\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 2.339605\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 2.308122\n",
      "Epoch: 2266/3000... Step: 72500... Loss: 0.767587... Val Loss: 2.357697\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 2.999169\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 2.029316\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 1.657939\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 1.489588\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 1.303666\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 1.928307\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 1.826939\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 2.121234\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 2.015659\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 1.998286\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 1.881755\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 1.854632\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 1.801525\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 2.307918\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 2.318332\n",
      "Epoch: 2269/3000... Step: 72600... Loss: 1.456233... Val Loss: 2.295645\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 2.513088\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 1.826717\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 1.398822\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 1.289590\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 1.318974\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 1.906029\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 1.887576\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 2.361822\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 2.257785\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 2.217700\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 2.090568\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 2.128951\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 2.042895\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 2.438280\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 2.401309\n",
      "Epoch: 2272/3000... Step: 72700... Loss: 1.081066... Val Loss: 2.361104\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 2.799449\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 1.838753\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 1.415888\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 1.250198\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 1.119674\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 1.671829\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 1.566284\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 1.902388\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 1.818453\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 1.834225\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 1.775430\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 1.801332\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 1.746649\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 2.256374\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 2.229295\n",
      "Epoch: 2275/3000... Step: 72800... Loss: 0.419824... Val Loss: 2.311966\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 2.870242\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 2.261521\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 1.764364\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 1.518342\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 2.967888\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 3.347589\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 2.941246\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 3.108780\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 2.920232\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 2.895499\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 2.743635\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 2.601669\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 2.490561\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 3.055213\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 3.071759\n",
      "Epoch: 2279/3000... Step: 72900... Loss: 1.165304... Val Loss: 3.811882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 2.824742\n",
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 1.936725\n",
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 1.459524\n",
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 1.360949\n",
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 1.340850\n",
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 1.906954\n",
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 1.859602\n",
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 2.218637\n",
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 2.095191\n",
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 2.072338\n",
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 2.075882\n",
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 2.021087\n",
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 1.945674\n",
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 2.371337\n",
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 2.328483\n",
      "Epoch: 2282/3000... Step: 73000... Loss: 0.340038... Val Loss: 2.316916\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 4.103830\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 2.629367\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 2.237212\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 2.176650\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 2.748034\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 3.090844\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 3.057682\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 3.919578\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 3.701589\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 3.483143\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 3.461951\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 3.428137\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 3.413673\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 3.739252\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 3.690138\n",
      "Epoch: 2285/3000... Step: 73100... Loss: 1.043920... Val Loss: 3.643910\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 2.864055\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 2.052142\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 1.511784\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 1.296590\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 1.250886\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 1.714302\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 1.555885\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 1.669789\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 1.611608\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 1.648746\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 1.714259\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 1.722349\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 1.674159\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 2.375674\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 2.359976\n",
      "Epoch: 2288/3000... Step: 73200... Loss: 0.578243... Val Loss: 2.348944\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 2.852726\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 2.167085\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 1.591377\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 1.423516\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 1.346995\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 1.804578\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 1.653801\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 1.710979\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 1.687151\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 1.719508\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 1.677854\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 1.620406\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 1.588736\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 2.276651\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 2.238074\n",
      "Epoch: 2291/3000... Step: 73300... Loss: 1.167377... Val Loss: 2.269138\n",
      "Validation loss decreased (2.274098 --> 2.269138).  Saving model ...\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 3.087140\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 1.948876\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 1.597103\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 1.379950\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 1.596313\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 2.032274\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 2.059177\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 2.784475\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 2.622468\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 2.635640\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 2.486855\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 2.538520\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 2.438839\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 2.751255\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 2.712417\n",
      "Epoch: 2294/3000... Step: 73400... Loss: 3.536735... Val Loss: 2.746985\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 2.840221\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 1.815027\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 1.426214\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 1.249378\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 1.634475\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 2.114557\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 2.143657\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 2.793473\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 2.647947\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 2.595564\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 2.451755\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 2.484178\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 2.381869\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 2.704591\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 2.643089\n",
      "Epoch: 2297/3000... Step: 73500... Loss: 0.710996... Val Loss: 2.589645\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 2.919130\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 2.143938\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 1.586775\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 1.417513\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 1.381826\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 2.021706\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 1.863012\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 2.101992\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 2.002884\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 2.068203\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 1.974580\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 2.030303\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 1.957142\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 2.465836\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 2.409135\n",
      "Epoch: 2300/3000... Step: 73600... Loss: 1.515359... Val Loss: 2.339852\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 2.987211\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 1.886156\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 1.448002\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 1.309110\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 1.412373\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 1.991636\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 1.936770\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 2.472917\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 2.325532\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 2.234461\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 2.179947\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 2.172621\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 2.081132\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 2.412613\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 2.438956\n",
      "Epoch: 2304/3000... Step: 73700... Loss: 2.190765... Val Loss: 2.381927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 2.677424\n",
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 2.126462\n",
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 1.713444\n",
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 1.578359\n",
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 1.600065\n",
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 2.282259\n",
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 2.221923\n",
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 2.537562\n",
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 2.421241\n",
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 2.459366\n",
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 2.400829\n",
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 2.418473\n",
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 2.326868\n",
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 2.659790\n",
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 2.575651\n",
      "Epoch: 2307/3000... Step: 73800... Loss: 0.371323... Val Loss: 2.575816\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 2.305347\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 1.766678\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 1.355393\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 1.260219\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 1.296174\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 1.938371\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 1.895879\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 2.150767\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 2.069182\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 2.079889\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 2.072606\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 2.058638\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 1.989594\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 2.388491\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 2.337341\n",
      "Epoch: 2310/3000... Step: 73900... Loss: 0.677627... Val Loss: 2.317761\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 2.645938\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 1.692735\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 1.391003\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 1.309674\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 1.399877\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 1.961188\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 1.977340\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 2.652148\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 2.490987\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 2.539973\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 2.452116\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 2.366591\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 2.283548\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 2.555293\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 2.524261\n",
      "Epoch: 2313/3000... Step: 74000... Loss: 1.073002... Val Loss: 2.517400\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 2.654963\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 1.935681\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 1.431584\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 1.260840\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 1.143906\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 1.725421\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 1.609626\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 2.075845\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 1.966221\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 1.969473\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 2.025737\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 1.987116\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 1.910533\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 2.312713\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 2.281573\n",
      "Epoch: 2316/3000... Step: 74100... Loss: 0.628125... Val Loss: 2.266762\n",
      "Validation loss decreased (2.269138 --> 2.266762).  Saving model ...\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 2.717881\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 2.088470\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 1.603266\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 1.461026\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 1.887750\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 2.420335\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 2.208616\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 2.489645\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 2.380004\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 2.408071\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 2.312989\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 2.374179\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 2.283211\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 2.681915\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 2.617432\n",
      "Epoch: 2319/3000... Step: 74200... Loss: 2.008102... Val Loss: 2.587438\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 2.610393\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 1.859225\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 1.453798\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 1.304195\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 1.205435\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 1.768851\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 1.679843\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 2.087633\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 1.994917\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 1.966984\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 1.910307\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 1.938395\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 1.872062\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 2.245142\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 2.220025\n",
      "Epoch: 2322/3000... Step: 74300... Loss: 1.625689... Val Loss: 2.218944\n",
      "Validation loss decreased (2.266762 --> 2.218944).  Saving model ...\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 2.472363\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 1.690066\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 1.513213\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 1.412160\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 1.393171\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 1.954307\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 1.889755\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 2.453532\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 2.301438\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 2.315662\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 2.188658\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 2.108537\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 2.021105\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 2.399396\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 2.397868\n",
      "Epoch: 2325/3000... Step: 74400... Loss: 0.679765... Val Loss: 2.335607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 2.963354\n",
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 2.104424\n",
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 1.716236\n",
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 1.515311\n",
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 1.611593\n",
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 2.057316\n",
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 1.851996\n",
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 1.932831\n",
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 1.934203\n",
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 1.902337\n",
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 1.866375\n",
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 1.767509\n",
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 1.756800\n",
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 2.425081\n",
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 2.444289\n",
      "Epoch: 2329/3000... Step: 74500... Loss: 1.606261... Val Loss: 2.483236\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 2.421609\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 1.929545\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 1.487220\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 1.420255\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 1.411556\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 2.054718\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 1.844550\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 2.130708\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 2.022898\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 2.073710\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 2.236682\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 2.462605\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 2.359468\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 2.925616\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 2.974835\n",
      "Epoch: 2332/3000... Step: 74600... Loss: 0.309214... Val Loss: 2.985059\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 3.164407\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 2.222699\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 1.883614\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 1.695414\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 1.996345\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 2.521395\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 2.469893\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 3.017720\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 2.840351\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 2.728545\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 2.719513\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 2.788292\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 2.700458\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 3.044083\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 3.134974\n",
      "Epoch: 2335/3000... Step: 74700... Loss: 0.621467... Val Loss: 3.217193\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 2.255855\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 1.532212\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 1.178545\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 1.037950\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 1.068723\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 1.667137\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 1.586229\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 1.956139\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 1.855175\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 1.844332\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 1.772722\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 1.740030\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 1.675652\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 2.104256\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 2.131313\n",
      "Epoch: 2338/3000... Step: 74800... Loss: 0.324004... Val Loss: 2.178382\n",
      "Validation loss decreased (2.218944 --> 2.178382).  Saving model ...\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 2.752829\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 2.140627\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 1.624472\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 1.409908\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 1.618466\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 2.210574\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 1.970042\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 2.063122\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 1.985509\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 1.981783\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 2.266818\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 2.179884\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 2.086974\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 2.687730\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 2.629459\n",
      "Epoch: 2341/3000... Step: 74900... Loss: 1.094988... Val Loss: 2.596142\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 2.625987\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 1.881964\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 1.472147\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 1.354305\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 1.322665\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 1.965098\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 1.957692\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 2.382960\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 2.264910\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 2.194053\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 2.309786\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 2.279574\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 2.187973\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 2.567484\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 2.553242\n",
      "Epoch: 2344/3000... Step: 75000... Loss: 2.837336... Val Loss: 2.546828\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 2.873424\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 1.833096\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 1.479411\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 1.390212\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 1.340970\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 1.969846\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 1.972058\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 2.574797\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 2.399219\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 2.351571\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 2.259356\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 2.387583\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 2.284613\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 2.580526\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 2.658176\n",
      "Epoch: 2347/3000... Step: 75100... Loss: 0.934811... Val Loss: 2.608328\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 3.555777\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 3.145878\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 2.528638\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 2.378323\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 2.416526\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 3.014638\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 2.855994\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 2.868085\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 2.853998\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 2.903590\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 2.819518\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 2.866805\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 2.790790\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 3.728864\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 3.629320\n",
      "Epoch: 2350/3000... Step: 75200... Loss: 3.107036... Val Loss: 3.722623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 3.095168\n",
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 1.992728\n",
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 1.495670\n",
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 1.367165\n",
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 1.473870\n",
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 2.080752\n",
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 2.103906\n",
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 2.709036\n",
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 2.521024\n",
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 2.445421\n",
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 2.424565\n",
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 2.429764\n",
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 2.315122\n",
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 2.583261\n",
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 2.611105\n",
      "Epoch: 2354/3000... Step: 75300... Loss: 0.299991... Val Loss: 2.593160\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 7.070106\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 7.032532\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 6.346111\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 6.080630\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 5.866285\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 6.446066\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 6.285002\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 6.265545\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 6.336216\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 6.441674\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 6.288371\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 6.313518\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 6.240166\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 6.905150\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 6.818516\n",
      "Epoch: 2357/3000... Step: 75400... Loss: 0.718205... Val Loss: 6.848256\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 2.672168\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 1.710051\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 1.393004\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 1.265774\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 1.297347\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 1.924251\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 1.904558\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 2.329939\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 2.205360\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 2.163191\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 2.119901\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 2.101454\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 2.022742\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 2.325944\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 2.327288\n",
      "Epoch: 2360/3000... Step: 75500... Loss: 0.417476... Val Loss: 2.311312\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 2.955459\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 2.060707\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 1.560828\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 1.398973\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 1.274863\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 1.822614\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 1.695136\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 1.901707\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 1.847892\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 1.830187\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 1.737625\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 1.710508\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 1.646670\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 2.141985\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 2.148041\n",
      "Epoch: 2363/3000... Step: 75600... Loss: 0.339641... Val Loss: 2.206745\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 2.901855\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 2.061950\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 1.620481\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 1.406034\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 1.652452\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 2.158334\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 2.002774\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 2.510498\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 2.352697\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 2.355996\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 2.265159\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 2.291057\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 2.204380\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 2.568121\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 2.550583\n",
      "Epoch: 2366/3000... Step: 75700... Loss: 0.616861... Val Loss: 2.581196\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 2.942542\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 1.928111\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 1.552746\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 1.409928\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 1.340029\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 1.924783\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 1.914025\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 2.308893\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 2.189053\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 2.314379\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 2.284729\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 2.371757\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 2.257992\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 2.592409\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 2.564575\n",
      "Epoch: 2369/3000... Step: 75800... Loss: 1.656209... Val Loss: 2.556765\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 3.685283\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 2.347397\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 1.862813\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 1.752039\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 1.848822\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 2.299661\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 2.281448\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 2.967604\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 2.754675\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 2.660361\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 2.546304\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 2.502358\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 2.420883\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 2.656681\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 2.683583\n",
      "Epoch: 2372/3000... Step: 75900... Loss: 0.890245... Val Loss: 2.619001\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 3.010659\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 1.959618\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 1.546792\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 1.388629\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 1.403884\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 1.960579\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 1.945742\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 2.269080\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 2.160246\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 2.096967\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 2.051955\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 2.030755\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 1.966756\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 2.281975\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 2.255255\n",
      "Epoch: 2375/3000... Step: 76000... Loss: 0.386620... Val Loss: 2.239621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 2.791699\n",
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 1.918125\n",
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 1.460250\n",
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 1.242496\n",
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 1.395254\n",
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 1.947191\n",
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 1.912703\n",
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 2.175976\n",
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 2.060369\n",
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 2.053208\n",
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 1.927612\n",
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 1.911639\n",
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 1.836714\n",
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 2.214644\n",
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 2.172252\n",
      "Epoch: 2379/3000... Step: 76100... Loss: 0.389058... Val Loss: 2.280612\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 3.017276\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 2.198640\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 1.732833\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 1.592765\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 1.771980\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 2.312889\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 2.160581\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 2.379904\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 2.315363\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 2.350922\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 2.284923\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 2.439187\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 2.355067\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 2.744576\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 2.711750\n",
      "Epoch: 2382/3000... Step: 76200... Loss: 0.386490... Val Loss: 2.735100\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 2.887209\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 2.138328\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 1.666040\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 1.428129\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 1.565576\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 2.116830\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 1.952075\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 2.276122\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 2.146993\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 2.101487\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 2.059770\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 2.112734\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 2.041703\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 2.364253\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 2.395228\n",
      "Epoch: 2385/3000... Step: 76300... Loss: 0.795096... Val Loss: 2.544436\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 3.081474\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 2.150395\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 1.825646\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 1.659892\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 1.832542\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 2.349825\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 2.324512\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 2.891515\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 2.728844\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 2.668004\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 2.634598\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 2.624384\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 2.552149\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 2.836079\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 2.801834\n",
      "Epoch: 2388/3000... Step: 76400... Loss: 0.764960... Val Loss: 2.830229\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 2.941504\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 2.061700\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 1.633039\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 1.562979\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 1.566232\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 2.123184\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 2.056885\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 2.385544\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 2.275781\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 2.295159\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 2.238147\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 2.219222\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 2.140031\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 2.578708\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 2.537625\n",
      "Epoch: 2391/3000... Step: 76500... Loss: 0.645900... Val Loss: 2.542963\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 2.920178\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 1.930791\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 1.503593\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 1.388837\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 1.604919\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 2.135672\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 2.187690\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 2.843760\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 2.661158\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 2.663965\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 2.541777\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 2.546521\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 2.435252\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 2.692544\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 2.629261\n",
      "Epoch: 2394/3000... Step: 76600... Loss: 4.134602... Val Loss: 2.575047\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 3.091644\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 2.067325\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 1.533643\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 1.376946\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 1.426721\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 2.005616\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 1.978993\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 2.545442\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 2.389471\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 2.463624\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 2.343646\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 2.317635\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 2.211809\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 2.458152\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 2.418467\n",
      "Epoch: 2397/3000... Step: 76700... Loss: 0.674020... Val Loss: 2.376819\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 2.536784\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 1.901409\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 1.418739\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 1.233162\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 1.417563\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 2.057390\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 1.943371\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 2.500521\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 2.328453\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 2.527732\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 2.336870\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 2.302714\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 2.201764\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 2.501954\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 2.460552\n",
      "Epoch: 2400/3000... Step: 76800... Loss: 2.737926... Val Loss: 2.468514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 2.180817\n",
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 1.689387\n",
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 1.319082\n",
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 1.179474\n",
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 1.117703\n",
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 1.729237\n",
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 1.580162\n",
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 1.726657\n",
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 1.650637\n",
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 1.737262\n",
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 1.729081\n",
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 1.720537\n",
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 1.667331\n",
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 2.310262\n",
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 2.250207\n",
      "Epoch: 2404/3000... Step: 76900... Loss: 0.443317... Val Loss: 2.240314\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 4.409557\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 3.262582\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 2.493983\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 2.236952\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 2.151383\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 2.674601\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 2.565934\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 3.180741\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 3.093704\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 3.087028\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 2.951608\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 2.858477\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 2.770043\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 3.051170\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 3.088185\n",
      "Epoch: 2407/3000... Step: 77000... Loss: 0.709329... Val Loss: 3.148260\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 2.218237\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 1.789910\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 1.366806\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 1.273727\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 1.321741\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 1.906105\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 1.865059\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 2.095443\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 2.029267\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 2.048563\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 1.939902\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 1.977256\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 1.904504\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 2.322363\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 2.284126\n",
      "Epoch: 2410/3000... Step: 77100... Loss: 0.699983... Val Loss: 2.343850\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 2.481194\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 1.714689\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 1.345816\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 1.217940\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 1.264276\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 1.840731\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 1.853721\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 2.276245\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 2.183106\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 2.131027\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 2.140068\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 2.086455\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 2.011251\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 2.332754\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 2.315105\n",
      "Epoch: 2413/3000... Step: 77200... Loss: 0.624164... Val Loss: 2.295114\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 2.985631\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 2.062459\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 1.631108\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 1.533962\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 1.586088\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 2.128217\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 2.181179\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 2.871180\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 2.751579\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 2.678130\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 2.629462\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 2.605824\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 2.505926\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 2.724259\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 2.730384\n",
      "Epoch: 2416/3000... Step: 77300... Loss: 0.763363... Val Loss: 2.701844\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 2.399751\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 1.791798\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 1.484206\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 1.381744\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 1.362313\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 1.914159\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 1.775361\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 1.933699\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 1.844776\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 1.816199\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 1.826107\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 1.883313\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 1.819102\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 2.270392\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 2.270830\n",
      "Epoch: 2419/3000... Step: 77400... Loss: 0.926402... Val Loss: 2.415926\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 2.979900\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 1.890206\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 1.512505\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 1.437778\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 1.395376\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 1.878956\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 1.952404\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 2.708047\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 2.528720\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 2.416642\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 2.301867\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 2.272859\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 2.174762\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 2.394449\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 2.383218\n",
      "Epoch: 2422/3000... Step: 77500... Loss: 0.619088... Val Loss: 2.400568\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 2.917960\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 1.870133\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 1.449779\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 1.288414\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 1.568705\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 2.038021\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 2.058810\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 2.775044\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 2.588187\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 2.483821\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 2.374790\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 2.394411\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 2.281064\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 2.529800\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 2.514057\n",
      "Epoch: 2425/3000... Step: 77600... Loss: 3.380008... Val Loss: 2.611541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 4.341751\n",
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 4.118320\n",
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 3.269575\n",
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 3.006585\n",
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 3.064890\n",
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 3.580117\n",
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 3.330907\n",
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 3.385932\n",
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 3.330270\n",
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 3.467919\n",
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 3.280088\n",
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 3.289468\n",
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 3.227640\n",
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 3.937342\n",
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 3.892501\n",
      "Epoch: 2429/3000... Step: 77700... Loss: 1.579533... Val Loss: 4.135663\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 2.832527\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 2.056016\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 1.559246\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 1.380683\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 1.258644\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 1.744825\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 1.703748\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 1.996469\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 1.932671\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 1.978602\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 1.967637\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 2.140354\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 2.059757\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 2.422497\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 2.372959\n",
      "Epoch: 2432/3000... Step: 77800... Loss: 0.400901... Val Loss: 2.340152\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 2.891774\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 2.213504\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 1.611425\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 1.346862\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 1.311482\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 1.964160\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 1.832808\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 2.211359\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 2.085774\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 2.044603\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 2.030275\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 2.096427\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 2.004781\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 2.473460\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 2.510426\n",
      "Epoch: 2435/3000... Step: 77900... Loss: 0.497527... Val Loss: 2.656114\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 3.061485\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 1.953988\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 1.549561\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 1.439805\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 1.472574\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 2.003192\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 1.948420\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 2.764505\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 2.628313\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 2.508745\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 2.557843\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 2.487095\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 2.385552\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 2.626908\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 2.606914\n",
      "Epoch: 2438/3000... Step: 78000... Loss: 0.469617... Val Loss: 2.620830\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 3.545096\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 2.239380\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 1.790923\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 1.596739\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 1.453034\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 2.063528\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 2.019860\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 2.302430\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 2.198009\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 2.252077\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 2.145897\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 2.074223\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 2.013950\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 2.312492\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 2.280801\n",
      "Epoch: 2441/3000... Step: 78100... Loss: 1.011161... Val Loss: 2.314828\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 3.087240\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 2.428870\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 1.837979\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 1.708863\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 1.596194\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 2.220953\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 2.204243\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 2.518425\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 2.432429\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 2.611660\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 2.544270\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 2.566642\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 2.467844\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 2.880089\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 2.857182\n",
      "Epoch: 2444/3000... Step: 78200... Loss: 2.788350... Val Loss: 2.814347\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 2.616559\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 1.739551\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 1.516278\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 1.340264\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 1.391047\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 2.008065\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 1.985658\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 2.512358\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 2.359638\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 2.294196\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 2.153287\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 2.043001\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 1.965825\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 2.248077\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 2.308266\n",
      "Epoch: 2447/3000... Step: 78300... Loss: 1.316994... Val Loss: 2.275408\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 4.012576\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 3.553046\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 2.671047\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 2.283555\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 2.317923\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 2.853469\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 2.512747\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 2.591731\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 2.482548\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 2.701276\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 2.527883\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 2.514607\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 2.495030\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 3.256462\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 3.217904\n",
      "Epoch: 2450/3000... Step: 78400... Loss: 1.870195... Val Loss: 3.341156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 2.638318\n",
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 1.817363\n",
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 1.376551\n",
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 1.233004\n",
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 1.373024\n",
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 2.227440\n",
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 2.134814\n",
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 2.660051\n",
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 2.470438\n",
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 2.397519\n",
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 2.294250\n",
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 2.212712\n",
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 2.104688\n",
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 2.385058\n",
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 2.337644\n",
      "Epoch: 2454/3000... Step: 78500... Loss: 0.502393... Val Loss: 2.401601\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 3.012517\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 2.013090\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 1.654829\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 1.501459\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 1.449161\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 2.279048\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 2.203073\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 2.621392\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 2.493091\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 2.463191\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 2.344624\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 2.266805\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 2.175258\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 2.547168\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 2.564632\n",
      "Epoch: 2457/3000... Step: 78600... Loss: 0.445715... Val Loss: 2.558565\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 2.881978\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 1.861656\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 1.407424\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 1.225942\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 1.271356\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 2.013308\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 1.936284\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 2.566695\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 2.382342\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 2.462234\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 2.372869\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 2.315510\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 2.206705\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 2.455544\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 2.449917\n",
      "Epoch: 2460/3000... Step: 78700... Loss: 0.324530... Val Loss: 2.469060\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 3.561848\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 2.699247\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 2.050144\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 1.864429\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 1.711117\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 2.223899\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 2.051600\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 2.269464\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 2.206857\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 2.205884\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 2.139778\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 2.116601\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 2.059003\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 2.446948\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 2.434553\n",
      "Epoch: 2463/3000... Step: 78800... Loss: 1.624561... Val Loss: 2.569610\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 3.017904\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 2.010987\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 1.536788\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 1.343864\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 1.316709\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 1.951273\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 1.893463\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 2.701910\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 2.520688\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 2.461094\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 2.411240\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 2.339359\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 2.255490\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 2.595158\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 2.558134\n",
      "Epoch: 2466/3000... Step: 78900... Loss: 0.537060... Val Loss: 2.522637\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 3.035686\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 2.088540\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 1.692228\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 1.512485\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 1.512945\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 2.171007\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 2.014880\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 2.301571\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 2.187780\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 2.160720\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 2.083824\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 2.022794\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 1.966241\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 2.378481\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 2.392783\n",
      "Epoch: 2469/3000... Step: 79000... Loss: 1.761169... Val Loss: 2.411096\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 2.678719\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 1.798468\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 1.300349\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 1.156253\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 1.109462\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 2.393197\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 2.244112\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 2.599744\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 2.442830\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 2.352847\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 2.220518\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 2.219846\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 2.124129\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 2.464657\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 2.414435\n",
      "Epoch: 2472/3000... Step: 79100... Loss: 0.787965... Val Loss: 2.485203\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 3.381481\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 2.139536\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 1.735934\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 1.506567\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 1.718499\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 2.228056\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 2.312474\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 3.198687\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 2.995987\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 2.901578\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 2.740925\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 2.682440\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 2.567987\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 2.756633\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 2.848924\n",
      "Epoch: 2475/3000... Step: 79200... Loss: 4.057497... Val Loss: 2.813754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 3.303103\n",
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 2.221880\n",
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 1.627021\n",
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 1.445196\n",
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 1.386281\n",
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 1.949328\n",
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 1.877265\n",
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 2.587802\n",
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 2.432636\n",
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 2.437349\n",
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 2.385647\n",
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 2.301885\n",
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 2.204904\n",
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 2.552317\n",
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 2.529643\n",
      "Epoch: 2479/3000... Step: 79300... Loss: 1.254228... Val Loss: 2.743825\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 2.467937\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 1.909947\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 1.454192\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 1.402962\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 1.386068\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 2.038981\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 2.026430\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 2.262697\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 2.202837\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 2.162878\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 2.075525\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 2.129504\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 2.054555\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 2.430978\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 2.393066\n",
      "Epoch: 2482/3000... Step: 79400... Loss: 0.430061... Val Loss: 2.387921\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 3.167110\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 2.043758\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 1.616585\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 1.395813\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 1.699996\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 2.254045\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 2.193717\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 2.674709\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 2.499618\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 2.462628\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 2.384166\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 2.423866\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 2.324805\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 2.626524\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 2.591610\n",
      "Epoch: 2485/3000... Step: 79500... Loss: 0.327325... Val Loss: 2.635856\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 2.722988\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 1.706721\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 1.337702\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 1.194070\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 1.421679\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 2.185052\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 2.168020\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 2.960896\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 2.797121\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 2.628968\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 2.525336\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 2.475796\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 2.390361\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 2.650385\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 2.701763\n",
      "Epoch: 2488/3000... Step: 79600... Loss: 0.479184... Val Loss: 2.747469\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 2.716854\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 1.803276\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 1.444623\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 1.299511\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 1.200555\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 1.767871\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 1.759149\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 1.909690\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 1.853104\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 1.861980\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 1.906317\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 1.930116\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 1.867835\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 2.308360\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 2.336665\n",
      "Epoch: 2491/3000... Step: 79700... Loss: 0.912075... Val Loss: 2.332328\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 3.057333\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 2.028649\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 1.553969\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 1.377743\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 1.278914\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 1.755481\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 1.639422\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 1.910555\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 1.826899\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 1.857502\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 1.885818\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 1.879842\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 1.821031\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 2.245961\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 2.222857\n",
      "Epoch: 2494/3000... Step: 79800... Loss: 1.844353... Val Loss: 2.229677\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 2.717067\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 2.009593\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 1.550023\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 1.306296\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 1.661912\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 2.123348\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 1.955732\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 2.031837\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 1.923988\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 1.918718\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 1.790394\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 1.807064\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 1.734695\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 2.389257\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 2.330310\n",
      "Epoch: 2497/3000... Step: 79900... Loss: 1.828380... Val Loss: 2.308519\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 3.139635\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 2.243050\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 1.811661\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 1.652763\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 1.854687\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 2.401755\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 2.310579\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 2.680566\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 2.552624\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 2.601064\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 2.444417\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 2.409584\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 2.299859\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 2.629978\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 2.652459\n",
      "Epoch: 2500/3000... Step: 80000... Loss: 1.516127... Val Loss: 2.573680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 2.477939\n",
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 1.904224\n",
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 1.442443\n",
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 1.284471\n",
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 1.502920\n",
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 2.068846\n",
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 1.890571\n",
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 1.966967\n",
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 1.883083\n",
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 1.848773\n",
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 1.772739\n",
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 1.739210\n",
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 1.681401\n",
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 2.262685\n",
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 2.241220\n",
      "Epoch: 2504/3000... Step: 80100... Loss: 0.522468... Val Loss: 2.303910\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 2.250660\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 1.686442\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 1.387727\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 1.281235\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 1.418370\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 2.272545\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 2.215547\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 2.532522\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 2.399434\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 2.430347\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 2.446737\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 2.463690\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 2.349162\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 2.714094\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 2.670574\n",
      "Epoch: 2507/3000... Step: 80200... Loss: 0.533012... Val Loss: 2.631080\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 2.593040\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 1.776806\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 1.385734\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 1.226017\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 1.198706\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 1.840335\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 1.822223\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 2.070333\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 1.967664\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 1.974601\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 1.890621\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 1.857954\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 1.803394\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 2.234747\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 2.186471\n",
      "Epoch: 2510/3000... Step: 80300... Loss: 0.266690... Val Loss: 2.220719\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 2.912053\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 2.260780\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 1.624483\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 1.487368\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 1.679302\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 2.166574\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 1.939980\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 2.147693\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 2.048641\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 2.146719\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 2.056575\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 1.947440\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 1.875654\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 2.345222\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 2.319635\n",
      "Epoch: 2513/3000... Step: 80400... Loss: 0.352897... Val Loss: 2.285332\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 2.643706\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 1.855065\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 1.499396\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 1.290861\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 1.310659\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 1.905327\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 1.803513\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 2.272269\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 2.150717\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 2.141915\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 2.390481\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 2.357858\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 2.243407\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 2.561019\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 2.552475\n",
      "Epoch: 2516/3000... Step: 80500... Loss: 0.737956... Val Loss: 2.566282\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 2.918580\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 1.995844\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 1.701780\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 1.546858\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 1.703156\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 2.223981\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 2.087600\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 2.270832\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 2.165057\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 2.179128\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 2.104446\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 2.051044\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 1.996994\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 2.442539\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 2.454246\n",
      "Epoch: 2519/3000... Step: 80600... Loss: 1.342062... Val Loss: 2.420698\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 5.861632\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 5.594592\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 4.745566\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 4.452098\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 4.475757\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 4.988315\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 4.655534\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 4.542254\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 4.492952\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 4.789401\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 4.552167\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 4.498938\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 4.472479\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 5.166605\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 5.085024\n",
      "Epoch: 2522/3000... Step: 80700... Loss: 1.978945... Val Loss: 5.322809\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 2.362731\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 1.617263\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 1.294450\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 1.162361\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 1.141743\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 1.729631\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 1.819535\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 2.498161\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 2.372079\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 2.299952\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 2.292493\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 2.263267\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 2.154948\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 2.451574\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 2.398801\n",
      "Epoch: 2525/3000... Step: 80800... Loss: 2.268903... Val Loss: 2.421576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 2.805628\n",
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 2.455665\n",
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 2.104127\n",
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 1.850693\n",
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 1.806129\n",
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 2.332353\n",
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 2.079520\n",
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 2.002723\n",
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 1.913853\n",
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 2.164281\n",
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 2.026543\n",
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 2.009957\n",
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 1.940804\n",
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 2.689178\n",
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 2.656793\n",
      "Epoch: 2529/3000... Step: 80900... Loss: 1.244769... Val Loss: 2.633094\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 2.729059\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 1.863157\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 1.401973\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 1.226816\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 1.218102\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 1.783294\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 1.727586\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 2.162192\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 2.035986\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 2.013012\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 1.993345\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 1.995550\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 1.912353\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 2.208498\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 2.216876\n",
      "Epoch: 2532/3000... Step: 81000... Loss: 0.281949... Val Loss: 2.237188\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 3.573067\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 2.185276\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 1.774569\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 1.628182\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 1.947498\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 2.421209\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 2.487561\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 3.158347\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 2.975589\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 2.798038\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 2.767096\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 2.805877\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 2.696722\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 2.900198\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 2.988155\n",
      "Epoch: 2535/3000... Step: 81100... Loss: 0.350457... Val Loss: 3.158229\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 2.740925\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 1.954422\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 1.473448\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 1.295698\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 1.360182\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 1.912604\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 1.818463\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 2.203383\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 2.104909\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 2.080744\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 2.011124\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 1.955040\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 1.870186\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 2.247381\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 2.213489\n",
      "Epoch: 2538/3000... Step: 81200... Loss: 0.279671... Val Loss: 2.245011\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 2.622783\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 2.043906\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 1.643657\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 1.518967\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 1.401747\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 2.016515\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 1.988019\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 2.166950\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 2.129921\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 2.068846\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 1.990650\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 2.062321\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 2.002829\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 2.811809\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 2.800856\n",
      "Epoch: 2541/3000... Step: 81300... Loss: 1.289690... Val Loss: 2.823584\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 3.438747\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 2.261256\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 1.849659\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 1.739446\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 2.168402\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 2.610873\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 2.636940\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 3.326680\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 3.116247\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 2.963104\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 2.792796\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 2.748790\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 2.630334\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 2.769435\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 2.822319\n",
      "Epoch: 2544/3000... Step: 81400... Loss: 2.429140... Val Loss: 2.987590\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 2.778639\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 1.755484\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 1.373892\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 1.236902\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 1.224809\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 1.814982\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 1.879460\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 2.529893\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 2.408984\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 2.360841\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 2.231617\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 2.229944\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 2.134075\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 2.360003\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 2.353399\n",
      "Epoch: 2547/3000... Step: 81500... Loss: 0.533383... Val Loss: 2.346203\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 2.753659\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 2.231597\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 1.707775\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 1.514648\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 2.288678\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 2.704223\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 2.458985\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 2.490276\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 2.371437\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 2.562950\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 2.405887\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 2.384273\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 2.277458\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 2.750575\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 2.667261\n",
      "Epoch: 2550/3000... Step: 81600... Loss: 3.229269... Val Loss: 2.993551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 2.616200\n",
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 2.005181\n",
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 1.510517\n",
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 1.342952\n",
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 1.230024\n",
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 1.807335\n",
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 1.654880\n",
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 1.772706\n",
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 1.734674\n",
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 1.745207\n",
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 1.718108\n",
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 1.719431\n",
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 1.661087\n",
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 2.225019\n",
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 2.214466\n",
      "Epoch: 2554/3000... Step: 81700... Loss: 0.804892... Val Loss: 2.374641\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 4.745966\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 3.217240\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 2.769254\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 2.567511\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 2.632664\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 3.068939\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 3.123052\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 4.140208\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 3.900234\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 3.703895\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 3.573034\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 3.637778\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 3.559855\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 3.722455\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 3.780347\n",
      "Epoch: 2557/3000... Step: 81800... Loss: 1.443558... Val Loss: 3.720434\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 2.814721\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 1.832357\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 1.411795\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 1.274195\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 1.259327\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 1.780363\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 1.746621\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 2.293885\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 2.160342\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 2.077918\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 1.971316\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 1.928840\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 1.844259\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 2.110502\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 2.139015\n",
      "Epoch: 2560/3000... Step: 81900... Loss: 0.163339... Val Loss: 2.166033\n",
      "Validation loss decreased (2.178382 --> 2.166033).  Saving model ...\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 2.912388\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 1.925307\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 1.705871\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 1.605390\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 1.899884\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 3.614667\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 3.429936\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 3.817013\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 3.575927\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 3.455259\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 3.347761\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 3.190194\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 3.054348\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 3.279205\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 3.250289\n",
      "Epoch: 2563/3000... Step: 82000... Loss: 0.812668... Val Loss: 3.297303\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 2.934417\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 1.987052\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 1.488645\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 1.290703\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 1.467532\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 1.947044\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 1.852864\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 2.213468\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 2.080361\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 2.062543\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 1.946670\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 1.919656\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 1.846668\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 2.179104\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 2.193985\n",
      "Epoch: 2566/3000... Step: 82100... Loss: 0.566603... Val Loss: 2.304110\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 1.963828\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 1.768805\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 1.375116\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 1.358815\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 1.345951\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 1.950215\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 1.954825\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 2.162140\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 2.081279\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 2.122727\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 2.067092\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 2.038372\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 1.943763\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 2.506675\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 2.502268\n",
      "Epoch: 2569/3000... Step: 82200... Loss: 2.145312... Val Loss: 2.556791\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 2.862513\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 2.585258\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 1.974421\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 1.822060\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 2.123662\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 2.598373\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 2.427992\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 2.581561\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 2.500796\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 2.584930\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 2.519376\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 2.476262\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 2.375740\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 2.889358\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 2.839241\n",
      "Epoch: 2572/3000... Step: 82300... Loss: 1.616243... Val Loss: 2.847240\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 2.547881\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 1.779573\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 1.410160\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 1.263083\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 1.369683\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 1.832423\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 1.721299\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 1.968853\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 1.882647\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 1.896068\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 1.847450\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 1.986976\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 1.922550\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 2.396697\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 2.367974\n",
      "Epoch: 2575/3000... Step: 82400... Loss: 0.781565... Val Loss: 2.362593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 2.894239\n",
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 2.093932\n",
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 1.570662\n",
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 1.399601\n",
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 1.317066\n",
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 1.761670\n",
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 1.678400\n",
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 1.841426\n",
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 1.798864\n",
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 1.789975\n",
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 1.781246\n",
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 1.754103\n",
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 1.692132\n",
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 2.344506\n",
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 2.323994\n",
      "Epoch: 2579/3000... Step: 82500... Loss: 1.187757... Val Loss: 2.470901\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 3.380946\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 2.390576\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 1.931135\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 1.742509\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 1.855152\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 2.278292\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 2.182911\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 2.345508\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 2.278332\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 2.251341\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 2.148467\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 2.234792\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 2.164661\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 2.964620\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 2.895812\n",
      "Epoch: 2582/3000... Step: 82600... Loss: 0.569089... Val Loss: 2.829981\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 2.765082\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 2.060646\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 1.487818\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 1.340067\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 1.204745\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 1.792775\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 1.717417\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 1.816758\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 1.747109\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 1.743185\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 1.734874\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 1.909384\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 1.826614\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 2.337571\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 2.299116\n",
      "Epoch: 2585/3000... Step: 82700... Loss: 0.218521... Val Loss: 2.348636\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 2.819654\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 2.075928\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 1.536933\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 1.300269\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 1.283609\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 1.887551\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 1.775017\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 1.976739\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 1.881308\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 1.867942\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 1.802020\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 1.778767\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 1.719564\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 2.133308\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 2.105980\n",
      "Epoch: 2588/3000... Step: 82800... Loss: 0.321307... Val Loss: 2.110580\n",
      "Validation loss decreased (2.166033 --> 2.110580).  Saving model ...\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 2.651609\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 1.973343\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 1.574621\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 1.361198\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 1.541256\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 2.043774\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 1.925632\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 1.976177\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 1.899095\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 1.851470\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 1.838728\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 1.889968\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 1.830965\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 2.478555\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 2.473580\n",
      "Epoch: 2591/3000... Step: 82900... Loss: 0.822298... Val Loss: 2.527064\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 4.973713\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 4.458016\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 3.803971\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 3.660678\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 3.672194\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 4.188386\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 4.128624\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 4.360527\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 4.311276\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 4.473953\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 4.282541\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 4.185824\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 4.086889\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 4.406140\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 4.322013\n",
      "Epoch: 2594/3000... Step: 83000... Loss: 3.208176... Val Loss: 4.347725\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 2.391995\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 1.609845\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 1.405155\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 1.232073\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 1.381684\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 2.095082\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 2.064684\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 2.469251\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 2.322766\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 2.235540\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 2.148936\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 2.070749\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 1.992054\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 2.274662\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 2.243176\n",
      "Epoch: 2597/3000... Step: 83100... Loss: 0.695077... Val Loss: 2.253154\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 2.517318\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 1.931366\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 1.609768\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 1.470315\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 1.764955\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 2.168833\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 2.196896\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 2.534489\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 2.382176\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 2.301640\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 2.178882\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 2.107408\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 2.035999\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 2.331593\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 2.342485\n",
      "Epoch: 2600/3000... Step: 83200... Loss: 0.845836... Val Loss: 2.484323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 2.612133\n",
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 1.709025\n",
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 1.385032\n",
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 1.225702\n",
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 1.194498\n",
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 1.771652\n",
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 1.793963\n",
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 2.431501\n",
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 2.305643\n",
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 2.262571\n",
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 2.221209\n",
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 2.168165\n",
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 2.069734\n",
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 2.273973\n",
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 2.224540\n",
      "Epoch: 2604/3000... Step: 83300... Loss: 0.297092... Val Loss: 2.219225\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 3.357826\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 2.080870\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 1.686951\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 1.508186\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 1.932777\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 2.319470\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 2.333571\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 2.993652\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 2.779171\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 2.714532\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 2.572373\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 2.572128\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 2.475315\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 2.708692\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 2.712665\n",
      "Epoch: 2607/3000... Step: 83400... Loss: 1.522604... Val Loss: 2.790605\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 2.523985\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 1.699615\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 1.273348\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 1.103208\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 1.178367\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 1.663337\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 1.591123\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 1.973704\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 1.870482\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 1.866748\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 1.918138\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 1.877849\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 1.799793\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 2.047679\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 2.067665\n",
      "Epoch: 2610/3000... Step: 83500... Loss: 0.157216... Val Loss: 2.092755\n",
      "Validation loss decreased (2.110580 --> 2.092755).  Saving model ...\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 4.296173\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 3.408492\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 2.962159\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 2.729775\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 2.591274\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 3.076455\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 2.935813\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 3.291059\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 3.224523\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 3.253739\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 3.139799\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 3.079782\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 2.974040\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 3.286567\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 3.299805\n",
      "Epoch: 2613/3000... Step: 83600... Loss: 1.421390... Val Loss: 3.401811\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 3.017344\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 2.240929\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 1.862144\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 1.773922\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 1.957196\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 2.467384\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 2.391147\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 2.757250\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 2.667592\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 2.704856\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 2.685570\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 2.606878\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 2.490596\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 2.762761\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 2.713955\n",
      "Epoch: 2616/3000... Step: 83700... Loss: 0.378675... Val Loss: 2.683858\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 3.464921\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 2.242424\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 1.965695\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 1.763871\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 1.904094\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 2.407321\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 2.395174\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 3.310594\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 3.095588\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 3.092112\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 2.938340\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 2.915759\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 2.820277\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 3.007076\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 3.033329\n",
      "Epoch: 2619/3000... Step: 83800... Loss: 2.604741... Val Loss: 2.951984\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 2.866947\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 2.101685\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 1.654666\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 1.544475\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 1.496391\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 2.015102\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 1.941335\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 2.335356\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 2.213132\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 2.309728\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 2.253441\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 2.197099\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 2.094692\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 2.367930\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 2.343117\n",
      "Epoch: 2622/3000... Step: 83900... Loss: 0.811925... Val Loss: 2.359707\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 2.362719\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 1.578342\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 1.207039\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 1.097067\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 1.228848\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 1.740469\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 1.806763\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 2.514695\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 2.357314\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 2.272695\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 2.129871\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 2.060206\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 1.969276\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 2.170932\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 2.173860\n",
      "Epoch: 2625/3000... Step: 84000... Loss: 2.028170... Val Loss: 2.194491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 3.803764\n",
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 3.217969\n",
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 2.673943\n",
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 2.433229\n",
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 2.590575\n",
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 3.160701\n",
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 3.010021\n",
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 3.073332\n",
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 3.029009\n",
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 3.273735\n",
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 3.161203\n",
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 3.196315\n",
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 3.094543\n",
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 3.584957\n",
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 3.572437\n",
      "Epoch: 2629/3000... Step: 84100... Loss: 2.549669... Val Loss: 3.698238\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 2.676024\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 1.834451\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 1.431767\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 1.220097\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 1.119647\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 1.742676\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 1.672583\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 2.083014\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 1.984331\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 1.976167\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 1.909221\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 1.925830\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 1.837860\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 2.160746\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 2.154527\n",
      "Epoch: 2632/3000... Step: 84200... Loss: 0.250463... Val Loss: 2.137198\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 3.416725\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 2.040702\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 1.737591\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 1.735057\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 1.860826\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 2.328715\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 2.393902\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 3.203946\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 3.026469\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 2.834726\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 2.822705\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 2.730832\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 2.668567\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 2.842207\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 2.946087\n",
      "Epoch: 2635/3000... Step: 84300... Loss: 0.485133... Val Loss: 2.895770\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 2.647310\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 1.818119\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 1.350579\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 1.183434\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 1.160174\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 1.686594\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 1.626397\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 1.976396\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 1.893745\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 1.870351\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 1.854296\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 1.780132\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 1.712656\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 2.044105\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 2.028447\n",
      "Epoch: 2638/3000... Step: 84400... Loss: 1.018137... Val Loss: 2.065643\n",
      "Validation loss decreased (2.092755 --> 2.065643).  Saving model ...\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 3.326741\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 2.484470\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 2.207584\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 1.919457\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 1.894271\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 2.406623\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 2.417487\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 2.573484\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 2.473195\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 2.640233\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 2.599723\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 2.613257\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 2.605765\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 2.997885\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 2.933239\n",
      "Epoch: 2641/3000... Step: 84500... Loss: 0.984858... Val Loss: 2.972260\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 3.542613\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 2.733916\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 2.189232\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 1.897413\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 1.840872\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 2.347601\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 2.211181\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 2.350529\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 2.233289\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 2.270726\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 2.156718\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 2.122376\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 2.049736\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 2.410660\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 2.416844\n",
      "Epoch: 2644/3000... Step: 84600... Loss: 1.868538... Val Loss: 2.369255\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 2.511525\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 1.907465\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 1.551833\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 1.384461\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 1.363131\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 1.906646\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 1.743016\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 1.849216\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 1.777292\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 1.886649\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 1.778765\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 1.718168\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 1.644852\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 2.108471\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 2.115644\n",
      "Epoch: 2647/3000... Step: 84700... Loss: 1.009798... Val Loss: 2.145042\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 7.468549\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 7.042293\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 6.099484\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 5.774425\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 5.454489\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 6.171524\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 5.949041\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 5.854552\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 5.858342\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 5.960429\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 5.824439\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 5.737564\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 5.662135\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 6.265352\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 6.232414\n",
      "Epoch: 2650/3000... Step: 84800... Loss: 4.834610... Val Loss: 6.145626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 2.926872\n",
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 1.867680\n",
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 1.478435\n",
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 1.310604\n",
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 1.436130\n",
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 1.977450\n",
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 1.844408\n",
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 2.283526\n",
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 2.178168\n",
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 2.153245\n",
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 2.132408\n",
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 2.050438\n",
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 1.975190\n",
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 2.222749\n",
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 2.204836\n",
      "Epoch: 2654/3000... Step: 84900... Loss: 0.701039... Val Loss: 2.376062\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 3.617802\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 2.522650\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 2.051315\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 1.872175\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 1.792610\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 2.370912\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 2.346586\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 2.927739\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 2.811156\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 2.809582\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 2.672484\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 2.567432\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 2.469487\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 2.676709\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 2.619469\n",
      "Epoch: 2657/3000... Step: 85000... Loss: 2.985953... Val Loss: 2.678698\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 2.450920\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 1.784297\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 1.358374\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 1.183831\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 1.152487\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 1.769377\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 1.625925\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 1.867535\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 1.780632\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 1.763392\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 1.735380\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 1.703576\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 1.637097\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 2.133575\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 2.105136\n",
      "Epoch: 2660/3000... Step: 85100... Loss: 0.307253... Val Loss: 2.100572\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 2.867429\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 2.003329\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 1.545440\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 1.407830\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 1.268611\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 1.884623\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 1.747957\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 1.876881\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 1.808615\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 1.794263\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 1.732861\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 1.978892\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 1.901010\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 2.430646\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 2.411729\n",
      "Epoch: 2663/3000... Step: 85200... Loss: 0.329052... Val Loss: 2.451436\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 3.671644\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 2.429120\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 2.003841\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 1.848001\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 1.720873\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 2.257166\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 2.153983\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 2.463509\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 2.355572\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 2.308169\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 2.244825\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 2.247152\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 2.183429\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 2.551782\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 2.554737\n",
      "Epoch: 2666/3000... Step: 85300... Loss: 0.681820... Val Loss: 2.531960\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 3.476115\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 2.263771\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 1.784983\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 1.561303\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 1.469250\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 1.958613\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 1.885891\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 2.228388\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 2.107020\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 2.102765\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 2.023940\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 2.040853\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 1.965458\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 2.385290\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 2.388127\n",
      "Epoch: 2669/3000... Step: 85400... Loss: 1.954987... Val Loss: 2.363547\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 2.745850\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 1.837415\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 1.435042\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 1.282548\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 1.247467\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 1.728262\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 1.648504\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 1.968433\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 1.881108\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 1.821428\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 1.822411\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 1.766131\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 1.708086\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 2.055488\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 2.064975\n",
      "Epoch: 2672/3000... Step: 85500... Loss: 2.519392... Val Loss: 2.100423\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 3.098332\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 1.908755\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 1.527780\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 1.307139\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 1.512210\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 1.960934\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 1.900594\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 2.447496\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 2.295025\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 2.336369\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 2.363151\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 2.303866\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 2.209445\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 2.406563\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 2.407566\n",
      "Epoch: 2675/3000... Step: 85600... Loss: 0.871308... Val Loss: 2.374184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 2.679683\n",
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 1.952244\n",
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 1.468434\n",
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 1.288184\n",
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 1.309931\n",
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 2.681654\n",
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 2.425101\n",
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 2.493101\n",
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 2.380883\n",
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 2.418850\n",
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 2.289817\n",
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 2.159556\n",
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 2.078796\n",
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 2.779597\n",
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 2.701836\n",
      "Epoch: 2679/3000... Step: 85700... Loss: 0.643089... Val Loss: 2.632983\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 3.309058\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 2.568196\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 2.018796\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 1.859671\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 1.694890\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 2.184968\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 2.092040\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 2.284059\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 2.248261\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 2.302335\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 2.216638\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 2.188279\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 2.128773\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 2.597113\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 2.591884\n",
      "Epoch: 2682/3000... Step: 85800... Loss: 0.403955... Val Loss: 2.625737\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 2.702472\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 2.357522\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 1.841585\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 1.705358\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 1.567728\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 2.692990\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 2.478082\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 2.446028\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 2.373738\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 2.374902\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 2.277022\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 2.181780\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 2.102329\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 2.812106\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 2.743983\n",
      "Epoch: 2685/3000... Step: 85900... Loss: 0.638613... Val Loss: 2.668748\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 2.758075\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 1.808573\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 1.402073\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 1.275306\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 1.235977\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 1.756900\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 1.675471\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 2.087859\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 2.006694\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 1.936446\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 2.029580\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 1.991941\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 1.929041\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 2.364236\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 2.372671\n",
      "Epoch: 2688/3000... Step: 86000... Loss: 0.335718... Val Loss: 2.393652\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 2.867869\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 2.129851\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 1.759021\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 1.574625\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 1.518745\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 2.018189\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 1.856876\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 1.969145\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 1.907352\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 1.944562\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 1.848459\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 1.839873\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 1.779428\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 2.238162\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 2.239155\n",
      "Epoch: 2691/3000... Step: 86100... Loss: 1.193276... Val Loss: 2.260255\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 3.671894\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 3.344854\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 2.622198\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 2.471232\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 2.336263\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 2.824060\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 2.662700\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 2.742721\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 2.694295\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 2.768090\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 2.738932\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 2.703181\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 2.626520\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 3.314764\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 3.277537\n",
      "Epoch: 2694/3000... Step: 86200... Loss: 1.668549... Val Loss: 3.318536\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 2.751607\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 1.837528\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 1.450706\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 1.292468\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 1.218298\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 1.750631\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 1.693588\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 1.941093\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 1.882992\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 1.868287\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 1.776027\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 1.773834\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 1.713469\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 2.053865\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 2.068675\n",
      "Epoch: 2697/3000... Step: 86300... Loss: 1.114515... Val Loss: 2.097800\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 4.980557\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 3.672421\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 3.228100\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 3.067195\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 3.135453\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 3.576809\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 3.626349\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 4.254659\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 4.123681\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 3.983937\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 3.924445\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 3.866221\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 3.789739\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 3.966377\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 3.967179\n",
      "Epoch: 2700/3000... Step: 86400... Loss: 4.246751... Val Loss: 3.943096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 3.367729\n",
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 2.177582\n",
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 1.765788\n",
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 1.522692\n",
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 1.453600\n",
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 1.916096\n",
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 1.864460\n",
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 2.369372\n",
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 2.247870\n",
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 2.242089\n",
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 2.106192\n",
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 2.029122\n",
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 1.942713\n",
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 2.223640\n",
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 2.246283\n",
      "Epoch: 2704/3000... Step: 86500... Loss: 0.354155... Val Loss: 2.233966\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 4.028543\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 3.053268\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 2.535278\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 2.243517\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 2.235737\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 2.712514\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 2.548685\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 2.681177\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 2.598474\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 2.636034\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 2.574323\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 2.508700\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 2.452650\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 3.234898\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 3.180029\n",
      "Epoch: 2707/3000... Step: 86600... Loss: 2.100186... Val Loss: 3.162388\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 2.971757\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 1.915893\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 1.450485\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 1.308678\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 1.360062\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 1.820914\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 1.783264\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 2.293629\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 2.178270\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 2.127536\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 2.090633\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 2.059301\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 1.986161\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 2.216392\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 2.288317\n",
      "Epoch: 2710/3000... Step: 86700... Loss: 0.142203... Val Loss: 2.314249\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 4.235286\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 3.025107\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 2.574844\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 2.497582\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 2.276318\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 2.707393\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 2.678227\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 2.890385\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 2.829826\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 2.771703\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 2.800945\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 2.966363\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 2.871809\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 3.295590\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 3.360393\n",
      "Epoch: 2713/3000... Step: 86800... Loss: 0.697125... Val Loss: 3.364944\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 2.837613\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 2.121653\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 1.625081\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 1.450358\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 1.348053\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 1.824486\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 1.795241\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 2.040777\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 1.971832\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 1.950778\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 2.028592\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 2.208385\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 2.123237\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 2.873449\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 2.836401\n",
      "Epoch: 2716/3000... Step: 86900... Loss: 1.428112... Val Loss: 2.845166\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 2.866809\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 1.917094\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 1.504392\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 1.392474\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 1.417046\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 1.852906\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 1.849758\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 2.409662\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 2.283440\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 2.241821\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 2.190766\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 2.111686\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 2.013425\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 2.252345\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 2.241569\n",
      "Epoch: 2719/3000... Step: 87000... Loss: 2.104067... Val Loss: 2.279051\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 2.589173\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 2.013683\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 1.502750\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 1.395493\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 1.596778\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 1.988448\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 1.865013\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 2.062793\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 1.948543\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 1.940240\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 1.845755\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 1.840473\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 1.755930\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 2.185188\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 2.177062\n",
      "Epoch: 2722/3000... Step: 87100... Loss: 0.743742... Val Loss: 2.291247\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 3.228608\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 2.078520\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 1.623258\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 1.413892\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 1.307887\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 1.789149\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 1.817738\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 2.586925\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 2.453767\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 2.438244\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 2.338890\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 2.302743\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 2.196809\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 2.371128\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 2.377664\n",
      "Epoch: 2725/3000... Step: 87200... Loss: 6.461629... Val Loss: 2.447955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 3.230422\n",
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 2.255754\n",
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 1.810758\n",
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 1.674463\n",
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 1.698542\n",
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 2.211833\n",
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 2.178668\n",
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 2.641557\n",
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 2.499629\n",
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 2.531464\n",
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 2.452637\n",
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 2.405146\n",
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 2.329578\n",
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 2.618641\n",
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 2.596133\n",
      "Epoch: 2729/3000... Step: 87300... Loss: 0.461099... Val Loss: 2.540820\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 2.854378\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 1.864462\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 1.384483\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 1.193671\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 1.172520\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 2.062855\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 1.927385\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 2.353751\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 2.217399\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 2.167577\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 2.005062\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 1.912076\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 1.849510\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 2.191296\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 2.179522\n",
      "Epoch: 2732/3000... Step: 87400... Loss: 0.422539... Val Loss: 2.214259\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 3.601369\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 2.452849\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 2.023891\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 1.787059\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 1.746028\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 2.156040\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 2.097208\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 2.405212\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 2.406187\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 2.349928\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 2.273815\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 2.248832\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 2.194381\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 2.507247\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 2.584859\n",
      "Epoch: 2735/3000... Step: 87500... Loss: 0.522649... Val Loss: 2.746656\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 2.745057\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 2.341404\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 1.726152\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 1.538886\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 1.448710\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 1.978052\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 1.813488\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 1.831102\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 1.808876\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 1.916779\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 1.788584\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 1.752698\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 1.697967\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 2.395301\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 2.331403\n",
      "Epoch: 2738/3000... Step: 87600... Loss: 0.331690... Val Loss: 2.255852\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 3.240058\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 2.335843\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 1.778099\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 1.571215\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 1.797775\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 2.303514\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 2.142725\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 2.291795\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 2.218562\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 2.257715\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 2.119391\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 2.170632\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 2.096423\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 2.812718\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 2.775742\n",
      "Epoch: 2741/3000... Step: 87700... Loss: 1.494263... Val Loss: 2.822468\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 3.572411\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 2.548918\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 1.971362\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 1.759918\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 1.702942\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 2.209043\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 2.217546\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 2.579478\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 2.489801\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 2.466649\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 2.352622\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 2.308836\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 2.222508\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 2.569420\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 2.561386\n",
      "Epoch: 2744/3000... Step: 87800... Loss: 2.554479... Val Loss: 2.822587\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 2.960777\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 1.943646\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 1.565819\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 1.400660\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 1.322646\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 1.811484\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 1.743618\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 2.300737\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 2.178552\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 2.143442\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 2.025919\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 1.988044\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 1.910095\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 2.126622\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 2.146219\n",
      "Epoch: 2747/3000... Step: 87900... Loss: 0.814598... Val Loss: 2.154612\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 3.956178\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 2.457579\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 1.827076\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 1.543585\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 1.504371\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 1.971666\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 1.805414\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 2.132850\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 2.044788\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 2.074578\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 2.068265\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 1.979411\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 1.914939\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 2.251093\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 2.224085\n",
      "Epoch: 2750/3000... Step: 88000... Loss: 0.974371... Val Loss: 2.373823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 2.676613\n",
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 1.815400\n",
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 1.371271\n",
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 1.224330\n",
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 1.184368\n",
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 1.678530\n",
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 1.646411\n",
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 1.988945\n",
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 1.909856\n",
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 1.879665\n",
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 1.794067\n",
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 1.775182\n",
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 1.706667\n",
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 1.951447\n",
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 1.969329\n",
      "Epoch: 2754/3000... Step: 88100... Loss: 0.481813... Val Loss: 2.028288\n",
      "Validation loss decreased (2.065643 --> 2.028288).  Saving model ...\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 13.137688\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 13.689892\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 12.969186\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 12.889554\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 12.782541\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 13.433345\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 13.488557\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 13.440943\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 13.456689\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 13.457823\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 13.263744\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 13.208422\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 13.021895\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 13.864230\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 13.736225\n",
      "Epoch: 2757/3000... Step: 88200... Loss: 8.442435... Val Loss: 13.799629\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 3.002311\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 1.995603\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 1.568073\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 1.476107\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 1.377224\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 1.911979\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 1.866677\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 2.139962\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 2.076634\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 2.049898\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 2.013596\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 1.976412\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 1.913719\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 2.385668\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 2.366684\n",
      "Epoch: 2760/3000... Step: 88300... Loss: 0.307964... Val Loss: 2.311024\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 2.713905\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 1.940603\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 1.534404\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 1.375174\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 1.287498\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 1.778587\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 1.707143\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 1.979157\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 1.923742\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 1.935424\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 1.852459\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 1.812354\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 1.745525\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 2.206480\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 2.218214\n",
      "Epoch: 2763/3000... Step: 88400... Loss: 0.552522... Val Loss: 2.183241\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 3.565018\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 2.360385\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 1.907700\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 1.730669\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 1.616811\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 2.135787\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 2.074278\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 2.426664\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 2.325133\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 2.251529\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 2.267485\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 2.220496\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 2.159841\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 2.662714\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 2.672438\n",
      "Epoch: 2766/3000... Step: 88500... Loss: 0.775625... Val Loss: 2.656640\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 3.492284\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 2.332482\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 1.872614\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 1.706330\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 1.713574\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 2.267462\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 2.205990\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 2.559101\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 2.429692\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 2.353682\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 2.396425\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 2.354102\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 2.274389\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 2.515874\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 2.520728\n",
      "Epoch: 2769/3000... Step: 88600... Loss: 1.894529... Val Loss: 2.561430\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 3.813164\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 2.765493\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 2.400940\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 2.155522\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 2.125093\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 2.566328\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 2.435358\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 2.666709\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 2.547640\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 2.534678\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 2.476882\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 2.456141\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 2.388020\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 2.771721\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 2.796539\n",
      "Epoch: 2772/3000... Step: 88700... Loss: 2.505444... Val Loss: 2.896885\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 2.878323\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 2.017183\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 1.501525\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 1.344694\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 1.274068\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 1.766005\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 1.674608\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 1.914833\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 1.853393\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 1.859074\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 1.776835\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 1.741881\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 1.667962\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 1.976131\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 1.991410\n",
      "Epoch: 2775/3000... Step: 88800... Loss: 0.258591... Val Loss: 2.040842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 3.178213\n",
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 2.596306\n",
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 1.930035\n",
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 1.729353\n",
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 1.626765\n",
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 2.118835\n",
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 1.925743\n",
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 1.981917\n",
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 1.965041\n",
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 2.123367\n",
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 1.997103\n",
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 1.965117\n",
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 1.894128\n",
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 2.436345\n",
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 2.398574\n",
      "Epoch: 2779/3000... Step: 88900... Loss: 2.100555... Val Loss: 2.320161\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 3.089988\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 2.211913\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 1.730255\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 1.514466\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 1.406198\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 1.897305\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 1.838339\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 2.112419\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 2.032565\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 2.016877\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 1.939841\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 1.871328\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 1.809403\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 2.196557\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 2.231288\n",
      "Epoch: 2782/3000... Step: 89000... Loss: 0.343048... Val Loss: 2.287822\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 3.183389\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 2.358579\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 1.824054\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 1.703876\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 1.558514\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 2.008873\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 2.032553\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 2.279603\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 2.204482\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 2.156735\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 2.144573\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 2.185213\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 2.119015\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 2.636719\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 2.652069\n",
      "Epoch: 2785/3000... Step: 89100... Loss: 0.338003... Val Loss: 2.646156\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 2.896299\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 2.187812\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 1.656281\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 1.495049\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 1.392410\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 1.829779\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 1.766155\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 1.864240\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 1.818180\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 1.934601\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 1.813786\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 1.799802\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 1.744298\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 2.286681\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 2.301842\n",
      "Epoch: 2788/3000... Step: 89200... Loss: 0.303740... Val Loss: 2.307887\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 2.736284\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 2.025875\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 1.530767\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 1.373092\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 1.307452\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 1.806517\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 1.641141\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 1.788097\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 1.726651\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 1.831293\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 1.765282\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 1.708851\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 1.650393\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 2.310036\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 2.252220\n",
      "Epoch: 2791/3000... Step: 89300... Loss: 1.933288... Val Loss: 2.186483\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 4.270377\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 3.038251\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 2.374121\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 2.076601\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 2.604233\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 2.918138\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 2.899502\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 4.158349\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 3.869792\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 3.670439\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 3.551738\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 3.600783\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 3.543624\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 3.583128\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 3.713240\n",
      "Epoch: 2794/3000... Step: 89400... Loss: 4.015483... Val Loss: 3.733847\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 3.640851\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 2.324489\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 1.771630\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 1.491037\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 1.608352\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 1.969953\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 1.879003\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 2.255483\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 2.114781\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 2.084168\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 2.071919\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 2.108321\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 2.007657\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 2.228581\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 2.217886\n",
      "Epoch: 2797/3000... Step: 89500... Loss: 1.472138... Val Loss: 2.617610\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 4.456463\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 3.657223\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 2.926507\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 2.677024\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 2.579144\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 2.947738\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 2.843791\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 3.009962\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 2.996604\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 2.997967\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 2.904696\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 2.865485\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 2.774429\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 3.537934\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 3.505286\n",
      "Epoch: 2800/3000... Step: 89600... Loss: 3.096050... Val Loss: 3.443261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 3.466351\n",
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 2.419806\n",
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 1.886281\n",
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 1.635980\n",
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 1.967227\n",
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 2.316731\n",
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 2.207585\n",
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 2.402229\n",
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 2.269636\n",
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 2.230212\n",
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 2.258510\n",
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 2.180698\n",
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 2.083038\n",
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 2.690881\n",
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 2.746495\n",
      "Epoch: 2804/3000... Step: 89700... Loss: 0.586480... Val Loss: 2.838837\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 3.569731\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 2.468699\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 1.930972\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 1.739490\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 1.772836\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 2.223200\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 2.096477\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 2.273379\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 2.166912\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 2.229853\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 2.113823\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 2.026292\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 1.965255\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 2.428222\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 2.433624\n",
      "Epoch: 2807/3000... Step: 89800... Loss: 0.293531... Val Loss: 2.367343\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 3.131988\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 2.060644\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 1.716940\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 1.494607\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 1.534053\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 2.053349\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 1.937697\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 2.278441\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 2.154431\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 2.133785\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 2.034169\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 1.917642\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 1.840317\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 2.217680\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 2.249156\n",
      "Epoch: 2810/3000... Step: 89900... Loss: 0.215373... Val Loss: 2.197357\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 2.541908\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 2.185086\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 1.670745\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 1.431481\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 1.346452\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 2.024323\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 1.867294\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 1.972184\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 1.907173\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 1.937594\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 1.892938\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 1.800565\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 1.722895\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 2.235300\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 2.220830\n",
      "Epoch: 2813/3000... Step: 90000... Loss: 0.359585... Val Loss: 2.201774\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 2.963759\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 2.082740\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 1.522940\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 1.301789\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 1.362898\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 1.861819\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 1.754086\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 2.137731\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 2.027228\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 2.089308\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 2.120647\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 2.124209\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 2.017413\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 2.318611\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 2.283279\n",
      "Epoch: 2816/3000... Step: 90100... Loss: 0.393308... Val Loss: 2.278186\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 3.080821\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 2.290901\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 1.793419\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 1.617791\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 1.530961\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 1.971311\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 1.867825\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 2.133633\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 2.047544\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 2.025724\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 1.965856\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 1.919473\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 1.845662\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 2.644902\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 2.639179\n",
      "Epoch: 2819/3000... Step: 90200... Loss: 1.468300... Val Loss: 2.563202\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 2.786938\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 1.991872\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 1.554482\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 1.329799\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 1.578674\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 1.995995\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 1.899231\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 2.188641\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 2.077331\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 2.005623\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 1.906672\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 1.842103\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 1.768089\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 2.405933\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 2.490101\n",
      "Epoch: 2822/3000... Step: 90300... Loss: 1.874676... Val Loss: 2.677305\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 2.938961\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 1.975024\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 1.487157\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 1.277153\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 1.231486\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 1.701861\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 1.647595\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 1.958684\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 1.878709\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 1.890270\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 1.859244\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 1.876386\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 1.796079\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 2.053131\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 2.073764\n",
      "Epoch: 2825/3000... Step: 90400... Loss: 0.818039... Val Loss: 2.132034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 3.944740\n",
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 2.556957\n",
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 1.906039\n",
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 1.704725\n",
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 1.778789\n",
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 2.161113\n",
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 1.940395\n",
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 2.113834\n",
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 2.027727\n",
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 2.051143\n",
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 1.938582\n",
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 1.836164\n",
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 1.776740\n",
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 2.548765\n",
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 2.606429\n",
      "Epoch: 2829/3000... Step: 90500... Loss: 1.322583... Val Loss: 2.772583\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 3.363440\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 2.170741\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 1.735291\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 1.439277\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 1.467792\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 1.913804\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 1.812891\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 2.034497\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 1.949068\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 1.969971\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 1.880562\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 1.821621\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 1.754341\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 2.112364\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 2.136915\n",
      "Epoch: 2832/3000... Step: 90600... Loss: 0.388246... Val Loss: 2.170284\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 3.484134\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 3.058861\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 2.385278\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 2.203662\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 2.197626\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 2.733132\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 2.646950\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 2.800327\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 2.745542\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 2.954056\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 2.902449\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 2.780401\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 2.680848\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 3.141457\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 3.119284\n",
      "Epoch: 2835/3000... Step: 90700... Loss: 0.961488... Val Loss: 3.125603\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 2.777629\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 2.192804\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 1.667215\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 1.424581\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 1.353723\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 1.866151\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 1.795270\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 2.035105\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 1.953560\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 1.972799\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 1.851010\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 1.830716\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 1.752984\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 2.422252\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 2.434250\n",
      "Epoch: 2838/3000... Step: 90800... Loss: 0.188783... Val Loss: 2.462031\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 3.011773\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 2.102495\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 1.726379\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 1.619450\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 1.577850\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 2.039877\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 2.025339\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 2.433708\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 2.337830\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 2.230881\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 2.148132\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 2.070149\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 1.995503\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 2.456436\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 2.492047\n",
      "Epoch: 2841/3000... Step: 90900... Loss: 1.038043... Val Loss: 2.452072\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 3.526826\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 2.625154\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 2.095100\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 1.929548\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 1.912694\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 2.380541\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 2.536330\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 3.513647\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 3.328391\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 3.172642\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 3.007501\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 2.950103\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 2.830882\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 2.900068\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 2.927309\n",
      "Epoch: 2844/3000... Step: 91000... Loss: 3.737405... Val Loss: 2.933724\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 3.102481\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 2.012172\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 1.526167\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 1.304573\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 1.214743\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 1.723645\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 1.772664\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 2.179176\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 2.095221\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 2.050590\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 1.937187\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 2.001152\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 1.919870\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 2.142288\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 2.139447\n",
      "Epoch: 2847/3000... Step: 91100... Loss: 0.693166... Val Loss: 2.131119\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 3.240904\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 2.690506\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 2.322383\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 1.919706\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 2.334451\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 2.751731\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 2.584928\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 2.643510\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 2.502072\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 2.679945\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 2.544480\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 2.438693\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 2.355655\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 3.189708\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 3.144621\n",
      "Epoch: 2850/3000... Step: 91200... Loss: 1.966137... Val Loss: 3.351115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 2.959839\n",
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 2.065963\n",
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 1.542945\n",
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 1.398601\n",
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 1.303233\n",
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 1.781584\n",
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 1.709033\n",
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 2.042851\n",
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 1.943669\n",
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 1.949130\n",
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 1.869647\n",
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 1.864552\n",
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 1.789950\n",
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 2.109706\n",
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 2.176968\n",
      "Epoch: 2854/3000... Step: 91300... Loss: 0.372901... Val Loss: 2.178577\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 4.285168\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 2.775716\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 2.048114\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 1.793243\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 2.086940\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 2.448425\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 2.307031\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 2.416011\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 2.319372\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 2.332230\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 2.233289\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 2.137303\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 2.061736\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 2.819389\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 2.812430\n",
      "Epoch: 2857/3000... Step: 91400... Loss: 1.183232... Val Loss: 3.105863\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 3.289924\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 2.358287\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 1.756811\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 1.555820\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 1.561492\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 2.054881\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 1.892265\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 2.058264\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 1.969672\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 2.019016\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 1.965984\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 1.874429\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 1.811812\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 2.356183\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 2.326380\n",
      "Epoch: 2860/3000... Step: 91500... Loss: 0.194908... Val Loss: 2.329201\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 4.013559\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 2.892054\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 2.446262\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 2.151597\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 2.015129\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 2.619674\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 2.586345\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 2.793784\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 2.732794\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 2.790569\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 2.644620\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 2.553815\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 2.461987\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 2.761515\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 2.760083\n",
      "Epoch: 2863/3000... Step: 91600... Loss: 0.339342... Val Loss: 2.811518\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 3.800387\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 2.752155\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 2.111853\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 1.910398\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 1.870632\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 2.381540\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 2.261551\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 2.695927\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 2.616219\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 2.641974\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 2.647721\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 2.664824\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 2.561102\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 2.763522\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 2.798225\n",
      "Epoch: 2866/3000... Step: 91700... Loss: 0.860479... Val Loss: 2.838360\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 3.886378\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 3.166560\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 2.550702\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 2.401149\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 2.343783\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 2.815769\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 2.741587\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 2.882264\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 2.855343\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 2.888445\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 2.730696\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 2.657856\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 2.572023\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 3.023457\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 3.059109\n",
      "Epoch: 2869/3000... Step: 91800... Loss: 2.967939... Val Loss: 3.110107\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 2.893275\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 1.963380\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 1.453607\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 1.308752\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 1.699434\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 2.177190\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 1.976748\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 2.098445\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 2.003874\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 1.985138\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 2.050797\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 2.019080\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 1.934879\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 2.413430\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 2.420136\n",
      "Epoch: 2872/3000... Step: 91900... Loss: 0.900974... Val Loss: 2.711932\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 3.147510\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 2.131241\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 1.693411\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 1.544862\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 1.476929\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 1.941054\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 1.840565\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 2.263242\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 2.145253\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 2.093522\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 1.998275\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 1.965379\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 1.887186\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 2.567297\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 2.592171\n",
      "Epoch: 2875/3000... Step: 92000... Loss: 0.672022... Val Loss: 2.530676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 3.291038\n",
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 2.238235\n",
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 1.888064\n",
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 1.695239\n",
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 1.989586\n",
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 2.591473\n",
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 2.456815\n",
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 2.927928\n",
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 2.753922\n",
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 2.684112\n",
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 2.570012\n",
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 2.456746\n",
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 2.366844\n",
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 2.710318\n",
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 2.678585\n",
      "Epoch: 2879/3000... Step: 92100... Loss: 1.490170... Val Loss: 2.619462\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 2.814842\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 2.055210\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 1.492943\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 1.316467\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 1.276840\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 1.879665\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 1.707846\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 1.868077\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 1.793208\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 1.848509\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 1.823468\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 1.759197\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 1.677974\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 2.110634\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 2.104562\n",
      "Epoch: 2882/3000... Step: 92200... Loss: 0.290618... Val Loss: 2.062464\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 4.761048\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 3.238643\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 3.073765\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 2.953634\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 3.491996\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 3.774680\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 3.728285\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 4.320039\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 4.152022\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 3.898640\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 3.806452\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 3.751116\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 3.759500\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 4.178737\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 4.284041\n",
      "Epoch: 2885/3000... Step: 92300... Loss: 0.208357... Val Loss: 4.884913\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 3.204320\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 2.276672\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 1.657802\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 1.506949\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 1.463531\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 1.931368\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 1.778779\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 1.917568\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 1.847514\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 1.897643\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 1.809373\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 1.818703\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 1.747018\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 2.285887\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 2.275544\n",
      "Epoch: 2888/3000... Step: 92400... Loss: 0.191901... Val Loss: 2.275598\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 2.785319\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 1.972530\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 1.576490\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 1.369438\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 1.380229\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 1.871959\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 1.890435\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 2.172478\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 2.100173\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 2.078115\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 1.980339\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 1.931875\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 1.858072\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 2.501344\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 2.517132\n",
      "Epoch: 2891/3000... Step: 92500... Loss: 1.136507... Val Loss: 2.517911\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 4.468620\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 3.093228\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 2.451878\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 2.091341\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 2.181656\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 2.608723\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 2.681601\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 3.946789\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 3.675128\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 3.470464\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 3.279421\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 3.160932\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 3.121189\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 3.210909\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 3.230456\n",
      "Epoch: 2894/3000... Step: 92600... Loss: 3.615687... Val Loss: 3.161858\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 2.462048\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 1.931871\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 1.528232\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 1.328353\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 1.292477\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 1.839881\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 1.760627\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 1.920699\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 1.846451\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 1.873395\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 1.803928\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 1.707466\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 1.639657\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 2.058145\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 2.059028\n",
      "Epoch: 2897/3000... Step: 92700... Loss: 0.798318... Val Loss: 2.064072\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 3.110748\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 2.126979\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 1.655877\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 1.484578\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 1.476297\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 1.927042\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 1.757181\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 1.908665\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 1.809678\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 1.872570\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 1.774508\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 1.727506\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 1.654615\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 2.054867\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 2.086442\n",
      "Epoch: 2900/3000... Step: 92800... Loss: 0.626417... Val Loss: 2.098829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 3.235913\n",
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 2.192342\n",
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 1.715955\n",
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 1.582310\n",
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 1.553794\n",
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 2.033498\n",
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 1.901427\n",
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 2.147498\n",
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 2.029130\n",
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 2.124541\n",
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 1.979508\n",
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 1.948359\n",
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 1.867336\n",
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 2.315964\n",
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 2.331246\n",
      "Epoch: 2904/3000... Step: 92900... Loss: 0.634637... Val Loss: 2.364766\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 3.070619\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 2.117451\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 1.661165\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 1.505810\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 1.449386\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 1.962116\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 1.814434\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 2.258514\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 2.141375\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 2.079391\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 1.956141\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 1.885469\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 1.816519\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 2.114308\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 2.118882\n",
      "Epoch: 2907/3000... Step: 93000... Loss: 0.953797... Val Loss: 2.098859\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 2.829734\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 1.874051\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 1.412526\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 1.194589\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 1.196789\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 1.924576\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 1.802909\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 2.093120\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 1.990235\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 2.016004\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 1.947987\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 1.867391\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 1.792284\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 2.128982\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 2.160798\n",
      "Epoch: 2910/3000... Step: 93100... Loss: 0.132659... Val Loss: 2.189244\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 2.543506\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 1.835656\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 1.525002\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 1.414848\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 1.409857\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 1.922967\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 1.808695\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 2.003883\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 1.948634\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 1.938886\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 1.856327\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 1.864127\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 1.815475\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 2.470904\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 2.477922\n",
      "Epoch: 2913/3000... Step: 93200... Loss: 0.164845... Val Loss: 2.398861\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 2.695200\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 2.066396\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 1.560140\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 1.290010\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 1.211122\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 1.805605\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 1.687728\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 1.805668\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 1.747368\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 1.810377\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 1.738073\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 1.687907\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 1.617674\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 2.134040\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 2.133324\n",
      "Epoch: 2916/3000... Step: 93300... Loss: 0.779179... Val Loss: 2.092119\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 3.415593\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 2.328050\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 1.821718\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 1.611299\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 1.705697\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 2.173713\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 2.042420\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 2.313797\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 2.192018\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 2.239333\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 2.120943\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 2.017105\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 1.939762\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 2.441041\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 2.437304\n",
      "Epoch: 2919/3000... Step: 93400... Loss: 1.545222... Val Loss: 2.406688\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 3.897930\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 3.158052\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 2.472503\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 2.200674\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 2.310059\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 2.796589\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 2.652768\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 2.828994\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 2.750855\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 2.881913\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 2.785086\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 2.713624\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 2.616077\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 2.942697\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 2.912267\n",
      "Epoch: 2922/3000... Step: 93500... Loss: 1.000022... Val Loss: 3.165163\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 3.413092\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 2.164721\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 1.722031\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 1.492756\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 1.333635\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 3.032695\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 2.788034\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 2.924237\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 2.728468\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 2.685572\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 2.633721\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 2.527814\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 2.405438\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 2.664048\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 2.671785\n",
      "Epoch: 2925/3000... Step: 93600... Loss: 1.684759... Val Loss: 2.750441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 4.481957\n",
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 3.835521\n",
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 3.094430\n",
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 2.880709\n",
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 3.928694\n",
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 4.275687\n",
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 3.931734\n",
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 3.762614\n",
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 3.614661\n",
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 3.700829\n",
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 3.490838\n",
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 3.352749\n",
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 3.277151\n",
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 3.862843\n",
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 3.810148\n",
      "Epoch: 2929/3000... Step: 93700... Loss: 1.753341... Val Loss: 4.215567\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 3.282465\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 2.281432\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 1.834555\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 1.689898\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 1.814460\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 2.238444\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 2.120384\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 2.408452\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 2.346701\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 2.380243\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 2.255104\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 2.168103\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 2.095112\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 2.375134\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 2.385901\n",
      "Epoch: 2932/3000... Step: 93800... Loss: 0.312492... Val Loss: 2.394950\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 3.383466\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 2.201929\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 1.741891\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 1.433330\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 1.610645\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 3.756425\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 3.573033\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 3.798053\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 3.526710\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 3.383335\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 3.151223\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 3.008972\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 2.881374\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 2.963717\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 2.945547\n",
      "Epoch: 2935/3000... Step: 93900... Loss: 0.837795... Val Loss: 3.070064\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 2.563539\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 1.840626\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 1.409559\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 1.380177\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 1.409534\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 1.861944\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 1.725164\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 1.894047\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 1.827937\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 1.925653\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 1.839889\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 1.755429\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 1.693420\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 2.036350\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 2.057300\n",
      "Epoch: 2938/3000... Step: 94000... Loss: 0.241010... Val Loss: 2.039684\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 3.235800\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 2.233178\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 1.749587\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 1.601673\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 1.434252\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 1.927347\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 1.891915\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 2.112542\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 2.044537\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 2.044678\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 2.174961\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 2.138351\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 2.066588\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 2.544023\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 2.549517\n",
      "Epoch: 2941/3000... Step: 94100... Loss: 0.874494... Val Loss: 2.545143\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 3.067987\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 2.074750\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 1.629331\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 1.447359\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 1.389509\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 1.786558\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 1.789571\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 2.143132\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 2.025921\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 2.068709\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 2.051873\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 2.017510\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 1.932598\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 2.187913\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 2.224696\n",
      "Epoch: 2944/3000... Step: 94200... Loss: 2.029522... Val Loss: 2.246405\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 2.822525\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 2.016909\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 1.506638\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 1.355926\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 1.350318\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 1.921744\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 1.811689\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 2.045460\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 1.969041\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 1.965443\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 1.925745\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 1.863770\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 1.791411\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 2.530509\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 2.569626\n",
      "Epoch: 2947/3000... Step: 94300... Loss: 2.400123... Val Loss: 2.583650\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 3.181954\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 3.072409\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 2.600962\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 2.535078\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 2.401581\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 2.901224\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 2.793717\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 2.906584\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 2.898536\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 3.051223\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 2.956275\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 2.854846\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 2.756702\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 3.399197\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 3.345576\n",
      "Epoch: 2950/3000... Step: 94400... Loss: 2.466016... Val Loss: 3.246397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 2.997448\n",
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 2.167777\n",
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 1.649902\n",
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 1.465804\n",
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 1.488961\n",
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 1.925581\n",
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 1.811825\n",
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 2.002219\n",
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 1.924941\n",
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 1.955949\n",
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 1.833231\n",
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 1.771185\n",
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 1.702402\n",
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 2.205614\n",
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 2.236624\n",
      "Epoch: 2954/3000... Step: 94500... Loss: 0.647774... Val Loss: 2.276825\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 4.569380\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 2.816944\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 2.043976\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 1.733727\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 1.855901\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 2.355764\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 2.212142\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 2.685861\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 2.559555\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 2.568061\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 2.429699\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 2.354623\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 2.279563\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 2.498853\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 2.490014\n",
      "Epoch: 2957/3000... Step: 94600... Loss: 0.838859... Val Loss: 2.675761\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 2.719172\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 1.866853\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 1.447797\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 1.283785\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 1.272052\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 1.835194\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 1.728350\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 2.034831\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 1.941610\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 1.952142\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 1.910820\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 1.815502\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 1.737224\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 2.057592\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 2.072310\n",
      "Epoch: 2960/3000... Step: 94700... Loss: 0.115394... Val Loss: 2.090423\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 2.522942\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 1.784984\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 1.460414\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 1.336189\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 1.423850\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 1.912672\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 1.853694\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 2.110322\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 2.014435\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 1.964428\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 2.018838\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 1.933756\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 1.881363\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 2.334727\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 2.351540\n",
      "Epoch: 2963/3000... Step: 94800... Loss: 0.291292... Val Loss: 2.330907\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 3.542186\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 2.389619\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 1.947652\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 1.757271\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 1.658781\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 2.104547\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 1.986345\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 2.528524\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 2.391525\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 2.427322\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 2.251584\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 2.225275\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 2.135527\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 2.348565\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 2.405717\n",
      "Epoch: 2966/3000... Step: 94900... Loss: 0.786206... Val Loss: 2.473631\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 2.641104\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 1.824402\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 1.446879\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 1.327565\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 1.269209\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 1.856913\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 1.781505\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 2.212471\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 2.118829\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 2.123442\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 2.030445\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 1.974595\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 1.888183\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 2.163749\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 2.201500\n",
      "Epoch: 2969/3000... Step: 95000... Loss: 1.724598... Val Loss: 2.219597\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 4.610487\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 3.762574\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 3.000090\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 2.810795\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 2.746837\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 3.245765\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 3.143243\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 3.526328\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 3.451908\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 3.562740\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 3.416295\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 3.336193\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 3.238844\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 3.585219\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 3.539414\n",
      "Epoch: 2972/3000... Step: 95100... Loss: 1.554631... Val Loss: 3.518024\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 2.735653\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 1.807278\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 1.369599\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 1.237832\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 1.371854\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 1.837858\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 1.779819\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 2.129044\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 2.027939\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 2.006785\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 1.871738\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 1.839555\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 1.760066\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 2.018202\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 2.059104\n",
      "Epoch: 2975/3000... Step: 95200... Loss: 0.508323... Val Loss: 2.050112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 3.307751\n",
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 2.364109\n",
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 1.859171\n",
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 1.655947\n",
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 1.775267\n",
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 2.254136\n",
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 2.190700\n",
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 2.613732\n",
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 2.469512\n",
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 2.443377\n",
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 2.321688\n",
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 2.328471\n",
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 2.225382\n",
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 2.380475\n",
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 2.381392\n",
      "Epoch: 2979/3000... Step: 95300... Loss: 0.467399... Val Loss: 2.406793\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 2.903008\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 2.024894\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 1.572673\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 1.404393\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 1.351379\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 1.868328\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 1.771911\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 1.937462\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 1.863787\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 1.903032\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 1.827427\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 1.741839\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 1.680330\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 1.964372\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 1.978303\n",
      "Epoch: 2982/3000... Step: 95400... Loss: 0.244652... Val Loss: 1.992560\n",
      "Validation loss decreased (2.028288 --> 1.992560).  Saving model ...\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 3.038157\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 2.132801\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 1.724000\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 1.562671\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 1.552119\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 2.120521\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 2.064763\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 2.399337\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 2.295653\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 2.314586\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 2.279665\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 2.222005\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 2.181186\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 2.605746\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 2.664902\n",
      "Epoch: 2985/3000... Step: 95500... Loss: 0.443448... Val Loss: 2.689927\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 3.338851\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 2.455530\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 1.913663\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 1.673848\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 1.555457\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 2.119091\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 2.018524\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 2.300713\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 2.193761\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 2.214066\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 2.074076\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 2.034693\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 1.960775\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 2.580128\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 2.610200\n",
      "Epoch: 2988/3000... Step: 95600... Loss: 0.308028... Val Loss: 2.640957\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 2.939623\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 2.074829\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 1.580776\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 1.442976\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 1.359872\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 1.867194\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 1.741660\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 1.961539\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 1.892959\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 1.934439\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 1.838041\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 1.789017\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 1.722349\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 2.118699\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 2.156324\n",
      "Epoch: 2991/3000... Step: 95700... Loss: 0.735348... Val Loss: 2.117712\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 3.086823\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 2.128417\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 1.640237\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 1.506199\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 1.525981\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 2.038469\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 1.902914\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 2.201542\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 2.127945\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 2.144071\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 2.079544\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 2.045506\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 1.952359\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 2.171343\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 2.268518\n",
      "Epoch: 2994/3000... Step: 95800... Loss: 1.407561... Val Loss: 2.360481\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 2.579906\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 1.787298\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 1.368198\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 1.146085\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 1.180344\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 1.741391\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 1.657467\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 1.871456\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 1.802434\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 1.829392\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 1.794991\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 1.756568\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 1.692834\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 2.025381\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 2.012361\n",
      "Epoch: 2997/3000... Step: 95900... Loss: 0.703989... Val Loss: 2.016326\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 8.252038\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 8.423800\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 7.586355\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 7.513523\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 7.285118\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 7.846441\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 7.761338\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 7.651743\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 7.697401\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 7.965716\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 7.776528\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 7.745132\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 7.663030\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 8.795694\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 8.644827\n",
      "Epoch: 3000/3000... Step: 96000... Loss: 8.674397... Val Loss: 8.528600\n"
     ]
    }
   ],
   "source": [
    "from utils import prep_data\n",
    "\n",
    "\n",
    "\n",
    "train_data_loader = prep_data(train_data, train_targets, batch_size = 32,first_n_epochs = 10 )\n",
    "test_data_loader = prep_data(test_data, test_targets, batch_size = 32,first_n_epochs = 10 )\n",
    "val_data_loader = prep_data(val_data, val_targets, batch_size = 32, first_n_epochs=10)\n",
    "\n",
    "\n",
    "model, optimizer, criterion, criterion_val = create_model(50, 3, bidirectional = True)\n",
    "train_validate_model(model, optimizer, criterion, criterion_val)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([83.5056, 81.3736, 80.0233, 81.2056, 83.1180, 81.3025, 84.1388, 80.9148,\n",
      "        84.1646, 70.7779, 59.4650, 70.4419, 81.4640, 80.5401, 82.1489, 83.8610,\n",
      "        81.8969, 59.4327, 79.8294, 83.2860, 83.6607, 78.5890, 71.3012, 75.5847,\n",
      "        51.6346, 73.3687, 78.2272, 81.9421, 81.5157, 82.1166, 76.8510, 81.7095])\n",
      "tensor([83.5508, 81.3063, 79.3927, 81.5711, 83.4153, 81.8422, 84.2683, 80.5153,\n",
      "        83.8900, 71.1488, 55.9860, 71.6234, 80.8638, 80.7991, 81.7856, 83.4737,\n",
      "        81.4938, 64.7265, 79.5146, 83.7675, 83.3762, 78.8027, 70.3407, 75.9860,\n",
      "        48.0201, 73.7338, 78.3844, 82.4203, 81.2067, 82.4773, 78.1876, 81.9994])\n",
      "tensor([82.3298, 83.9643, 59.0322, 83.9191, 84.3455, 84.1840, 80.6177, 84.4812,\n",
      "        80.9665, 82.7756, 84.0871, 82.8983, 83.1890, 78.0204, 88.0346, 84.0806,\n",
      "        83.8351, 82.6916, 89.1717, 80.6952, 68.0062, 77.3679, 72.8195, 84.2938,\n",
      "        83.0727, 84.6233, 74.5768, 83.2472, 84.1194, 82.5688, 82.0390, 83.3247])\n",
      "tensor([81.9162, 84.1669, 59.4689, 83.4477, 83.9707, 84.3442, 81.3375, 84.3582,\n",
      "        81.4626, 83.0879, 84.3770, 82.9553, 83.5252, 78.6198, 87.3309, 84.1904,\n",
      "        83.3548, 83.0273, 88.4037, 80.5047, 70.0683, 80.7132, 77.6390, 83.7365,\n",
      "        82.2175, 84.9072, 74.0266, 83.0906, 83.7329, 83.1538, 82.1480, 83.3152])\n",
      "tensor([83.7124, 84.2680, 84.9787, 79.2415, 57.7465, 84.3197, 83.4022, 84.5393,\n",
      "        76.3212, 60.0078, 84.8947, 73.8726, 57.7852, 76.7606, 89.1782, 75.0549,\n",
      "        83.3699, 84.6169, 83.9385, 84.5329, 73.2265, 81.4511, 78.6794, 85.0627,\n",
      "        85.2436, 84.7913, 82.9888, 84.2680, 78.8474, 81.4317, 81.2379, 83.9837])\n",
      "tensor([84.2415, 84.3811, 85.9322, 80.1794, 58.0381, 84.5161, 83.5300, 84.5047,\n",
      "        74.5087, 59.6450, 83.7010, 72.6240, 57.3745, 78.0108, 88.0440, 76.5247,\n",
      "        82.6854, 84.5124, 84.1824, 84.8584, 70.8019, 81.5456, 78.4353, 85.9283,\n",
      "        85.2667, 84.6411, 83.2334, 84.3409, 78.6875, 81.8307, 81.7084, 83.9116])\n",
      "tensor([60.8218, 45.2901, 83.2536, 57.2490, 79.4870, 79.8747, 84.3391, 58.6316,\n",
      "        72.1669, 83.6671, 83.5379, 83.4346, 79.7842, 79.2092, 75.8108, 24.5962,\n",
      "        76.5603, 77.1224, 85.4116, 83.9256, 81.3090, 82.0843, 79.2609, 79.6162,\n",
      "        76.8316, 82.3298, 78.7763, 82.8918, 62.3853, 83.4798, 84.2874, 76.7541])\n",
      "tensor([65.9426, 42.6463, 83.6402, 58.5076, 79.5351, 79.7598, 84.0884, 63.8965,\n",
      "        74.9031, 83.6911, 83.2184, 83.7436, 79.5050, 79.7316, 76.0933, 25.1324,\n",
      "        77.0891, 78.6092, 85.7208, 84.0352, 81.4189, 81.6434, 80.1288, 78.8331,\n",
      "        75.8427, 82.5826, 78.5835, 82.9808, 70.5916, 83.4387, 84.2629, 77.0294])\n",
      "tensor([77.1288, 77.8524, 81.8840, 82.9565, 82.8466, 71.7987, 83.2536, 63.3674,\n",
      "        83.1309, 81.6126, 71.1138, 83.6607, 79.9910, 60.4406, 83.5508, 83.6413,\n",
      "        63.4643, 81.1281, 30.0943, 77.8137, 85.0239, 67.1469, 85.0110, 81.7871,\n",
      "        73.0004, 84.7655, 81.3800, 80.7469, 82.4913, 82.6463, 82.0067, 77.3291])\n",
      "tensor([77.3657, 77.7779, 82.2708, 82.8506, 82.6948, 69.7284, 83.2427, 64.2307,\n",
      "        82.7301, 81.8568, 71.4705, 83.8813, 80.3450, 62.2517, 83.9142, 83.8759,\n",
      "        67.1183, 81.7155, 34.7021, 78.1715, 86.1301, 66.3451, 85.1614, 81.8558,\n",
      "        74.1082, 84.3264, 81.8285, 80.9549, 83.2485, 82.7808, 82.7710, 77.3273])\n",
      "tensor([74.9128, 74.6737, 83.2278, 81.3800, 85.7475, 83.6090, 78.8280, 81.6708,\n",
      "        82.4331, 81.1668, 80.4884, 83.5379, 69.1175, 87.7568, 83.5250, 82.0067,\n",
      "        83.9256, 83.4152, 85.5925, 84.1969, 57.7077, 68.0256, 83.6994, 76.3600,\n",
      "        84.2357, 54.1155, 81.5997, 76.6830, 79.6033, 87.7116, 83.2278, 86.1352])\n",
      "tensor([74.9132, 78.3622, 83.3336, 81.5132, 86.5884, 84.0205, 78.8106, 82.6705,\n",
      "        82.3653, 80.8679, 80.7420, 83.4003, 69.2965, 87.8366, 83.9446, 82.6589,\n",
      "        84.0170, 82.4639, 86.2220, 84.3130, 55.5805, 67.7523, 84.0591, 75.9974,\n",
      "        83.2293, 58.2162, 82.0834, 77.0458, 79.6886, 87.7223, 82.4555, 85.9958])\n",
      "tensor([82.3556, 55.3883, 75.8948, 78.8086, 76.2631, 78.2788, 80.8309, 60.7507,\n",
      "        82.8079, 73.5754, 80.5530, 77.9041, 70.6164, 81.3154, 59.1872, 78.5308,\n",
      "        66.0421, 82.1618, 79.2350, 83.9514, 82.9694, 75.7591, 84.2421, 70.4484,\n",
      "        79.7519, 81.2120, 81.2185, 73.2976, 83.7124, 74.2538, 79.9199, 83.3053])\n",
      "tensor([82.5789, 55.8445, 76.3801, 79.0767, 76.6152, 78.4457, 80.9160, 60.3674,\n",
      "        82.3792, 72.0115, 80.5592, 78.4851, 69.8212, 81.3196, 60.3957, 78.0296,\n",
      "        64.9534, 83.1852, 78.0085, 85.0110, 82.9000, 75.9182, 84.3990, 72.6411,\n",
      "        79.5672, 81.0714, 81.2281, 73.7490, 83.7686, 74.2508, 79.8517, 82.9927])\n",
      "tensor([70.2416, 84.2486, 60.2274, 83.2407, 83.5056, 83.9450, 79.2092, 82.9758,\n",
      "        83.7641, 84.1646, 86.1481, 58.2569, 83.1890, 82.7432, 88.9521, 79.3255,\n",
      "        76.0822, 75.2229, 79.9199, 83.1438, 69.6279, 81.8775, 77.6909, 80.1654,\n",
      "        83.8287, 84.1065, 80.3592, 82.4848, 70.8425, 82.8595, 79.9070, 70.9717])\n",
      "tensor([70.8327, 84.8854, 59.6241, 83.5047, 83.6429, 84.2297, 79.7032, 82.3048,\n",
      "        83.5645, 84.1550, 86.0595, 59.3851, 83.3511, 82.4784, 87.3785, 79.6084,\n",
      "        73.9434, 75.5894, 80.3505, 82.9224, 71.0028, 82.0385, 77.3182, 80.3709,\n",
      "        83.2653, 84.2767, 80.1343, 83.0127, 70.9419, 82.6172, 79.3783, 71.1648])\n",
      "tensor([82.3039, 77.3485, 83.3312, 83.2020, 76.7283, 84.1969, 72.4835, 82.9306,\n",
      "        83.2795, 84.1775, 79.4612, 85.6829, 74.3959, 77.6715, 79.4353, 69.8217,\n",
      "        70.9329, 84.0742, 84.6104, 82.6528, 71.6372, 83.0469, 83.5702, 78.9508,\n",
      "        75.9917, 85.4439, 79.5322, 83.1503, 83.6155, 81.5609, 80.9924, 83.8028])\n",
      "tensor([82.7904, 78.4902, 83.8201, 83.3872, 77.2952, 84.4829, 70.0837, 82.9311,\n",
      "        83.2923, 84.3895, 78.5818, 85.5255, 76.1196, 77.6770, 79.4457, 70.9860,\n",
      "        73.2016, 84.4206, 83.5760, 83.6791, 70.0439, 83.2706, 83.7606, 78.5284,\n",
      "        78.2170, 86.5475, 80.0185, 83.4310, 83.7594, 81.2230, 81.3778, 84.0092])\n",
      "tensor([84.2745, 83.1761, 83.0275, 58.9094, 81.3477, 68.8203, 82.2135, 80.2429,\n",
      "        84.2228, 84.4618, 83.4152, 61.1255, 75.9400, 76.2049, 80.7533, 79.5775,\n",
      "        67.2438, 82.9500, 80.7921, 77.0384, 78.3241, 83.9062, 77.5552, 67.3278,\n",
      "        60.7249, 81.0053, 83.2536, 79.5258, 82.9435, 84.1323, 81.6061, 84.5329])\n",
      "tensor([84.5587, 83.0886, 83.2027, 61.9174, 81.1834, 66.8784, 82.2826, 79.7103,\n",
      "        84.0813, 84.4447, 83.6893, 59.0814, 76.3625, 74.4596, 80.8765, 79.2413,\n",
      "        67.3913, 82.1751, 80.7066, 77.2702, 78.5413, 84.6452, 78.2021, 67.1552,\n",
      "        62.1704, 80.9279, 82.9767, 80.7620, 82.9305, 83.9189, 81.3880, 84.1227])\n",
      "tensor([83.5185, 76.2825, 81.9098, 83.3699, 80.2429, 80.9472, 86.9944, 83.6413,\n",
      "        81.7418, 81.2831, 80.9665, 76.1533, 76.8898, 81.1474, 82.1424, 80.4820,\n",
      "        82.8725, 83.3829, 84.9464, 80.2623, 79.6938, 81.9873, 72.0119, 83.8545,\n",
      "        73.2394, 84.1711, 84.2615, 84.1775, 73.3234, 77.6974, 66.0163, 59.4457])\n",
      "tensor([83.3540, 75.8513, 82.6841, 83.5555, 80.2132, 80.3985, 86.5795, 83.8442,\n",
      "        82.4387, 81.9912, 81.3945, 75.4973, 77.2866, 81.0286, 83.1159, 80.8289,\n",
      "        80.8895, 83.1664, 84.9706, 80.1890, 79.8802, 81.4566, 72.8557, 83.9514,\n",
      "        72.9396, 84.0721, 84.4084, 83.9830, 65.5097, 78.9594, 65.4708, 57.1683])\n",
      "tensor([78.7763, 83.2084, 84.6427, 75.6428, 81.5480, 76.3342, 84.2098, 72.6580,\n",
      "        70.6487, 82.1166, 83.0340, 84.6556, 76.3212, 46.4078, 71.8891, 83.1180,\n",
      "        83.7253, 83.5315, 82.3298, 82.1166, 75.2746, 85.0174, 80.5078, 82.8725,\n",
      "        81.3929, 75.5330, 73.4074, 84.1646, 78.6730, 82.1166, 77.9558, 73.0069])\n",
      "tensor([79.0311, 82.8393, 84.4019, 76.1124, 82.0002, 76.5994, 84.2037, 73.8637,\n",
      "        62.8638, 82.8910, 84.1841, 85.4096, 75.5817, 46.8763, 69.6294, 83.9138,\n",
      "        82.8365, 83.5542, 83.4163, 81.4101, 75.5434, 85.2770, 80.5290, 82.9101,\n",
      "        81.5475, 75.3444, 70.3478, 84.2167, 80.3879, 81.0835, 78.1062, 70.2747])\n",
      "tensor([46.7955, 84.7332, 83.6025, 75.7850, 84.6556, 84.4230, 74.4088, 84.1905,\n",
      "        80.2042, 84.9658, 75.8754, 87.0784, 82.1812, 85.8767, 85.7152, 84.1840,\n",
      "        84.0483, 85.3276, 78.0656, 59.8979, 82.8725, 83.7770, 84.7655, 74.9645,\n",
      "        83.6607, 82.3491, 73.8274, 75.8884, 72.6450, 89.2945, 84.2745, 80.4626])\n",
      "tensor([43.3088, 84.6884, 83.7017, 75.1875, 84.5375, 84.6309, 75.9240, 84.6759,\n",
      "        80.5463, 85.3184, 76.4753, 86.5317, 82.0062, 85.8359, 86.4656, 84.3717,\n",
      "        84.2003, 85.6842, 78.8307, 64.6034, 81.8815, 83.6836, 84.8988, 69.0645,\n",
      "        83.5681, 81.9321, 74.3836, 75.2918, 71.6073, 88.5480, 85.0936, 79.4432])\n",
      "tensor([79.5839, 88.2995, 83.4087, 82.7949, 79.9716, 83.0275, 72.0571, 81.7354,\n",
      "        78.8668, 77.7168, 83.4669, 87.1883, 81.1345, 83.2795, 53.7020, 81.3865,\n",
      "        56.8743, 60.7701, 79.0025, 75.7398, 82.1101, 82.8466, 82.8660, 55.9698,\n",
      "        82.6011, 86.7037, 61.8555, 76.3342, 84.5264, 76.5861, 65.1570, 84.0677])\n",
      "tensor([78.3185, 86.7669, 83.5154, 82.2270, 79.9835, 82.8409, 72.2526, 81.2779,\n",
      "        79.4639, 78.5692, 83.6893, 86.6492, 80.8447, 82.5278, 62.6109, 80.7410,\n",
      "        67.0734, 58.3804, 79.9849, 75.9581, 81.7824, 82.5222, 83.0416, 54.6620,\n",
      "        83.0848, 86.0867, 65.2008, 77.6274, 84.4834, 77.6433, 66.5751, 83.5053])\n",
      "tensor([83.9450, 83.0663, 80.7792, 79.6292, 84.5200, 76.2049, 84.9851, 77.9817,\n",
      "        87.3239, 81.5092, 68.1354, 79.2738, 76.0951, 80.8696, 82.1036, 80.9665,\n",
      "        63.3480, 79.9328, 80.1912, 80.1008, 81.2250, 75.1971, 82.9435, 83.3441,\n",
      "        84.2809, 84.5716, 76.9738, 77.3808, 77.1159, 76.7153, 66.6688, 81.2379])\n",
      "tensor([83.7136, 83.5034, 80.9741, 79.8726, 84.3745, 75.8693, 84.8586, 77.8099,\n",
      "        86.0839, 82.2371, 61.4360, 79.8084, 80.7717, 80.4525, 82.5371, 80.9842,\n",
      "        67.3977, 80.1446, 79.8562, 80.4924, 81.2064, 74.5787, 82.9795, 83.2713,\n",
      "        84.0346, 84.9040, 76.6776, 77.7849, 76.7821, 75.8726, 67.3207, 81.7319])\n",
      "tensor([82.6205, 84.3326, 81.6061, 69.6925, 84.0871, 75.9207, 82.0196, 84.4037,\n",
      "        85.1854, 83.2536, 70.0478, 82.9371, 83.4087, 69.6343, 85.1854, 75.6105,\n",
      "        74.0276, 85.8444, 75.5976, 81.5092])\n",
      "tensor([82.8905, 84.5668, 81.8222, 67.8786, 83.4398, 75.5009, 81.5894, 84.1059,\n",
      "        84.8027, 83.2663, 69.7925, 82.2270, 83.4865, 67.7645, 85.5572, 76.0312,\n",
      "        67.1415, 86.6566, 75.2070, 81.9900])\n",
      "16\n",
      "TOTAL Loss: 34.287\n",
      "Msqrt: 2.143\n",
      "Test loss: 2.143\n"
     ]
    }
   ],
   "source": [
    "test_acc, msqrt = test_model(model, criterion, criterion_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382\n",
      "618\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(len([x for x in train_targets if x < 80.0]))\n",
    "print(len([x for x in train_targets if x > 80.0]))\n",
    "\n",
    "\n",
    "plt.hist(train_targets,bins=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import logging\n",
    "\n",
    "from hpbandster.core.worker import Worker\n",
    "import hpbandster.core.nameserver as hpns\n",
    "import hpbandster.core.result as hpres\n",
    "from hpbandster.optimizers import BOHB\n",
    "\n",
    "logging.getLogger('hpbandster').setLevel(logging.DEBUG)\n",
    "\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "import torch.nn as nn\n",
    "\n",
    "hbster = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### evaluate function\n",
    "\n",
    "def evaluate_model(model, criterion, dataloader):\n",
    "    \n",
    "    test_losses = []\n",
    "    msqrt = 0\n",
    "    counter = 0\n",
    "    test_loss_sum_list = []\n",
    "\n",
    "    model.eval()\n",
    "    for inputs,configs, labels in dataloader:\n",
    "        batch_size_calc = len(labels)\n",
    "        counter = counter +1\n",
    "        h = model.init_hidden(batch_size_calc)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output, h = model(inputs,configs, h)\n",
    "        test_loss = criterion(output.squeeze(), labels.float())\n",
    "        test_losses.append(test_loss.item())\n",
    "        \n",
    "        test_loss_sum = criterion_val(output.squeeze(), labels.float())/(batch_size_calc)\n",
    "        test_loss_sum_list.append(test_loss_sum.item())\n",
    "\n",
    "        \n",
    "        pred = (output.squeeze())\n",
    "        \n",
    "        \n",
    "    print(\"Test loss: {:.3f}\".format(np.mean(test_loss_sum_list)))\n",
    "\n",
    "    return np.mean(test_loss_sum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(model, criterion, dataloader):\n",
    "\n",
    "\n",
    "    val_losses = []\n",
    "    model.eval()\n",
    "    test_loss_sum_list = []\n",
    "\n",
    "    for inp,configs, lab in self.validation_loader:\n",
    "\n",
    "        batch_size_calc = len(lab)\n",
    "\n",
    "        val_h = model.init_hidden(batch_size = batch_size_calc)\n",
    "\n",
    "\n",
    "        inp, lab = inp.to(device), lab.to(device)\n",
    "        out, val_h = model(inp, configs,val_h)#\n",
    "        \n",
    "        test_loss_sum = criterion_val(out.squeeze(), lab.float())/(batch_size_calc)\n",
    "        test_loss_sum_list = test_loss_sum_list.append(test_loss_sum)\n",
    "\n",
    "        \n",
    "        val_loss = abs(criterion(out.squeeze(), lab.float()))\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        print(\"Epoch: {}/{}...\".format(epoch, int(budget)),\n",
    "              \"Step: {}...\".format(counter),\n",
    "              \"Loss: {:.6f}...\".format(loss.item()),\n",
    "              \"Val Loss: {:.6f}\".format(np.mean(test_loss_sum_list)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model_BOHB(hidden_dim, num_layers, relative_size = 0.75, dropout = 0.5):\n",
    "    \n",
    "    \n",
    "    def weights_init(m):\n",
    "        for name, param in m.named_parameters():\n",
    "            torch.nn.init.uniform_(param) \n",
    "            \n",
    "    \n",
    "    outcome_dim = 1\n",
    "    seq_length = 10\n",
    "    config_size = 7\n",
    "    input_size = 1\n",
    "\n",
    "    model = LSTM_Net(input_size, outcome_dim, hidden_dim, seq_length, num_layers, config_size,\n",
    "                     relative_size = relative_size, drop_prob = dropout)\n",
    "    model.to(device)\n",
    "    \n",
    "    model.apply(weights_init)\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchWorker(Worker):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_loader = prep_data(train_data, train_targets, batch_size = 32,first_n_epochs = 10)\n",
    "        self.test_loader = prep_data(test_data, test_targets, batch_size = 32,first_n_epochs = 10)\n",
    "        self.validation_loader = prep_data(val_data, val_targets, batch_size = 32, first_n_epochs=10)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_model(config: CS.Configuration) -> nn.Module:\n",
    "        \"\"\" Define a configurable convolution model.\n",
    "            \n",
    "        See description of get_conv_model above for more details on the model.\n",
    "        \"\"\"\n",
    "        # START TODO ################ (1point)\n",
    "        # raise NotImplementedError  \n",
    "        \n",
    "        hidden_dim = config['hidden_dim']\n",
    "        num_layers = config['num_layers']\n",
    "        relative_size = config['relative_size']\n",
    "        dropout = config['dropout']\n",
    "        \n",
    "        model = create_model_BOHB(hidden_dim, num_layers, relative_size = relative_size, dropout = dropout)\n",
    "        return model\n",
    "    \n",
    "        # END TODO ################\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_configspace() -> CS.Configuration:\n",
    "        \"\"\" Define a conditional hyperparameter search-space.\n",
    "    \n",
    "        \"\"\"\n",
    "\n",
    "        cs = CS.ConfigurationSpace()\n",
    "        \n",
    "        \n",
    "        num_layers_cs = CSH.UniformIntegerHyperparameter(\"num_layers\",lower=1,upper=5)\n",
    "        hidden_dim_cs = CSH.UniformIntegerHyperparameter(\"hidden_dim\",lower=5,upper=500)\n",
    "        \n",
    "        lr = CSH.UniformFloatHyperparameter(\"lr\",lower=1e-6,upper=1e-1,log=True)\n",
    "        weight_decay = CSH.UniformFloatHyperparameter(\"weight_decay\",lower=1e-6,upper=1e-1,log=True)\n",
    "        relative_size = CSH.UniformFloatHyperparameter(\"relative_size\",lower=0.1,upper=1.0)\n",
    "        sgd_momentum = CSH.UniformFloatHyperparameter(\"sgd_momentum\",lower=0.00,upper=0.99)\n",
    "        dropout = CSH.UniformFloatHyperparameter(\"dropout\",lower=0.1,upper=0.8)\n",
    "\n",
    "        optimizer = CSH.CategoricalHyperparameter('optimizer', choices=['Adam', 'SGD'])\n",
    "        t_max =  CSH.UniformIntegerHyperparameter(\"T_max\",lower=10,upper=max_budget)\n",
    "        \n",
    "        cs.add_hyperparameters([num_layers_cs,dropout,t_max ,hidden_dim_cs, weight_decay, lr, relative_size, sgd_momentum, optimizer])\n",
    "    \n",
    "        condition3 = CS.EqualsCondition(sgd_momentum,optimizer,'SGD')\n",
    "        cs.add_condition(condition3)\n",
    "        \n",
    "    \n",
    "        \n",
    "        return cs\n",
    "\n",
    "    def compute(self, config: CS.Configuration, budget: float, working_directory: str,\n",
    "                *args, **kwargs) -> dict:\n",
    "        \"\"\"Evaluate a function with the given config and budget and return a loss.\n",
    "        \n",
    "        Bohb tries to minimize the returned loss.\n",
    "        \n",
    "        In our case the function is the training and validation of a model,\n",
    "        the budget is the number of epochs and the loss is the validation error.\n",
    "        \"\"\"\n",
    "        model = self.get_model(config)\n",
    "        \n",
    "        criterion = torch.nn.MSELoss()\n",
    "        if config['optimizer'] == 'Adam':\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay = config[\"weight_decay\"])\n",
    "        \n",
    "        else:\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=config['sgd_momentum'], weight_decay = config[\"weight_decay\"])\n",
    "\n",
    "                \n",
    "        t_max = config['T_max']\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, t_max)\n",
    "\n",
    "        \n",
    "        clip = 5\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for epoch in range(int(budget)):\n",
    "            \n",
    "            for batches, configs , labels in self.train_loader:\n",
    "                batch_size_calc = len(labels)\n",
    "                hidden = model.init_hidden(batch_size=batch_size_calc)\n",
    "                model.zero_grad()\n",
    "                output, hidden = model(batches, configs, hidden)\n",
    "                loss = criterion(output.squeeze(), labels.float())\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "                \n",
    "            scheduler.step()\n",
    "\n",
    "            \n",
    "        train_loss = evaluate_model(model, criterion,  self.train_loader)\n",
    "        val_loss = evaluate_model(model, criterion, self.validation_loader)\n",
    "        test_loss = evaluate_model(model, criterion, self.test_loader)\n",
    "\n",
    "        \n",
    "                \n",
    "        return ({\n",
    "                'loss': val_loss,  # remember: HpBandSter minimizes the loss!\n",
    "                'info': {'test_accuracy': test_loss,\n",
    "                         'train_accuracy': train_loss,\n",
    "                         'valid_accuracy': val_loss,\n",
    "                         'model': str(model)}\n",
    "                })\n",
    "        \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = '/home/sven/LCBench/'\n",
    "# minimum budget that BOHB uses\n",
    "min_budget = 200\n",
    "# largest budget BOHB will use\n",
    "max_budget = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T_max': 287, 'dropout': 0.43885485900234245, 'hidden_dim': 116, 'lr': 0.004965710218824816, 'num_layers': 2, 'optimizer': 'Adam', 'relative_size': 0.7660771906581066, 'weight_decay': 0.00011755110999779851}\n",
      "Test loss: 4.806\n",
      "Test loss: 5.662\n",
      "Test loss: 4.164\n",
      "{'loss': 5.6617428213357925, 'info': {'test_accuracy': 4.1643142476677895, 'train_accuracy': 4.805712576955557, 'valid_accuracy': 5.6617428213357925, 'model': 'LSTM_Net(\\n  (dropout): Dropout(p=0.43885485900234245, inplace=False)\\n  (rnn1): RNN(1, 116, num_layers=2)\\n  (linear1): Linear(in_features=7, out_features=88, bias=True)\\n  (linear2): Linear(in_features=88, out_features=88, bias=True)\\n  (linear3): Linear(in_features=204, out_features=204, bias=True)\\n  (linear4): Linear(in_features=204, out_features=58, bias=True)\\n  (linear5): Linear(in_features=58, out_features=1, bias=True)\\n  (dropout2): Dropout(p=0.43885485900234245, inplace=False)\\n  (linear_reduce1): Linear(in_features=1160, out_features=580, bias=True)\\n  (linear_reduce2): Linear(in_features=580, out_features=116, bias=True)\\n  (linear_reduce3): Linear(in_features=116, out_features=116, bias=True)\\n)'}}\n"
     ]
    }
   ],
   "source": [
    "if hbster:\n",
    "    worker = PyTorchWorker(run_id='0')\n",
    "    cs = worker.get_configspace()\n",
    "\n",
    "    config = cs.sample_configuration().get_dictionary()\n",
    "    print(config)\n",
    "\n",
    "    res = worker.compute(config=config, budget=min_budget, working_directory=working_dir)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hbster:    \n",
    "    \n",
    "    result_file = os.path.join(working_dir, 'bohb_result.pkl')\n",
    "    nic_name = 'lo0'\n",
    "    port = 0\n",
    "    run_id = 'bohb_run_1'\n",
    "    n_bohb_iterations = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:57:49 wait_for_workers trying to get the condition\n",
      "18:57:49 DISPATCHER: started the 'discover_worker' thread\n",
      "18:57:49 WORKER: Connected to nameserver <Pyro4.core.Proxy at 0x7f448ef42320; connected IPv4; for PYRO:Pyro.NameServer@127.0.0.1:44085>\n",
      "18:57:49 DISPATCHER: started the 'job_runner' thread\n",
      "18:57:49 WORKER: No dispatcher found. Waiting for one to initiate contact.\n",
      "18:57:49 WORKER: start listening for jobs\n",
      "18:57:49 DISPATCHER: Pyro daemon running on 127.0.0.1:43121\n",
      "18:57:49 DISPATCHER: Starting worker discovery\n",
      "18:57:49 DISPATCHER: Found 1 potential workers, 0 currently in the pool.\n",
      "18:57:49 DISPATCHER: discovered new worker, hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768\n",
      "18:57:49 HBMASTER: number of workers changed to 1\n",
      "18:57:49 Enough workers to start this run!\n",
      "18:57:49 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "18:57:49 adjust_queue_size: lock accquired\n",
      "18:57:49 HBMASTER: starting run at 1580925469.8871536\n",
      "18:57:49 HBMASTER: adjusted queue size to (0, 1)\n",
      "18:57:49 DISPATCHER: Finished worker discovery\n",
      "18:57:49 start sampling a new configuration.\n",
      "18:57:49 DISPATCHER: Trying to submit another job.\n",
      "18:57:49 done sampling a new configuration.\n",
      "18:57:49 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "18:57:49 HBMASTER: schedule new run for iteration 0\n",
      "18:57:49 HBMASTER: trying submitting job (0, 0, 0) to dispatcher\n",
      "18:57:49 HBMASTER: submitting job (0, 0, 0) to dispatcher\n",
      "18:57:49 DISPATCHER: trying to submit job (0, 0, 0)\n",
      "18:57:49 DISPATCHER: trying to notify the job_runner thread.\n",
      "18:57:49 HBMASTER: job (0, 0, 0) submitted to dispatcher\n",
      "18:57:49 DISPATCHER: Trying to submit another job.\n",
      "18:57:49 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "18:57:49 DISPATCHER: starting job (0, 0, 0) on hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768\n",
      "18:57:49 DISPATCHER: job (0, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768\n",
      "18:57:49 WORKER: start processing job (0, 0, 0)\n",
      "18:57:49 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "18:57:49 WORKER: args: ()\n",
      "18:57:49 WORKER: kwargs: {'config': {'T_max': 174, 'dropout': 0.4673956674368789, 'hidden_dim': 365, 'lr': 0.008121916032262325, 'num_layers': 1, 'optimizer': 'SGD', 'relative_size': 0.4391985504432502, 'weight_decay': 7.713318355328748e-06, 'sgd_momentum': 0.136064389218762}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "18:58:49 DISPATCHER: Starting worker discovery\n",
      "18:58:49 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:58:49 DISPATCHER: Finished worker discovery\n",
      "18:59:49 DISPATCHER: Starting worker discovery\n",
      "18:59:49 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:59:49 DISPATCHER: Finished worker discovery\n",
      "19:00:49 DISPATCHER: Starting worker discovery\n",
      "19:00:49 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:00:49 DISPATCHER: Finished worker discovery\n",
      "19:01:49 DISPATCHER: Starting worker discovery\n",
      "19:01:49 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:01:49 DISPATCHER: Finished worker discovery\n",
      "19:02:49 DISPATCHER: Starting worker discovery\n",
      "19:02:49 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:02:49 DISPATCHER: Finished worker discovery\n",
      "19:03:49 DISPATCHER: Starting worker discovery\n",
      "19:03:49 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:03:49 DISPATCHER: Finished worker discovery\n",
      "19:04:49 DISPATCHER: Starting worker discovery\n",
      "19:04:49 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:04:49 DISPATCHER: Finished worker discovery\n",
      "19:05:49 DISPATCHER: Starting worker discovery\n",
      "19:05:49 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:05:49 DISPATCHER: Finished worker discovery\n",
      "19:06:49 DISPATCHER: Starting worker discovery\n",
      "19:06:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:06:50 DISPATCHER: Finished worker discovery\n",
      "19:07:50 DISPATCHER: Starting worker discovery\n",
      "19:07:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:07:50 DISPATCHER: Finished worker discovery\n",
      "19:08:50 DISPATCHER: Starting worker discovery\n",
      "19:08:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:08:50 DISPATCHER: Finished worker discovery\n",
      "19:09:50 DISPATCHER: Starting worker discovery\n",
      "19:09:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:09:50 DISPATCHER: Finished worker discovery\n",
      "19:10:50 DISPATCHER: Starting worker discovery\n",
      "19:10:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:10:50 DISPATCHER: Finished worker discovery\n",
      "19:11:50 DISPATCHER: Starting worker discovery\n",
      "19:11:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:11:50 DISPATCHER: Finished worker discovery\n",
      "19:12:50 DISPATCHER: Starting worker discovery\n",
      "19:12:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:12:50 DISPATCHER: Finished worker discovery\n",
      "19:13:50 DISPATCHER: Starting worker discovery\n",
      "19:13:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:13:50 DISPATCHER: Finished worker discovery\n",
      "19:14:50 DISPATCHER: Starting worker discovery\n",
      "19:14:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:14:50 DISPATCHER: Finished worker discovery\n",
      "19:15:50 DISPATCHER: Starting worker discovery\n",
      "19:15:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:15:50 DISPATCHER: Finished worker discovery\n",
      "19:16:50 DISPATCHER: Starting worker discovery\n",
      "19:16:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:16:50 DISPATCHER: Finished worker discovery\n",
      "19:17:50 DISPATCHER: Starting worker discovery\n",
      "19:17:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:17:50 DISPATCHER: Finished worker discovery\n",
      "19:18:50 DISPATCHER: Starting worker discovery\n",
      "19:18:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:18:50 DISPATCHER: Finished worker discovery\n",
      "19:19:50 DISPATCHER: Starting worker discovery\n",
      "19:19:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:19:50 DISPATCHER: Finished worker discovery\n",
      "19:20:50 DISPATCHER: Starting worker discovery\n",
      "19:20:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:20:50 DISPATCHER: Finished worker discovery\n",
      "19:21:50 DISPATCHER: Starting worker discovery\n",
      "19:21:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:21:50 DISPATCHER: Finished worker discovery\n",
      "19:22:50 DISPATCHER: Starting worker discovery\n",
      "19:22:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:22:50 DISPATCHER: Finished worker discovery\n",
      "19:23:50 DISPATCHER: Starting worker discovery\n",
      "19:23:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:23:50 DISPATCHER: Finished worker discovery\n",
      "19:24:50 DISPATCHER: Starting worker discovery\n",
      "19:24:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:24:50 DISPATCHER: Finished worker discovery\n",
      "19:25:50 DISPATCHER: Starting worker discovery\n",
      "19:25:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:25:50 DISPATCHER: Finished worker discovery\n",
      "19:26:50 DISPATCHER: Starting worker discovery\n",
      "19:26:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:26:50 DISPATCHER: Finished worker discovery\n",
      "19:27:50 DISPATCHER: Starting worker discovery\n",
      "19:27:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:27:50 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1345817536018180582374067142656.000\n",
      "Test loss: 1281483020841830882511851356160.000\n",
      "Test loss: 1346225697236844730114354708480.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:28:15 WORKER: done with job (0, 0, 0), trying to register it.\n",
      "19:28:15 WORKER: registered result for job (0, 0, 0) with dispatcher\n",
      "19:28:15 DISPATCHER: job (0, 0, 0) finished\n",
      "19:28:15 DISPATCHER: register_result: lock acquired\n",
      "19:28:16 DISPATCHER: job (0, 0, 0) on hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768 finished\n",
      "19:28:16 job_id: (0, 0, 0)\n",
      "kwargs: {'config': {'T_max': 174, 'dropout': 0.4673956674368789, 'hidden_dim': 365, 'lr': 0.008121916032262325, 'num_layers': 1, 'optimizer': 'SGD', 'relative_size': 0.4391985504432502, 'weight_decay': 7.713318355328748e-06, 'sgd_momentum': 0.136064389218762}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 1.281483020841831e+30, 'info': {'test_accuracy': 1.3462256972368447e+30, 'train_accuracy': 1.3458175360181806e+30, 'valid_accuracy': 1.281483020841831e+30, 'model': 'LSTM_Net(\\n  (dropout): Dropout(p=0.4673956674368789, inplace=False)\\n  (rnn1): RNN(1, 365)\\n  (linear1): Linear(in_features=7, out_features=160, bias=True)\\n  (linear2): Linear(in_features=160, out_features=160, bias=True)\\n  (linear3): Linear(in_features=525, out_features=525, bias=True)\\n  (linear4): Linear(in_features=525, out_features=182, bias=True)\\n  (linear5): Linear(in_features=182, out_features=1, bias=True)\\n  (dropout2): Dropout(p=0.4673956674368789, inplace=False)\\n  (linear_reduce1): Linear(in_features=3650, out_features=1825, bias=True)\\n  (linear_reduce2): Linear(in_features=1825, out_features=365, bias=True)\\n  (linear_reduce3): Linear(in_features=365, out_features=365, bias=True)\\n)'}}\n",
      "exception: None\n",
      "\n",
      "19:28:16 job_callback for (0, 0, 0) started\n",
      "19:28:16 DISPATCHER: Trying to submit another job.\n",
      "19:28:16 job_callback for (0, 0, 0) got condition\n",
      "19:28:16 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "19:28:16 Only 1 run(s) for budget 333.333333 available, need more than 11 -> can't build model!\n",
      "19:28:16 HBMASTER: Trying to run another job!\n",
      "19:28:16 job_callback for (0, 0, 0) finished\n",
      "19:28:16 start sampling a new configuration.\n",
      "19:28:17 done sampling a new configuration.\n",
      "19:28:17 HBMASTER: schedule new run for iteration 0\n",
      "19:28:17 HBMASTER: trying submitting job (0, 0, 1) to dispatcher\n",
      "19:28:17 HBMASTER: submitting job (0, 0, 1) to dispatcher\n",
      "19:28:17 DISPATCHER: trying to submit job (0, 0, 1)\n",
      "19:28:17 DISPATCHER: trying to notify the job_runner thread.\n",
      "19:28:17 HBMASTER: job (0, 0, 1) submitted to dispatcher\n",
      "19:28:17 DISPATCHER: Trying to submit another job.\n",
      "19:28:17 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "19:28:17 DISPATCHER: starting job (0, 0, 1) on hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768\n",
      "19:28:17 DISPATCHER: job (0, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768\n",
      "19:28:17 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "19:28:17 WORKER: start processing job (0, 0, 1)\n",
      "19:28:17 WORKER: args: ()\n",
      "19:28:17 WORKER: kwargs: {'config': {'T_max': 717, 'dropout': 0.49081563972406295, 'hidden_dim': 38, 'lr': 0.021898813588612762, 'num_layers': 4, 'optimizer': 'Adam', 'relative_size': 0.4553952479483574, 'weight_decay': 0.00010637272577328577}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "19:28:50 DISPATCHER: Starting worker discovery\n",
      "19:28:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:28:50 DISPATCHER: Finished worker discovery\n",
      "19:29:50 DISPATCHER: Starting worker discovery\n",
      "19:29:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:29:50 DISPATCHER: Finished worker discovery\n",
      "19:30:50 DISPATCHER: Starting worker discovery\n",
      "19:30:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:30:50 DISPATCHER: Finished worker discovery\n",
      "19:31:50 DISPATCHER: Starting worker discovery\n",
      "19:31:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:31:50 DISPATCHER: Finished worker discovery\n",
      "19:32:50 DISPATCHER: Starting worker discovery\n",
      "19:32:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:32:50 DISPATCHER: Finished worker discovery\n",
      "19:33:50 DISPATCHER: Starting worker discovery\n",
      "19:33:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:33:50 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 76.030\n",
      "Test loss: 68.757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:34:03 WORKER: done with job (0, 0, 1), trying to register it.\n",
      "19:34:03 DISPATCHER: job (0, 0, 1) finished\n",
      "19:34:03 WORKER: registered result for job (0, 0, 1) with dispatcher\n",
      "19:34:03 DISPATCHER: register_result: lock acquired\n",
      "19:34:03 DISPATCHER: job (0, 0, 1) on hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768 finished\n",
      "19:34:03 job_id: (0, 0, 1)\n",
      "kwargs: {'config': {'T_max': 717, 'dropout': 0.49081563972406295, 'hidden_dim': 38, 'lr': 0.021898813588612762, 'num_layers': 4, 'optimizer': 'Adam', 'relative_size': 0.4553952479483574, 'weight_decay': 0.00010637272577328577}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 68.75661373138428, 'info': {'test_accuracy': 66.12546408176422, 'train_accuracy': 76.02973634004593, 'valid_accuracy': 68.75661373138428, 'model': 'LSTM_Net(\\n  (dropout): Dropout(p=0.49081563972406295, inplace=False)\\n  (rnn1): RNN(1, 38, num_layers=4)\\n  (linear1): Linear(in_features=7, out_features=17, bias=True)\\n  (linear2): Linear(in_features=17, out_features=17, bias=True)\\n  (linear3): Linear(in_features=55, out_features=55, bias=True)\\n  (linear4): Linear(in_features=55, out_features=19, bias=True)\\n  (linear5): Linear(in_features=19, out_features=1, bias=True)\\n  (dropout2): Dropout(p=0.49081563972406295, inplace=False)\\n  (linear_reduce1): Linear(in_features=380, out_features=190, bias=True)\\n  (linear_reduce2): Linear(in_features=190, out_features=38, bias=True)\\n  (linear_reduce3): Linear(in_features=38, out_features=38, bias=True)\\n)'}}\n",
      "exception: None\n",
      "\n",
      "19:34:03 job_callback for (0, 0, 1) started\n",
      "19:34:03 DISPATCHER: Trying to submit another job.\n",
      "19:34:03 job_callback for (0, 0, 1) got condition\n",
      "19:34:03 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "19:34:03 Only 2 run(s) for budget 333.333333 available, need more than 11 -> can't build model!\n",
      "19:34:03 HBMASTER: Trying to run another job!\n",
      "19:34:03 job_callback for (0, 0, 1) finished\n",
      "19:34:03 start sampling a new configuration.\n",
      "19:34:03 done sampling a new configuration.\n",
      "19:34:03 HBMASTER: schedule new run for iteration 0\n",
      "19:34:03 HBMASTER: trying submitting job (0, 0, 2) to dispatcher\n",
      "19:34:03 HBMASTER: submitting job (0, 0, 2) to dispatcher\n",
      "19:34:03 DISPATCHER: trying to submit job (0, 0, 2)\n",
      "19:34:03 DISPATCHER: trying to notify the job_runner thread.\n",
      "19:34:03 HBMASTER: job (0, 0, 2) submitted to dispatcher\n",
      "19:34:03 DISPATCHER: Trying to submit another job.\n",
      "19:34:03 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "19:34:03 DISPATCHER: starting job (0, 0, 2) on hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768\n",
      "19:34:03 DISPATCHER: job (0, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768\n",
      "19:34:03 WORKER: start processing job (0, 0, 2)\n",
      "19:34:03 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "19:34:03 WORKER: args: ()\n",
      "19:34:03 WORKER: kwargs: {'config': {'T_max': 92, 'dropout': 0.14482248402059622, 'hidden_dim': 409, 'lr': 1.0958586902536608e-06, 'num_layers': 4, 'optimizer': 'SGD', 'relative_size': 0.7940162045133444, 'weight_decay': 0.0011624974152256386, 'sgd_momentum': 0.6759900433576366}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 66.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:34:50 DISPATCHER: Starting worker discovery\n",
      "19:34:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:34:50 DISPATCHER: Finished worker discovery\n",
      "19:35:50 DISPATCHER: Starting worker discovery\n",
      "19:35:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:35:50 DISPATCHER: Finished worker discovery\n",
      "19:36:50 DISPATCHER: Starting worker discovery\n",
      "19:36:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:36:50 DISPATCHER: Finished worker discovery\n",
      "19:37:50 DISPATCHER: Starting worker discovery\n",
      "19:37:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:37:50 DISPATCHER: Finished worker discovery\n",
      "19:38:50 DISPATCHER: Starting worker discovery\n",
      "19:38:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:38:50 DISPATCHER: Finished worker discovery\n",
      "19:39:50 DISPATCHER: Starting worker discovery\n",
      "19:39:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:39:50 DISPATCHER: Finished worker discovery\n",
      "19:40:50 DISPATCHER: Starting worker discovery\n",
      "19:40:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:40:50 DISPATCHER: Finished worker discovery\n",
      "19:41:50 DISPATCHER: Starting worker discovery\n",
      "19:41:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:41:50 DISPATCHER: Finished worker discovery\n",
      "19:42:50 DISPATCHER: Starting worker discovery\n",
      "19:42:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:42:50 DISPATCHER: Finished worker discovery\n",
      "19:43:50 DISPATCHER: Starting worker discovery\n",
      "19:43:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:43:50 DISPATCHER: Finished worker discovery\n",
      "19:44:50 DISPATCHER: Starting worker discovery\n",
      "19:44:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:44:50 DISPATCHER: Finished worker discovery\n",
      "19:45:50 DISPATCHER: Starting worker discovery\n",
      "19:45:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:45:50 DISPATCHER: Finished worker discovery\n",
      "19:46:50 DISPATCHER: Starting worker discovery\n",
      "19:46:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:46:50 DISPATCHER: Finished worker discovery\n",
      "19:47:50 DISPATCHER: Starting worker discovery\n",
      "19:47:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:47:50 DISPATCHER: Finished worker discovery\n",
      "19:48:50 DISPATCHER: Starting worker discovery\n",
      "19:48:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:48:50 DISPATCHER: Finished worker discovery\n",
      "19:49:50 DISPATCHER: Starting worker discovery\n",
      "19:49:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:49:50 DISPATCHER: Finished worker discovery\n",
      "19:50:50 DISPATCHER: Starting worker discovery\n",
      "19:50:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:50:50 DISPATCHER: Finished worker discovery\n",
      "19:51:50 DISPATCHER: Starting worker discovery\n",
      "19:51:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:51:50 DISPATCHER: Finished worker discovery\n",
      "19:52:50 DISPATCHER: Starting worker discovery\n",
      "19:52:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:52:50 DISPATCHER: Finished worker discovery\n",
      "19:53:50 DISPATCHER: Starting worker discovery\n",
      "19:53:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:53:50 DISPATCHER: Finished worker discovery\n",
      "19:54:50 DISPATCHER: Starting worker discovery\n",
      "19:54:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:54:50 DISPATCHER: Finished worker discovery\n",
      "19:55:50 DISPATCHER: Starting worker discovery\n",
      "19:55:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:55:50 DISPATCHER: Finished worker discovery\n",
      "19:56:50 DISPATCHER: Starting worker discovery\n",
      "19:56:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:56:50 DISPATCHER: Finished worker discovery\n",
      "19:57:50 DISPATCHER: Starting worker discovery\n",
      "19:57:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:57:50 DISPATCHER: Finished worker discovery\n",
      "19:58:50 DISPATCHER: Starting worker discovery\n",
      "19:58:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:58:50 DISPATCHER: Finished worker discovery\n",
      "19:59:50 DISPATCHER: Starting worker discovery\n",
      "19:59:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "19:59:50 DISPATCHER: Finished worker discovery\n",
      "20:00:50 DISPATCHER: Starting worker discovery\n",
      "20:00:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:00:50 DISPATCHER: Finished worker discovery\n",
      "20:01:50 DISPATCHER: Starting worker discovery\n",
      "20:01:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:01:50 DISPATCHER: Finished worker discovery\n",
      "20:02:50 DISPATCHER: Starting worker discovery\n",
      "20:02:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:02:50 DISPATCHER: Finished worker discovery\n",
      "20:03:50 DISPATCHER: Starting worker discovery\n",
      "20:03:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:03:50 DISPATCHER: Finished worker discovery\n",
      "20:04:50 DISPATCHER: Starting worker discovery\n",
      "20:04:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:04:50 DISPATCHER: Finished worker discovery\n",
      "20:05:50 DISPATCHER: Starting worker discovery\n",
      "20:05:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:05:50 DISPATCHER: Finished worker discovery\n",
      "20:06:50 DISPATCHER: Starting worker discovery\n",
      "20:06:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:06:50 DISPATCHER: Finished worker discovery\n",
      "20:07:50 DISPATCHER: Starting worker discovery\n",
      "20:07:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:07:50 DISPATCHER: Finished worker discovery\n",
      "20:08:50 DISPATCHER: Starting worker discovery\n",
      "20:08:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:08:50 DISPATCHER: Finished worker discovery\n",
      "20:09:50 DISPATCHER: Starting worker discovery\n",
      "20:09:50 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:09:51 DISPATCHER: Finished worker discovery\n",
      "20:10:51 DISPATCHER: Starting worker discovery\n",
      "20:10:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:10:51 DISPATCHER: Finished worker discovery\n",
      "20:11:51 DISPATCHER: Starting worker discovery\n",
      "20:11:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:11:51 DISPATCHER: Finished worker discovery\n",
      "20:12:51 DISPATCHER: Starting worker discovery\n",
      "20:12:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:12:51 DISPATCHER: Finished worker discovery\n",
      "20:13:51 DISPATCHER: Starting worker discovery\n",
      "20:13:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:13:51 DISPATCHER: Finished worker discovery\n",
      "20:14:51 DISPATCHER: Starting worker discovery\n",
      "20:14:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:14:51 DISPATCHER: Finished worker discovery\n",
      "20:15:51 DISPATCHER: Starting worker discovery\n",
      "20:15:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:15:51 DISPATCHER: Finished worker discovery\n",
      "20:16:51 DISPATCHER: Starting worker discovery\n",
      "20:16:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:16:51 DISPATCHER: Finished worker discovery\n",
      "20:17:51 DISPATCHER: Starting worker discovery\n",
      "20:17:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:17:51 DISPATCHER: Finished worker discovery\n",
      "20:18:51 DISPATCHER: Starting worker discovery\n",
      "20:18:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:18:51 DISPATCHER: Finished worker discovery\n",
      "20:19:51 DISPATCHER: Starting worker discovery\n",
      "20:19:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:19:51 DISPATCHER: Finished worker discovery\n",
      "20:20:51 DISPATCHER: Starting worker discovery\n",
      "20:20:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:20:51 DISPATCHER: Finished worker discovery\n",
      "20:21:51 DISPATCHER: Starting worker discovery\n",
      "20:21:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:21:51 DISPATCHER: Finished worker discovery\n",
      "20:22:51 DISPATCHER: Starting worker discovery\n",
      "20:22:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:22:51 DISPATCHER: Finished worker discovery\n",
      "20:23:51 DISPATCHER: Starting worker discovery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:23:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:23:51 DISPATCHER: Finished worker discovery\n",
      "20:24:51 DISPATCHER: Starting worker discovery\n",
      "20:24:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:24:51 DISPATCHER: Finished worker discovery\n",
      "20:25:51 DISPATCHER: Starting worker discovery\n",
      "20:25:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:25:51 DISPATCHER: Finished worker discovery\n",
      "20:26:51 DISPATCHER: Starting worker discovery\n",
      "20:26:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:26:51 DISPATCHER: Finished worker discovery\n",
      "20:27:51 DISPATCHER: Starting worker discovery\n",
      "20:27:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:27:51 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 8049043577466798926151439876096.000\n",
      "Test loss: 8019157250043457391359007129600.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:28:38 WORKER: done with job (0, 0, 2), trying to register it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 8426634273362101045969431822336.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:28:39 WORKER: registered result for job (0, 0, 2) with dispatcher\n",
      "20:28:39 DISPATCHER: job (0, 0, 2) finished\n",
      "20:28:39 DISPATCHER: register_result: lock acquired\n",
      "20:28:39 DISPATCHER: job (0, 0, 2) on hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768 finished\n",
      "20:28:39 job_id: (0, 0, 2)\n",
      "kwargs: {'config': {'T_max': 92, 'dropout': 0.14482248402059622, 'hidden_dim': 409, 'lr': 1.0958586902536608e-06, 'num_layers': 4, 'optimizer': 'SGD', 'relative_size': 0.7940162045133444, 'weight_decay': 0.0011624974152256386, 'sgd_momentum': 0.6759900433576366}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 8.019157250043457e+30, 'info': {'test_accuracy': 8.426634273362101e+30, 'train_accuracy': 8.049043577466799e+30, 'valid_accuracy': 8.019157250043457e+30, 'model': 'LSTM_Net(\\n  (dropout): Dropout(p=0.14482248402059622, inplace=False)\\n  (rnn1): RNN(1, 409, num_layers=4)\\n  (linear1): Linear(in_features=7, out_features=324, bias=True)\\n  (linear2): Linear(in_features=324, out_features=324, bias=True)\\n  (linear3): Linear(in_features=733, out_features=733, bias=True)\\n  (linear4): Linear(in_features=733, out_features=204, bias=True)\\n  (linear5): Linear(in_features=204, out_features=1, bias=True)\\n  (dropout2): Dropout(p=0.14482248402059622, inplace=False)\\n  (linear_reduce1): Linear(in_features=4090, out_features=2045, bias=True)\\n  (linear_reduce2): Linear(in_features=2045, out_features=409, bias=True)\\n  (linear_reduce3): Linear(in_features=409, out_features=409, bias=True)\\n)'}}\n",
      "exception: None\n",
      "\n",
      "20:28:39 job_callback for (0, 0, 2) started\n",
      "20:28:39 DISPATCHER: Trying to submit another job.\n",
      "20:28:39 job_callback for (0, 0, 2) got condition\n",
      "20:28:39 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "20:28:39 Only 3 run(s) for budget 333.333333 available, need more than 11 -> can't build model!\n",
      "20:28:39 HBMASTER: Trying to run another job!\n",
      "20:28:39 job_callback for (0, 0, 2) finished\n",
      "20:28:39 ITERATION: Advancing config (0, 0, 1) to next budget 1000.000000\n",
      "20:28:39 HBMASTER: schedule new run for iteration 0\n",
      "20:28:39 HBMASTER: trying submitting job (0, 0, 1) to dispatcher\n",
      "20:28:39 HBMASTER: submitting job (0, 0, 1) to dispatcher\n",
      "20:28:39 DISPATCHER: trying to submit job (0, 0, 1)\n",
      "20:28:39 DISPATCHER: trying to notify the job_runner thread.\n",
      "20:28:39 HBMASTER: job (0, 0, 1) submitted to dispatcher\n",
      "20:28:39 DISPATCHER: Trying to submit another job.\n",
      "20:28:39 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "20:28:39 DISPATCHER: starting job (0, 0, 1) on hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768\n",
      "20:28:39 DISPATCHER: job (0, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768\n",
      "20:28:39 WORKER: start processing job (0, 0, 1)\n",
      "20:28:39 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "20:28:39 WORKER: args: ()\n",
      "20:28:39 WORKER: kwargs: {'config': {'T_max': 717, 'dropout': 0.49081563972406295, 'hidden_dim': 38, 'lr': 0.021898813588612762, 'num_layers': 4, 'optimizer': 'Adam', 'relative_size': 0.4553952479483574, 'weight_decay': 0.00010637272577328577}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "20:28:51 DISPATCHER: Starting worker discovery\n",
      "20:28:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:28:51 DISPATCHER: Finished worker discovery\n",
      "20:29:51 DISPATCHER: Starting worker discovery\n",
      "20:29:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:29:51 DISPATCHER: Finished worker discovery\n",
      "20:30:51 DISPATCHER: Starting worker discovery\n",
      "20:30:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:30:51 DISPATCHER: Finished worker discovery\n",
      "20:31:51 DISPATCHER: Starting worker discovery\n",
      "20:31:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:31:51 DISPATCHER: Finished worker discovery\n",
      "20:32:51 DISPATCHER: Starting worker discovery\n",
      "20:32:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:32:51 DISPATCHER: Finished worker discovery\n",
      "20:33:51 DISPATCHER: Starting worker discovery\n",
      "20:33:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:33:51 DISPATCHER: Finished worker discovery\n",
      "20:34:51 DISPATCHER: Starting worker discovery\n",
      "20:34:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:34:51 DISPATCHER: Finished worker discovery\n",
      "20:35:51 DISPATCHER: Starting worker discovery\n",
      "20:35:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:35:51 DISPATCHER: Finished worker discovery\n",
      "20:36:51 DISPATCHER: Starting worker discovery\n",
      "20:36:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:36:51 DISPATCHER: Finished worker discovery\n",
      "20:37:51 DISPATCHER: Starting worker discovery\n",
      "20:37:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:37:51 DISPATCHER: Finished worker discovery\n",
      "20:38:51 DISPATCHER: Starting worker discovery\n",
      "20:38:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:38:51 DISPATCHER: Finished worker discovery\n",
      "20:39:51 DISPATCHER: Starting worker discovery\n",
      "20:39:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:39:51 DISPATCHER: Finished worker discovery\n",
      "20:40:51 DISPATCHER: Starting worker discovery\n",
      "20:40:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:40:51 DISPATCHER: Finished worker discovery\n",
      "20:41:51 DISPATCHER: Starting worker discovery\n",
      "20:41:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:41:51 DISPATCHER: Finished worker discovery\n",
      "20:42:51 DISPATCHER: Starting worker discovery\n",
      "20:42:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:42:51 DISPATCHER: Finished worker discovery\n",
      "20:43:51 DISPATCHER: Starting worker discovery\n",
      "20:43:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:43:51 DISPATCHER: Finished worker discovery\n",
      "20:44:51 DISPATCHER: Starting worker discovery\n",
      "20:44:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:44:51 DISPATCHER: Finished worker discovery\n",
      "20:45:51 DISPATCHER: Starting worker discovery\n",
      "20:45:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:45:51 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 60.407\n",
      "Test loss: 53.319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:46:06 WORKER: done with job (0, 0, 1), trying to register it.\n",
      "20:46:06 WORKER: registered result for job (0, 0, 1) with dispatcher\n",
      "20:46:06 DISPATCHER: job (0, 0, 1) finished\n",
      "20:46:06 DISPATCHER: register_result: lock acquired\n",
      "20:46:06 DISPATCHER: job (0, 0, 1) on hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768 finished\n",
      "20:46:06 job_id: (0, 0, 1)\n",
      "kwargs: {'config': {'T_max': 717, 'dropout': 0.49081563972406295, 'hidden_dim': 38, 'lr': 0.021898813588612762, 'num_layers': 4, 'optimizer': 'Adam', 'relative_size': 0.4553952479483574, 'weight_decay': 0.00010637272577328577}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 53.3189001083374, 'info': {'test_accuracy': 52.64789843559265, 'train_accuracy': 60.407166600227356, 'valid_accuracy': 53.3189001083374, 'model': 'LSTM_Net(\\n  (dropout): Dropout(p=0.49081563972406295, inplace=False)\\n  (rnn1): RNN(1, 38, num_layers=4)\\n  (linear1): Linear(in_features=7, out_features=17, bias=True)\\n  (linear2): Linear(in_features=17, out_features=17, bias=True)\\n  (linear3): Linear(in_features=55, out_features=55, bias=True)\\n  (linear4): Linear(in_features=55, out_features=19, bias=True)\\n  (linear5): Linear(in_features=19, out_features=1, bias=True)\\n  (dropout2): Dropout(p=0.49081563972406295, inplace=False)\\n  (linear_reduce1): Linear(in_features=380, out_features=190, bias=True)\\n  (linear_reduce2): Linear(in_features=190, out_features=38, bias=True)\\n  (linear_reduce3): Linear(in_features=38, out_features=38, bias=True)\\n)'}}\n",
      "exception: None\n",
      "\n",
      "20:46:06 job_callback for (0, 0, 1) started\n",
      "20:46:06 DISPATCHER: Trying to submit another job.\n",
      "20:46:06 job_callback for (0, 0, 1) got condition\n",
      "20:46:06 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "20:46:06 Only 1 run(s) for budget 1000.000000 available, need more than 11 -> can't build model!\n",
      "20:46:06 HBMASTER: Trying to run another job!\n",
      "20:46:06 job_callback for (0, 0, 1) finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 52.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:46:06 start sampling a new configuration.\n",
      "20:46:07 done sampling a new configuration.\n",
      "20:46:07 HBMASTER: schedule new run for iteration 1\n",
      "20:46:07 HBMASTER: trying submitting job (1, 0, 0) to dispatcher\n",
      "20:46:07 HBMASTER: submitting job (1, 0, 0) to dispatcher\n",
      "20:46:07 DISPATCHER: trying to submit job (1, 0, 0)\n",
      "20:46:07 DISPATCHER: trying to notify the job_runner thread.\n",
      "20:46:07 HBMASTER: job (1, 0, 0) submitted to dispatcher\n",
      "20:46:07 DISPATCHER: Trying to submit another job.\n",
      "20:46:07 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "20:46:07 DISPATCHER: starting job (1, 0, 0) on hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768\n",
      "20:46:07 DISPATCHER: job (1, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768\n",
      "20:46:07 WORKER: start processing job (1, 0, 0)\n",
      "20:46:07 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "20:46:07 WORKER: args: ()\n",
      "20:46:07 WORKER: kwargs: {'config': {'T_max': 483, 'dropout': 0.13334096417072255, 'hidden_dim': 434, 'lr': 6.22275631228386e-06, 'num_layers': 3, 'optimizer': 'SGD', 'relative_size': 0.8723980040827953, 'weight_decay': 0.0026305535337607534, 'sgd_momentum': 0.6566070338228434}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "20:46:51 DISPATCHER: Starting worker discovery\n",
      "20:46:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:46:51 DISPATCHER: Finished worker discovery\n",
      "20:47:51 DISPATCHER: Starting worker discovery\n",
      "20:47:51 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:47:51 DISPATCHER: Finished worker discovery\n",
      "20:48:51 DISPATCHER: Starting worker discovery\n",
      "20:48:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:48:52 DISPATCHER: Finished worker discovery\n",
      "20:49:52 DISPATCHER: Starting worker discovery\n",
      "20:49:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:49:52 DISPATCHER: Finished worker discovery\n",
      "20:50:52 DISPATCHER: Starting worker discovery\n",
      "20:50:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:50:52 DISPATCHER: Finished worker discovery\n",
      "20:51:52 DISPATCHER: Starting worker discovery\n",
      "20:51:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:51:52 DISPATCHER: Finished worker discovery\n",
      "20:52:52 DISPATCHER: Starting worker discovery\n",
      "20:52:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:52:52 DISPATCHER: Finished worker discovery\n",
      "20:53:52 DISPATCHER: Starting worker discovery\n",
      "20:53:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:53:52 DISPATCHER: Finished worker discovery\n",
      "20:54:52 DISPATCHER: Starting worker discovery\n",
      "20:54:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:54:52 DISPATCHER: Finished worker discovery\n",
      "20:55:52 DISPATCHER: Starting worker discovery\n",
      "20:55:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:55:52 DISPATCHER: Finished worker discovery\n",
      "20:56:52 DISPATCHER: Starting worker discovery\n",
      "20:56:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:56:52 DISPATCHER: Finished worker discovery\n",
      "20:57:52 DISPATCHER: Starting worker discovery\n",
      "20:57:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:57:52 DISPATCHER: Finished worker discovery\n",
      "20:58:52 DISPATCHER: Starting worker discovery\n",
      "20:58:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:58:52 DISPATCHER: Finished worker discovery\n",
      "20:59:52 DISPATCHER: Starting worker discovery\n",
      "20:59:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "20:59:52 DISPATCHER: Finished worker discovery\n",
      "21:00:52 DISPATCHER: Starting worker discovery\n",
      "21:00:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:00:52 DISPATCHER: Finished worker discovery\n",
      "21:01:52 DISPATCHER: Starting worker discovery\n",
      "21:01:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:01:52 DISPATCHER: Finished worker discovery\n",
      "21:02:52 DISPATCHER: Starting worker discovery\n",
      "21:02:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:02:52 DISPATCHER: Finished worker discovery\n",
      "21:03:52 DISPATCHER: Starting worker discovery\n",
      "21:03:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:03:52 DISPATCHER: Finished worker discovery\n",
      "21:04:52 DISPATCHER: Starting worker discovery\n",
      "21:04:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:04:52 DISPATCHER: Finished worker discovery\n",
      "21:05:52 DISPATCHER: Starting worker discovery\n",
      "21:05:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:05:52 DISPATCHER: Finished worker discovery\n",
      "21:06:52 DISPATCHER: Starting worker discovery\n",
      "21:06:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:06:52 DISPATCHER: Finished worker discovery\n",
      "21:07:52 DISPATCHER: Starting worker discovery\n",
      "21:07:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:07:52 DISPATCHER: Finished worker discovery\n",
      "21:08:52 DISPATCHER: Starting worker discovery\n",
      "21:08:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:08:52 DISPATCHER: Finished worker discovery\n",
      "21:09:52 DISPATCHER: Starting worker discovery\n",
      "21:09:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:09:52 DISPATCHER: Finished worker discovery\n",
      "21:10:52 DISPATCHER: Starting worker discovery\n",
      "21:10:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:10:52 DISPATCHER: Finished worker discovery\n",
      "21:11:52 DISPATCHER: Starting worker discovery\n",
      "21:11:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:11:52 DISPATCHER: Finished worker discovery\n",
      "21:12:52 DISPATCHER: Starting worker discovery\n",
      "21:12:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:12:52 DISPATCHER: Finished worker discovery\n",
      "21:13:52 DISPATCHER: Starting worker discovery\n",
      "21:13:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:13:52 DISPATCHER: Finished worker discovery\n",
      "21:14:52 DISPATCHER: Starting worker discovery\n",
      "21:14:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:14:52 DISPATCHER: Finished worker discovery\n",
      "21:15:52 DISPATCHER: Starting worker discovery\n",
      "21:15:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:15:52 DISPATCHER: Finished worker discovery\n",
      "21:16:52 DISPATCHER: Starting worker discovery\n",
      "21:16:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:16:52 DISPATCHER: Finished worker discovery\n",
      "21:17:52 DISPATCHER: Starting worker discovery\n",
      "21:17:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:17:52 DISPATCHER: Finished worker discovery\n",
      "21:18:52 DISPATCHER: Starting worker discovery\n",
      "21:18:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:18:52 DISPATCHER: Finished worker discovery\n",
      "21:19:52 DISPATCHER: Starting worker discovery\n",
      "21:19:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:19:52 DISPATCHER: Finished worker discovery\n",
      "21:20:52 DISPATCHER: Starting worker discovery\n",
      "21:20:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:20:52 DISPATCHER: Finished worker discovery\n",
      "21:21:52 DISPATCHER: Starting worker discovery\n",
      "21:21:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:21:52 DISPATCHER: Finished worker discovery\n",
      "21:22:52 DISPATCHER: Starting worker discovery\n",
      "21:22:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:22:52 DISPATCHER: Finished worker discovery\n",
      "21:23:52 DISPATCHER: Starting worker discovery\n",
      "21:23:52 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "21:23:52 DISPATCHER: Finished worker discovery\n",
      "21:24:52 HBMASTER: shutdown initiated, shutdown_workers = True\n",
      "21:24:52 WORKER: shutting down now!\n",
      "21:24:53 DISPATCHER: Starting worker discovery\n",
      "21:24:53 DISPATCHER: Found 0 potential workers, 1 currently in the pool.\n",
      "21:24:54 DISPATCHER: removing dead worker, hpbandster.run_bohb_run_1.worker.sven-Lenovo-IdeaPad-L340-17API.8384139935532400768\n",
      "21:24:54 Job (1, 0, 0) was not completed\n",
      "21:24:54 HBMASTER: number of workers changed to 0\n",
      "21:24:54 DISPATCHER: Dispatcher shutting down\n",
      "21:24:54 DISPATCHER: Trying to submit another job.\n",
      "21:24:54 DISPATCHER: job_runner shutting down\n"
     ]
    }
   ],
   "source": [
    "if hbster:\n",
    "    \n",
    "    try:\n",
    "        # Start a nameserver\n",
    "        #host = hpns.nic_name_to_host(nic_name)\n",
    "        ns = hpns.NameServer(run_id=run_id, host='127.0.0.1', port=port,\n",
    "                             working_directory=working_dir)\n",
    "        ns_host, ns_port = ns.start()\n",
    "        #run(ns_host)\n",
    "        # Start local worker\n",
    "        w = PyTorchWorker(run_id=run_id, host='127.0.0.1', nameserver=ns_host,\n",
    "                          nameserver_port=ns_port, timeout=500)\n",
    "        w.run(background=True)\n",
    "\n",
    "        # Run an optimizer\n",
    "        bohb = BOHB(configspace=worker.get_configspace(),\n",
    "                    run_id=run_id,\n",
    "                    host='127.0.0.1',\n",
    "                    nameserver=ns_host,\n",
    "                    nameserver_port=ns_port,\n",
    "                    min_budget=min_budget, max_budget=max_budget)\n",
    "\n",
    "        result = bohb.run(n_iterations=n_bohb_iterations)\n",
    "        print(\"Write result to file {}\".format(result_file))\n",
    "        with open(result_file, 'wb') as f:\n",
    "            pickle.dump(result, f)\n",
    "    finally:\n",
    "        bohb.shutdown(shutdown_workers=True)\n",
    "        ns.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load a saved result object if necessary\n",
    "if hbster:  \n",
    "    \n",
    "    with open(result_file, 'rb') as f:\n",
    "        result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hbster:    \n",
    "    \n",
    "    all_runs_largest_budget = result.get_all_runs(only_largest_budget=True)\n",
    "    val_accs= []\n",
    "    for i in range(len(all_runs_largest_budget)):\n",
    "      val_accs.append(all_runs_largest_budget[i]['info']['valid_accuracy'])\n",
    "    val_accs = np.array(val_accs)\n",
    "    arg = np.argmin(val_accs)\n",
    "    print(all_runs_largest_budget[arg]['info']['model'])\n",
    "    print(all_runs_largest_budget[arg]['info']['valid_accuracy'])\n",
    "    print(all_runs_largest_budget[arg]['config'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLearningCurvePredictor():\n",
    "    \"\"\"A learning curve predictor that predicts the last observed epoch of the validation accuracy as final performance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for datapoint in X:\n",
    "            predictions.append(datapoint[\"Train/val_accuracy\"][-1])\n",
    "        return predictions\n",
    "    \n",
    "def score(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & tuning\n",
    "predictor = SimpleLearningCurvePredictor()\n",
    "for data in train_data:\n",
    "    data['Train/val_accuracy']=data['Train/val_accuracy'][0:10]\n",
    "\n",
    "predictor.fit(train_data, train_targets)\n",
    "preds = predictor.predict(val_data)\n",
    "mse = score(val_targets, preds)\n",
    "print(\"Score on validation set:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation (after tuning)\n",
    "final_preds = predictor.predict(test_data)\n",
    "final_score = score(test_targets, final_preds)\n",
    "print(\"Final test score:\", final_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
